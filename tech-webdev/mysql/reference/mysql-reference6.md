      
'age'              The age of the oldest item within the slab
                   class, in seconds.
                   
'evicted'          The number of items evicted to make way for
                   new entries.
                   
'evicted_time'     The time of the last evicted entry
                   
'evicted_nonzero'  The time of the last evicted non-zero entry     1.4.0
                                                                   
'outofmemory'      The number of items for this slab class that
                   have triggered an out of memory error (only
                   value when the '-M' command line option is in
                   effect).
                   
'tailrepairs'      Number of times the entries for a particular
                   ID need repairing

Item level statistics can be used to determine how many items are stored
within a given slab and their freshness and recycle rate.  You can use
this to help identify whether there are certain slab classes that are
triggering a much larger number of evictions that others.


File: manual.info.tmp,  Node: ha-memcached-stats-sizes,  Next: ha-memcached-stats-detail,  Prev: ha-memcached-stats-items,  Up: ha-memcached-stats

16.2.4.4 'memcached' Size Statistics
....................................

To get size statistics, use the 'stats sizes' command, or the API
equivalent.

The size statistics provide information about the sizes and number of
items of each size within the cache.  The information is returned as two
columns, the first column is the size of the item (rounded up to the
nearest 32 byte boundary), and the second column is the count of the
number of items of that size within the cache:

     96 35
     128 38
     160 807
     192 804
     224 410
     256 222
     288 83
     320 39
     352 53
     384 33
     416 64
     448 51
     480 30
     512 54
     544 39
     576 10065

*Caution*:

Running this statistic locks up your cache as each item is read from the
cache and its size calculated.  On a large cache, this may take some
time and prevent any set or get operations until the process completes.

The item size statistics are useful only to determine the sizes of the
objects you are storing.  Since the actual memory allocation is relevant
only in terms of the chunk size and page size, the information is only
useful during a careful debugging or diagnostic session.


File: manual.info.tmp,  Node: ha-memcached-stats-detail,  Next: ha-memcached-stats-memcached-tool,  Prev: ha-memcached-stats-sizes,  Up: ha-memcached-stats

16.2.4.5 'memcached' Detail Statistics
......................................

For 'memcached' 1.3.x and higher, you can enable and obtain detailed
statistics about the get, set, and del operations on theindividual keys
stored in the cache, and determine whether the attempts hit (found) a
particular key.  These operations are only recorded while the detailed
stats analysis is turned on.

To enable detailed statistics, you must send the 'stats detail on'
command to the 'memcached' server:

     $ telnet localhost 11211
     Trying 127.0.0.1...
     Connected to tiger.
     Escape character is '^]'.stats detail on
     OK

Individual statistics are recorded for every 'get', 'set' and 'del'
operation on a key, including keys that are not currently stored in the
server.  For example, if an attempt is made to obtain the value of key
'abckey' and it does not exist, the 'get' operating on the specified key
are recorded while detailed statistics are in effect, even if the key is
not currently stored.  The 'hits', that is, the number of 'get' or 'del'
operations for a key that exists in the server are also counted.

To turn detailed statistics off, send the 'stats detail off' command to
the 'memcached' server:

     $ telnet localhost 11211
     Trying 127.0.0.1...
     Connected to tiger.
     Escape character is '^]'.stats detail off
     OK

To obtain the detailed statistics recorded during the process, send the
'stats detail dump' command to the 'memcached' server:

     stats detail dump
     PREFIX hykkey get 0 hit 0 set 1 del 0
     PREFIX xyzkey get 0 hit 0 set 1 del 0
     PREFIX yukkey get 1 hit 0 set 0 del 0
     PREFIX abckey get 3 hit 3 set 1 del 0
     END

You can use the detailed statistics information to determine whether
your 'memcached' clients are using a large number of keys that do not
exist in the server by comparing the 'hit' and 'get' or 'del' counts.
Because the information is recorded by key, you can also determine
whether the failures or operations are clustered around specific keys.


File: manual.info.tmp,  Node: ha-memcached-stats-memcached-tool,  Prev: ha-memcached-stats-detail,  Up: ha-memcached-stats

16.2.4.6 Using 'memcached-tool'
...............................

The 'memcached-tool', located within the 'scripts' directory within the
'memcached' source directory.  The tool provides convenient access to
some reports and statistics from any 'memcached' instance.

The basic format of the command is:

     shell> ./memcached-tool hostname:port [command]

The default output produces a list of the slab allocations and usage.
For example:

     shell> memcached-tool localhost:11211 display
       #  Item_Size  Max_age   Pages   Count   Full?  Evicted Evict_Time OOM
       1      80B        93s       1      20      no        0        0    0
       2     104B        93s       1      16      no        0        0    0
       3     136B      1335s       1      28      no        0        0    0
       4     176B      1335s       1      24      no        0        0    0
       5     224B      1335s       1      32      no        0        0    0
       6     280B      1335s       1      34      no        0        0    0
       7     352B      1335s       1      36      no        0        0    0
       8     440B      1335s       1      46      no        0        0    0
       9     552B      1335s       1      58      no        0        0    0
      10     696B      1335s       1      66      no        0        0    0
      11     872B      1335s       1      89      no        0        0    0
      12     1.1K      1335s       1     112      no        0        0    0
      13     1.3K      1335s       1     145      no        0        0    0
      14     1.7K      1335s       1     123      no        0        0    0
      15     2.1K      1335s       1     198      no        0        0    0
      16     2.6K      1335s       1     199      no        0        0    0
      17     3.3K      1335s       1     229      no        0        0    0
      18     4.1K      1335s       1     248     yes       36        2    0
      19     5.2K      1335s       2     328      no        0        0    0
      20     6.4K      1335s       2     316     yes      387        1    0
      21     8.1K      1335s       3     381     yes      492        1    0
      22    10.1K      1335s       3     303     yes      598        2    0
      23    12.6K      1335s       5     405     yes      605        1    0
      24    15.8K      1335s       6     384     yes      766        2    0
      25    19.7K      1335s       7     357     yes      908      170    0
      26    24.6K      1336s       7     287     yes     1012        1    0
      27    30.8K      1336s       7     231     yes     1193      169    0
      28    38.5K      1336s       4     104     yes     1323      169    0
      29    48.1K      1336s       1      21     yes     1287        1    0
      30    60.2K      1336s       1      17     yes     1093      169    0
      31    75.2K      1337s       1      13     yes      713      168    0
      32    94.0K      1337s       1      10     yes      278      168    0
      33   117.5K      1336s       1       3      no        0        0    0

This output is the same if you specify the 'command' as 'display':

     shell> memcached-tool localhost:11211 display
       #  Item_Size  Max_age   Pages   Count   Full?  Evicted Evict_Time OOM
       1      80B        93s       1      20      no        0        0    0
       2     104B        93s       1      16      no        0        0    0
     ...

The output shows a summarized version of the output from the 'slabs'
statistics.  The columns provided in the output are shown below:

   * '#': The slab number

   * 'Item_Size': The size of the slab

   * 'Max_age': The age of the oldest item in the slab

   * 'Pages': The number of pages allocated to the slab

   * 'Count': The number of items in this slab

   * 'Full?': Whether the slab is fully populated

   * 'Evicted': The number of objects evicted from this slab

   * 'Evict_Time': The time (in seconds) since the last eviction

   * 'OOM': The number of items that have triggered an out of memory
     error

You can also obtain a dump of the general statistics for the server
using the 'stats' command:

     shell> memcached-tool localhost:11211 stats
     #localhost:11211   Field       Value
              accepting_conns           1
                        bytes         162
                   bytes_read         485
                bytes_written        6820
                   cas_badval           0
                     cas_hits           0
                   cas_misses           0
                    cmd_flush           0
                      cmd_get           4
                      cmd_set           2
                  conn_yields           0
        connection_structures          11
             curr_connections          10
                   curr_items           2
                    decr_hits           0
                  decr_misses           1
                  delete_hits           0
                delete_misses           0
                    evictions           0
                     get_hits           4
                   get_misses           0
                    incr_hits           0
                  incr_misses           2
               limit_maxbytes    67108864
          listen_disabled_num           0
                          pid       12981
                 pointer_size          32
                rusage_system    0.013911
                  rusage_user    0.011876
                      threads           4
                         time  1255518565
            total_connections          20
                  total_items           2
                       uptime         880
                      version       1.4.2


File: manual.info.tmp,  Node: ha-memcached-faq,  Prev: ha-memcached-stats,  Up: ha-memcached

16.2.5 'memcached' FAQ
----------------------

Can memcached be run on a Windows environment?

No.  Currently 'memcached' is available only on the Unix/Linux platform.
There is an unofficial port available, see
<http://www.codeplex.com/memcachedproviders>.

What is the maximum size of an object you can store in memcached?  Is
that configurable?

The default maximum object size is 1MB. In 'memcached' 1.4.2 and later,
you can change the maximum size of an object using the '-I' command line
option.

For versions before this, to increase this size, you have to re-compile
'memcached'.  You can modify the value of the 'POWER_BLOCK' within the
'slabs.c' file within the source.

In 'memcached' 1.4.2 and higher, you can configure the maximum supported
object size by using the '-I' command-line option.  For example, to
increase the maximum object size to 5MB:

     $ memcached -I 5m

If an object is larger than the maximum object size, you must manually
split it.  'memcached' is very simple: you give it a key and some data,
it tries to cache it in RAM. If you try to store more than the default
maximum size, the value is just truncated for speed reasons.

Is it true 'memcached' will be much more effective with
db-read-intensive applications than with db-write-intensive
applications?

Yes.  'memcached' plays no role in database writes, it is a method of
caching data already read from the database in RAM.

Is there any overhead in not using persistent connections?  If
persistent is always recommended, what are the downsides (for example,
locking up)?

If you don't use persistent connections when communicating with
'memcached', there will be a small increase in the latency of opening
the connection each time.  The effect is comparable to use nonpersistent
connections with MySQL.

In general, the chance of locking or other issues with persistent
connections is minimal, because there is very little locking within
'memcached'.  If there is a problem, eventually your request will time
out and return no result, so your application will need to load from
MySQL again.

How is an event such as a crash of one of the 'memcached' servers
handled by the 'memcached' client?

There is no automatic handling of this.  If your client fails to get a
response from a server, code a fallback mechanism to load the data from
the MySQL database.

The client APIs all provide the ability to add and remove 'memcached'
instances on the fly.  If within your application you notice that
'memcached' server is no longer responding, you can remove the server
from the list of servers, and keys will automatically be redistributed
to another 'memcached' server in the list.  If retaining the cache
content on all your servers is important, make sure you use an API that
supports a consistent hashing algorithm.  For more information, see
*note ha-memcached-using-hashtypes::.

What is a recommended hardware configuration for a memcached server?

'memcached' has a very low processing overhead.  All that is required is
spare physical RAM capacity.  A 'memcached' server does not require a
dedicated machine.  If you have web, application, or database servers
that have spare RAM capacity, then use them with 'memcached'.

To build and deploy a dedicated 'memcached' server, use a relatively
low-power CPU, lots of RAM, and one or more Gigabit Ethernet interfaces.

Is memcached more effective for video and audio as opposed to textual
read/writes?

'memcached' works equally well for all kinds of data.  To 'memcached',
any value you store is just a stream of data.  Remember, though, that
the maximum size of an object you can store in 'memcached' is 1MB, but
can be configured to be larger by using the '-I' option in 'memcached'
1.4.2 and later, or by modifying the source in versions before 1.4.2.
If you plan on using 'memcached' with audio and video content, you will
probably want to increase the maximum object size.  Also remember that
'memcached' is a solution for caching information for reading.  It
shouldn't be used for writes, except when updating the information in
the cache.

Can 'memcached' work with ASPX?

There are ports and interfaces for many languages and environments.
ASPX relies on an underlying language such as C# or VisualBasic, and if
you are using ASP.NET then there is a C# 'memcached' library.  For more
information, see <https://sourceforge.net/projects/memcacheddotnet/>.

How expensive is it to establish a memcache connection?  Should those
connections be pooled?

Opening the connection is relatively inexpensive, because there is no
security, authentication or other handshake taking place before you can
start sending requests and getting results.  Most APIs support a
persistent connection to a 'memcached' instance to reduce the latency.
Connection pooling would depend on the API you are using, but if you are
communicating directly over TCP/IP, then connection pooling would
provide some small performance benefit.

How is the data handled when the 'memcached' server is down?

The behavior is entirely application dependent.  Most applications fall
back to loading the data from the database (just as if they were
updating the 'memcached' information).  If you are using multiple
'memcached' servers, you might also remove a downed server from the list
to prevent it from affecting performance.  Otherwise, the client will
still attempt to communicate with the 'memcached' server that
corresponds to the key you are trying to load.

How are auto-increment columns in the MySQL database coordinated across
multiple instances of memcached?

They aren't.  There is no relationship between MySQL and 'memcached'
unless your application (or, if you are using the MySQL UDFs for
'memcached', your database definition) creates one.

If you are storing information based on an auto-increment key into
multiple instances of 'memcached', the information is only stored on one
of the 'memcached' instances anyway.  The client uses the key value to
determine which 'memcached' instance to store the information.  It
doesn't store the same information across all the instances, as that
would be a waste of cache memory.

Is compression available?

Yes.  Most of the client APIs support some sort of compression, and some
even allow you to specify the threshold at which a value is deemed
appropriate for compression during storage.

Can we implement different types of 'memcached' as different nodes in
the same server, so can there be deterministic and non-deterministic in
the same server?

Yes.  You can run multiple instances of 'memcached' on a single server,
and in your client configuration you choose the list of servers you want
to use.

What are best practices for testing an implementation, to ensure that it
improves performance, and to measure the impact of 'memcached'
configuration changes?  And would you recommend keeping the
configuration very simple to start?

The best way to test the performance is to start up a 'memcached'
instance.  First, modify your application so that it stores the data
just before the data is about to be used or displayed into 'memcached'.
Since the APIs handle the serialization of the data, it should just be a
one-line modification to your code.  Then, modify the start of the
process that would normally load that information from MySQL with the
code that requests the data from 'memcached'.  If the data cannot be
loaded from 'memcached', default to the MySQL process.

All of the changes required will probably amount to just a few lines of
code.  To get the best benefit, make sure you cache entire objects (for
example, all the components of a web page, blog post, discussion thread,
and so on), rather than using 'memcached' as a simple cache of
individual rows of MySQL tables.

Keeping the configuration simple at the start, or even over the long
term, is easy with 'memcached'.  Once you have the basic structure up
and running, often the only ongoing change is to add more servers into
the list of servers used by your applications.  You don't need to manage
the 'memcached' servers, and there is no complex configuration; just add
more servers to the list and let the client API and the 'memcached'
servers make the decisions.


File: manual.info.tmp,  Node: replication,  Next: mysql-cluster,  Prev: ha-overview,  Up: Top

17 Replication
**************

* Menu:

* replication-configuration::    Replication Configuration
* replication-implementation::   Replication Implementation
* replication-solutions::        Replication Solutions
* replication-notes::            Replication Notes and Tips

Replication enables data from one MySQL database server (the master) to
be replicated to one or more MySQL database servers (the slaves).
Replication is asynchronous by default, therefore slaves do not need to
be connected permanently to receive updates from the master.  This means
that updates can occur over long-distance connections and even over
temporary or intermittent connections such as a dial-up service.
Depending on the configuration, you can replicate all databases,
selected databases, or even selected tables within a database.

For answers to some questions often asked by those who are new to MySQL
Replication, see *note faqs-replication::.

Advantages of replication in MySQL include:

   * Scale-out solutions - spreading the load among multiple slaves to
     improve performance.  In this environment, all writes and updates
     must take place on the master server.  Reads, however, may take
     place on one or more slaves.  This model can improve the
     performance of writes (since the master is dedicated to updates),
     while dramatically increasing read speed across an increasing
     number of slaves.

   * Data security - because data is replicated to the slave, and the
     slave can pause the replication process, it is possible to run
     backup services on the slave without corrupting the corresponding
     master data.

   * Analytics - live data can be created on the master, while the
     analysis of the information can take place on the slave without
     affecting the performance of the master.

   * Long-distance data distribution - if a branch office would like to
     work with a copy of your main data, you can use replication to
     create a local copy of the data for their use without requiring
     permanent access to the master.

Replication in MySQL features support for one-way, asynchronous
replication, in which one server acts as the master, while one or more
other servers act as slaves.  This is in contrast to the _synchronous_
replication which is a characteristic of NDB Cluster (see *note
mysql-cluster::).  In MySQL 5.5, an interface to semisynchronous
replication is supported in addition to the built-in asynchronous
replication.  With semisynchronous replication, a commit performed on
the master side blocks before returning to the session that performed
the transaction until at least one slave acknowledges that it has
received and logged the events for the transaction.  See *note
replication-semisync::

There are a number of solutions available for setting up replication
between two servers, but the best method to use depends on the presence
of data and the engine types you are using.  For more information on the
available options, see *note replication-howto::.

There are two core types of replication format, Statement Based
Replication (SBR), which replicates entire SQL statements, and Row Based
Replication (RBR), which replicates only the changed rows.  You may also
use a third variety, Mixed Based Replication (MBR). For more information
on the different replication formats, see *note replication-formats::.
In MySQL 5.5, statement-based format is the default.

Replication is controlled through a number of different options and
variables.  These control the core operation of the replication,
timeouts, and the databases and filters that can be applied on databases
and tables.  For more information on the available options, see *note
replication-options::.

You can use replication to solve a number of different problems,
including problems with performance, supporting the backup of different
databases, and as part of a larger solution to alleviate system
failures.  For information on how to address these issues, see *note
replication-solutions::.

For notes and tips on how different data types and statements are
treated during replication, including details of replication features,
version compatibility, upgrades, and problems and their resolution,
including an FAQ, see *note replication-notes::.

For detailed information on the implementation of replication, how
replication works, the process and contents of the binary log,
background threads and the rules used to decide how statements are
recorded and replication, see *note replication-implementation::.


File: manual.info.tmp,  Node: replication-configuration,  Next: replication-implementation,  Prev: replication,  Up: replication

17.1 Replication Configuration
==============================

* Menu:

* replication-howto::            How to Set Up Replication
* replication-formats::          Replication Formats
* replication-options::          Replication and Binary Logging Options and Variables
* replication-administration::   Common Replication Administration Tasks

Replication between servers in MySQL is based on the binary logging
mechanism.  The MySQL instance operating as the master (the source of
the database changes) writes updates and changes as 'events' to the
binary log.  The information in the binary log is stored in different
logging formats according to the database changes being recorded.
Slaves are configured to read the binary log from the master and to
execute the events in the binary log on the slave's local database.

The master is 'dumb' in this scenario.  Once binary logging has been
enabled, all statements are recorded in the binary log.  Each slave
receives a copy of the entire contents of the binary log.  It is the
responsibility of the slave to decide which statements in the binary log
should be executed; you cannot configure the master to log only certain
events.  If you do not specify otherwise, all events in the master
binary log are executed on the slave.  If required, you can configure
the slave to process only events that apply to particular databases or
tables.

Each slave keeps a record of the binary log coordinates: The file name
and position within the file that it has read and processed from the
master.  This means that multiple slaves can be connected to the master
and executing different parts of the same binary log.  Because the
slaves control this process, individual slaves can be connected and
disconnected from the server without affecting the master's operation.
Also, because each slave remembers the position within the binary log,
it is possible for slaves to be disconnected, reconnect and then 'catch
up' by continuing from the recorded position.

The master and each slave must be configured with a unique ID (using the
'server_id' system variable).  In addition, each slave must be
configured with information about the master host name, log file name,
and position within that file.  These details can be controlled from
within a MySQL session using the *note 'CHANGE MASTER TO':
change-master-to. statement on the slave.  The details are stored within
the slave's 'master.info' file.

This section describes the setup and configuration required for a
replication environment, including step-by-step instructions for
creating a new replication environment.  The major components of this
section are:

   * For a guide to setting up two or more servers for replication,
     *note replication-howto::, deals with the configuration of the
     systems and provides methods for copying data between the master
     and slaves.

   * Events in the binary log are recorded using a number of formats.
     These are referred to as statement-based replication (SBR) or
     row-based replication (RBR). A third type, mixed-format replication
     (MIXED), uses SBR or RBR replication automatically to take
     advantage of the benefits of both SBR and RBR formats when
     appropriate.  The different formats are discussed in *note
     replication-formats::.

   * Detailed information on the different configuration options and
     variables that apply to replication is provided in *note
     replication-options::.

   * Once started, the replication process should require little
     administration or monitoring.  However, for advice on common tasks
     that you may want to execute, see *note
     replication-administration::.


File: manual.info.tmp,  Node: replication-howto,  Next: replication-formats,  Prev: replication-configuration,  Up: replication-configuration

17.1.1 How to Set Up Replication
--------------------------------

* Menu:

* replication-howto-masterbaseconfig::  Setting the Replication Master Configuration
* replication-howto-slavebaseconfig::  Setting the Replication Slave Configuration
* replication-howto-repuser::    Creating a User for Replication
* replication-howto-masterstatus::  Obtaining the Replication Master Binary Log Coordinates
* replication-howto-mysqldump::  Creating a Data Snapshot Using mysqldump
* replication-howto-rawdata::    Creating a Data Snapshot Using Raw Data Files
* replication-howto-newservers::  Setting Up Replication with New Master and Slaves
* replication-howto-existingdata::  Setting Up Replication with Existing Data
* replication-howto-additionalslaves::  Introducing Additional Slaves to an Existing Replication Environment
* replication-howto-slaveinit::  Setting the Master Configuration on the Slave

This section describes how to set up complete replication of a MySQL
server.  There are a number of different methods for setting up
replication, and the exact method to use depends on how you are setting
up replication, and whether you already have data within your master
database.

There are some generic tasks that are common to all replication setups:

   * On the master, you must enable binary logging and configure a
     unique server ID. This might require a server restart.  See *note
     replication-howto-masterbaseconfig::.

   * On each slave that you want to connect to the master, you must
     configure a unique server ID. This might require a server restart.
     See *note replication-howto-slavebaseconfig::.

   * You may want to create a separate user that will be used by your
     slaves to authenticate with the master to read the binary log for
     replication.  The step is optional.  See *note
     replication-howto-repuser::.

   * Before creating a data snapshot or starting the replication
     process, you should record the position of the binary log on the
     master.  You will need this information when configuring the slave
     so that the slave knows where within the binary log to start
     executing events.  See *note replication-howto-masterstatus::.

   * If you already have data on your master and you want to use it to
     synchronize your slave, you will need to create a data snapshot.
     You can create a snapshot using *note 'mysqldump': mysqldump. (see
     *note replication-howto-mysqldump::) or by copying the data files
     directly (see *note replication-howto-rawdata::).

   * You will need to configure the slave with settings for connecting
     to the master, such as the host name, login credentials, and binary
     log file name and position.  See *note
     replication-howto-slaveinit::.

Once you have configured the basic options, you will need to follow the
instructions for your replication setup.  A number of alternatives are
provided:

   * If you are establishing a new MySQL master and one or more slaves,
     you need only set up the configuration, as you have no data to
     exchange.  For guidance on setting up replication in this
     situation, see *note replication-howto-newservers::.

   * If you are already running a MySQL server, and therefore already
     have data that must be transferred to your slaves before
     replication starts, have not previously configured the binary log
     and are able to shut down your MySQL server for a short period
     during the process, see *note replication-howto-existingdata::.

   * If you are adding slaves to an existing replication environment,
     you can set up the slaves without affecting the master.  See *note
     replication-howto-additionalslaves::.

If you will be administering MySQL replication servers, we suggest that
you read this entire chapter through and try all statements mentioned in
*note replication-statements-master::, and *note
replication-statements-slave::.  You should also familiarize yourself
with the replication startup options described in *note
replication-options::.

*Note*:

Note that certain steps within the setup process require the 'SUPER'
privilege.  If you do not have this privilege, it might not be possible
to enable replication.


File: manual.info.tmp,  Node: replication-howto-masterbaseconfig,  Next: replication-howto-slavebaseconfig,  Prev: replication-howto,  Up: replication-howto

17.1.1.1 Setting the Replication Master Configuration
.....................................................

On a replication master, you must enable binary logging and establish a
unique server ID. If this has not already been done, this part of master
setup requires a server restart.

Binary logging _must_ be enabled on the master because the binary log is
the basis for sending data changes from the master to its slaves.  If
binary logging is not enabled, replication will not be possible.

Each server within a replication group must be configured with a unique
server ID. This ID is used to identify individual servers within the
group, and must be a positive integer between 1 and (2^32)−1.  How you
organize and select the numbers is entirely up to you.

To configure the binary log and server ID options, you will need to shut
down your MySQL server and edit the 'my.cnf' or 'my.ini' file.  Add the
following options to the configuration file within the '[mysqld]'
section.  If these options already exist, but are commented out,
uncomment the options and alter them according to your needs.  For
example, to enable binary logging using a log file name prefix of
'mysql-bin', and configure a server ID of 1, use these lines:

     [mysqld]
     log-bin=mysql-bin
     server-id=1

After making the changes, restart the server.

*Note*:

If you do not set 'server_id' (or set it explicitly to its default value
of 0), a master refuses connections from all slaves.

*Note*:

For the greatest possible durability and consistency in a replication
setup using 'InnoDB' with transactions, you should use
'innodb_flush_log_at_trx_commit=1' and 'sync_binlog=1' in the master
'my.cnf' file.

*Note*:

Ensure that the 'skip_networking' system variable is not enabled on your
replication master.  If networking has been disabled, your slave will
not able to communicate with the master and replication will fail.


File: manual.info.tmp,  Node: replication-howto-slavebaseconfig,  Next: replication-howto-repuser,  Prev: replication-howto-masterbaseconfig,  Up: replication-howto

17.1.1.2 Setting the Replication Slave Configuration
....................................................

On a replication slave, you must establish a unique server ID. If this
has not already been done, this part of slave setup requires a server
restart.

If the slave server ID is not already set, or the current value
conflicts with the value that you have chosen for the master server, you
should shut down your slave server and edit the configuration to specify
a unique server ID. For example:

     [mysqld]
     server-id=2

After making the changes, restart the server.

If you are setting up multiple slaves, each one must have a unique
'server_id' value that differs from that of the master and from each of
the other slaves.  Think of 'server_id' values as something similar to
IP addresses: These IDs uniquely identify each server instance in the
community of replication partners.

*Note*:

If you do not set 'server_id' (or set it explicitly to its default value
of 0), a slave refuses to connect to a master.

You do not have to enable binary logging on the slave for replication to
be enabled.  However, if you enable binary logging on the slave, you can
use the binary log for data backups and crash recovery on the slave, and
also use the slave as part of a more complex replication topology (for
example, where the slave acts as a master to other slaves).


File: manual.info.tmp,  Node: replication-howto-repuser,  Next: replication-howto-masterstatus,  Prev: replication-howto-slavebaseconfig,  Up: replication-howto

17.1.1.3 Creating a User for Replication
........................................

Each slave must connect to the master using a MySQL user name and
password, so there must be a user account on the master that the slave
can use to connect.  Any account can be used for this operation,
providing it has been granted the 'REPLICATION SLAVE' privilege.  You
may wish to create a different account for each slave, or connect to the
master using the same account for each slave.

You need not create an account specifically for replication.  However,
you should be aware that the user name and password will be stored in
plain text within the 'master.info' file (see *note
slave-logs-status::).  Therefore, you may want to create a separate
account that has privileges only for the replication process, to
minimize the possibility of compromise to other accounts.

To create a new account, use *note 'CREATE USER': create-user.  To grant
this account the privileges required for replication, use the *note
'GRANT': grant. statement.  If you create an account solely for the
purposes of replication, that account needs only the 'REPLICATION SLAVE'
privilege.  For example, to set up a new user, 'repl', that can connect
for replication from any host within the 'example.com' domain, issue
these statements on the master:

     mysql> CREATE USER 'repl'@'%.example.com' IDENTIFIED BY 'PASSWORD';
     mysql> GRANT REPLICATION SLAVE ON *.* TO 'repl'@'%.example.com';

See *note account-management-statements::, for more information on
statements for manipulation of user accounts.


File: manual.info.tmp,  Node: replication-howto-masterstatus,  Next: replication-howto-mysqldump,  Prev: replication-howto-repuser,  Up: replication-howto

17.1.1.4 Obtaining the Replication Master Binary Log Coordinates
................................................................

To configure replication on the slave you must determine the master's
current coordinates within its binary log.  You will need this
information so that when the slave starts the replication process, it is
able to start processing events from the binary log at the correct
point.

If you have existing data on your master that you want to synchronize on
your slaves before starting the replication process, you must stop
processing statements on the master, and then obtain its current binary
log coordinates and dump its data, before permitting the master to
continue executing statements.  If you do not stop the execution of
statements, the data dump and the master status information that you use
will not match and you will end up with inconsistent or corrupted
databases on the slaves.

To obtain the master binary log coordinates, follow these steps:

  1. Start a session on the master by connecting to it with the
     command-line client, and flush all tables and block write
     statements by executing the 'FLUSH TABLES WITH READ LOCK'
     statement:

          mysql> FLUSH TABLES WITH READ LOCK;

     For 'InnoDB' tables, note that 'FLUSH TABLES WITH READ LOCK' also
     blocks *note 'COMMIT': commit. operations.

     *Warning*:

     Leave the client from which you issued the 'FLUSH TABLES' statement
     running so that the read lock remains in effect.  If you exit the
     client, the lock is released.

  2. In a different session on the master, use the *note 'SHOW MASTER
     STATUS': show-master-status. statement to determine the current
     binary log file name and position:

          mysql > SHOW MASTER STATUS;
          +------------------+----------+--------------+------------------+
          | File             | Position | Binlog_Do_DB | Binlog_Ignore_DB |
          +------------------+----------+--------------+------------------+
          | mysql-bin.000003 | 73       | test         | manual,mysql     |
          +------------------+----------+--------------+------------------+

     The 'File' column shows the name of the log file and 'Position'
     shows the position within the file.  In this example, the binary
     log file is 'mysql-bin.000003' and the position is 73.  Record
     these values.  You need them later when you are setting up the
     slave.  They represent the replication coordinates at which the
     slave should begin processing new updates from the master.

     If the master has been running previously without binary logging
     enabled, the log file name and position values displayed by *note
     'SHOW MASTER STATUS': show-master-status. or *note 'mysqldump
     --master-data': mysqldump. will be empty.  In that case, the values
     that you need to use later when specifying the slave's log file and
     position are the empty string ('''') and '4'.

You now have the information you need to enable the slave to start
reading from the binary log in the correct place to start replication.

If you have existing data that needs be to synchronized with the slave
before you start replication, leave the client running so that the lock
remains in place and then proceed to *note
replication-howto-mysqldump::, or *note replication-howto-rawdata::.
The idea here is to prevent any further changes so that the data copied
to the slaves is in synchrony with the master.

If you are setting up a brand new master and slave replication group,
you can exit the first session to release the read lock.


File: manual.info.tmp,  Node: replication-howto-mysqldump,  Next: replication-howto-rawdata,  Prev: replication-howto-masterstatus,  Up: replication-howto

17.1.1.5 Creating a Data Snapshot Using mysqldump
.................................................

One way to create a snapshot of the data in an existing master database
is to use the *note 'mysqldump': mysqldump. tool to create a dump of all
the databases you want to replicate.  Once the data dump has been
completed, you then import this data into the slave before starting the
replication process.

The example shown here dumps all databases to a file named 'dbdump.db',
and includes the '--master-data' option which automatically appends the
*note 'CHANGE MASTER TO': change-master-to. statement required on the
slave to start the replication process:

     shell> mysqldump --all-databases --master-data > dbdump.db

If you do not use '--master-data', then it is necessary to lock all
tables in a separate session manually (using 'FLUSH TABLES WITH READ
LOCK') prior to running *note 'mysqldump': mysqldump, then exiting or
running UNLOCK TABLES from the second session to release the locks.  You
must also obtain binary log position information matching the snapshot,
using *note 'SHOW MASTER STATUS': show-master-status, and use this to
issue the appropriate *note 'CHANGE MASTER TO': change-master-to.
statement when starting the slave.

When choosing databases to include in the dump, remember that you need
to filter out databases on each slave that you do not want to include in
the replication process.

To import the data, either copy the dump file to the slave, or access
the file from the master when connecting remotely to the slave.


File: manual.info.tmp,  Node: replication-howto-rawdata,  Next: replication-howto-newservers,  Prev: replication-howto-mysqldump,  Up: replication-howto

17.1.1.6 Creating a Data Snapshot Using Raw Data Files
......................................................

If your database is large, copying the raw data files can be more
efficient than using *note 'mysqldump': mysqldump. and importing the
file on each slave.  This technique skips the overhead of updating
indexes as the 'INSERT' statements are replayed.

Using this method with tables in storage engines with complex caching or
logging algorithms requires extra steps to produce a perfect 'point in
time' snapshot: the initial copy command might leave out cache
information and logging updates, even if you have acquired a global read
lock.  How the storage engine responds to this depends on its crash
recovery abilities.

This method also does not work reliably if the master and slave have
different values for 'ft_stopword_file', 'ft_min_word_len', or
'ft_max_word_len' and you are copying tables having full-text indexes.

If you use *note 'InnoDB': innodb-storage-engine. tables, you can use
the 'mysqlbackup' command from the MySQL Enterprise Backup component to
produce a consistent snapshot.  This command records the log name and
offset corresponding to the snapshot to be later used on the slave.
MySQL Enterprise Backup is a commercial product that is included as part
of a MySQL Enterprise subscription.  See *note mysql-enterprise-backup::
for detailed information.

Otherwise, use the cold backup technique to obtain a reliable binary
snapshot of 'InnoDB' tables: copy all data files after doing a slow
shutdown of the MySQL Server.

To create a raw data snapshot of 'MyISAM' tables, you can use standard
copy tools such as 'cp' or 'copy', a remote copy tool such as 'scp' or
'rsync', an archiving tool such as 'zip' or 'tar', or a file system
snapshot tool such as 'dump', providing that your MySQL data files exist
on a single file system.  If you are replicating only certain databases,
copy only those files that relate to those tables.  (For 'InnoDB', all
tables in all databases are stored in the system tablespace files,
unless you have the 'innodb_file_per_table' option enabled.)

You might want to specifically exclude the following files from your
archive:

   * Files relating to the 'mysql' database.

   * The 'master.info' file.

   * The master's binary log files.

   * Any relay log files.

To get the most consistent results with a raw data snapshot, shut down
the master server during the process, as follows:

  1. Acquire a read lock and get the master's status.  See *note
     replication-howto-masterstatus::.

  2. In a separate session, shut down the master server:

          shell> mysqladmin shutdown

  3. Make a copy of the MySQL data files.  The following examples show
     common ways to do this.  You need to choose only one of them:

          shell> tar cf /TMP/DB.TAR ./DATA
          shell> zip -r /TMP/DB.ZIP ./DATA
          shell> rsync --recursive ./DATA /TMP/DBDATA

  4. Restart the master server.

If you are not using 'InnoDB' tables, you can get a snapshot of the
system from a master without shutting down the server as described in
the following steps:

  1. Acquire a read lock and get the master's status.  See *note
     replication-howto-masterstatus::.

  2. Make a copy of the MySQL data files.  The following examples show
     common ways to do this.  You need to choose only one of them:

          shell> tar cf /TMP/DB.TAR ./DATA
          shell> zip -r /TMP/DB.ZIP ./DATA
          shell> rsync --recursive ./DATA /TMP/DBDATA

  3. In the client where you acquired the read lock, release the lock:

          mysql> UNLOCK TABLES;

Once you have created the archive or copy of the database, copy the
files to each slave before starting the slave replication process.


File: manual.info.tmp,  Node: replication-howto-newservers,  Next: replication-howto-existingdata,  Prev: replication-howto-rawdata,  Up: replication-howto

17.1.1.7 Setting Up Replication with New Master and Slaves
..........................................................

The easiest and most straightforward method for setting up replication
is to use new master and slave servers.

You can also use this method if you are setting up new servers but have
an existing dump of the databases from a different server that you want
to load into your replication configuration.  By loading the data into a
new master, the data will be automatically replicated to the slaves.

To set up replication between a new master and slave:

  1. Configure the MySQL master with the necessary configuration
     properties.  See *note replication-howto-masterbaseconfig::.

  2. Start up the MySQL master.

  3. Set up a user.  See *note replication-howto-repuser::.

  4. Obtain the master status information.  See *note
     replication-howto-masterstatus::.

  5. On the master, release the read lock:

          mysql> UNLOCK TABLES;

  6. On the slave, edit the MySQL configuration.  See *note
     replication-howto-slavebaseconfig::.

  7. Start up the MySQL slave.

  8. Execute a *note 'CHANGE MASTER TO': change-master-to. statement to
     set the master replication server configuration.  See *note
     replication-howto-slaveinit::.

Perform the slave setup steps on each slave.

Because there is no data to load or exchange on a new server
configuration you do not need to copy or import any information.

If you are setting up a new replication environment using the data from
a different existing database server, you will now need to run the dump
file generated from that server on the new master.  The database updates
will automatically be propagated to the slaves:

     shell> mysql -h master < fulldb.dump


File: manual.info.tmp,  Node: replication-howto-existingdata,  Next: replication-howto-additionalslaves,  Prev: replication-howto-newservers,  Up: replication-howto

17.1.1.8 Setting Up Replication with Existing Data
..................................................

When setting up replication with existing data, you will need to decide
how best to get the data from the master to the slave before starting
the replication service.

The basic process for setting up replication with existing data is as
follows:

  1. With the MySQL master running, create a user to be used by the
     slave when connecting to the master during replication.  See *note
     replication-howto-repuser::.

  2. If you have not already configured the 'server_id' system variable
     and enabled binary logging on the master server, you will need to
     shut it down to configure these options.  See *note
     replication-howto-masterbaseconfig::.

     If you have to shut down your master server, this is a good
     opportunity to take a snapshot of its databases.  You should obtain
     the master status (see *note replication-howto-masterstatus::)
     before taking down the master, updating the configuration and
     taking a snapshot.  For information on how to create a snapshot
     using raw data files, see *note replication-howto-rawdata::.

  3. If your master server is already correctly configured, obtain its
     status (see *note replication-howto-masterstatus::) and then use
     *note 'mysqldump': mysqldump. to take a snapshot (see *note
     replication-howto-mysqldump::) or take a raw snapshot of the live
     server using the guide in *note replication-howto-rawdata::.

  4. Update the configuration of the slave.  See *note
     replication-howto-slavebaseconfig::.

  5. The next step depends on how you created the snapshot of data on
     the master.

     If you used *note 'mysqldump': mysqldump.:

       1. Start the slave, using the '--skip-slave-start' option so that
          replication does not start.

       2. Import the dump file:

               shell> mysql < fulldb.dump

     If you created a snapshot using the raw data files:

       1. Extract the data files into your slave data directory.  For
          example:

               shell> tar xvf dbdump.tar

          You may need to set permissions and ownership on the files so
          that the slave server can access and modify them.

       2. Start the slave, using the '--skip-slave-start' option so that
          replication does not start.

  6. Configure the slave with the replication coordinates from the
     master.  This tells the slave the binary log file and position
     within the file where replication needs to start.  Also, configure
     the slave with the login credentials and host name of the master.
     For more information on the *note 'CHANGE MASTER TO':
     change-master-to. statement required, see *note
     replication-howto-slaveinit::.

  7. Start the slave threads:

          mysql> START SLAVE;

After you have performed this procedure, the slave should connect to the
master and catch up on any updates that have occurred since the snapshot
was taken.

If you have forgotten to set the 'server_id' system variable for the
master, slaves cannot connect to it.

If you have forgotten to set the 'server_id' system variable for the
slave, you get the following error in the slave's error log:

     Warning: You should set server-id to a non-0 value if master_host
     is set; we will force server id to 2, but this MySQL server will
     not act as a slave.

You also find error messages in the slave's error log if it is not able
to replicate for any other reason.

Once a slave is replicating, you can find in its data directory one file
named 'master.info' and another named 'relay-log.info'.  The slave uses
these two files to keep track of how much of the master's binary log it
has processed.  Do _not_ remove or edit these files unless you know
exactly what you are doing and fully understand the implications.  Even
in that case, it is preferred that you use the *note 'CHANGE MASTER TO':
change-master-to. statement to change replication parameters.  The slave
will use the values specified in the statement to update the status
files automatically.

*Note*:

The content of 'master.info' overrides some of the server options
specified on the command line or in 'my.cnf'.  See *note
replication-options::, for more details.

A single snapshot of the master suffices for multiple slaves.  To set up
additional slaves, use the same master snapshot and follow the slave
portion of the procedure just described.


File: manual.info.tmp,  Node: replication-howto-additionalslaves,  Next: replication-howto-slaveinit,  Prev: replication-howto-existingdata,  Up: replication-howto

17.1.1.9 Introducing Additional Slaves to an Existing Replication Environment
.............................................................................

To add another slave to an existing replication configuration, you can
do so without stopping the master.  Instead, set up the new slave by
making a copy of an existing slave, except that you configure the new
slave with a different 'server_id' value.

To duplicate an existing slave:

  1. Shut down the existing slave:

          shell> mysqladmin shutdown

  2. Copy the data directory from the existing slave to the new slave.
     You can do this by creating an archive using 'tar' or 'WinZip', or
     by performing a direct copy using a tool such as 'cp' or 'rsync'.
     Ensure that you also copy the log files and relay log files.

     A common problem that is encountered when adding new replication
     slaves is that the new slave fails with a series of warning and
     error messages like these:

          071118 16:44:10 [Warning] Neither --relay-log nor --relay-log-index were used; so
          replication may break when this MySQL server acts as a slave and has his hostname
          changed!! Please use '--relay-log=NEW_SLAVE_HOSTNAME-relay-bin' to avoid this problem.
          071118 16:44:10 [ERROR] Failed to open the relay log './OLD_SLAVE_HOSTNAME-relay-bin.003525'
          (relay_log_pos 22940879)
          071118 16:44:10 [ERROR] Could not find target log during relay log initialization
          071118 16:44:10 [ERROR] Failed to initialize the master info structure

     This is due to the fact that, if the 'relay_log' system variable is
     not specified, the relay log files contain the host name as part of
     their file names.  This is also true of the relay log index file if
     the 'relay_log_index' system variable is not used.  For more
     information about these variables, see *note replication-options::.

     To avoid this problem, use the same value for 'relay_log' on the
     new slave that was used on the existing slave.  (If this option was
     not set explicitly on the existing slave, use
     'EXISTING_SLAVE_HOSTNAME-relay-bin'.)  If this is not feasible,
     copy the existing slave's relay log index file to the new slave and
     set the 'relay_log_index' system variable on the new slave to match
     what was used on the existing slave.  (If this option was not set
     explicitly on the existing slave, use
     'EXISTING_SLAVE_HOSTNAME-relay-bin.index'.)  Alternatively--if you
     have already tried to start the new slave (after following the
     remaining steps in this section) and have encountered errors like
     those described previously--then perform the following steps:

       1. If you have not already done so, issue a *note 'STOP SLAVE':
          stop-slave. on the new slave.

          If you have already started the existing slave again, issue a
          *note 'STOP SLAVE': stop-slave. on the existing slave as well.

       2. Copy the contents of the existing slave's relay log index file
          into the new slave's relay log index file, making sure to
          overwrite any content already in the file.

       3. Proceed with the remaining steps in this section.

  3. Copy the 'master.info' and 'relay-log.info' files from the existing
     slave to the new slave if they were not located in the data
     directory.  These files hold the current log coordinates for the
     master's binary log and the slave's relay log.

  4. Start the existing slave.

  5. On the new slave, edit the configuration and give the new slave a
     unique server ID (using the 'server_id' system variable) that is
     not used by the master or any of the existing slaves.

  6. Start the new slave.  The slave will use the information in its
     'master.info' file to start the replication process.


File: manual.info.tmp,  Node: replication-howto-slaveinit,  Prev: replication-howto-additionalslaves,  Up: replication-howto

17.1.1.10 Setting the Master Configuration on the Slave
.......................................................

To set up the slave to communicate with the master for replication, you
must tell the slave the necessary connection information.  To do this,
execute the following statement on the slave, replacing the option
values with the actual values relevant to your system:

     mysql> CHANGE MASTER TO
         ->     MASTER_HOST='MASTER_HOST_NAME',
         ->     MASTER_USER='REPLICATION_USER_NAME',
         ->     MASTER_PASSWORD='REPLICATION_PASSWORD',
         ->     MASTER_LOG_FILE='RECORDED_LOG_FILE_NAME',
         ->     MASTER_LOG_POS=RECORDED_LOG_POSITION;

*Note*:

Replication cannot use Unix socket files.  You must be able to connect
to the master MySQL server using TCP/IP.

The *note 'CHANGE MASTER TO': change-master-to. statement has other
options as well.  For example, it is possible to set up secure
replication using SSL. For a full list of options, and information about
the maximum permissible length for the string-valued options, see *note
change-master-to::.


File: manual.info.tmp,  Node: replication-formats,  Next: replication-options,  Prev: replication-howto,  Up: replication-configuration

17.1.2 Replication Formats
--------------------------

* Menu:

* replication-sbr-rbr::          Advantages and Disadvantages of Statement-Based and Row-Based Replication
* replication-rbr-usage::        Usage of Row-Based Logging and Replication
* replication-rbr-safe-unsafe::  Determination of Safe and Unsafe Statements in Binary Logging

Replication works because events written to the binary log are read from
the master and then processed on the slave.  The events are recorded
within the binary log in different formats according to the type of
event.  The different replication formats used correspond to the binary
logging format used when the events were recorded in the master's binary
log.  The correlation between binary logging formats and the terms used
during replication are:

   * Replication capabilities in MySQL originally were based on
     propagation of SQL statements from master to slave.  This is called
     _statement-based replication_ (often abbreviated as _SBR_), which
     corresponds to the standard statement-based binary logging format.
     In older versions of MySQL (5.1.4 and earlier), binary logging and
     replication used this format exclusively.

   * Row-based binary logging logs changes in individual table rows.
     When used with MySQL replication, this is known as _row-based
     replication_ (often abbreviated as _RBR_). In row-based
     replication, the master writes _events_ to the binary log that
     indicate how individual table rows are changed.

   * The server can change the binary logging format in real time
     according to the type of event using _mixed-format logging_.

     When the mixed format is in effect, statement-based logging is used
     by default, but automatically switches to row-based logging in
     particular cases as described later.  Replication using the mixed
     format is often referred to as _mixed-based replication_ or
     _mixed-format replication_.  For more information, see *note
     binary-log-mixed::.

In MySQL 5.5, statement-based format is the default.

NDB Cluster

The default binary logging format in all MySQL NDB Cluster 7.2 releases,
beginning with MySQL NDB Cluster 7.2.1, is 'STATEMENT'.  (This is a
change from previous versions of NDB Cluster.)  You should note that NDB
Cluster Replication always uses row-based replication, and that the
*note 'NDB': mysql-cluster. storage engine is incompatible with
statement-based replication.  This means that you must manually set the
format to 'ROW' prior to enabling NDB Cluster Replication.  See *note
mysql-cluster-replication-general::, for more information.

When using 'MIXED' format, the binary logging format is determined in
part by the storage engine being used and the statement being executed.
For more information on mixed-format logging and the rules governing the
support of different logging formats, see *note binary-log-mixed::.

The logging format in a running MySQL server is controlled by setting
the 'binlog_format' server system variable.  This variable can be set
with session or global scope.  The rules governing when and how the new
setting takes effect are the same as for other MySQL server system
variables.  Setting the variable for the current session lasts only
until the end of that session, and the change is not visible to other
sessions.  Setting the variable globally takes effect for clients that
connect after the change, but not for any current client sessions,
including the session where the variable setting was changed.  To make
the global system variable setting permanent so that it applies across
server restarts, you must set it in an option file.  For more
information, see *note set-variable::.

There are conditions under which you cannot change the binary logging
format at runtime or doing so causes replication to fail.  See *note
binary-log-setting::.

Changing the global 'binlog_format' value requires privileges sufficient
to set global system variables.  Changing the session 'binlog_format'
value requires privileges sufficient to set restricted session system
variables.  See *note system-variable-privileges::.

The statement-based and row-based replication formats have different
issues and limitations.  For a comparison of their relative advantages
and disadvantages, see *note replication-sbr-rbr::.

With statement-based replication, you may encounter issues with
replicating stored routines or triggers.  You can avoid these issues by
using row-based replication instead.  For more information, see *note
stored-programs-logging::.


File: manual.info.tmp,  Node: replication-sbr-rbr,  Next: replication-rbr-usage,  Prev: replication-formats,  Up: replication-formats

17.1.2.1 Advantages and Disadvantages of Statement-Based and Row-Based Replication
..................................................................................

Each binary logging format has advantages and disadvantages.  For most
users, the mixed replication format should provide the best combination
of data integrity and performance.  If, however, you want to take
advantage of the features specific to the statement-based or row-based
replication format when performing certain tasks, you can use the
information in this section, which provides a summary of their relative
advantages and disadvantages, to determine which is best for your needs.

   * Advantages of statement-based replication

   * Disadvantages of statement-based replication

   * Advantages of row-based replication

   * Disadvantages of row-based replication

*Advantages of statement-based replication*

   * Proven technology.

   * Less data written to log files.  When updates or deletes affect
     many rows, this results in _much_ less storage space required for
     log files.  This also means that taking and restoring from backups
     can be accomplished more quickly.

   * Log files contain all statements that made any changes, so they can
     be used to audit the database.

*Disadvantages of statement-based replication*

   * Statements that are unsafe for SBR

     Not all statements which modify data (such as *note 'INSERT':
     insert. *note 'DELETE': delete, *note 'UPDATE': update, and *note
     'REPLACE': replace. statements) can be replicated using
     statement-based replication.  Any nondeterministic behavior is
     difficult to replicate when using statement-based replication.
     Examples of such DML (Data Modification Language) statements
     include the following:

        * A statement that depends on a UDF or stored program that is
          nondeterministic, since the value returned by such a UDF or
          stored program or depends on factors other than the parameters
          supplied to it.  (Row-based replication, however, simply
          replicates the value returned by the UDF or stored program, so
          its effect on table rows and data is the same on both the
          master and slave.)  See *note replication-features-invoked::,
          for more information.

        * *note 'DELETE': delete. and *note 'UPDATE': update. statements
          that use a 'LIMIT' clause without an 'ORDER BY' are
          nondeterministic.  See *note replication-features-limit::.

        * Statements using any of the following functions cannot be
          replicated properly using statement-based replication:

             * 'LOAD_FILE()'

             * 'UUID()', 'UUID_SHORT()'

             * 'USER()'

             * 'FOUND_ROWS()'

             * 'SYSDATE()' (unless both the master and the slave are
               started with the '--sysdate-is-now' option)

             * 'GET_LOCK()'

             * 'IS_FREE_LOCK()'

             * 'IS_USED_LOCK()'

             * 'MASTER_POS_WAIT()'

             * 'RAND()'

             * 'RELEASE_LOCK()'

             * 'SLEEP()'

             * 'VERSION()'

          However, all other functions are replicated correctly using
          statement-based replication, including 'NOW()' and so forth.

          For more information, see *note
          replication-features-functions::.

     Statements that cannot be replicated correctly using
     statement-based replication are logged with a warning like the one
     shown here:

          [Warning] Statement is not safe to log in statement format.

     A similar warning is also issued to the client in such cases.  The
     client can display it using *note 'SHOW WARNINGS': show-warnings.

   * *note 'INSERT ... SELECT': insert. requires a greater number of
     row-level locks than with row-based replication.

   * *note 'UPDATE': update. statements that require a table scan
     (because no index is used in the 'WHERE' clause) must lock a
     greater number of rows than with row-based replication.

   * For *note 'InnoDB': innodb-storage-engine.: An *note 'INSERT':
     insert. statement that uses 'AUTO_INCREMENT' blocks other
     nonconflicting *note 'INSERT': insert. statements.

   * For complex statements, the statement must be evaluated and
     executed on the slave before the rows are updated or inserted.
     With row-based replication, the slave only has to modify the
     affected rows, not execute the full statement.

   * If there is an error in evaluation on the slave, particularly when
     executing complex statements, statement-based replication may
     slowly increase the margin of error across the affected rows over
     time.  See *note replication-features-slaveerrors::.

   * Stored functions execute with the same 'NOW()' value as the calling
     statement.  However, this is not true of stored procedures.

   * Deterministic UDFs must be applied on the slaves.

   * Table definitions must be (nearly) identical on master and slave.
     See *note replication-features-differing-tables::, for more
     information.

*Advantages of row-based replication*

   * All changes can be replicated.  This is the safest form of
     replication.

     The 'mysql' database is not replicated.  The 'mysql' database is
     instead seen as a node-specific database.  Row-based replication is
     not supported on tables in this database.  Instead, statements that
     would normally update this information--such as *note 'GRANT':
     grant, *note 'REVOKE': revoke. and the manipulation of triggers,
     stored routines (including stored procedures), and views--are all
     replicated to slaves using statement-based replication.

     For statements such as *note 'CREATE TABLE ... SELECT':
     create-table, a 'CREATE' statement is generated from the table
     definition and replicated using statement-based format, while the
     row insertions are replicated using row-based format.

   * The technology is the same as in most other database management
     systems; knowledge about other systems transfers to MySQL.

   * Fewer row locks are required on the master, which thus achieves
     higher concurrency, for the following types of statements:

        * *note 'INSERT ... SELECT': insert-select.

        * *note 'INSERT': insert. statements with 'AUTO_INCREMENT'

        * *note 'UPDATE': update. or *note 'DELETE': delete. statements
          with 'WHERE' clauses that do not use keys or do not change
          most of the examined rows.

   * Fewer row locks are required on the slave for any *note 'INSERT':
     insert, *note 'UPDATE': update, or *note 'DELETE': delete.
     statement.

*Disadvantages of row-based replication*

   * RBR tends to generate more data that must be logged.  To replicate
     a DML statement (such as an *note 'UPDATE': update. or *note
     'DELETE': delete. statement), statement-based replication writes
     only the statement to the binary log.  By contrast, row-based
     replication writes each changed row to the binary log.  If the
     statement changes many rows, row-based replication may write
     significantly more data to the binary log; this is true even for
     statements that are rolled back.  This also means that taking and
     restoring from backup can require more time.  In addition, the
     binary log is locked for a longer time to write the data, which may
     cause concurrency problems.

   * Deterministic UDFs that generate large *note 'BLOB': blob. values
     take longer to replicate with row-based replication than with
     statement-based replication.  This is because the *note 'BLOB':
     blob. column value is logged, rather than the statement generating
     the data.

   * You cannot examine the logs to see what statements were executed,
     nor can you see on the slave what statements were received from the
     master and executed.

     However, you can see what data was changed using *note
     'mysqlbinlog': mysqlbinlog. with the options
     '--base64-output=DECODE-ROWS' and '--verbose'.

   * For tables using the *note 'MyISAM': myisam-storage-engine. storage
     engine, a stronger lock is required on the slave for *note
     'INSERT': insert. statements when applying them as row-based events
     to the binary log than when applying them as statements.  This
     means that concurrent inserts on *note 'MyISAM':
     myisam-storage-engine. tables are not supported when using
     row-based replication.


File: manual.info.tmp,  Node: replication-rbr-usage,  Next: replication-rbr-safe-unsafe,  Prev: replication-sbr-rbr,  Up: replication-formats

17.1.2.2 Usage of Row-Based Logging and Replication
...................................................

Major changes in the replication environment and in the behavior of
applications can result from using row-based logging (RBL) or row-based
replication (RBR) rather than statement-based logging or replication.
This section describes a number of issues known to exist when using
row-based logging or replication, and discusses some best practices for
taking advantage of row-based logging and replication.

For additional information, see *note replication-formats::, and *note
replication-sbr-rbr::.

For information about issues specific to NDB Cluster Replication (which
depends on row-based replication), see *note
mysql-cluster-replication-issues::.

   * RBL, RBR, and temporary tables

     As noted in *note replication-features-temptables::, temporary
     tables are not replicated when using row-based format.  When mixed
     format is in effect, 'safe' statements involving temporary tables
     are logged using statement-based format.  For more information, see
     *note replication-sbr-rbr::.

     Temporary tables are not replicated when using row-based format
     because there is no need.  In addition, because temporary tables
     can be read only from the thread which created them, there is
     seldom if ever any benefit obtained from replicating them, even
     when using statement-based format.

     Beginning with MySQL 5.5.5, you can switch from statement-based to
     row-based binary logging mode even when temporary tables have been
     created.  However, while using the row-based format, the MySQL
     server cannot determine the logging mode that was in effect when a
     given temporary table was created.  For this reason, the server in
     such cases logs a *note 'DROP TEMPORARY TABLE IF EXISTS':
     drop-table. statement for each temporary table that still exists
     for a given client session when that session ends.  (Bug #11760229,
     Bug #11762267) While this means that it is possible that an
     unnecessary 'DROP TEMPORARY TABLE' statement might be logged in
     some cases, the statement is harmless, and does not cause an error
     even if the table does not exist, due to the presence of the 'IF
     EXISTS' option.

   * RBL and synchronization of nontransactional tables

     When many rows are affected, the set of changes is split into
     several events; when the statement commits, all of these events are
     written to the binary log.  When executing on the slave, a table
     lock is taken on all tables involved, and then the rows are applied
     in batch mode.  (This may or may not be effective, depending on the
     engine used for the slave's copy of the table.)

   * Latency and binary log size

     Because RBL writes changes for each row to the binary log, its size
     can increase quite rapidly.  In a replication environment, this can
     significantly increase the time required to make changes on the
     slave that match those on the master.  You should be aware of the
     potential for this delay in your applications.

   * Reading the binary log

     *note 'mysqlbinlog': mysqlbinlog. displays row-based events in the
     binary log using the 'BINLOG' statement (see *note binlog::).  This
     statement displays an event in printable form, but as a base
     64-encoded string the meaning of which is not evident.  When
     invoked with the '--base64-output=DECODE-ROWS' and '--verbose'
     options, *note 'mysqlbinlog': mysqlbinlog. formats the contents of
     the binary log in a manner that is easily human readable.  This is
     helpful when binary log events were written in row-based format if
     you want to read or recover from a replication or database failure
     using the contents of the binary log.  For more information, see
     *note mysqlbinlog-row-events::.

   * Binary log execution errors and slave_exec_mode

     If 'slave_exec_mode' is 'IDEMPOTENT', a failure to apply changes
     from RBL because the original row cannot be found does not trigger
     an error or cause replication to fail.  This means that it is
     possible that updates are not applied on the slave, so that the
     master and slave are no longer synchronized.  Latency issues and
     use of nontransactional tables with RBR when 'slave_exec_mode' is
     'IDEMPOTENT' can cause the master and slave to diverge even
     further.  For more information about 'slave_exec_mode', see *note
     server-system-variables::.

     *Note*:

     'slave_exec_mode=IDEMPOTENT' is generally useful only for circular
     replication or multi-master replication with NDB Cluster, for which
     'IDEMPOTENT' is the default value (see *note
     mysql-cluster-replication::).

     For other scenarios, setting 'slave_exec_mode' to 'STRICT' is
     normally sufficient; this is the default value for storage engines
     other than *note 'NDB': mysql-cluster.

   * Lack of binary log checksums

     RBL uses no checksums.  This means that network, disk, and other
     errors may not be identified when processing the binary log.  To
     ensure that data is transmitted without network corruption, you may
     want to consider using SSL, which adds another layer of
     checksumming, for replication connections.  The *note 'CHANGE
     MASTER TO': change-master-to. statement has options to enable
     replication over SSL. See also *note change-master-to::, for
     general information about setting up MySQL with SSL.

     This is not an issue in MySQL 5.6 and later, which support
     checksums for the binary log; see Checksum options
     (https://dev.mysql.com/doc/refman/5.6/en/replication-options-binary-log.html#replication-optvars-binlog-checksums).

   * Filtering based on server ID not supported

     A common practice is to filter out changes on some slaves by using
     a 'WHERE' clause that includes the relation '@@server_id <>
     ID_VALUE' clause with *note 'UPDATE': update. and *note 'DELETE':
     delete. statements, a simple example of such a clause being 'WHERE
     @@server_id <> 1'.  However, this does not work correctly with
     row-based logging.  If you must use the 'server_id' system variable
     for statement filtering, you must also use
     '--binlog_format=STATEMENT'.

     In MySQL 5.5, you can do filtering based on server ID by using the
     'IGNORE_SERVER_IDS' option for the *note 'CHANGE MASTER TO':
     change-master-to. statement.  This option works with the
     statement-based and row-based logging formats.

   * Database-level replication options

     The effects of the '--replicate-do-db', '--replicate-ignore-db',
     and '--replicate-rewrite-db' options differ considerably depending
     on whether row-based or statement-based logging is used.  Because
     of this, it is recommended to avoid database-level options and
     instead use table-level options such as '--replicate-do-table' and
     '--replicate-ignore-table'.  For more information about these
     options and the impact that your choice of replication format has
     on how they operate, see *note replication-options::.

   * RBL, nontransactional tables, and stopped slaves

     When using row-based logging, if the slave server is stopped while
     a slave thread is updating a nontransactional table, the slave
     database may reaches an inconsistent state.  For this reason, it is
     recommended that you use a transactional storage engine such as
     *note 'InnoDB': innodb-storage-engine. for all tables replicated
     using the row-based format.

     Use of *note 'STOP SLAVE': stop-slave. (or *note 'STOP SLAVE
     SQL_THREAD': stop-slave. in MySQL 5.5.9 and later) prior to
     shutting down the slave MySQL server helps prevent such issues from
     occurring, and is always recommended regardless of the logging
     format or storage engines employed.


File: manual.info.tmp,  Node: replication-rbr-safe-unsafe,  Prev: replication-rbr-usage,  Up: replication-formats

17.1.2.3 Determination of Safe and Unsafe Statements in Binary Logging
......................................................................

When speaking of the 'safeness' of a statement in MySQL Replication, we
are referring to whether a statement and its effects can be replicated
correctly using statement-based format.  If this is true of the
statement, we refer to the statement as _safe_; otherwise, we refer to
it as _unsafe_.

In general, a statement is safe if it deterministic, and unsafe if it is
not.  However, certain nondeterministic functions are _not_ considered
unsafe (see *note replication-rbr-safe-unsafe-not::, later in this
section).  In addition, statements using results from floating-point
math functions--which are hardware-dependent--are always considered
unsafe (see *note replication-features-floatvalues::).

Handling of safe and unsafe statements

A statement is treated differently depending on whether the statement is
considered safe, and with respect to the binary logging format (that is,
the current value of 'binlog_format').

   * No distinction is made in the treatment of safe and unsafe
     statements when the binary logging mode is 'ROW'.

   * If the binary logging format is 'MIXED', statements flagged as
     unsafe are logged using the row-based format; statements regarded
     as safe are logged using the statement-based format.

   * If the binary logging format is 'STATEMENT', statements flagged as
     being unsafe generate a warning to this effect.  (Safe statements
     are logged normally.)

Each statement flagged as unsafe generates a warning.  Formerly, in
cases where a great many such statements were executed on the master,
this could lead to very large error log files, sometimes even filling up
an entire disk unexpectedly.  To guard against this, MySQL 5.5.27
introduced a warning suppression mechanism, which behaves as follows:
Whenever the 50 most recent 'ER_BINLOG_UNSAFE_STATEMENT' warnings have
been generated more than 50 times in any 50-second period, warning
suppression is enabled.  When activated, this causes such warnings not
to be written to the error log; instead, for each 50 warnings of this
type, a note 'The last warning was repeated N times in last S seconds'
is written to the error log.  This continues as long as the 50 most
recent such warnings were issued in 50 seconds or less; once the rate
has decreased below this threshold, the warnings are once again logged
normally.  Warning suppression has no effect on how the safety of
statements for statement-based logging is determined, nor on how
warnings are sent to the client (MySQL clients still receive one warning
for each such statement).

For more information, see *note replication-formats::.

Statements considered unsafe

Statements having the following characteristics are considered unsafe:

   * Statements containing system functions that may return a different
     value on slave.

     These functions include 'FOUND_ROWS()', 'GET_LOCK()',
     'IS_FREE_LOCK()', 'IS_USED_LOCK()', 'LOAD_FILE()',
     'MASTER_POS_WAIT()', 'RAND()', 'RELEASE_LOCK()', 'ROW_COUNT()',
     'SESSION_USER()', 'SLEEP()', 'SYSDATE()', 'SYSTEM_USER()',
     'USER()', 'UUID()', and 'UUID_SHORT()'.

     Nondeterministic functions not considered unsafe

     Although these functions are not deterministic, they are treated as
     safe for purposes of logging and replication: 'CONNECTION_ID()',
     'CURDATE()', 'CURRENT_DATE()', 'CURRENT_TIME()',
     'CURRENT_TIMESTAMP()', 'CURTIME()', 'LOCALTIME()',
     'LOCALTIMESTAMP()', 'NOW()', 'UNIX_TIMESTAMP()', 'UTC_DATE()',
     'UTC_TIME()', 'UTC_TIMESTAMP()', and 'LAST_INSERT_ID()'

     For more information, see *note replication-features-functions::.

   * References to system variables

     Most system variables are not replicated correctly using the
     statement-based format.  For exceptions, see *note
     binary-log-mixed::.

     See *note replication-features-variables::.

   * UDFs

     Since we have no control over what a UDF does, we must assume that
     it is executing unsafe statements.

   * Updates a table having an AUTO_INCREMENT column

     Prior to MySQL 5.5.3, all such statements were always considered
     unsafe because the order in which the rows are updated could differ
     on the master and the slave.  In MySQL 5.3.3 and later, these
     statements are unsafe only when they are executed by a trigger or
     stored program (Bug #50192, Bug #11758052).

     An *note 'INSERT': insert. into a table that has a composite
     primary key containing an 'AUTO_INCREMENT' column that is not the
     first column of this composite key is unsafe.

     For more information, see *note
     replication-features-auto-increment::.

   * INSERT DELAYED statement

     This statement is considered unsafe because the insertion of the
     rows may interleave with concurrently executing statements.

   * INSERT ...  ON DUPLICATE KEY UPDATE statements on tables with
     multiple primary or unique keys

     When executed against a table that contains more than one primary
     or unique key, this statement is considered unsafe, being sensitive
     to the order in which the storage engine checks the keys, which is
     not deterministic, and on which the choice of rows updated by the
     MySQL Server depends.

     An *note 'INSERT ... ON DUPLICATE KEY UPDATE': insert-on-duplicate.
     statement against a table having more than one unique or primary
     key is marked as unsafe for statement-based replication beginning
     with MySQL 5.5.24.  (Bug #11765650, Bug #58637)

   * Updates using LIMIT

     The order in which rows are retrieved is not specified.

     See *note replication-features-limit::.

   * Accesses or references log tables

     The contents of the system log table may differ between master and
     slave.

   * Nontransactional operations after transactional operations

     Within a transaction, allowing any nontransactional reads or writes
     to execute after any transactional reads or writes is considered
     unsafe.

     For more information, see *note
     replication-features-transactions::.

   * Accesses or references self-logging tables

     All reads and writes to self-logging tables are considered unsafe.
     Within a transaction, any statement following a read or write to
     self-logging tables is also considered unsafe.

   * LOAD DATA statements

     Beginning with MySQL 5.5.6, *note 'LOAD DATA': load-data. is
     treated as unsafe and when 'binlog_format=mixed' the statement is
     logged in row-based format.  When 'binlog_format=statement' *note
     'LOAD DATA': load-data. does not generate a warning, unlike other
     unsafe statements.

For additional information, see *note replication-features::.


File: manual.info.tmp,  Node: replication-options,  Next: replication-administration,  Prev: replication-formats,  Up: replication-configuration

17.1.3 Replication and Binary Logging Options and Variables
-----------------------------------------------------------

* Menu:

* replication-options-reference::  Replication and Binary Logging Option and Variable Reference
* replication-options-master::   Replication Master Options and Variables
* replication-options-slave::    Replication Slave Options and Variables
* replication-options-binary-log::  Binary Log Options and Variables

The next few sections contain information about *note 'mysqld': mysqld.
options and server variables that are used in replication and for
controlling the binary log.  Options and variables for use on
replication masters and replication slaves are covered separately, as
are options and variables relating to binary logging.  A set of
quick-reference tables providing basic information about these options
and variables is also included (in the next section following this one).

Of particular importance is the 'server_id' system variable.

Property               Value
                       
*Command-Line          '--server-id=#'
Format*                

*System Variable*      'server_id'
                       
*Scope*                Global
                       
*Dynamic*              Yes
                       
*Type*                 Integer
                       
*Default Value*        '0'
                       
*Minimum Value*        '0'
                       
*Maximum Value*        '4294967295'

This variable specifies the server ID.

On a replication master and each replication slave, you _must_ specify
'server_id' to establish a unique replication ID in the range from 1 to
2^32 − 1.  'Unique', means that each ID must be different from every
other ID in use by any other replication master or slave.  For
additional information, see *note replication-options-master::, and
*note replication-options-slave::.

If you do not specify 'server_id', the default server ID is 0.  If the
server ID is set to 0, binary logging takes place, but a master with a
server ID of 0 refuses any connections from slaves, and a slave with a
server ID of 0 refuses to connect to a master.  Note that although you
can change the server ID dynamically to a nonzero value, doing so does
not enable replication to start immediately.  You must change the server
ID and then restart the server to initialize the replication slave.

In MySQL 5.5, whether the server ID is set to 0 explicitly or the
default is allowed to be used, the server sets the 'server_id' system
variable to 1; this is a known issue that is fixed in MySQL 5.7.

For more information, see *note replication-howto-slavebaseconfig::.


File: manual.info.tmp,  Node: replication-options-reference,  Next: replication-options-master,  Prev: replication-options,  Up: replication-options

17.1.3.1 Replication and Binary Logging Option and Variable Reference
.....................................................................

The following lists provide basic information about the MySQL
command-line options and system variables applicable to replication and
the binary log.

   * 'abort-slave-event-count': Option used by mysql-test for debugging
     and testing of replication

   * 'auto_increment_increment': AUTO_INCREMENT columns are incremented
     by this value

   * 'auto_increment_offset': Offset added to AUTO_INCREMENT columns

   * 'Com_change_master': Count of CHANGE MASTER TO statements

   * 'Com_show_master_status': Count of SHOW MASTER STATUS statements

   * 'Com_show_new_master': Count of SHOW NEW MASTER statements

   * 'Com_show_slave_hosts': Count of SHOW SLAVE HOSTS statements

   * 'Com_show_slave_status': Count of SHOW SLAVE STATUS statements

   * 'Com_slave_start': Count of START SLAVE statements

   * 'Com_slave_stop': Count of STOP SLAVE statements

   * 'disconnect-slave-event-count': Option used by mysql-test for
     debugging and testing of replication

   * 'expire_logs_days': Purge binary logs after this many days

   * 'init_slave': Statements that are executed when a slave connects to
     a master

   * 'log_bin_trust_function_creators': If equal to 0 (the default),
     then when -log-bin is used, creation of a stored function is
     allowed only to users having the SUPER privilege and only if the
     function created does not break binary logging

   * 'log_bin_trust_routine_creators': (deprecated) Use
     log-bin-trust-function-creators instead

   * 'master-connect-retry': Number of seconds the slave thread will
     sleep before retrying to connect to the master in case the master
     goes down or the connection is lost

   * 'master-host': Master host name or IP address for replication

   * 'master-info-file': The location and name of the file that
     remembers the master and where the I/O replication thread is in the
     master's binary logs

   * 'master-password': The password the slave thread will authenticate
     with when connecting to master

   * 'master-port': The port the master is listening on

   * 'master-retry-count': Number of tries the slave makes to connect to
     the master before giving up

   * 'master-ssl': Enable the slave to connect to the master using SSL

   * 'master-ssl-ca': Master SSL Certificate Authority file; applies
     only if master-ssl is enabled

   * 'master-ssl-capath': Master SSL Certificate Authority path; applies
     only if master-ssl is enabled

   * 'master-ssl-cert': Master SSL certificate file name; applies only
     if master-ssl is enabled

   * 'master-ssl-cipher': Master SSL cipher; applies only if master-ssl
     is enabled

   * 'master-ssl-key': Master SSL key file name; applies only if
     master-ssl is enabled

   * 'master-user': The user name the slave thread will use for
     authentication when connecting to master.  The user must have FILE
     privilege.  If the master user is not set, user test is assumed.
     The value in master.info will take precedence if it can be read

   * 'max_relay_log_size': If nonzero, relay log is rotated
     automatically when its size exceeds this value.  If zero, size at
     which rotation occurs is determined by the value of
     max_binlog_size.

   * 'relay_log': The location and base name to use for relay logs

   * 'relay_log_index': The location and name to use for the file that
     keeps a list of the last relay logs

   * 'relay_log_info_file': File in which the slave records information
     about the relay logs

   * 'relay_log_purge': Determines whether relay logs are purged

   * 'relay_log_recovery': Whether automatic recovery of relay log files
     from master at startup is enabled; must be enabled for a crash-safe
     slave

   * 'relay_log_space_limit': Maximum space to use for all relay logs

   * 'replicate-do-db': Tells the slave SQL thread to restrict
     replication to the specified database

   * 'replicate-do-table': Tells the slave SQL thread to restrict
     replication to the specified table

   * 'replicate-ignore-db': Tells the slave SQL thread not to replicate
     to the specified database

   * 'replicate-ignore-table': Tells the slave SQL thread not to
     replicate to the specified table

   * 'replicate-rewrite-db': Updates to a database with a different name
     than the original

   * 'replicate-same-server-id': In replication, if enabled, do not skip
     events having our server id

   * 'replicate-wild-do-table': Tells the slave thread to restrict
     replication to the tables that match the specified wildcard pattern

   * 'replicate-wild-ignore-table': Tells the slave thread not to
     replicate to the tables that match the given wildcard pattern

   * 'report_host': Host name or IP of the slave to be reported to the
     master during slave registration

   * 'report_password': An arbitrary password that the slave server
     should report to the master.  Not the same as the password for the
     MySQL replication user account.

   * 'report_port': Port for connecting to slave reported to the master
     during slave registration

   * 'report_user': An arbitrary user name that a slave server should
     report to the master.  Not the same as the name used with the MySQL
     replication user account.

   * 'rpl_recovery_rank': Not used; removed in later versions

   * 'Rpl_semi_sync_master_clients': Number of semisynchronous slaves

   * 'rpl_semi_sync_master_enabled': Whether semisynchronous replication
     is enabled on the master

   * 'Rpl_semi_sync_master_net_avg_wait_time': The average time the
     master waited for a slave reply

   * 'Rpl_semi_sync_master_net_wait_time': The total time the master
     waited for slave replies

   * 'Rpl_semi_sync_master_net_waits': The total number of times the
     master waited for slave replies

   * 'Rpl_semi_sync_master_no_times': Number of times the master turned
     off semisynchronous replication

   * 'Rpl_semi_sync_master_no_tx': Number of commits not acknowledged
     successfully

   * 'Rpl_semi_sync_master_status': Whether semisynchronous replication
     is operational on the master

   * 'Rpl_semi_sync_master_timefunc_failures': Number of times the
     master failed when calling time functions

   * 'rpl_semi_sync_master_timeout': Number of milliseconds to wait for
     slave acknowledgment

   * 'rpl_semi_sync_master_trace_level': The semisynchronous replication
     debug trace level on the master

   * 'Rpl_semi_sync_master_tx_avg_wait_time': The average time the
     master waited for each transaction

   * 'Rpl_semi_sync_master_tx_wait_time': The total time the master
     waited for transactions

   * 'Rpl_semi_sync_master_tx_waits': The total number of times the
     master waited for transactions

   * 'rpl_semi_sync_master_wait_no_slave': Whether master waits for
     timeout even with no slaves

   * 'Rpl_semi_sync_master_wait_pos_backtraverse': The total number of
     times the master waited for an event with binary coordinates lower
     than events waited for previously

   * 'Rpl_semi_sync_master_wait_sessions': Number of sessions currently
     waiting for slave replies

   * 'Rpl_semi_sync_master_yes_tx': Number of commits acknowledged
     successfully

   * 'rpl_semi_sync_slave_enabled': Whether semisynchronous replication
     is enabled on slave

   * 'Rpl_semi_sync_slave_status': Whether semisynchronous replication
     is operational on slave

   * 'rpl_semi_sync_slave_trace_level': The semisynchronous replication
     debug trace level on the slave

   * 'Rpl_status': The status of fail-safe replication (not implemented)

   * 'show-slave-auth-info': Show user name and password in SHOW SLAVE
     HOSTS on this master

   * 'skip-slave-start': If set, slave is not autostarted

   * 'slave_load_tmpdir': The location where the slave should put its
     temporary files when replicating LOAD DATA statements

   * 'slave_net_timeout': Number of seconds to wait for more data from a
     master/slave connection before aborting the read

   * 'slave-skip-errors': Tells the slave thread to continue replication
     when a query returns an error from the provided list

   * 'slave_compressed_protocol': Use compression of master/slave
     protocol

   * 'slave_exec_mode': Allows for switching the slave thread between
     IDEMPOTENT mode (key and some other errors suppressed) and STRICT
     mode; STRICT mode is the default, except for NDB Cluster, where
     IDEMPOTENT is always used

   * 'Slave_heartbeat_period': The slave's replication heartbeat
     interval, in seconds

   * 'slave_max_allowed_packet': Maximum size, in bytes, of a packet
     that can be sent from a replication master to a slave; overrides
     max_allowed_packet

   * 'Slave_open_temp_tables': Number of temporary tables that the slave
     SQL thread currently has open

   * 'Slave_received_heartbeats': Number of heartbeats received by a
     replication slave since previous reset

   * 'Slave_retried_transactions': The total number of times since
     startup that the replication slave SQL thread has retried
     transactions

   * 'Slave_running': The state of this server as a replication slave
     (slave I/O thread status)

   * 'slave_transaction_retries': Number of times the slave SQL thread
     will retry a transaction in case it failed with a deadlock or
     elapsed lock wait timeout, before giving up and stopping

   * 'slave_type_conversions': Controls type conversion mode on
     replication slave.  Value is a list of zero or more elements from
     the list: ALL_LOSSY, ALL_NON_LOSSY. Set to an empty string to
     disallow type conversions between master and slave.

   * 'sql_log_bin': Controls binary logging for the current session

   * 'sql_slave_skip_counter': Number of events from the master that a
     slave server should skip.  Not compatible with GTID replication.

   * 'sync_master_info': Synchronize master.info to disk after every #th
     event

   * 'sync_relay_log': Synchronize relay log to disk after every #th
     event

   * 'sync_relay_log_info': Synchronize relay.info file to disk after
     every #th event

*note replication-options-master::, provides more detailed information
about options and variables relating to replication master servers.  For
more information about options and variables relating to replication
slaves, see *note replication-options-slave::.

   * 'binlog-do-db': Limits binary logging to specific databases

   * 'binlog_format': Specifies the format of the binary log

   * 'binlog-ignore-db': Tells the master that updates to the given
     database should not be logged to the binary log

   * 'binlog-row-event-max-size': Binary log max event size

   * 'Binlog_cache_disk_use': Number of transactions that used a
     temporary file instead of the binary log cache

   * 'binlog_cache_size': Size of the cache to hold the SQL statements
     for the binary log during a transaction

   * 'Binlog_cache_use': Number of transactions that used the temporary
     binary log cache

   * 'binlog_direct_non_transactional_updates': Causes updates using
     statement format to nontransactional engines to be written directly
     to binary log.  See documentation before using.

   * 'Binlog_stmt_cache_disk_use': Number of nontransactional statements
     that used a temporary file instead of the binary log statement
     cache

   * 'binlog_stmt_cache_size': Size of the cache to hold
     nontransactional statements for the binary log during a transaction

   * 'Binlog_stmt_cache_use': Number of statements that used the
     temporary binary log statement cache

   * 'Com_show_binlog_events': Count of SHOW BINLOG EVENTS statements

   * 'Com_show_binlogs': Count of SHOW BINLOGS statements

   * 'log-bin': Specifies the base name for binary log files

   * 'log-bin-index': Specifies the name for the binary log index file

   * 'log_bin': Whether the binary log is enabled

   * 'log_bin_use_v1_row_events': Whether server is using version 1
     binary log row events

   * 'log_slave_updates': Whether the slave should log the updates
     performed by its SQL thread to its own binary log

   * 'max-binlog-dump-events': Option used by mysql-test for debugging
     and testing of replication

   * 'max_binlog_cache_size': Can be used to restrict the total size
     used to cache a multi-statement transaction

   * 'max_binlog_size': Binary log will be rotated automatically when
     size exceeds this value

   * 'max_binlog_stmt_cache_size': Can be used to restrict the total
     size used to cache all nontransactional statements during a
     transaction

   * 'sporadic-binlog-dump-fail': Option used by mysql-test for
     debugging and testing of replication

   * 'sync_binlog': Synchronously flush binary log to disk after every
     #th event

*note replication-options-binary-log::, provides more detailed
information about options and variables relating to binary logging.  For
additional general information about the binary log, see *note
binary-log::.

For information about the 'sql_log_bin' and 'sql_log_off' variables, see
*note server-system-variables::.

For a lsiting with _all_ command-line options, system and status
variables used with *note 'mysqld': mysqld, see *note
server-option-variable-reference::.


File: manual.info.tmp,  Node: replication-options-master,  Next: replication-options-slave,  Prev: replication-options-reference,  Up: replication-options

17.1.3.2 Replication Master Options and Variables
.................................................

This section describes the server options and system variables that you
can use on replication master servers.  You can specify the options
either on the *note command line: command-line-options. or in an *note
option file: option-files.  You can specify system variable values using
*note 'SET': set-variable.

On the master and each slave, you must set the 'server_id' system
variable to establish a unique replication ID. For each server, you
should pick a unique positive integer in the range from 1 to 2^32 − 1,
and each ID must be different from every other ID in use by any other
replication master or slave.  Example: 'server-id=3'.

For options used on the master for controlling binary logging, see *note
replication-options-binary-log::.

*Startup Options for Replication Masters*

The following list describes startup options for controlling replication
master servers.  Replication-related system variables are discussed
later in this section.

   * 
     '--show-slave-auth-info'

     Property               Value
                            
     *Command-Line          '--show-slave-auth-info[={OFF|ON}]'
     Format*                

     *Type*                 Boolean
                            
     *Default Value*        'OFF'

     Display slave user names and passwords in the output of *note 'SHOW
     SLAVE HOSTS': show-slave-hosts. on the master server for slaves
     started with the '--report-user' and '--report-password' options.

*System Variables Used on Replication Masters*

The following system variables are used in controlling replication
masters:

   * 
     'auto_increment_increment'

     Property               Value
                            
     *Command-Line          '--auto-increment-increment=#'
     Format*                

     *System Variable*      'auto_increment_increment'
                            
     *Scope*                Global, Session
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '1'
                            
     *Minimum Value*        '1'
                            
     *Maximum Value*        '65535'

     'auto_increment_increment' and 'auto_increment_offset' are intended
     for use with master-to-master replication, and can be used to
     control the operation of 'AUTO_INCREMENT' columns.  Both variables
     have global and session values, and each can assume an integer
     value between 1 and 65,535 inclusive.  Setting the value of either
     of these two variables to 0 causes its value to be set to 1
     instead.  Attempting to set the value of either of these two
     variables to an integer greater than 65,535 or less than 0 causes
     its value to be set to 65,535 instead.  Attempting to set the value
     of 'auto_increment_increment' or 'auto_increment_offset' to a
     noninteger value gives rise to an error, and the actual value of
     the variable remains unchanged.

     *Note*:

     'auto_increment_increment' is also supported for use with *note
     'NDB': mysql-cluster. tables.

     These two variables affect 'AUTO_INCREMENT' column behavior as
     follows:

        * 'auto_increment_increment' controls the interval between
          successive column values.  For example:

               mysql> SHOW VARIABLES LIKE 'auto_inc%';
               +--------------------------+-------+
               | Variable_name            | Value |
               +--------------------------+-------+
               | auto_increment_increment | 1     |
               | auto_increment_offset    | 1     |
               +--------------------------+-------+
               2 rows in set (0.00 sec)

               mysql> CREATE TABLE autoinc1
                   -> (col INT NOT NULL AUTO_INCREMENT PRIMARY KEY);
                 Query OK, 0 rows affected (0.04 sec)

               mysql> SET @@auto_increment_increment=10;
               Query OK, 0 rows affected (0.00 sec)

               mysql> SHOW VARIABLES LIKE 'auto_inc%';
               +--------------------------+-------+
               | Variable_name            | Value |
               +--------------------------+-------+
               | auto_increment_increment | 10    |
               | auto_increment_offset    | 1     |
               +--------------------------+-------+
               2 rows in set (0.01 sec)

               mysql> INSERT INTO autoinc1 VALUES (NULL), (NULL), (NULL), (NULL);
               Query OK, 4 rows affected (0.00 sec)
               Records: 4  Duplicates: 0  Warnings: 0

               mysql> SELECT col FROM autoinc1;
               +-----+
               | col |
               +-----+
               |   1 |
               |  11 |
               |  21 |
               |  31 |
               +-----+
               4 rows in set (0.00 sec)

        * 'auto_increment_offset' determines the starting point for the
          'AUTO_INCREMENT' column value.  Consider the following,
          assuming that these statements are executed during the same
          session as the example given in the description for
          'auto_increment_increment':

               mysql> SET @@auto_increment_offset=5;
               Query OK, 0 rows affected (0.00 sec)

               mysql> SHOW VARIABLES LIKE 'auto_inc%';
               +--------------------------+-------+
               | Variable_name            | Value |
               +--------------------------+-------+
               | auto_increment_increment | 10    |
               | auto_increment_offset    | 5     |
               +--------------------------+-------+
               2 rows in set (0.00 sec)

               mysql> CREATE TABLE autoinc2
                   -> (col INT NOT NULL AUTO_INCREMENT PRIMARY KEY);
               Query OK, 0 rows affected (0.06 sec)

               mysql> INSERT INTO autoinc2 VALUES (NULL), (NULL), (NULL), (NULL);
               Query OK, 4 rows affected (0.00 sec)
               Records: 4  Duplicates: 0  Warnings: 0

               mysql> SELECT col FROM autoinc2;
               +-----+
               | col |
               +-----+
               |   5 |
               |  15 |
               |  25 |
               |  35 |
               +-----+
               4 rows in set (0.02 sec)

          If the value of 'auto_increment_offset' is greater than that
          of 'auto_increment_increment', the value of
          'auto_increment_offset' is ignored.

     Should one or both of these variables be changed and then new rows
     inserted into a table containing an 'AUTO_INCREMENT' column, the
     results may seem counterintuitive because the series of
     'AUTO_INCREMENT' values is calculated without regard to any values
     already present in the column, and the next value inserted is the
     least value in the series that is greater than the maximum existing
     value in the 'AUTO_INCREMENT' column.  In other words, the series
     is calculated like so:

     'auto_increment_offset' + N x 'auto_increment_increment'

     where N is a positive integer value in the series [1, 2, 3, ...].
     For example:

          mysql> SHOW VARIABLES LIKE 'auto_inc%';
          +--------------------------+-------+
          | Variable_name            | Value |
          +--------------------------+-------+
          | auto_increment_increment | 10    |
          | auto_increment_offset    | 5     |
          +--------------------------+-------+
          2 rows in set (0.00 sec)

          mysql> SELECT col FROM autoinc1;
          +-----+
          | col |
          +-----+
          |   1 |
          |  11 |
          |  21 |
          |  31 |
          +-----+
          4 rows in set (0.00 sec)

          mysql> INSERT INTO autoinc1 VALUES (NULL), (NULL), (NULL), (NULL);
          Query OK, 4 rows affected (0.00 sec)
          Records: 4  Duplicates: 0  Warnings: 0

          mysql> SELECT col FROM autoinc1;
          +-----+
          | col |
          +-----+
          |   1 |
          |  11 |
          |  21 |
          |  31 |
          |  35 |
          |  45 |
          |  55 |
          |  65 |
          +-----+
          8 rows in set (0.00 sec)

     The values shown for 'auto_increment_increment' and
     'auto_increment_offset' generate the series 5 + N x 10, that is,
     [5, 15, 25, 35, 45, ...].  The greatest value present in the 'col'
     column prior to the *note 'INSERT': insert. is 31, and the next
     available value in the 'AUTO_INCREMENT' series is 35, so the
     inserted values for 'col' begin at that point and the results are
     as shown for the *note 'SELECT': select. query.

     It is not possible to confine the effects of these two variables to
     a single table, and thus they do not take the place of the
     sequences offered by some other database management systems; these
     variables control the behavior of all 'AUTO_INCREMENT' columns in
     _all_ tables on the MySQL server.  If the global value of either
     variable is set, its effects persist until the global value is
     changed or overridden by setting the session value, or until *note
     'mysqld': mysqld. is restarted.  If the local value is set, the new
     value affects 'AUTO_INCREMENT' columns for all tables into which
     new rows are inserted by the current user for the duration of the
     session, unless the values are changed during that session.

     The default value of 'auto_increment_increment' is 1.  See *note
     replication-features-auto-increment::.

   * 
     'auto_increment_offset'

     Property               Value
                            
     *Command-Line          '--auto-increment-offset=#'
     Format*                

     *System Variable*      'auto_increment_offset'
                            
     *Scope*                Global, Session
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '1'
                            
     *Minimum Value*        '1'
                            
     *Maximum Value*        '65535'

     This variable has a default value of 1.  For particulars, see the
     description for 'auto_increment_increment'.

     *Note*:

     'auto_increment_offset' is also supported for use with *note 'NDB':
     mysql-cluster. tables.


File: manual.info.tmp,  Node: replication-options-slave,  Next: replication-options-binary-log,  Prev: replication-options-master,  Up: replication-options

17.1.3.3 Replication Slave Options and Variables
................................................

   * *note replication-optvars-slaves::

   * *note replication-optvars-slaves-obsolete::

   * *note replication-sysvars-slaves::

This section describes the server options and system variables that
apply to slave replication servers.  You can specify the options either
on the *note command line: command-line-options. or in an *note option
file: option-files.  Many of the options can be set while the server is
running by using the *note 'CHANGE MASTER TO': change-master-to.
statement.  You can specify system variable values using *note 'SET':
set-variable.

Server ID

On the master and each slave, you must set the 'server_id' system
variable to establish a unique replication ID in the range from 1 to
2^32 − 1.  'Unique' means that each ID must be different from every
other ID in use by any other replication master or slave.  Example
'my.cnf' file:

     [mysqld]
     server-id=3

*Startup Options for Replication Slaves*

The following list describes startup options for controlling replication
slave servers.  Many of these options can be set while the server is
running by using the *note 'CHANGE MASTER TO': change-master-to.
statement.  Others, such as the '--replicate-*' options, can be set only
when the slave server starts.  Replication-related system variables are
discussed later in this section.

   * 
     '--abort-slave-event-count'

     Property               Value
                            
     *Command-Line          '--abort-slave-event-count=#'
     Format*                

     *Type*                 Integer
                            
     *Default Value*        '0'
                            
     *Minimum Value*        '0'

     When this option is set to some positive integer VALUE other than 0
     (the default) it affects replication behavior as follows: After the
     slave SQL thread has started, VALUE log events are permitted to be
     executed; after that, the slave SQL thread does not receive any
     more events, just as if the network connection from the master were
     cut.  The slave thread continues to run, and the output from *note
     'SHOW SLAVE STATUS': show-slave-status. displays 'Yes' in both the
     'Slave_IO_Running' and the 'Slave_SQL_Running' columns, but no
     further events are read from the relay log.

     This option is used internally by the MySQL test suite for
     replication testing and debugging.  It is not intended for use in a
     production setting.

   * 
     '--disconnect-slave-event-count'

     Property               Value
                            
     *Command-Line          '--disconnect-slave-event-count=#'
     Format*                

     *Type*                 Integer
                            
     *Default Value*        '0'

     This option is used internally by the MySQL test suite for
     replication testing and debugging.

   * 
     '--log-slow-slave-statements'

     Property               Value
                            
     *Command-Line          '--log-slow-slave-statements[={OFF|ON}]'
     Format*                

     *Type*                 Boolean
                            
     *Default Value*        'OFF'

     When the slow query log is enabled, this option enables logging for
     queries that have taken more than 'long_query_time' seconds to
     execute on the slave.

     Note that all statements logged in row format in the master will
     not be logged in the slave's slow log, even if this option is
     enabled.

   * 
     '--log-warnings[=LEVEL]'

     Property               Value
                            
     *Command-Line          '--log-warnings[=#]'
     Format*                

     *System Variable*      'log_warnings'
                            
     *Scope*                Global, Session
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '1'
                            
     *Minimum Value*        '0'
                            
     *Maximum Value*        '18446744073709551615'
     (64-bit platforms,     
     >= 5.5.3)

     *Maximum Value*        '18446744073709547520'
     (64-bit platforms,     
     <= 5.5.2)

     *Maximum Value*        '4294967295'
     (32-bit platforms)

     This option causes a server to print more messages to the error log
     about what it is doing.  With respect to replication, the server
     generates warnings that it succeeded in reconnecting after a
     network/connection failure, and informs you as to how each slave
     thread started.  This option is enabled (1) by default; to disable
     it, use '--log-warnings=0'.  If the value is greater than 1,
     aborted connections are written to the error log, and access-denied
     errors for new connection attempts are written.  See *note
     communication-errors::.

     *Note*:

     The effects of this option are not limited to replication.  It
     affects diagnostic messages across a spectrum of server activities.

   * 
     '--master-info-file=FILE_NAME'

     Property               Value
                            
     *Command-Line          '--master-info-file=file_name'
     Format*                

     *Type*                 File name
                            
     *Default Value*        'master.info'

     The name to use for the file in which the slave records information
     about the master.  The default name is 'master.info' in the data
     directory.  For information about the format of this file, see
     *note slave-logs-status::.

   * 
     '--master-retry-count=COUNT'

     Property               Value
                            
     *Command-Line          '--master-retry-count=#'
     Format*                

     *Type*                 Integer
                            
     *Default Value*        '86400'
                            
     *Minimum Value*        '0'
                            
     *Maximum Value*        '18446744073709551615'
     (64-bit platforms)     

     *Maximum Value*        '4294967295'
     (32-bit platforms)

     The number of times that the slave tries to connect to the master
     before giving up.  Reconnects are attempted at intervals set by the
     'MASTER_CONNECT_RETRY' option of the *note 'CHANGE MASTER TO':
     change-master-to. statement (default 60).  Reconnection attempts
     are triggered when the slave reaches its connection timeout
     (specified by the 'slave_net_timeout' system variable) without
     receiving data from the master.  The default value is 86400.  A
     value of 0 means 'infinite'; the slave attempts to connect forever.

   * 
     '--max-relay-log-size=SIZE'

     Property               Value
                            
     *Command-Line          '--max-relay-log-size=#'
     Format*                

     *System Variable*      'max_relay_log_size'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '0'
                            
     *Minimum Value*        '0'
                            
     *Maximum Value*        '1073741824'

     The size at which the server rotates relay log files automatically.
     If this value is nonzero, the relay log is rotated automatically
     when its size exceeds this value.  If this value is zero (the
     default), the size at which relay log rotation occurs is determined
     by the value of 'max_binlog_size'.  For more information, see *note
     slave-logs-relaylog::.

   * 
     '--relay-log-purge={0|1}'

     Property               Value
                            
     *Command-Line          '--relay-log-purge[={OFF|ON}]'
     Format*                

     *System Variable*      'relay_log_purge'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Boolean
                            
     *Default Value*        'ON'

     Disable or enable automatic purging of relay logs as soon as they
     are no longer needed.  The default value is 1 (enabled).  This is a
     global variable that can be changed dynamically with 'SET GLOBAL
     relay_log_purge = N'.

   * 
     '--relay-log-space-limit=SIZE'

     Property               Value
                            
     *Command-Line          '--relay-log-space-limit=#'
     Format*                

     *System Variable*      'relay_log_space_limit'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Integer
                            
     *Default Value*        '0'
                            
     *Minimum Value*        '0'
                            
     *Maximum Value*        '18446744073709551615'
     (64-bit platforms,     
     >= 5.5.3)

     *Maximum Value*        '18446744073709547520'
     (64-bit platforms,     
     <= 5.5.2)

     *Maximum Value*        '4294967295'
     (32-bit platforms)

     This option places an upper limit on the total size in bytes of all
     relay logs on the slave.  A value of 0 means 'no limit.' This is
     useful for a slave server host that has limited disk space.  When
     the limit is reached, the I/O thread stops reading binary log
     events from the master server until the SQL thread has caught up
     and deleted some unused relay logs.  Note that this limit is not
     absolute: There are cases where the SQL thread needs more events
     before it can delete relay logs.  In that case, the I/O thread
     exceeds the limit until it becomes possible for the SQL thread to
     delete some relay logs because not doing so would cause a deadlock.
     You should not set '--relay-log-space-limit' to less than twice the
     value of '--max-relay-log-size' (or '--max-binlog-size' if
     '--max-relay-log-size' is 0).  In that case, there is a chance that
     the I/O thread waits for free space because
     '--relay-log-space-limit' is exceeded, but the SQL thread has no
     relay log to purge and is unable to satisfy the I/O thread.  This
     forces the I/O thread to ignore '--relay-log-space-limit'
     temporarily.

   * 
     '--replicate-do-db=DB_NAME'

     Property               Value
                            
     *Command-Line          '--replicate-do-db=name'
     Format*                

     *Type*                 String

     The effects of this option depend on whether statement-based or
     row-based replication is in use.

     Statement-based replication

     Tell the slave SQL thread to restrict replication to statements
     where the default database (that is, the one selected by *note
     'USE': use.) is DB_NAME.  To specify more than one database, use
     this option multiple times, once for each database; however, doing
     so does _not_ replicate cross-database statements such as 'UPDATE
     SOME_DB.SOME_TABLE SET foo='bar'' while a different database (or no
     database) is selected.

     *Warning*:

     To specify multiple databases you _must_ use multiple instances of
     this option.  Because database names can contain commas, if you
     supply a comma separated list then the list will be treated as the
     name of a single database.

     An example of what does not work as you might expect when using
     statement-based replication: If the slave is started with
     '--replicate-do-db=sales' and you issue the following statements on
     the master, the *note 'UPDATE': update. statement is _not_
     replicated:

          USE prices;
          UPDATE sales.january SET amount=amount+1000;

     The main reason for this 'check just the default database' behavior
     is that it is difficult from the statement alone to know whether it
     should be replicated (for example, if you are using multiple-table
     *note 'DELETE': delete. statements or multiple-table *note
     'UPDATE': update. statements that act across multiple databases).
     It is also faster to check only the default database rather than
     all databases if there is no need.

     Row-based replication

     Tells the slave SQL thread to restrict replication to database
     DB_NAME.  Only tables belonging to DB_NAME are changed; the current
     database has no effect on this.  Suppose that the slave is started
     with '--replicate-do-db=sales' and row-based replication is in
     effect, and then the following statements are run on the master:

          USE prices;
          UPDATE sales.february SET amount=amount+100;

     The 'february' table in the 'sales' database on the slave is
     changed in accordance with the *note 'UPDATE': update. statement;
     this occurs whether or not the *note 'USE': use. statement was
     issued.  However, issuing the following statements on the master
     has no effect on the slave when using row-based replication and
     '--replicate-do-db=sales':

          USE prices;
          UPDATE prices.march SET amount=amount-25;

     Even if the statement 'USE prices' were changed to 'USE sales', the
     *note 'UPDATE': update. statement's effects would still not be
     replicated.

     Another important difference in how '--replicate-do-db' is handled
     in statement-based replication as opposed to row-based replication
     occurs with regard to statements that refer to multiple databases.
     Suppose that the slave is started with '--replicate-do-db=db1', and
     the following statements are executed on the master:

          USE db1;
          UPDATE db1.table1 SET col1 = 10, db2.table2 SET col2 = 20;

     If you are using statement-based replication, then both tables are
     updated on the slave.  However, when using row-based replication,
     only 'table1' is affected on the slave; since 'table2' is in a
     different database, 'table2' on the slave is not changed by the
     *note 'UPDATE': update.  Now suppose that, instead of the 'USE db1'
     statement, a 'USE db4' statement had been used:

          USE db4;
          UPDATE db1.table1 SET col1 = 10, db2.table2 SET col2 = 20;

     In this case, the *note 'UPDATE': update. statement would have no
     effect on the slave when using statement-based replication.
     However, if you are using row-based replication, the *note
     'UPDATE': update. would change 'table1' on the slave, but not
     'table2'--in other words, only tables in the database named by
     '--replicate-do-db' are changed, and the choice of default database
     has no effect on this behavior.

     If you need cross-database updates to work, use
     '--replicate-wild-do-table=DB_NAME.%' instead.  See *note
     replication-rules::.

     *Note*:

     This option affects replication in the same manner that
     '--binlog-do-db' affects binary logging, and the effects of the
     replication format on how '--replicate-do-db' affects replication
     behavior are the same as those of the logging format on the
     behavior of '--binlog-do-db'.

     This option has no effect on *note 'BEGIN': commit, *note 'COMMIT':
     commit, or *note 'ROLLBACK': commit. statements.

   * 
     '--replicate-ignore-db=DB_NAME'

     Property               Value
                            
     *Command-Line          '--replicate-ignore-db=name'
     Format*                

     *Type*                 String

     As with '--replicate-do-db', the effects of this option depend on
     whether statement-based or row-based replication is in use.

     Statement-based replication

     Tells the slave SQL thread not to replicate any statement where the
     default database (that is, the one selected by *note 'USE': use.)
     is DB_NAME.

     Row-based replication

     Tells the slave SQL thread not to update any tables in the database
     DB_NAME.  The default database has no effect.

     When using statement-based replication, the following example does
     not work as you might expect.  Suppose that the slave is started
     with '--replicate-ignore-db=sales' and you issue the following
     statements on the master:

          USE prices;
          UPDATE sales.january SET amount=amount+1000;

     The *note 'UPDATE': update. statement _is_ replicated in such a
     case because '--replicate-ignore-db' applies only to the default
     database (determined by the *note 'USE': use. statement).  Because
     the 'sales' database was specified explicitly in the statement, the
     statement has not been filtered.  However, when using row-based
     replication, the *note 'UPDATE': update. statement's effects are
     _not_ propagated to the slave, and the slave's copy of the
     'sales.january' table is unchanged; in this instance,
     '--replicate-ignore-db=sales' causes _all_ changes made to tables
     in the master's copy of the 'sales' database to be ignored by the
     slave.

     To specify more than one database to ignore, use this option
     multiple times, once for each database.  Because database names can
     contain commas, if you supply a comma separated list then the list
     will be treated as the name of a single database.

     You should not use this option if you are using cross-database
     updates and you do not want these updates to be replicated.  See
     *note replication-rules::.

     If you need cross-database updates to work, use
     '--replicate-wild-ignore-table=DB_NAME.%' instead.  See *note
     replication-rules::.

     *Note*:

     This option affects replication in the same manner that
     '--binlog-ignore-db' affects binary logging, and the effects of the
     replication format on how '--replicate-ignore-db' affects
     replication behavior are the same as those of the logging format on
     the behavior of '--binlog-ignore-db'.

     This option has no effect on *note 'BEGIN': commit, *note 'COMMIT':
     commit, or *note 'ROLLBACK': commit. statements.

   * 
     '--replicate-do-table=DB_NAME.TBL_NAME'

     Property               Value
                            
     *Command-Line          '--replicate-do-table=name'
     Format*                

     *Type*                 String

     Tells the slave SQL thread to restrict replication to the specified
     table.  To specify more than one table, use this option multiple
     times, once for each table.  This works for both cross-database
     updates and default database updates, in contrast to
     '--replicate-do-db'.  See *note replication-rules::.

     This option affects only statements that apply to tables.  It does
     not affect statements that apply only to other database objects,
     such as stored routines.  To filter statements operating on stored
     routines, use one or more of the '--replicate-*-db' options.

   * 
     '--replicate-ignore-table=DB_NAME.TBL_NAME'

     Property               Value
                            
     *Command-Line          '--replicate-ignore-table=name'
     Format*                

     *Type*                 String

     Tells the slave SQL thread not to replicate any statement that
     updates the specified table, even if any other tables might be
     updated by the same statement.  To specify more than one table to
     ignore, use this option multiple times, once for each table.  This
     works for cross-database updates, in contrast to
     '--replicate-ignore-db'.  See *note replication-rules::.

     This option affects only statements that apply to tables.  It does
     not affect statements that apply only to other database objects,
     such as stored routines.  To filter statements operating on stored
     routines, use one or more of the '--replicate-*-db' options.

   * 
     '--replicate-rewrite-db=FROM_NAME->TO_NAME'

     Property               Value
                            
     *Command-Line          '--replicate-rewrite-db=old_name->new_name'
     Format*                

     *Type*                 String

     Tells the slave to translate the default database (that is, the one
     selected by *note 'USE': use.) to TO_NAME if it was FROM_NAME on
     the master.  Only statements involving tables are affected (not
     statements such as *note 'CREATE DATABASE': create-database, *note
     'DROP DATABASE': drop-database, and *note 'ALTER DATABASE':
     alter-database.), and only if FROM_NAME is the default database on
     the master.  To specify multiple rewrites, use this option multiple
     times.  The server uses the first one with a FROM_NAME value that
     matches.  The database name translation is done _before_ the
     '--replicate-*' rules are tested.

     Statements in which table names are qualified with database names
     when using this option do not work with table-level replication
     filtering options such as '--replicate-do-table'.  Suppose we have
     a database named 'a' on the master, one named 'b' on the slave,
     each containing a table 't', and have started the master with
     '--replicate-rewrite-db='a->b''.  At a later point in time, we
     execute *note 'DELETE FROM a.t': delete.  In this case, no relevant
     filtering rule works, for the reasons shown here:

       1. '--replicate-do-table=a.t' does not work because the slave has
          table 't' in database 'b'.

       2. '--replicate-do-table=b.t' does not match the original
          statement and so is ignored.

       3. '--replicate-do-table=*.t' is handled identically to
          '--replicate-do-table=a.t', and thus does not work, either.

     Similarly, the '--replication-rewrite-db' option does not work with
     cross-database updates.

     If you use this option on the command line and the '>' character is
     special to your command interpreter, quote the option value.  For
     example:

          shell> mysqld --replicate-rewrite-db="OLDDB->NEWDB"

   * 
     '--replicate-same-server-id'

     Property               Value
                            
     *Command-Line          '--replicate-same-server-id[={OFF|ON}]'
     Format*                

     *Type*                 Boolean
                            
     *Default Value*        'OFF'

     To be used on slave servers.  Usually you should use the default
     setting of 0, to prevent infinite loops caused by circular
     replication.  If set to 1, the slave does not skip events having
     its own server ID. Normally, this is useful only in rare
     configurations.  Cannot be set to 1 if 'log_slave_updates' is
     enabled.  By default, the slave I/O thread does not write binary
     log events to the relay log if they have the slave's server ID
     (this optimization helps save disk usage).  If you want to use
     '--replicate-same-server-id', be sure to start the slave with this
     option before you make the slave read its own events that you want
     the slave SQL thread to execute.

   * 
     '--replicate-wild-do-table=DB_NAME.TBL_NAME'

     Property               Value
                            
     *Command-Line          '--replicate-wild-do-table=name'
     Format*                

     *Type*                 String

     Tells the slave thread to restrict replication to statements where
     any of the updated tables match the specified database and table
     name patterns.  Patterns can contain the '%' and '_' wildcard
     characters, which have the same meaning as for the 'LIKE'
     pattern-matching operator.  To specify more than one table, use
     this option multiple times, once for each table.  This works for
     cross-database updates.  See *note replication-rules::.

     This option applies to tables, views, and triggers.  It does not
     apply to stored procedures and functions, or events.  To filter
     statements operating on the latter objects, use one or more of the
     '--replicate-*-db' options.

     Example: '--replicate-wild-do-table=foo%.bar%' replicates only
     updates that use a table where the database name starts with 'foo'
     and the table name starts with 'bar'.

     If the table name pattern is '%', it matches any table name and the
     option also applies to database-level statements (*note 'CREATE
     DATABASE': create-database, *note 'DROP DATABASE': drop-database,
     and *note 'ALTER DATABASE': alter-database.).  For example, if you
     use '--replicate-wild-do-table=foo%.%', database-level statements
     are replicated if the database name matches the pattern 'foo%'.

     To include literal wildcard characters in the database or table
     name patterns, escape them with a backslash.  For example, to
     replicate all tables of a database that is named 'my_own%db', but
     not replicate tables from the 'my1ownAABCdb' database, you should
     escape the '_' and '%' characters like this:
     '--replicate-wild-do-table=my\_own\%db'.  If you use the option on
     the command line, you might need to double the backslashes or quote
     the option value, depending on your command interpreter.  For
     example, with the 'bash' shell, you would need to type
     '--replicate-wild-do-table=my\\_own\\%db'.

   * 
     '--replicate-wild-ignore-table=DB_NAME.TBL_NAME'

     Property               Value
                            
     *Command-Line          '--replicate-wild-ignore-table=name'
     Format*                

     *Type*                 String

     Tells the slave thread not to replicate a statement where any table
     matches the given wildcard pattern.  To specify more than one table
     to ignore, use this option multiple times, once for each table.
     This works for cross-database updates.  See *note
     replication-rules::.

     Example: '--replicate-wild-ignore-table=foo%.bar%' does not
     replicate updates that use a table where the database name starts
     with 'foo' and the table name starts with 'bar'.

     For information about how matching works, see the description of
     the '--replicate-wild-do-table' option.  The rules for including
     literal wildcard characters in the option value are the same as for
     '--replicate-wild-ignore-table' as well.

   * 
     '--skip-slave-start'

     Property               Value
                            
     *Command-Line          '--skip-slave-start[={OFF|ON}]'
     Format*                

     *Type*                 Boolean
                            
     *Default Value*        'OFF'

     Tells the slave server not to start the slave threads when the
     server starts.  To start the threads later, use a *note 'START
     SLAVE': start-slave. statement.

   * 
     '--slave_compressed_protocol={0|1}'

     Property               Value
                            
     *Command-Line          '--slave-compressed-protocol[={OFF|ON}]'
     Format*                

     *System Variable*      'slave_compressed_protocol'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Boolean
                            
     *Default Value*        'OFF'

     If this option is set to 1, use compression for the slave/master
     protocol if both the slave and the master support it.  The default
     is 0 (no compression).

   * 
     '--slave-skip-errors=[ERR_CODE1,ERR_CODE2,...|all]'

     (_MySQL NDB Cluster 7.2.6 and higher:)_
     '--slave-skip-errors=[ERR_CODE1,ERR_CODE2,...|all|ddl_exist_errors]'

     Property               Value
                            
     *Command-Line          '--slave-skip-errors=name'
     Format*                

     *System Variable*      'slave_skip_errors'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 String
                            
     *Default Value*        'OFF'
                            
     *Valid Values*         'OFF' '[list of error codes]' 'all'
                            'ddl_exist_errors'

     Normally, replication stops when an error occurs on the slave.
     This gives you the opportunity to resolve the inconsistency in the
     data manually.  This option tells the slave SQL thread to continue
     replication when a statement returns any of the errors listed in
     the option value.

     Do not use this option unless you fully understand why you are
     getting errors.  If there are no bugs in your replication setup and
     client programs, and no bugs in MySQL itself, an error that stops
     replication should never occur.  Indiscriminate use of this option
     results in slaves becoming hopelessly out of synchrony with the
     master, with you having no idea why this has occurred.

     For error codes, you should use the numbers provided by the error
     message in your slave error log and in the output of *note 'SHOW
     SLAVE STATUS': show-slave-status.  *note error-handling::, lists
     server error codes.

     You can also (but should not) use the very nonrecommended value of
     'all' to cause the slave to ignore all error messages and keeps
     going regardless of what happens.  Needless to say, if you use
     'all', there are no guarantees regarding the integrity of your
     data.  Please do not complain (or file bug reports) in this case if
     the slave's data is not anywhere close to what it is on the master.
     _You have been warned_.

     MySQL NDB Cluster 7.2.6 and higher support an additional shorthand
     value 'ddl_exist_errors' for use with the enhanced failover
     mechanism which is implemented beginning with that version of NDB
     Cluster.  This value is equivalent to the error code list
     '1007,1008,1050,1051,1054,1060,1061,1068,1094,1146'.  _This value
     is not supported by the *note 'mysqld': mysqld. binary included
     with the MySQL Server 5.5 distribution_.  (Bug #11762277, Bug
     #54854) For more information, see *note
     mysql-cluster-replication-failover::.

     Examples:

          --slave-skip-errors=1062,1053
          --slave-skip-errors=all
          --slave-skip-errors=ddl_exist_errors

*Obsolete Replication Slave Options*

_The following options are removed in MySQL 5.5.  If you attempt to
start *note 'mysqld': mysqld. with any of these options in MySQL 5.5,
the server aborts with an 'unknown variable' error_.  To set the
replication parameters formerly associated with these options, you must
use the 'CHANGE MASTER TO ...' statement (see *note change-master-to::).

The options affected are shown in this list:

   * '--master-host'

   * '--master-user'

   * '--master-password'

   * '--master-port'

   * '--master-connect-retry'

   * '--master-ssl'

   * '--master-ssl-ca'

   * '--master-ssl-capath'

   * '--master-ssl-cert'

   * '--master-ssl-cipher'

   * '--master-ssl-key'

*System Variables Used on Replication Slaves*

The following list describes system variables for controlling
replication slave servers.  They can be set at server startup and some
of them can be changed at runtime using *note 'SET': set-variable.
Server options used with replication slaves are listed earlier in this
section.

   * 
     'init_slave'

     Property               Value
                            
     *Command-Line          '--init-slave=name'
     Format*                

     *System Variable*      'init_slave'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 String

     This variable is similar to 'init_connect', but is a string to be
     executed by a slave server each time the SQL thread starts.  The
     format of the string is the same as for the 'init_connect'
     variable.

     *Note*:

     The SQL thread sends an acknowledgment to the client before it
     executes 'init_slave'.  Therefore, it is not guaranteed that
     'init_slave' has been executed when *note 'START SLAVE':
     start-slave. returns.  See *note start-slave::, for more
     information.

   * 'relay_log'

     Property               Value
                            
     *Command-Line          '--relay-log=file_name'
     Format*                

     *System Variable*      'relay_log'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 File name

     The base name for the relay log.  The default base name is
     'HOST_NAME-relay-bin'.

     The server writes the file in the data directory unless the base
     name is given with a leading absolute path name to specify a
     different directory.  The server creates relay log files in
     sequence by adding a numeric suffix to the base name.

     Due to the manner in which MySQL parses server options, if you
     specify this variable at server startup, you must supply a value;
     _the default base name is used only if the option is not actually
     specified_.  If you specify the 'relay_log' system variable at
     server startup without specifying a value, unexpected behavior is
     likely to result; this behavior depends on the other options used,
     the order in which they are specified, and whether they are
     specified on the command line or in an option file.  For more
     information about how MySQL handles server options, see *note
     program-options::.

     If you specify this variable, the value specified is also used as
     the base name for the relay log index file.  You can override this
     behavior by specifying a different relay log index file base name
     using the 'relay_log_index' system variable.

     Starting with MySQL 5.5.20, when the server reads an entry from the
     index file, it checks whether the entry contains a relative path.
     If it does, the relative part of the path in replaced with the
     absolute path set using the 'relay_log' system variable.  An
     absolute path remains unchanged; in such a case, the index must be
     edited manually to enable the new path or paths to be used.  Prior
     to MySQL 5.5.20, manual intervention was required whenever
     relocating the binary log or relay log files.  (Bug #11745230, Bug
     #12133)

     You may find the 'relay_log' system variable useful in performing
     the following tasks:

        * Creating relay logs whose names are independent of host names.

        * If you need to put the relay logs in some area other than the
          data directory because your relay logs tend to be very large
          and you do not want to decrease 'max_relay_log_size'.

        * To increase speed by using load-balancing between disks.

   * 'relay_log_index'

     Property               Value
                            
     *Command-Line          '--relay-log-index=file_name'
     Format*                

     *System Variable*      'relay_log_index'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 File name
                            
     *Default Value*        '*host_name*-relay-bin.index'

     The name for the relay log index file.  The default name is
     'HOST_NAME-relay-bin.index' in the data directory, where HOST_NAME
     is the name of the slave server.

     Due to the manner in which MySQL parses server options, if you
     specify this variable at server startup, you must supply a value;
     _the default base name is used only if the option is not actually
     specified_.  If you specify the 'relay_log_index' system variable
     at server startup without specifying a value, unexpected behavior
     is likely to result; this behavior depends on the other options
     used, the order in which they are specified, and whether they are
     specified on the command line or in an option file.  For more
     information about how MySQL handles server options, see *note
     program-options::.

   * 'relay_log_info_file'

     Property               Value
                            
     *Command-Line          '--relay-log-info-file=file_name'
     Format*                

     *System Variable*      'relay_log_info_file'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 File name
                            
     *Default Value*        'relay-log.info'

     The name of the file in which the slave records information about
     the relay logs.  The default name is 'relay-log.info' in the data
     directory.  For information about the format of this file, see
     *note slave-logs-status::.

   * 'relay_log_recovery'

     Property               Value
                            
     *Command-Line          '--relay-log-recovery[={OFF|ON}]'
     Format*                

     *System Variable*      'relay_log_recovery'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Boolean
                            
     *Default Value*        'OFF'

     If enabled, this variable enables automatic relay log recovery
     immediately following server startup, which means that the
     replication slave discards all unprocessed relay logs and retrieves
     them from the replication master.  This should be used following a
     crash on the replication slave to ensure that no possibly corrupted
     relay logs are processed.  The default value is 0 (disabled).  This
     global variable can be changed dynamically at runtime, or set with
     the '--relay-log-recovery' option at slave startup.

   * 
     'report_host'

     Property               Value
                            
     *Command-Line          '--report-host=host_name'
     Format*                

     *System Variable*      'report_host'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 String

     The host name or IP address of the slave to be reported to the
     master during slave registration.  This value appears in the output
     of *note 'SHOW SLAVE HOSTS': show-slave-hosts. on the master
     server.  Leave the value unset if you do not want the slave to
     register itself with the master.

     *Note*:

     It is not sufficient for the master to simply read the IP address
     of the slave from the TCP/IP socket after the slave connects.  Due
     to NAT and other routing issues, that IP may not be valid for
     connecting to the slave from the master or other hosts.

   * 
     'report_password'

     Property               Value
                            
     *Command-Line          '--report-password=name'
     Format*                

     *System Variable*      'report_password'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 String

     The account password of the slave to be reported to the master
     during slave registration.  This value appears in the output of
     *note 'SHOW SLAVE HOSTS': show-slave-hosts. on the master server if
     the master was started with '--show-slave-auth-info'.

     Although the name of this variable might imply otherwise,
     'report_password' is not connected to the MySQL user privilege
     system and so is not necessarily (or even likely to be) the same as
     the password for the MySQL replication user account.

   * 
     'report_port'

     Property               Value
                            
     *Command-Line          '--report-port=port_num'
     Format*                

     *System Variable*      'report_port'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Integer
                            
     *Default Value* (>=    '0'
     5.5.23)                

     *Default Value* (<=    '3306'
     5.5.22)                

     *Minimum Value*        '0'
                            
     *Maximum Value*        '65535'

     The TCP/IP port number for connecting to the slave, to be reported
     to the master during slave registration.  Set this only if the
     slave is listening on a nondefault port or if you have a special
     tunnel from the master or other clients to the slave.  If you are
     not sure, do not use this option.

     Prior to MySQL 5.5.23, the default value for this option was 3306.
     In MySQL 5.5.23 and higher, the value shown is the port number
     actually used by the slave (Bug #13333431).  This change also
     affects the default value displayed by *note 'SHOW SLAVE HOSTS':
     show-slave-hosts.

   * 
     'report_user'

     Property               Value
                            
     *Command-Line          '--report-user=name'
     Format*                

     *System Variable*      'report_user'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 String

     The account user name of the slave to be reported to the master
     during slave registration.  This value appears in the output of
     *note 'SHOW SLAVE HOSTS': show-slave-hosts. on the master server if
     the master was started with '--show-slave-auth-info'.

     Although the name of this variable might imply otherwise,
     'report_user' is not connected to the MySQL user privilege system
     and so is not necessarily (or even likely to be) the same as the
     name of the MySQL replication user account.

   * 'rpl_recovery_rank'

     This variable is unused, and is removed in MySQL 5.6.

   * 
     'slave_compressed_protocol'

     Property               Value
                            
     *Command-Line          '--slave-compressed-protocol[={OFF|ON}]'
     Format*                

     *System Variable*      'slave_compressed_protocol'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Boolean
                            
     *Default Value*        'OFF'

     Whether to use compression of the master/slave protocol if both
     master and slave support it.  If this variable is disabled (the
     default), connections are uncompressed.  See also *note
     connection-compression-control::.

   * 
     'slave_exec_mode'

     Property               Value
                            
     *Command-Line          '--slave-exec-mode=mode'
     Format*                

     *System Variable*      'slave_exec_mode'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Enumeration
                            
     *Default Value*        'IDEMPOTENT' (NDB) 'STRICT' (Other)
                            
     *Valid Values*         'IDEMPOTENT' 'STRICT'

     Controls how a slave thread resolves conflicts and errors during
     replication.  'IDEMPOTENT' mode causes suppression of duplicate-key
     and no-key-found errors; 'STRICT' means no such suppression takes
     place.

     'IDEMPOTENT' mode is intended for use in multi-master replication,
     circular replication, and some other special replication scenarios
     for NDB Cluster Replication.  (See *note
     mysql-cluster-replication-multi-master::, and *note
     mysql-cluster-replication-conflict-resolution::, for more
     information.)  NDB Cluster ignores any value explicitly set for
     'slave_exec_mode', and always treats it as 'IDEMPOTENT'.

     In MySQL Server 5.5, 'STRICT' mode is the default value.

     For storage engines other than *note 'NDB': mysql-cluster,
     _'IDEMPOTENT' mode should be used only when you are absolutely sure
     that duplicate-key errors and key-not-found errors can safely be
     ignored_.  It is meant to be used in fail-over scenarios for NDB
     Cluster where multi-master replication or circular replication is
     employed, and is not recommended for use in other cases.

   * 'slave_load_tmpdir'

     Property               Value
                            
     *Command-Line          '--slave-load-tmpdir=dir_name'
     Format*                

     *System Variable*      'slave_load_tmpdir'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Directory name
                            
     *Default Value*        'Value of --tmpdir'

     The name of the directory where the slave creates temporary files.
     The variable value is by default equal to the value of the 'tmpdir'
     system variable.

     When the slave SQL thread replicates a *note 'LOAD DATA':
     load-data. statement, it extracts the file to be loaded from the
     relay log into temporary files, and then loads these into the
     table.  If the file loaded on the master is huge, the temporary
     files on the slave are huge, too.  Therefore, it might be advisable
     to use this option to tell the slave to put temporary files in a
     directory located in some file system that has a lot of available
     space.  In that case, the relay logs are huge as well, so you might
     also want to set the 'relay_log' system variable to place the relay
     logs in that file system.

     The directory specified by this option should be located in a
     disk-based file system (not a memory-based file system) so that the
     temporary files used to replicate *note 'LOAD DATA': load-data.
     statements can survive machine restarts.  The directory also should
     not be one that is cleared by the operating system during the
     system startup process.

   * 'slave_max_allowed_packet'

     Property               Value
                            
     *Command-Line          '--slave-max-allowed-packet=#'
     Format*                

     *Introduced*           5.5.26
                            
     *System Variable*      'slave_max_allowed_packet'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '1073741824'
                            
     *Minimum Value*        '1024'
                            
     *Maximum Value*        '1073741824'

     In MySQL 5.5.26 and higher, this variable sets the maximum packet
     size for the slave SQL and I/O threads, so that large updates using
     row-based replication do not cause replication to fail because an
     update exceeded 'max_allowed_packet'.

     This global variable always has a value that is a positive integer
     multiple of 1024; if you set it to some value that is not, the
     value is rounded down to the next highest multiple of 1024 for it
     is stored or used; setting 'slave_max_allowed_packet' to 0 causes
     1024 to be used.  (A truncation warning is issued in all such
     cases.)  The default and maximum value is 1073741824 (1 GB); the
     minimum is 1024.

   * 'slave_net_timeout'

     Property               Value
                            
     *Command-Line          '--slave-net-timeout=#'
     Format*                

     *System Variable*      'slave_net_timeout'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '3600'
                            
     *Minimum Value*        '1'

     The number of seconds to wait for more data from the master before
     the slave considers the connection broken, aborts the read, and
     tries to reconnect.  The first retry occurs immediately after the
     timeout.  The interval between retries is controlled by the
     'MASTER_CONNECT_RETRY' option for the *note 'CHANGE MASTER TO':
     change-master-to. statement, and the number of reconnection
     attempts is limited by the '--master-retry-count' option.  The
     default is 3600 seconds (one hour).

   * 
     'slave_skip_errors'

     Property               Value
                            
     *Command-Line          '--slave-skip-errors=name'
     Format*                

     *System Variable*      'slave_skip_errors'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 String
                            
     *Default Value*        'OFF'
                            
     *Valid Values*         'OFF' '[list of error codes]' 'all'
                            'ddl_exist_errors'

     Normally, replication stops when an error occurs on the slave.
     This gives you the opportunity to resolve the inconsistency in the
     data manually.  This variable tells the slave SQL thread to
     continue replication when a statement returns any of the errors
     listed in the variable value.

   * 
     'slave_transaction_retries'

     Property               Value
                            
     *Command-Line          '--slave-transaction-retries=#'
     Format*                

     *System Variable*      'slave_transaction_retries'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '10'
                            
     *Minimum Value*        '0'
                            
     *Maximum Value*        '18446744073709551615'
     (64-bit platforms,     
     >= 5.5.3)

     *Maximum Value*        '18446744073709547520'
     (64-bit platforms,     
     <= 5.5.2)

     *Maximum Value*        '4294967295'
     (32-bit platforms)

     If a replication slave SQL thread fails to execute a transaction
     because of an *note 'InnoDB': innodb-storage-engine. deadlock or
     because the transaction's execution time exceeded *note 'InnoDB':
     innodb-storage-engine.'s 'innodb_lock_wait_timeout' or *note
     'NDBCLUSTER': mysql-cluster.'s
     'TransactionDeadlockDetectionTimeout' or
     'TransactionInactiveTimeout', it automatically retries
     'slave_transaction_retries' times before stopping with an error.
     The default value is 10.

   * 
     'slave_type_conversions'

     Property               Value
                            
     *Command-Line          '--slave-type-conversions=set'
     Format*                

     *Introduced*           5.5.3
                            
     *System Variable*      'slave_type_conversions'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Set
                            
     *Default Value*        ''
                            
     *Valid Values*         'ALL_LOSSY' 'ALL_NON_LOSSY'

     Controls the type conversion mode in effect on the slave when using
     row-based replication, including NDB Cluster Replication.  Its
     value is a comma-delimited set of zero or more elements from the
     list: 'ALL_LOSSY', 'ALL_NON_LOSSY'.  Set this variable to an empty
     string to disallow type conversions between the master and the
     slave.  Changes require a restart of the slave to take effect.

     For additional information on type conversion modes applicable to
     attribute promotion and demotion in row-based replication, see
     *note replication-features-attribute-promotion::.

   * 
     'sql_slave_skip_counter'

     Property               Value
                            
     *System Variable*      'sql_slave_skip_counter'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer

     The number of events from the master that a slave server should
     skip.

     *Important*:

     If skipping the number of events specified by setting this variable
     would cause the slave to begin in the middle of an event group, the
     slave continues to skip until it finds the beginning of the next
     event group and begins from that point.  For more information, see
     *note set-global-sql-slave-skip-counter::.

   * 
     'sync_master_info'

     Property               Value
                            
     *Command-Line          '--sync-master-info=#'
     Format*                

     *System Variable*      'sync_master_info'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '0'
                            
     *Minimum Value*        '0'
                            
     *Maximum Value*        '18446744073709551615'
     (64-bit platforms,     
     >= 5.5.3)

     *Maximum Value*        '18446744073709547520'
     (64-bit platforms,     
     <= 5.5.2)

     *Maximum Value*        '4294967295'
     (32-bit platforms)

     If the value of this variable is greater than 0, a replication
     slave synchronizes its 'master.info' file to disk (using
     'fdatasync()') after every 'sync_master_info' events.  The default
     value is 0 (recommended in most situations), which does not force
     any synchronization to disk by the MySQL server; in this case, the
     server relies on the operating system to flush the 'master.info'
     file's contents from time to time as for any other file.

   * 
     'sync_relay_log'

     Property               Value
                            
     *Command-Line          '--sync-relay-log=#'
     Format*                

     *System Variable*      'sync_relay_log'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '0'
                            
     *Minimum Value*        '0'
                            
     *Maximum Value*        '18446744073709551615'
     (64-bit platforms,     
     >= 5.5.3)

     *Maximum Value*        '18446744073709547520'
     (64-bit platforms,     
     <= 5.5.2)

     *Maximum Value*        '4294967295'
     (32-bit platforms)

     If the value of this variable is greater than 0, the MySQL server
     synchronizes its relay log to disk (using 'fdatasync()') after
     every 'sync_relay_log' events are written to the relay log.

     The default value of 'sync_relay_log' is 0, which does no
     synchronizing to disk; in this case, the server relies on the
     operating system to flush the relay log's contents from time to
     time as for any other file.

     A value of 1 is the safest choice because in the event of a crash
     you lose at most one event from the relay log.  However, it is also
     the slowest choice (unless the disk has a battery-backed cache,
     which makes synchronization very fast).

   * 
     'sync_relay_log_info'

     Property               Value
                            
     *Command-Line          '--sync-relay-log-info=#'
     Format*                

     *System Variable*      'sync_relay_log_info'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '0'
                            
     *Minimum Value*        '0'
                            
     *Maximum Value*        '18446744073709551615'
     (64-bit platforms,     
     >= 5.5.3)

     *Maximum Value*        '18446744073709547520'
     (64-bit platforms,     
     <= 5.5.2)

     *Maximum Value*        '4294967295'
     (32-bit platforms)

     If the value of this variable is greater than 0, a replication
     slave synchronizes its 'relay-log.info' file to disk (using
     'fdatasync()') after every 'sync_relay_log_info' transactions.  A
     value of 1 is the generally the best choice.  The default value of
     'sync_relay_log_info' is 0, which does not force any
     synchronization to disk by the MySQL server--in this case, the
     server relies on the operating system to flush the 'relay-log.info'
     file's contents from time to time as for any other file.


File: manual.info.tmp,  Node: replication-options-binary-log,  Prev: replication-options-slave,  Up: replication-options

17.1.3.4 Binary Log Options and Variables
.........................................

   * *note replication-optvars-binlog::

   * *note replication-sysvars-binlog::

You can use the *note 'mysqld': mysqld. options and system variables
that are described in this section to affect the operation of the binary
log as well as to control which statements are written to the binary
log.  For additional information about the binary log, see *note
binary-log::.  For additional information about using MySQL server
options and system variables, see *note server-options::, and *note
server-system-variables::.

*Startup Options Used with Binary Logging*

The following list describes startup options for enabling and
configuring the binary log.  System variables used with binary logging
are discussed later in this section.

   * 
     '--binlog-row-event-max-size=N'

     Property               Value
                            
     *Command-Line          '--binlog-row-event-max-size=#'
     Format*                

     *Type*                 Integer
                            
     *Default Value*        '1024'
                            
     *Minimum Value*        '256'
                            
     *Maximum Value*        '18446744073709551615'
     (64-bit platforms,     
     >= 5.5.3)

     *Maximum Value*        '18446744073709547520'
     (64-bit platforms,     
     <= 5.5.2)

     *Maximum Value*        '4294967295'
     (32-bit platforms)

     Specify the maximum size of a row-based binary log event, in bytes.
     Rows are grouped into events smaller than this size if possible.
     The value should be a multiple of 256.  The default is 1024.  See
     *note replication-formats::.

   * 
     '--log-bin[=BASE_NAME]'

     Property               Value
                            
     *Command-Line          '--log-bin=file_name'
     Format*                

     *Type*                 File name

     Enable binary logging.  The server logs all statements that change
     data to the binary log, which is used for backup and replication.
     See *note binary-log::.

     The option value, if given, is the base name for the log sequence.
     The server creates binary log files in sequence by adding a numeric
     suffix to the base name.  It is recommended that you specify a base
     name (see *note known-issues::, for the reason).  Otherwise, MySQL
     uses 'HOST_NAME-bin' as the base name.

     In MySQL 5.5.20 and higher, when the server reads an entry from the
     index file, it checks whether the entry contains a relative path,
     and if it does, the relative part of the path is replaced with the
     absolute path set using the '--log-bin' option.  An absolute path
     remains unchanged; in such a case, the index must be edited
     manually to enable the new path or paths to be used.  Previous to
     MySQL 5.5.20, manual intervention was required whenever relocating
     the binary log or relay log files.  (Bug #11745230, Bug #12133)

     Setting this option causes the 'log_bin' system variable to be set
     to 'ON' (or '1'), and not to the base name.  This is a known issue;
     see Bug #19614 for more information.

   * 
     '--log-bin-index[=FILE_NAME]'

     Property               Value
                            
     *Command-Line          '--log-bin-index=file_name'
     Format*                

     *Type*                 File name

     The name for the binary log index file, which contains the names of
     the binary log files.  By default, it has the same location and
     base name as the value specified for the binary log files using the
     '--log-bin' option, plus the extension '.index'.  If you do not
     specify '--log-bin', the default binary log index file name is
     'binlog.index'.  If you omit the file name and do not specify one
     with '--log-bin', the default binary log index file name is
     'HOST_NAME-bin.index', using the name of the host machine.

     For information on the format and management of the binary log, see
     *note binary-log::.

Statement selection options

The options in the following list affect which statements are written to
the binary log, and thus sent by a replication master server to its
slaves.  There are also options for slave servers that control which
statements received from the master should be executed or ignored.  For
details, see *note replication-options-slave::.

   * 
     '--binlog-do-db=DB_NAME'

     Property               Value
                            
     *Command-Line          '--binlog-do-db=name'
     Format*                

     *Type*                 String

     This option affects binary logging in a manner similar to the way
     that '--replicate-do-db' affects replication.

     The effects of this option depend on whether the statement-based or
     row-based logging format is in use, in the same way that the
     effects of '--replicate-do-db' depend on whether statement-based or
     row-based replication is in use.  You should keep in mind that the
     format used to log a given statement may not necessarily be the
     same as that indicated by the value of 'binlog_format'.  For
     example, DDL statements such as *note 'CREATE TABLE': create-table.
     and *note 'ALTER TABLE': alter-table. are always logged as
     statements, without regard to the logging format in effect, so the
     following statement-based rules for '--binlog-do-db' always apply
     in determining whether or not the statement is logged.

     Statement-based logging

     Only those statements are written to the binary log where the
     default database (that is, the one selected by *note 'USE': use.)
     is DB_NAME.  To specify more than one database, use this option
     multiple times, once for each database; however, doing so does
     _not_ cause cross-database statements such as 'UPDATE
     SOME_DB.SOME_TABLE SET foo='bar'' to be logged while a different
     database (or no database) is selected.

     *Warning*:

     To specify multiple databases you _must_ use multiple instances of
     this option.  Because database names can contain commas, the list
     will be treated as the name of a single database if you supply a
     comma-separated list.

     An example of what does not work as you might expect when using
     statement-based logging: If the server is started with
     '--binlog-do-db=sales' and you issue the following statements, the
     *note 'UPDATE': update. statement is _not_ logged:

          USE prices;
          UPDATE sales.january SET amount=amount+1000;

     The main reason for this 'just check the default database' behavior
     is that it is difficult from the statement alone to know whether it
     should be replicated (for example, if you are using multiple-table
     *note 'DELETE': delete. statements or multiple-table *note
     'UPDATE': update. statements that act across multiple databases).
     It is also faster to check only the default database rather than
     all databases if there is no need.

     Another case which may not be self-evident occurs when a given
     database is replicated even though it was not specified when
     setting the option.  If the server is started with
     '--binlog-do-db=sales', the following *note 'UPDATE': update.
     statement is logged even though 'prices' was not included when
     setting '--binlog-do-db':


          USE sales;
          UPDATE prices.discounts SET percentage = percentage + 10;

     Because 'sales' is the default database when the *note 'UPDATE':
     update. statement is issued, the *note 'UPDATE': update. is logged.

     Row-based logging

     Logging is restricted to database DB_NAME.  Only changes to tables
     belonging to DB_NAME are logged; the default database has no effect
     on this.  Suppose that the server is started with
     '--binlog-do-db=sales' and row-based logging is in effect, and then
     the following statements are executed:

          USE prices;
          UPDATE sales.february SET amount=amount+100;

     The changes to the 'february' table in the 'sales' database are
     logged in accordance with the *note 'UPDATE': update. statement;
     this occurs whether or not the *note 'USE': use. statement was
     issued.  However, when using the row-based logging format and
     '--binlog-do-db=sales', changes made by the following *note
     'UPDATE': update. are not logged:

          USE prices;
          UPDATE prices.march SET amount=amount-25;

     Even if the 'USE prices' statement were changed to 'USE sales', the
     *note 'UPDATE': update. statement's effects would still not be
     written to the binary log.

     Another important difference in '--binlog-do-db' handling for
     statement-based logging as opposed to the row-based logging occurs
     with regard to statements that refer to multiple databases.
     Suppose that the server is started with '--binlog-do-db=db1', and
     the following statements are executed:

          USE db1;
          UPDATE db1.table1 SET col1 = 10, db2.table2 SET col2 = 20;

     If you are using statement-based logging, the updates to both
     tables are written to the binary log.  However, when using the
     row-based format, only the changes to 'table1' are logged; 'table2'
     is in a different database, so it is not changed by the *note
     'UPDATE': update.  Now suppose that, instead of the 'USE db1'
     statement, a 'USE db4' statement had been used:

          USE db4;
          UPDATE db1.table1 SET col1 = 10, db2.table2 SET col2 = 20;

     In this case, the *note 'UPDATE': update. statement is not written
     to the binary log when using statement-based logging.  However,
     when using row-based logging, the change to 'table1' is logged, but
     not that to 'table2'--in other words, only changes to tables in the
     database named by '--binlog-do-db' are logged, and the choice of
     default database has no effect on this behavior.

   * 
     '--binlog-ignore-db=DB_NAME'

     Property               Value
                            
     *Command-Line          '--binlog-ignore-db=name'
     Format*                

     *Type*                 String

     This option affects binary logging in a manner similar to the way
     that '--replicate-ignore-db' affects replication.

     The effects of this option depend on whether the statement-based or
     row-based logging format is in use, in the same way that the
     effects of '--replicate-ignore-db' depend on whether
     statement-based or row-based replication is in use.  You should
     keep in mind that the format used to log a given statement may not
     necessarily be the same as that indicated by the value of
     'binlog_format'.  For example, DDL statements such as *note 'CREATE
     TABLE': create-table. and *note 'ALTER TABLE': alter-table. are
     always logged as statements, without regard to the logging format
     in effect, so the following statement-based rules for
     '--binlog-ignore-db' always apply in determining whether or not the
     statement is logged.

     Statement-based logging

     Tells the server to not log any statement where the default
     database (that is, the one selected by *note 'USE': use.) is
     DB_NAME.

     Prior to MySQL 5.5.32, this option caused any statements containing
     fully qualified table names not to be logged if there was no
     default database specified (that is, when *note 'SELECT': select.
     'DATABASE()' returned 'NULL').  In MySQL 5.5.32 and higher, when
     there is no default database, no '--binlog-ignore-db' options are
     applied, and such statements are always logged.  (Bug #11829838,
     Bug #60188)

     Row-based format

     Tells the server not to log updates to any tables in the database
     DB_NAME.  The current database has no effect.

     When using statement-based logging, the following example does not
     work as you might expect.  Suppose that the server is started with
     '--binlog-ignore-db=sales' and you issue the following statements:

          USE prices;
          UPDATE sales.january SET amount=amount+1000;

     The *note 'UPDATE': update. statement _is_ logged in such a case
     because '--binlog-ignore-db' applies only to the default database
     (determined by the *note 'USE': use. statement).  Because the
     'sales' database was specified explicitly in the statement, the
     statement has not been filtered.  However, when using row-based
     logging, the *note 'UPDATE': update. statement's effects are _not_
     written to the binary log, which means that no changes to the
     'sales.january' table are logged; in this instance,
     '--binlog-ignore-db=sales' causes _all_ changes made to tables in
     the master's copy of the 'sales' database to be ignored for
     purposes of binary logging.

     To specify more than one database to ignore, use this option
     multiple times, once for each database.  Because database names can
     contain commas, the list will be treated as the name of a single
     database if you supply a comma-separated list.

     You should not use this option if you are using cross-database
     updates and you do not want these updates to be logged.

Testing and debugging options

The following binary log options are used in replication testing and
debugging.  They are not intended for use in normal operations.

   * 
     '--max-binlog-dump-events=N'

     Property               Value
                            
     *Command-Line          '--max-binlog-dump-events=#'
     Format*                

     *Type*                 Integer
                            
     *Default Value*        '0'

     This option is used internally by the MySQL test suite for
     replication testing and debugging.

   * 
     '--sporadic-binlog-dump-fail'

     Property               Value
                            
     *Command-Line          '--sporadic-binlog-dump-fail[={OFF|ON}]'
     Format*                

     *Type*                 Boolean
                            
     *Default Value*        'OFF'

     This option is used internally by the MySQL test suite for
     replication testing and debugging.

*System Variables Used with Binary Logging*

The following list describes system variables for controlling binary
logging.  They can be set at server startup and some of them can be
changed at runtime using *note 'SET': set-variable.  Server options used
to control binary logging are listed earlier in this section.  For
information about the 'sql_log_bin' and 'sql_log_off' variables, see
*note server-system-variables::.

   * 
     'binlog_cache_size'

     Property               Value
                            
     *Command-Line          '--binlog-cache-size=#'
     Format*                

     *System Variable*      'binlog_cache_size'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '32768'
                            
     *Minimum Value*        '4096'
                            
     *Maximum Value*        '18446744073709551615'
     (64-bit platforms,     
     >= 5.5.3)

     *Maximum Value*        '18446744073709547520'
     (64-bit platforms,     
     <= 5.5.2)

     *Maximum Value*        '4294967295'
     (32-bit platforms)

     The size of the cache to hold changes to the binary log during a
     transaction.  A binary log cache is allocated for each client if
     the server supports any transactional storage engines and if the
     server has the binary log enabled ('--log-bin' option).  If you
     often use large transactions, you can increase this cache size to
     get better performance.  The 'Binlog_cache_use' and
     'Binlog_cache_disk_use' status variables can be useful for tuning
     the size of this variable.  See *note binary-log::.

     In MySQL 5.5.3, a separate binary log cache (the binary log
     statement cache) was introduced for nontransactional statements and
     in MySQL 5.5.3 through 5.5.8, this variable sets.the size for both
     caches.  This means that, in these MySQL versions, the total memory
     used for these caches is double the value set for
     'binlog_cache_size'.

     Beginning with MySQL 5.5.9, 'binlog_cache_size' sets the size for
     the transaction cache only, and the size of the statement cache is
     governed by the 'binlog_stmt_cache_size' system variable.

   * 
     'binlog_direct_non_transactional_updates'

     Property               Value
                            
     *Command-Line          '--binlog-direct-non-transactional-updates[={OFF|ON}]'
     Format*                

     *Introduced*           5.5.2
                            
     *System Variable*      'binlog_direct_non_transactional_updates'
                            
     *Scope*                Global, Session
                            
     *Dynamic*              Yes
                            
     *Type*                 Boolean
                            
     *Default Value*        'OFF'

     Due to concurrency issues, a slave can become inconsistent when a
     transaction contains updates to both transactional and
     nontransactional tables.  MySQL tries to preserve causality among
     these statements by writing nontransactional statements to the
     transaction cache, which is flushed upon commit.  However, problems
     arise when modifications done to nontransactional tables on behalf
     of a transaction become immediately visible to other connections
     because these changes may not be written immediately into the
     binary log.

     Beginning with MySQL 5.5.2, the
     'binlog_direct_non_transactional_updates' variable offers one
     possible workaround to this issue.  By default, this variable is
     disabled.  Enabling 'binlog_direct_non_transactional_updates'
     causes updates to nontransactional tables to be written directly to
     the binary log, rather than to the transaction cache.

     _'binlog_direct_non_transactional_updates' works only for
     statements that are replicated using the statement-based binary
     logging format_; that is, it works only when the value of
     'binlog_format' is 'STATEMENT', or when 'binlog_format' is 'MIXED'
     and a given statement is being replicated using the statement-based
     format.  This variable has no effect when the binary log format is
     'ROW', or when 'binlog_format' is set to 'MIXED' and a given
     statement is replicated using the row-based format.

     *Important*:

     Before enabling this variable, you must make certain that there are
     no dependencies between transactional and nontransactional tables;
     an example of such a dependency would be the statement 'INSERT INTO
     MYISAM_TABLE SELECT * FROM INNODB_TABLE'.  Otherwise, such
     statements are likely to cause the slave to diverge from the
     master.

     Beginning with MySQL 5.5.5, this variable has no effect when the
     binary log format is 'ROW' or 'MIXED'.  (Bug #51291)

   * 
     'binlog_format'

     Property               Value
                            
     *Command-Line          '--binlog-format=format'
     Format*                

     *System Variable*      'binlog_format'
                            
     *Scope*                Global, Session
                            
     *Dynamic*              Yes
                            
     *Type*                 Enumeration
                            
     *Default Value* (>=    'MIXED'
     5.5.31-ndb-7.2.13)     

     *Default Value* (>=    'STATEMENT'
     5.5.15-ndb-7.2.1, <=   
     5.5.30-ndb-7.2.12)

     *Default Value*        'STATEMENT'
                            
     *Valid Values*         'ROW' 'STATEMENT' 'MIXED'

     This variable sets the binary logging format, and can be any one of
     'STATEMENT', 'ROW', or 'MIXED'.  See *note replication-formats::.
     Setting the binary logging format without enabling binary logging
     sets the 'binlog_format' global system variable and logs a warning.

     'binlog_format' can be set at startup or at runtime, except that
     under some conditions, changing this variable at runtime is not
     possible or causes replication to fail.  For more information, see
     *note binary-log-setting::.

     *Note*:

     While you can change the logging format at runtime, it is _not_
     recommended that you change it while replication is ongoing.  This
     is due in part to the fact that slaves do not honor the master's
     'binlog_format' setting; a given MySQL Server can change only its
     own logging format.

     In MySQL 5.5, the default format is 'STATEMENT'.  This is also true
     for MySQL NDB Cluster 7.2.1 and higher.  See *note
     replication-formats::.

     Exception

     In MySQL NDB Cluster 7.2.1 through MySQL NDB Cluster 7.2.12, the
     default for this variable is 'STATEMENT'.  In MySQL NDB Cluster
     7.2.13 and higher, when the *note 'NDB': mysql-cluster. storage
     engine is enabled, the default is 'MIXED'.  (Bug #16417224) See
     also *note mysql-cluster-replication-starting::, and *note
     mysql-cluster-replication-two-channels::.

     Setting the session value of this system variable is a restricted
     operation.  The session user must have privileges sufficient to set
     restricted session variables.  See *note
     system-variable-privileges::.

     The rules governing when changes to this variable take effect and
     how long the effect lasts are the same as for other MySQL server
     system variables.  For more information, see *note set-variable::.

     When 'MIXED' is specified, statement-based replication is used,
     except for cases where only row-based replication is guaranteed to
     lead to proper results.  For example, this happens when statements
     contain user-defined functions (UDF) or the 'UUID()' function.

     For details of how stored programs (stored procedures and
     functions, triggers, and events) are handled when each binary
     logging format is set, see *note stored-programs-logging::.

     There are exceptions when you cannot switch the replication format
     at runtime:

        * From within a stored function or a trigger.

        * If the session is currently in row-based replication mode and
          has open temporary tables.

        * Beginning with MySQL 5.5.3, within a transaction.  (Bug
          #47863)

     Trying to switch the format in those cases results in an error.

     *Note*:

     Prior to MySQL NDB Cluster 7.2.1, it was also not possible to
     change the binary logging format at runtime when the *note
     'NDBCLUSTER': mysql-cluster. storage engine was enabled.  In MySQL
     NDB Cluster 7.2.1 and higher, this restriction is removed.

     The binary log format affects the behavior of the following server
     options:

        * '--replicate-do-db'

        * '--replicate-ignore-db'

        * '--binlog-do-db'

        * '--binlog-ignore-db'

     These effects are discussed in detail in the descriptions of the
     individual options.

   * 
     'binlog_stmt_cache_size'

     Property               Value
                            
     *Command-Line          '--binlog-stmt-cache-size=#'
     Format*                

     *Introduced*           5.5.9
                            
     *System Variable*      'binlog_stmt_cache_size'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '32768'
                            
     *Minimum Value*        '4096'
                            
     *Maximum Value*        '18446744073709551615'
     (64-bit platforms)     

     *Maximum Value*        '4294967295'
     (32-bit platforms)

     Beginning with MySQL 5.5.9, this variable determines the size of
     the cache for the binary log to hold nontransactional statements
     issued during a transaction.  In MySQL 5.5.3 and higher, separate
     binary log transaction and statement caches are allocated for each
     client if the server supports any transactional storage engines and
     if the server has the binary log enabled ('--log-bin' option).  If
     you often use large nontransactional statements during
     transactions, you can increase this cache size to get more
     performance.  The 'Binlog_stmt_cache_use' and
     'Binlog_stmt_cache_disk_use' status variables can be useful for
     tuning the size of this variable.  See *note binary-log::.

     In MySQL 5.5.3 through 5.5.8, the size for both caches is set using
     'binlog_cache_size'.  This means that, in these MySQL versions, the
     total memory used for these caches is double the value set for
     'binlog_cache_size'.  Beginning with MySQL 5.5.9,
     'binlog_cache_size' sets the size for the transaction cache only.

   * 
     'log_bin'

     Property               Value
                            
     *System Variable*      'log_bin'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Boolean

     Whether the binary log is enabled.  If the '--log-bin' option is
     used, then the value of this variable is 'ON'; otherwise it is
     'OFF'.  This variable reports only on the status of binary logging
     (enabled or disabled); it does not actually report the value to
     which '--log-bin' is set.

     See *note binary-log::.

   * 
     'log_bin_trust_function_creators'

     Property               Value
                            
     *Command-Line          '--log-bin-trust-function-creators[={OFF|ON}]'
     Format*                

     *System Variable*      'log_bin_trust_function_creators'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Boolean
                            
     *Default Value*        'OFF'

     This variable applies when binary logging is enabled.  It controls
     whether stored function creators can be trusted not to create
     stored functions that will cause unsafe events to be written to the
     binary log.  If set to 0 (the default), users are not permitted to
     create or alter stored functions unless they have the 'SUPER'
     privilege in addition to the 'CREATE ROUTINE' or 'ALTER ROUTINE'
     privilege.  A setting of 0 also enforces the restriction that a
     function must be declared with the 'DETERMINISTIC' characteristic,
     or with the 'READS SQL DATA' or 'NO SQL' characteristic.  If the
     variable is set to 1, MySQL does not enforce these restrictions on
     stored function creation.  This variable also applies to trigger
     creation.  See *note stored-programs-logging::.

   * 
     'log_bin_trust_routine_creators'

     This is the old name for 'log_bin_trust_function_creators'.  This
     variable is deprecated and was removed in MySQL 5.5.3.

   * 
     'log_bin_use_v1_row_events'

     Property               Value
                            
     *Command-Line          '--log-bin-use-v1-row-events[={OFF|ON}]'
     Format*                

     *Introduced*           5.5.15-ndb-7.2.1
                            
     *System Variable*      'log_bin_use_v1_row_events'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Boolean
                            
     *Default Value*        'OFF'

     Whether Version 2 binary logging is in use.  If this variable is 0
     (disabled, the default), Version 2 binary log events are in use.
     If this variable is 1 (enabled), the server writes the binary log
     using Version 1 logging events (the only version of binary log
     events used in previous releases), and thus produces a binary log
     that can be read by older slaves.

     Version 2 binary log row events are used by default beginning with
     MySQL NDB Cluster 7.2.1; however, Version 2 events cannot be read
     by previous NDB Cluster releases.  Enabling
     'log_bin_use_v1_row_events' causes *note 'mysqld': mysqld. to write
     the binary log using Version 1 logging events.

     This variable is read-only at runtime.  To switch between Version 1
     and Version 2 binary event binary logging, it is necessary to set
     'log_bin_use_v1_row_events' at server startup.

     Other than when performing upgrades of NDB Cluster Replication,
     'log_bin_use_v1_row_events' is chiefly of interest when setting up
     replication conflict detection and resolution using
     'NDB$EPOCH_TRANS()' as the conflict detection function, which
     requires Version 2 binary log row events.  Thus, this variable and
     '--ndb-log-transaction-id' are not compatible.

     *Note*:

     Version 2 binary log row events are also available in MySQL NDB
     Cluster 7.0.27 and MySQL NDB Cluster 7.1.6 and later MySQL NDB
     Cluster 7.0 and 7.1 releases.  However, prior to MySQL NDB Cluster
     7.2.1, Version 1 events are the default (and so the default value
     for this option is 1 in those versions).  You should keep this mind
     when planning upgrades for setups using NDB Cluster Replication.

     This variable is not supported in mainline MySQL Server 5.5.

     For more information, see *note
     mysql-cluster-replication-conflict-resolution::.

   * 'log_slave_updates'

     Property               Value
                            
     *Command-Line          '--log-slave-updates[={OFF|ON}]'
     Format*                

     *System Variable*      'log_slave_updates'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Boolean
                            
     *Default Value*        'OFF'

     Whether updates received by a slave server from a master server
     should be logged to the slave's own binary log.

     Normally, a slave does not log to its own binary log any updates
     that are received from a master server.  Enabling this variable
     tells the slave to log the updates performed by its SQL thread to
     its own binary log.  For this variable to have any effect, the
     slave must also be started with the '--log-bin' option to enable
     binary logging.  See *note replication-options::.  Prior to MySQL
     5.5, the server would not start when enabling 'log_slave_updates'
     without also starting the server with the '--log-bin' option, and
     would fail with an error; in MySQL 5.5, only a warning is
     generated.  (Bug #44663)

     'log_slave_updates' is enabled when you want to chain replication
     servers.  For example, you might want to set up replication servers
     using this arrangement:

          A -> B -> C

     Here, 'A' serves as the master for the slave 'B', and 'B' serves as
     the master for the slave 'C'.  For this to work, 'B' must be both a
     master _and_ a slave.  You must start both 'A' and 'B' with
     '--log-bin' to enable binary logging, and 'B' with
     'log_slave_updates' enabled so that updates received from 'A' are
     logged by 'B' to its binary log.

   * 
     'max_binlog_cache_size'

     Property               Value
                            
     *Command-Line          '--max-binlog-cache-size=#'
     Format*                

     *System Variable*      'max_binlog_cache_size'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value* (>=    '18446744073709551615'
     5.5.3)                 

     *Default Value* (<=    '18446744073709547520'
     5.5.2)                 

     *Minimum Value*        '4096'
                            
     *Maximum Value* (>=    '18446744073709551615'
     5.5.3)                 

     *Maximum Value* (<=    '18446744073709547520'
     5.5.2)

     If a transaction requires more than this many bytes of memory, the
     server generates a 'Multi-statement transaction required more than
     'max_binlog_cache_size' bytes of storage' error.  The minimum value
     is 4096.  The maximum possible value is 16EB (exabytes).  The
     maximum recommended value is 4GB; this is due to the fact that
     MySQL currently cannot work with binary log positions greater than
     4GB.

     *Note*:

     Prior to MySQL 5.5.28, 64-bit Windows platforms truncated the
     stored value for this variable to 4G, even when it was set to a
     greater value (Bug #13961678).

     In MySQL 5.5.3, a separate binary log cache (the binary log
     statement cache) was introduced for nontransactional statements and
     in MySQL 5.5.3 through 5.5.8, this variable sets.the upper limit
     for both caches.  This means that, in these MySQL versions, the
     effective maximum for these caches is double the value set for
     'max_binlog_cache_size'.

     Beginning with MySQL 5.5.9, 'max_binlog_cache_size' sets the size
     for the transaction cache only, and the upper limit for the
     statement cache is governed by the 'max_binlog_stmt_cache_size'
     system variable.

     Also beginning with MySQL 5.5.9, the session visibility of the
     'max_binlog_cache_size' system variable matches that of the
     'binlog_cache_size' system variable: In MySQL 5.5.8 and earlier
     releases, a change in 'max_binlog_cache_size' took immediate
     effect; in MySQL 5.5.9 and higher, a change in
     'max_binlog_cache_size' takes effect only for new sessions that
     started after the value is changed.

   * 
     'max_binlog_size'

     Property               Value
                            
     *Command-Line          '--max-binlog-size=#'
     Format*                

     *System Variable*      'max_binlog_size'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '1073741824'
                            
     *Minimum Value*        '4096'
                            
     *Maximum Value*        '1073741824'

     If a write to the binary log causes the current log file size to
     exceed the value of this variable, the server rotates the binary
     logs (closes the current file and opens the next one).  The minimum
     value is 4096 bytes.  The maximum and default value is 1GB.

     A transaction is written in one chunk to the binary log, so it is
     never split between several binary logs.  Therefore, if you have
     big transactions, you might see binary log files larger than
     'max_binlog_size'.

     If 'max_relay_log_size' is 0, the value of 'max_binlog_size'
     applies to relay logs as well.

   * 
     'max_binlog_stmt_cache_size'

     Property               Value
                            
     *Command-Line          '--max-binlog-stmt-cache-size=#'
     Format*                

     *Introduced*           5.5.9
                            
     *System Variable*      'max_binlog_stmt_cache_size'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '18446744073709547520'
                            
     *Minimum Value*        '4096'
                            
     *Maximum Value*        '18446744073709547520'

     If nontransactional statements within a transaction require more
     than this many bytes of memory, the server generates an error.  The
     minimum value is 4096.  The maximum and default values are 4GB on
     32-bit platforms and 16EB (exabytes) on 64-bit platforms.

     *Note*:

     Prior to MySQL 5.5.28, 64-bit Windows platforms truncated the
     stored value for this variable to 4G, even when it was set to a
     greater value (Bug #13961678).

     In MySQL 5.5.3, a separate binary log cache (the binary log
     statement cache) was introduced for nontransactional statements and
     in MySQL 5.5.3 through 5.5.8, this variable sets.the upper limit
     for both caches.  This means that, in these MySQL versions, the
     effective maximum for these caches is double the value set for
     'max_binlog_cache_size'.

     Beginning with MySQL 5.5.9, 'max_binlog_stmt_cache_size' sets the
     size for the statement cache only, and the upper limit for the
     transaction cache is governed exclusively by the
     'max_binlog_cache_size' system variable.

   * 
     'sync_binlog'

     Property               Value
                            
     *Command-Line          '--sync-binlog=#'
     Format*                

     *System Variable*      'sync_binlog'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '0'
                            
     *Minimum Value*        '0'
                            
     *Maximum Value*        '4294967295'

     Controls how often the MySQL server synchronizes the binary log to
     disk.

        * 'sync_binlog=0': Disables synchronization of the binary log to
          disk by the MySQL server.  Instead, the MySQL server relies on
          the operating system to flush the binary log to disk from time
          to time as it does for any other file.  This setting provides
          the best performance, but in the event of a power failure or
          operating system crash, it is possible that the server has
          committed transactions that have not been synchronized to the
          binary log.

        * 'sync_binlog=1': Enables synchronization of the binary log to
          disk before transactions are committed.  This is the safest
          setting but can have a negative impact on performance due to
          the increased number of disk writes.  In the event of a power
          failure or operating system crash, you lose at most one
          statement or transaction from the binary log.

        * 'sync_binlog=N', where N is a value other than 0 or 1: The
          binary log is synchronized to disk after every 'N' writes to
          the binary log.  There is one write to the binary log per
          statement if autocommit is enabled, and one write per
          transaction otherwise.  In the event of a power failure or
          operating system crash, it is possible that the server has
          committed transactions that have not been flushed to the
          binary log.  This setting can have a negative impact on
          performance due to the increased number of disk writes.  A
          higher value improves performance, but with an increased risk
          of data loss.

     For the greatest possible durability and consistency in a
     replication setup that uses 'InnoDB' with transactions, use these
     settings:

        * 'sync_binlog=1'.

        * 'innodb_flush_log_at_trx_commit=1'.

     *Caution*:

     Many operating systems and some disk hardware fool the
     flush-to-disk operation.  They may tell *note 'mysqld': mysqld.
     that the flush has taken place, even though it has not.  In this
     case, the durability of transactions is not guaranteed even with
     the recommended settings, and in the worst case, a power outage can
     corrupt 'InnoDB' data.  Using a battery-backed disk cache in the
     SCSI disk controller or in the disk itself speeds up file flushes,
     and makes the operation safer.  You can also try to disable the
     caching of disk writes in hardware caches.


File: manual.info.tmp,  Node: replication-administration,  Prev: replication-options,  Up: replication-configuration

17.1.4 Common Replication Administration Tasks
----------------------------------------------

* Menu:

* replication-administration-status::  Checking Replication Status
* replication-administration-pausing::  Pausing Replication on the Slave

Once replication has been started it should execute without requiring
much regular administration.  Depending on your replication environment,
you will want to check the replication status of each slave
periodically, daily, or even more frequently.


File: manual.info.tmp,  Node: replication-administration-status,  Next: replication-administration-pausing,  Prev: replication-administration,  Up: replication-administration

17.1.4.1 Checking Replication Status
....................................

The most common task when managing a replication process is to ensure
that replication is taking place and that there have been no errors
between the slave and the master.  The primary statement for this is
*note 'SHOW SLAVE STATUS': show-slave-status, which you must execute on
each slave:

     mysql> SHOW SLAVE STATUS\G
     *************************** 1. row ***************************
                    Slave_IO_State: Waiting for master to send event
                       Master_Host: master1
                       Master_User: root
                       Master_Port: 3306
                     Connect_Retry: 60
                   Master_Log_File: mysql-bin.000004
               Read_Master_Log_Pos: 931
                    Relay_Log_File: slave1-relay-bin.000056
                     Relay_Log_Pos: 950
             Relay_Master_Log_File: mysql-bin.000004
                  Slave_IO_Running: Yes
                 Slave_SQL_Running: Yes
                   Replicate_Do_DB:
               Replicate_Ignore_DB:
                Replicate_Do_Table:
            Replicate_Ignore_Table:
           Replicate_Wild_Do_Table:
       Replicate_Wild_Ignore_Table:
                        Last_Errno: 0
                        Last_Error:
                      Skip_Counter: 0
               Exec_Master_Log_Pos: 931
                   Relay_Log_Space: 1365
                   Until_Condition: None
                    Until_Log_File:
                     Until_Log_Pos: 0
                Master_SSL_Allowed: No
                Master_SSL_CA_File:
                Master_SSL_CA_Path:
                   Master_SSL_Cert:
                 Master_SSL_Cipher:
                    Master_SSL_Key:
             Seconds_Behind_Master: 0
     Master_SSL_Verify_Server_Cert: No
                     Last_IO_Errno: 0
                     Last_IO_Error:
                    Last_SQL_Errno: 0
                    Last_SQL_Error:
       Replicate_Ignore_Server_Ids: 0

The key fields from the status report to examine are:

   * 'Slave_IO_State': The current status of the slave.  See *note
     slave-io-thread-states::, and *note slave-sql-thread-states::, for
     more information.

   * 'Slave_IO_Running': Whether the I/O thread for reading the master's
     binary log is running.  Normally, you want this to be 'Yes' unless
     you have not yet started replication or have explicitly stopped it
     with *note 'STOP SLAVE': stop-slave.

   * 'Slave_SQL_Running': Whether the SQL thread for executing events in
     the relay log is running.  As with the I/O thread, this should
     normally be 'Yes'.

   * 'Last_IO_Error', 'Last_SQL_Error': The last errors registered by
     the I/O and SQL threads when processing the relay log.  Ideally
     these should be blank, indicating no errors.

   * 'Seconds_Behind_Master': The number of seconds that the slave SQL
     thread is behind processing the master binary log.  A high number
     (or an increasing one) can indicate that the slave is unable to
     handle events from the master in a timely fashion.

     A value of 0 for 'Seconds_Behind_Master' can usually be interpreted
     as meaning that the slave has caught up with the master, but there
     are some cases where this is not strictly true.  For example, this
     can occur if the network connection between master and slave is
     broken but the slave I/O thread has not yet noticed this--that is,
     'slave_net_timeout' has not yet elapsed.

     It is also possible that transient values for
     'Seconds_Behind_Master' may not reflect the situation accurately.
     When the slave SQL thread has caught up on I/O,
     'Seconds_Behind_Master' displays 0; but when the slave I/O thread
     is still queuing up a new event, 'Seconds_Behind_Master' may show a
     large value until the SQL thread finishes executing the new event.
     This is especially likely when the events have old timestamps; in
     such cases, if you execute *note 'SHOW SLAVE STATUS':
     show-slave-status. several times in a relatively short period, you
     may see this value change back and forth repeatedly between 0 and a
     relatively large value.

Several pairs of fields provide information about the progress of the
slave in reading events from the master binary log and processing them
in the relay log:

   * ('Master_Log_file', 'Read_Master_Log_Pos'): Coordinates in the
     master binary log indicating how far the slave I/O thread has read
     events from that log.

   * ('Relay_Master_Log_File', 'Exec_Master_Log_Pos'): Coordinates in
     the master binary log indicating how far the slave SQL thread has
     executed events received from that log.

   * ('Relay_Log_File', 'Relay_Log_Pos'): Coordinates in the slave relay
     log indicating how far the slave SQL thread has executed the relay
     log.  These correspond to the preceding coordinates, but are
     expressed in slave relay log coordinates rather than master binary
     log coordinates.

The *note 'SHOW STATUS': show-status. statement also provides some
information relating specifically to replication slaves.  The
replication heartbeat information displayed by *note 'SHOW STATUS':
show-status. lets you check that the replication connection is active
even if the master has not sent events to the slave recently.  The
master sends a heartbeat signal to a slave if there are no updates to,
and no unsent events in, the binary log for a longer period than the
heartbeat interval.  The 'MASTER_HEARTBEAT_PERIOD' setting on the master
(set by the *note 'CHANGE MASTER TO': change-master-to. statement)
specifies the frequency of the heartbeat, which defaults to half of the
connection timeout interval for the slave ('slave_net_timeout').  The
'Slave_last_heartbeat'
(https://dev.mysql.com/doc/refman/5.6/en/server-status-variables.html#statvar_Slave_last_heartbeat)
variable for *note 'SHOW STATUS': show-status. shows when the
replication slave last received a heartbeat signal.

On the master, you can check the status of connected slaves using *note
'SHOW PROCESSLIST': show-processlist. to examine the list of running
processes.  Slave connections have 'Binlog Dump' in the 'Command' field:

     mysql> SHOW PROCESSLIST \G;
     *************************** 4. row ***************************
          Id: 10
        User: root
        Host: slave1:58371
          db: NULL
     Command: Binlog Dump
        Time: 777
       State: Has sent all binlog to slave; waiting for binlog to be updated
        Info: NULL

Because it is the slave that drives the replication process, very little
information is available in this report.

For slaves that were started with the '--report-host' option and are
connected to the master, the *note 'SHOW SLAVE HOSTS': show-slave-hosts.
statement on the master shows basic information about the slaves.  The
output includes the ID of the slave server, the value of the
'--report-host' option, the connecting port, and master ID:

     mysql> SHOW SLAVE HOSTS;
     +-----------+--------+------+-------------------+-----------+
     | Server_id | Host   | Port | Rpl_recovery_rank | Master_id |
     +-----------+--------+------+-------------------+-----------+
     |        10 | slave1 | 3306 |                 0 |         1 |
     +-----------+--------+------+-------------------+-----------+
     1 row in set (0.00 sec)


File: manual.info.tmp,  Node: replication-administration-pausing,  Prev: replication-administration-status,  Up: replication-administration

17.1.4.2 Pausing Replication on the Slave
.........................................

You can stop and start the replication of statements on the slave using
the *note 'STOP SLAVE': stop-slave. and *note 'START SLAVE':
start-slave. statements.

To stop processing of the binary log from the master, use *note 'STOP
SLAVE': stop-slave.:

     mysql> STOP SLAVE;

When replication is stopped, the slave I/O thread stops reading events
from the master binary log and writing them to the relay log, and the
SQL thread stops reading events from the relay log and executing them.
You can pause the I/O or SQL thread individually by specifying the
thread type:

     mysql> STOP SLAVE IO_THREAD;
     mysql> STOP SLAVE SQL_THREAD;

To start execution again, use the *note 'START SLAVE': start-slave.
statement:

     mysql> START SLAVE;

To start a particular thread, specify the thread type:

     mysql> START SLAVE IO_THREAD;
     mysql> START SLAVE SQL_THREAD;

For a slave that performs updates only by processing events from the
master, stopping only the SQL thread can be useful if you want to
perform a backup or other task.  The I/O thread will continue to read
events from the master but they are not executed.  This makes it easier
for the slave to catch up when you restart the SQL thread.

Stopping only the I/O thread enables the events in the relay log to be
executed by the SQL thread up to the point where the relay log ends.
This can be useful when you want to pause execution to catch up with
events already received from the master, when you want to perform
administration on the slave but also ensure that it has processed all
updates to a specific point.  This method can also be used to pause
event receipt on the slave while you conduct administration on the
master.  Stopping the I/O thread but permitting the SQL thread to run
helps ensure that there is not a massive backlog of events to be
executed when replication is started again.


File: manual.info.tmp,  Node: replication-implementation,  Next: replication-solutions,  Prev: replication-configuration,  Up: replication

17.2 Replication Implementation
===============================

* Menu:

* replication-implementation-details::  Replication Implementation Details
* slave-logs::                   Replication Relay and Status Logs
* replication-rules::            How Servers Evaluate Replication Filtering Rules

Replication is based on the master server keeping track of all changes
to its databases (updates, deletes, and so on) in its binary log.  The
binary log serves as a written record of all events that modify database
structure or content (data) from the moment the server was started.
Typically, *note 'SELECT': select. statements are not recorded because
they modify neither database structure nor content.

Each slave that connects to the master requests a copy of the binary
log.  That is, it pulls the data from the master, rather than the master
pushing the data to the slave.  The slave also executes the events from
the binary log that it receives.  This has the effect of repeating the
original changes just as they were made on the master.  Tables are
created or their structure modified, and data is inserted, deleted, and
updated according to the changes that were originally made on the
master.

Because each slave is independent, the replaying of the changes from the
master's binary log occurs independently on each slave that is connected
to the master.  In addition, because each slave receives a copy of the
binary log only by requesting it from the master, the slave is able to
read and update the copy of the database at its own pace and can start
and stop the replication process at will without affecting the ability
to update to the latest database status on either the master or slave
side.

For more information on the specifics of the replication implementation,
see *note replication-implementation-details::.

Masters and slaves report their status in respect of the replication
process regularly so that you can monitor them.  See *note
thread-information::, for descriptions of all replicated-related states.

The master binary log is written to a local relay log on the slave
before it is processed.  The slave also records information about the
current position with the master's binary log and the local relay log.
See *note slave-logs::.

Database changes are filtered on the slave according to a set of rules
that are applied according to the various configuration options and
variables that control event evaluation.  For details on how these rules
are applied, see *note replication-rules::.


File: manual.info.tmp,  Node: replication-implementation-details,  Next: slave-logs,  Prev: replication-implementation,  Up: replication-implementation

17.2.1 Replication Implementation Details
-----------------------------------------

MySQL replication capabilities are implemented using three threads, one
on the master server and two on the slave:

   * Binlog dump thread

     The master creates a thread to send the binary log contents to a
     slave when the slave connects.  This thread can be identified in
     the output of *note 'SHOW PROCESSLIST': show-processlist. on the
     master as the 'Binlog Dump' thread.

     The binary log dump thread acquires a lock on the master's binary
     log for reading each event that is to be sent to the slave.  As
     soon as the event has been read, the lock is released, even before
     the event is sent to the slave.

   * Slave I/O thread

     When a *note 'START SLAVE': start-slave. statement is issued on a
     slave server, the slave creates an I/O thread, which connects to
     the master and asks it to send the updates recorded in its binary
     logs.

     The slave I/O thread reads the updates that the master's 'Binlog
     Dump' thread sends (see previous item) and copies them to local
     files that comprise the slave's relay log.

     The state of this thread is shown as 'Slave_IO_running' in the
     output of *note 'SHOW SLAVE STATUS': show-slave-status. or as
     'Slave_running' in the output of *note 'SHOW STATUS': show-status.

   * Slave SQL thread

     The slave creates an SQL thread to read the relay log that is
     written by the slave I/O thread and execute the events contained
     therein.

In the preceding description, there are three threads per master/slave
connection.  A master that has multiple slaves creates one binary log
dump thread for each currently connected slave, and each slave has its
own I/O and SQL threads.

A slave uses two threads to separate reading updates from the master and
executing them into independent tasks.  Thus, the task of reading
statements is not slowed down if statement execution is slow.  For
example, if the slave server has not been running for a while, its I/O
thread can quickly fetch all the binary log contents from the master
when the slave starts, even if the SQL thread lags far behind.  If the
slave stops before the SQL thread has executed all the fetched
statements, the I/O thread has at least fetched everything so that a
safe copy of the statements is stored locally in the slave's relay logs,
ready for execution the next time that the slave starts.  This enables
the master server to purge its binary logs sooner because it no longer
needs to wait for the slave to fetch their contents.

The *note 'SHOW PROCESSLIST': show-processlist. statement provides
information that tells you what is happening on the master and on the
slave regarding replication.  For information on master states, see
*note master-thread-states::.  For slave states, see *note
slave-io-thread-states::, and *note slave-sql-thread-states::.

The following example illustrates how the three threads show up in the
output from *note 'SHOW PROCESSLIST': show-processlist.

On the master server, the output from *note 'SHOW PROCESSLIST':
show-processlist. looks like this:

     mysql> SHOW PROCESSLIST\G
     *************************** 1. row ***************************
          Id: 2
        User: root
        Host: localhost:32931
          db: NULL
     Command: Binlog Dump
        Time: 94
       State: Has sent all binlog to slave; waiting for binlog to
              be updated
        Info: NULL

Here, thread 2 is a 'Binlog Dump' replication thread that services a
connected slave.  The 'State' information indicates that all outstanding
updates have been sent to the slave and that the master is waiting for
more updates to occur.  If you see no 'Binlog Dump' threads on a master
server, this means that replication is not running; that is, no slaves
are currently connected.

On a slave server, the output from *note 'SHOW PROCESSLIST':
show-processlist. looks like this:

     mysql> SHOW PROCESSLIST\G
     *************************** 1. row ***************************
          Id: 10
        User: system user
        Host:
          db: NULL
     Command: Connect
        Time: 11
       State: Waiting for master to send event
        Info: NULL
     *************************** 2. row ***************************
          Id: 11
        User: system user
        Host:
          db: NULL
     Command: Connect
        Time: 11
       State: Has read all relay log; waiting for the slave I/O
              thread to update it
        Info: NULL

The 'State' information indicates that thread 10 is the I/O thread that
is communicating with the master server, and thread 11 is the SQL thread
that is processing the updates stored in the relay logs.  At the time
that *note 'SHOW PROCESSLIST': show-processlist. was run, both threads
were idle, waiting for further updates.

The value in the 'Time' column can show how late the slave is compared
to the master.  See *note faqs-replication::.  If sufficient time
elapses on the master side without activity on the 'Binlog Dump' thread,
the master determines that the slave is no longer connected.  As for any
other client connection, the timeouts for this depend on the values of
'net_write_timeout' and 'net_retry_count'; for more information about
these, see *note server-system-variables::.

The *note 'SHOW SLAVE STATUS': show-slave-status. statement provides
additional information about replication processing on a slave server.
See *note replication-administration-status::.


File: manual.info.tmp,  Node: slave-logs,  Next: replication-rules,  Prev: replication-implementation-details,  Up: replication-implementation

17.2.2 Replication Relay and Status Logs
----------------------------------------

* Menu:

* slave-logs-relaylog::          The Slave Relay Log
* slave-logs-status::            Slave Status Logs

During replication, a slave server creates several logs that hold the
binary log events relayed from the master to the slave, and to record
information about the current status and location within the relay log.
There are three types of logs used in the process, listed here:

   * The _relay log_ consists of the events read from the binary log of
     the master and written by the slave I/O thread.  Events in the
     relay log are executed on the slave as part of the SQL thread.

   * The _master info log_ contains status and current configuration
     information for the slave's connection to the master.  This log
     holds information on the master host name, login credentials, and
     coordinates indicating how far the slave has read from the master's
     binary log.

   * The _relay log info log_ holds status information about the
     execution point within the slave's relay log.


File: manual.info.tmp,  Node: slave-logs-relaylog,  Next: slave-logs-status,  Prev: slave-logs,  Up: slave-logs

17.2.2.1 The Slave Relay Log
............................

The relay log, like the binary log, consists of a set of numbered files
containing events that describe database changes, and an index file that
contains the names of all used relay log files.

The term 'relay log file' generally denotes an individual numbered file
containing database events.  The term 'relay log' collectively denotes
the set of numbered relay log files plus the index file.

Relay log files have the same format as binary log files and can be read
using *note 'mysqlbinlog': mysqlbinlog. (see *note mysqlbinlog::).

By default, relay log file names have the form
'HOST_NAME-relay-bin.NNNNNN' in the data directory, where HOST_NAME is
the name of the slave server host and NNNNNN is a sequence number.
Successive relay log files are created using successive sequence
numbers, beginning with '000001'.  The slave uses an index file to track
the relay log files currently in use.  The default relay log index file
name is 'HOST_NAME-relay-bin.index' in the data directory.

The default relay log file and relay log index file names can be
overridden with, respectively, the 'relay_log' and 'relay_log_index'
system variables (see *note replication-options::).

If a slave uses the default host-based relay log file names, changing a
slave's host name after replication has been set up can cause
replication to fail with the errors 'Failed to open the relay log' and
'Could not find target log during relay log initialization'.  This is a
known issue (see Bug #2122).  If you anticipate that a slave's host name
might change in the future (for example, if networking is set up on the
slave such that its host name can be modified using DHCP), you can avoid
this issue entirely by using the 'relay_log' and 'relay_log_index'
system variables to specify relay log file names explicitly when you
initially set up the slave.  This will make the names independent of
server host name changes.

If you encounter the issue after replication has already begun, one way
to work around it is to stop the slave server, prepend the contents of
the old relay log index file to the new one, and then restart the slave.
On a Unix system, this can be done as shown here:

     shell> cat NEW_RELAY_LOG_NAME.index >> OLD_RELAY_LOG_NAME.index
     shell> mv OLD_RELAY_LOG_NAME.index NEW_RELAY_LOG_NAME.index

A slave server creates a new relay log file under the following
conditions:

   * Each time the I/O thread starts.

   * When the logs are flushed (for example, with 'FLUSH LOGS' or *note
     'mysqladmin flush-logs': mysqladmin.).

   * When the size of the current relay log file becomes 'too large,'
     determined as follows:

        * If the value of 'max_relay_log_size' is greater than 0, that
          is the maximum relay log file size.

        * If the value of 'max_relay_log_size' is 0, 'max_binlog_size'
          determines the maximum relay log file size.

The SQL thread automatically deletes each relay log file after it has
executed all events in the file and no longer needs it.  There is no
explicit mechanism for deleting relay logs because the SQL thread takes
care of doing so.  However, 'FLUSH LOGS' rotates relay logs, which
influences when the SQL thread deletes them.


File: manual.info.tmp,  Node: slave-logs-status,  Prev: slave-logs-relaylog,  Up: slave-logs

17.2.2.2 Slave Status Logs
..........................

A replication slave server creates two logs.  By default, these logs are
files named 'master.info' and 'relay-log.info' and created in the data
directory.  The names and locations of these files can be changed by
using the '--master-info-file' option and 'relay_log_info_file' system
variable, respectively.  See *note replication-options::.

The two status logs contain information similar to that shown in the
output of the *note 'SHOW SLAVE STATUS': show-slave-status. statement,
which is discussed in *note replication-statements-slave::.  Because the
status logs are stored on disk, they survive a slave server's shutdown.
The next time the slave starts up, it reads the two logs to determine
how far it has proceeded in reading binary logs from the master and in
processing its own relay logs.

Access to the master info log should be restricted because it contains
the password for connecting to the master.  See *note
password-logging::.

The slave I/O thread updates the master info log.  The following table
shows the correspondence between the lines in the 'master.info' file and
the columns displayed by *note 'SHOW SLAVE STATUS': show-slave-status.

'master.info''SHOW SLAVE STATUS' Column   Description
File Line                                 

1                                         Number of lines in the file
                                          
2           'Master_Log_File'             The name of the master binary
                                          log currently being read from
                                          the master
                                          
3           'Read_Master_Log_Pos'         The current position within
                                          the master binary log that
                                          have been read from the master
                                          
4           'Master_Host'                 The host name of the master
                                          
5           'Master_User'                 The user name used to connect
                                          to the master
                                          
6           Password (not shown by        The password used to connect
            *note 'SHOW SLAVE STATUS': show-slave-status.)to the master
                                          
7           'Master_Port'                 The network port used to
                                          connect to the master
                                          
8           'Connect_Retry'               The period (in seconds) that
                                          the slave will wait before
                                          trying to reconnect to the
                                          master
                                          
9           'Master_SSL_Allowed'          Indicates whether the server
                                          supports SSL connections
                                          
10          'Master_SSL_CA_File'          The file used for the
                                          Certificate Authority (CA)
                                          certificate
                                          
11          'Master_SSL_CA_Path'          The path to the Certificate
                                          Authority (CA) certificates
                                          
12          'Master_SSL_Cert'             The name of the SSL
                                          certificate file
                                          
13          'Master_SSL_Cipher'           The list of possible ciphers
                                          used in the handshake for the
                                          SSL connection
                                          
14          'Master_SSL_Key'              The name of the SSL key file
                                          
15          'Master_SSL_Verify_Server_Cert'Whether to verify the server
                                          certificate
                                          
17          'Replicate_Ignore_Server_Ids' The number of server IDs to be
                                          ignored, followed by the
                                          actual server IDs

The slave SQL thread updates the relay log info log.  The following
table shows the correspondence between the lines in the 'relay-log.info'
file and the columns displayed by *note 'SHOW SLAVE STATUS':
show-slave-status.

Line in     'SHOW SLAVE STATUS'       Description
'relay-log.info'Column                
            
1           'Relay_Log_File'          The name of the current relay log
                                      file
                                      
2           'Relay_Log_Pos'           The current position within the
                                      relay log file; events up to this
                                      position have been executed on the
                                      slave database
                                      
3           'Relay_Master_Log_File'   The name of the master binary log
                                      file from which the events in the
                                      relay log file were read
                                      
4           'Exec_Master_Log_Pos'     The equivalent position within the
                                      master's binary log file of events
                                      that have already been executed

The contents of the 'relay-log.info' file and the states shown by the
*note 'SHOW SLAVE STATUS': show-slave-status. statement might not match
if the 'relay-log.info' file has not been flushed to disk.  Ideally, you
should only view 'relay-log.info' on a slave that is offline (that is,
'mysqld' is not running).  For a running system, *note 'SHOW SLAVE
STATUS': show-slave-status. should be used.

When you back up the slave's data, you should back up these two status
logs, along with the relay log files.  The status logs are needed to
resume replication after you restore the data from the slave.  If you
lose the relay logs but still have the relay log info log, you can check
it to determine how far the SQL thread has executed in the master binary
logs.  Then you can use *note 'CHANGE MASTER TO': change-master-to. with
the 'MASTER_LOG_FILE' and 'MASTER_LOG_POS' options to tell the slave to
re-read the binary logs from that point.  Of course, this requires that
the binary logs still exist on the master.


File: manual.info.tmp,  Node: replication-rules,  Prev: slave-logs,  Up: replication-implementation

17.2.3 How Servers Evaluate Replication Filtering Rules
-------------------------------------------------------

* Menu:

* replication-rules-db-options::  Evaluation of Database-Level Replication and Binary Logging Options
* replication-rules-table-options::  Evaluation of Table-Level Replication Options
* replication-rules-examples::   Replication Rule Application

If a master server does not write a statement to its binary log, the
statement is not replicated.  If the server does log the statement, the
statement is sent to all slaves and each slave determines whether to
execute it or ignore it.

On the master, you can control which databases to log changes for by
using the '--binlog-do-db' and '--binlog-ignore-db' options to control
binary logging.  For a description of the rules that servers use in
evaluating these options, see *note replication-rules-db-options::.  You
should not use these options to control which databases and tables are
replicated.  Instead, use filtering on the slave to control the events
that are executed on the slave.

On the slave side, decisions about whether to execute or ignore
statements received from the master are made according to the
'--replicate-*' options that the slave was started with.  (See *note
replication-options::.)

In the simplest case, when there are no '--replicate-*' options, the
slave executes all statements that it receives from the master.
Otherwise, the result depends on the particular options given.

Database-level options ('--replicate-do-db', '--replicate-ignore-db')
are checked first; see *note replication-rules-db-options::, for a
description of this process.  If no database-level options are used,
option checking proceeds to any table-level options that may be in use
(see *note replication-rules-table-options::, for a discussion of
these).  If one or more database-level options are used but none are
matched, the statement is not replicated.

To make it easier to determine what effect an option set will have, it
is recommended that you avoid mixing 'do' and 'ignore' options, or
wildcard and nonwildcard options.  An example of the latter that may
have unintended effects is the use of '--replicate-do-db' and
'--replicate-wild-do-table' together, where '--replicate-wild-do-table'
uses a pattern for the database name that matches the name given for
'--replicate-do-db'.  Suppose a replication slave is started with
'--replicate-do-db=dbx' '--replicate-wild-do-table=db%.t1'.  Then,
suppose that on the master, you issue the statement *note 'CREATE
DATABASE dbx': create-database.  Although you might expect it, this
statement is not replicated because it does not reference a table named
't1'.

If any '--replicate-rewrite-db' options were specified, they are applied
before the '--replicate-*' filtering rules are tested.

*Note*:

Database-level filtering options are case-sensitive on platforms
supporting case sensitivity in filenames, whereas table-level filtering
options are not (regardless of platform).  This is true regardless of
the value of the 'lower_case_table_names' system variable.


File: manual.info.tmp,  Node: replication-rules-db-options,  Next: replication-rules-table-options,  Prev: replication-rules,  Up: replication-rules

17.2.3.1 Evaluation of Database-Level Replication and Binary Logging Options
............................................................................

When evaluating replication options, the slave begins by checking to see
whether there are any '--replicate-do-db' or '--replicate-ignore-db'
options that apply.  When using '--binlog-do-db' or
'--binlog-ignore-db', the process is similar, but the options are
checked on the master.

The database that is checked for a match depends on the binary log
format of the statement that is being handled.  If the statement has
been logged using the row format, the database where data is to be
changed is the database that is checked.  If the statement has been
logged using the statement format, the default database (specified with
a *note 'USE': use. statement) is the database that is checked.

*Note*:

Only DML statements can be logged using the row format.  DDL statements
are always logged as statements, even when 'binlog_format=ROW'.  All DDL
statements are therefore always filtered according to the rules for
statement-based replication.  This means that you must select the
default database explicitly with a *note 'USE': use. statement in order
for a DDL statement to be applied.

For replication, the steps involved are listed here:

  1. Which logging format is used?

        * STATEMENT

          Test the default database.

        * ROW

          Test the database affected by the changes.

  2. Are there any '--replicate-do-db' options?

        * Yes

          Does the database match any of them?

             * Yes

               Continue to Step 4.

             * No

               Ignore the update and exit.

        * No

          Continue to step 3.

  3. Are there any '--replicate-ignore-db' options?

        * Yes

          Does the database match any of them?

             * Yes

               Ignore the update and exit.

             * No

               Continue to step 4.

        * No

          Continue to step 4.

  4. Proceed to checking the table-level replication options, if there
     are any.  For a description of how these options are checked, see
     *note replication-rules-table-options::.

     *Important*:

     A statement that is still permitted at this stage is not yet
     actually executed.  The statement is not executed until all
     table-level options (if any) have also been checked, and the
     outcome of that process permits execution of the statement.

For binary logging, the steps involved are listed here:

  1. Are there any '--binlog-do-db' or '--binlog-ignore-db' options?

        * Yes

          Continue to step 2.

        * No

          Log the statement and exit.

  2. Is there a default database (has any database been selected by
     *note 'USE': use.)?

        * Yes

          Continue to step 3.

        * No

          Ignore the statement and exit.

  3. There is a default database.  Are there any '--binlog-do-db'
     options?

        * Yes

          Do any of them match the database?

             * Yes

               Log the statement and exit.

             * No

               Ignore the statement and exit.

        * No

          Continue to step 4.

  4. Do any of the '--binlog-ignore-db' options match the database?

        * Yes

          Ignore the statement and exit.

        * No

          Log the statement and exit.

*Important*:

For statement-based logging, an exception is made in the rules just
given for the *note 'CREATE DATABASE': create-database, *note 'ALTER
DATABASE': alter-database, and *note 'DROP DATABASE': drop-database.
statements.  In those cases, the database being _created, altered, or
dropped_ replaces the default database when determining whether to log
or ignore updates.

'--binlog-do-db' can sometimes mean 'ignore other databases'.  For
example, when using statement-based logging, a server running with only
'--binlog-do-db=sales' does not write to the binary log statements for
which the default database differs from 'sales'.  When using row-based
logging with the same option, the server logs only those updates that
change data in 'sales'.


File: manual.info.tmp,  Node: replication-rules-table-options,  Next: replication-rules-examples,  Prev: replication-rules-db-options,  Up: replication-rules

17.2.3.2 Evaluation of Table-Level Replication Options
......................................................

The slave checks for and evaluates table options only if either of the
following two conditions is true:

   * No matching database options were found.

   * One or more database options were found, and were evaluated to
     arrive at an 'execute' condition according to the rules described
     in the previous section (see *note replication-rules-db-options::).

First, as a preliminary condition, the slave checks whether
statement-based replication is enabled.  If so, and the statement occurs
within a stored function, the slave executes the statement and exits.
If row-based replication is enabled, the slave does not know whether a
statement occurred within a stored function on the master, so this
condition does not apply.

*Note*:

For statement-based replication, replication events represent statements
(all changes making up a given event are associated with a single SQL
statement); for row-based replication, each event represents a change in
a single table row (thus a single statement such as 'UPDATE mytable SET
mycol = 1' may yield many row-based events).  When viewed in terms of
events, the process of checking table options is the same for both
row-based and statement-based replication.

Having reached this point, if there are no table options, the slave
simply executes all events.  If there are any '--replicate-do-table' or
'--replicate-wild-do-table' options, the event must match one of these
if it is to be executed; otherwise, it is ignored.  If there are any
'--replicate-ignore-table' or '--replicate-wild-ignore-table' options,
all events are executed except those that match any of these options.

The following steps describe this evaluation in more detail.  The
starting point is the end of the evaluation of the database-level
options, as described in *note replication-rules-db-options::.

  1. Are there any table replication options?

        * Yes

          Continue to step 2.

        * No

          Execute the update and exit.

  2. Which logging format is used?

        * STATEMENT

          Carry out the remaining steps for each statement that performs
          an update.

        * ROW

          Carry out the remaining steps for each update of a table row.

  3. Are there any '--replicate-do-table' options?

        * Yes

          Does the table match any of them?

             * Yes

               Execute the update and exit.

             * No

               Continue to step 4.

        * No

          Continue to step 4.

  4. Are there any '--replicate-ignore-table' options?

        * Yes

          Does the table match any of them?

             * Yes

               Ignore the update and exit.

             * No

               Continue to step 5.

        * No

          Continue to step 5.

  5. Are there any '--replicate-wild-do-table' options?

        * Yes

          Does the table match any of them?

             * Yes

               Execute the update and exit.

             * No

               Continue to step 6.

        * No

          Continue to step 6.

  6. Are there any '--replicate-wild-ignore-table' options?

        * Yes

          Does the table match any of them?

             * Yes

               Ignore the update and exit.

             * No

               Continue to step 7.

        * No

          Continue to step 7.

  7. Is there another table to be tested?

        * Yes

          Go back to step 3.

        * No

          Continue to step 8.

  8. Are there any '--replicate-do-table' or '--replicate-wild-do-table'
     options?

        * Yes

          Ignore the update and exit.

        * No

          Execute the update and exit.

*Note*:

Statement-based replication stops if a single SQL statement operates on
both a table that is included by a '--replicate-do-table' or
'--replicate-wild-do-table' option, and another table that is ignored by
a '--replicate-ignore-table' or '--replicate-wild-ignore-table' option.
The slave must either execute or ignore the complete statement (which
forms a replication event), and it cannot logically do this.  This also
applies to row-based replication for DDL statements, because DDL
statements are always logged as statements, without regard to the
logging format in effect.  The only type of statement that can update
both an included and an ignored table and still be replicated
successfully is a DML statement that has been logged with
'binlog_format=ROW'.


File: manual.info.tmp,  Node: replication-rules-examples,  Prev: replication-rules-table-options,  Up: replication-rules

17.2.3.3 Replication Rule Application
.....................................

This section provides additional explanation and examples of usage for
different combinations of replication filtering options.

Some typical combinations of replication filter rule types are given in
the following table:

Condition (Types of           Outcome
Options)                      

No '--replicate-*' options    The slave executes all events that it
at all:                       receives from the master.
                              
'--replicate-*-db' options,   The slave accepts or ignores events using
but no table options:         the database options.  It executes all
                              events permitted by those options because
                              there are no table restrictions.
                              
'--replicate-*-table'         All events are accepted at the
options, but no database      database-checking stage because there are
options:                      no database conditions.  The slave
                              executes or ignores events based solely
                              on the table options.
                              
A combination of database     The slave accepts or ignores events using
and table options:            the database options.  Then it evaluates
                              all events permitted by those options
                              according to the table options.  This can
                              sometimes lead to results that seem
                              counterintuitive, and that may be
                              different depending on whether you are
                              using statement-based or row-based
                              replication; see the text for an example.

A more complex example follows, in which we examine the outcomes for
both statement-based and row-based settings.

Suppose that we have two tables 'mytbl1' in database 'db1' and 'mytbl2'
in database 'db2' on the master, and the slave is running with the
following options (and no other replication filtering options):

     replicate-ignore-db = db1
     replicate-do-table  = db2.tbl2

Now we execute the following statements on the master:

     USE db1;
     INSERT INTO db2.tbl2 VALUES (1);

The results on the slave vary considerably depending on the binary log
format, and may not match initial expectations in either case.

Statement-based replication

The 'USE' statement causes 'db1' to be the default database.  Thus the
'--replicate-ignore-db' option matches, _and the *note 'INSERT': insert.
statement is ignored_.  The table options are not checked.

Row-based replication

The default database has no effect on how the slave reads database
options when using row-based replication.  Thus, the *note 'USE': use.
statement makes no difference in how the '--replicate-ignore-db' option
is handled: the database specified by this option does not match the
database where the *note 'INSERT': insert. statement changes data, so
the slave proceeds to check the table options.  The table specified by
'--replicate-do-table' matches the table to be updated, _and the row is
inserted_.


File: manual.info.tmp,  Node: replication-solutions,  Next: replication-notes,  Prev: replication-implementation,  Up: replication

17.3 Replication Solutions
==========================

* Menu:

* replication-solutions-backups::  Using Replication for Backups
* replication-solutions-diffengines::  Using Replication with Different Master and Slave Storage Engines
* replication-solutions-scaleout::  Using Replication for Scale-Out
* replication-solutions-partitioning::  Replicating Different Databases to Different Slaves
* replication-solutions-performance::  Improving Replication Performance
* replication-solutions-switch::  Switching Masters During Failover
* replication-solutions-encrypted-connections::  Setting Up Replication to Use Encrypted Connections
* replication-semisync::         Semisynchronous Replication

Replication can be used in many different environments for a range of
purposes.  This section provides general notes and advice on using
replication for specific solution types.

For information on using replication in a backup environment, including
notes on the setup, backup procedure, and files to back up, see *note
replication-solutions-backups::.

For advice and tips on using different storage engines on the master and
slaves, see *note replication-solutions-diffengines::.

Using replication as a scale-out solution requires some changes in the
logic and operation of applications that use the solution.  See *note
replication-solutions-scaleout::.

For performance or data distribution reasons, you may want to replicate
different databases to different replication slaves.  See *note
replication-solutions-partitioning::

As the number of replication slaves increases, the load on the master
can increase and lead to reduced performance (because of the need to
replicate the binary log to each slave).  For tips on improving your
replication performance, including using a single secondary server as a
replication master, see *note replication-solutions-performance::.

For guidance on switching masters, or converting slaves into masters as
part of an emergency failover solution, see *note
replication-solutions-switch::.

To secure your replication communication, you can encrypt the
communication channel.  For step-by-step instructions, see *note
replication-solutions-encrypted-connections::.


File: manual.info.tmp,  Node: replication-solutions-backups,  Next: replication-solutions-diffengines,  Prev: replication-solutions,  Up: replication-solutions

17.3.1 Using Replication for Backups
------------------------------------

* Menu:

* replication-solutions-backups-mysqldump::  Backing Up a Slave Using mysqldump
* replication-solutions-backups-rawdata::  Backing Up Raw Data from a Slave
* replication-solutions-backups-read-only::  Backing Up a Master or Slave by Making It Read Only

To use replication as a backup solution, replicate data from the master
to a slave, and then back up the data slave.  The slave can be paused
and shut down without affecting the running operation of the master, so
you can produce an effective snapshot of 'live' data that would
otherwise require the master to be shut down.

How you back up a database depends on its size and whether you are
backing up only the data, or the data and the replication slave state so
that you can rebuild the slave in the event of failure.  There are
therefore two choices:

   * If you are using replication as a solution to enable you to back up
     the data on the master, and the size of your database is not too
     large, the *note 'mysqldump': mysqldump. tool may be suitable.  See
     *note replication-solutions-backups-mysqldump::.

   * For larger databases, where *note 'mysqldump': mysqldump. would be
     impractical or inefficient, you can back up the raw data files
     instead.  Using the raw data files option also means that you can
     back up the binary and relay logs that will enable you to recreate
     the slave in the event of a slave failure.  For more information,
     see *note replication-solutions-backups-rawdata::.

Another backup strategy, which can be used for either master or slave
servers, is to put the server in a read-only state.  The backup is
performed against the read-only server, which then is changed back to
its usual read/write operational status.  See *note
replication-solutions-backups-read-only::.


File: manual.info.tmp,  Node: replication-solutions-backups-mysqldump,  Next: replication-solutions-backups-rawdata,  Prev: replication-solutions-backups,  Up: replication-solutions-backups

17.3.1.1 Backing Up a Slave Using mysqldump
...........................................

Using *note 'mysqldump': mysqldump. to create a copy of a database
enables you to capture all of the data in the database in a format that
enables the information to be imported into another instance of MySQL
Server (see *note mysqldump::).  Because the format of the information
is SQL statements, the file can easily be distributed and applied to
running servers in the event that you need access to the data in an
emergency.  However, if the size of your data set is very large, *note
'mysqldump': mysqldump. may be impractical.

When using *note 'mysqldump': mysqldump, you should stop replication on
the slave before starting the dump process to ensure that the dump
contains a consistent set of data:

  1. Stop the slave from processing requests.  You can stop replication
     completely on the slave using *note 'mysqladmin': mysqladmin.:

          shell> mysqladmin stop-slave

     Alternatively, you can stop only the slave SQL thread to pause
     event execution:

          shell> mysql -e 'STOP SLAVE SQL_THREAD;'

     This enables the slave to continue to receive data change events
     from the master's binary log and store them in the relay logs using
     the I/O thread, but prevents the slave from executing these events
     and changing its data.  Within busy replication environments,
     permitting the I/O thread to run during backup may speed up the
     catch-up process when you restart the slave SQL thread.

  2. Run *note 'mysqldump': mysqldump. to dump your databases.  You may
     either dump all databases or select databases to be dumped.  For
     example, to dump all databases:

          shell> mysqldump --all-databases > fulldb.dump

  3. Once the dump has completed, start slave operations again:

          shell> mysqladmin start-slave

In the preceding example, you may want to add login credentials (user
name, password) to the commands, and bundle the process up into a script
that you can run automatically each day.

If you use this approach, make sure you monitor the slave replication
process to ensure that the time taken to run the backup does not affect
the slave's ability to keep up with events from the master.  See *note
replication-administration-status::.  If the slave is unable to keep up,
you may want to add another slave and distribute the backup process.
For an example of how to configure this scenario, see *note
replication-solutions-partitioning::.


File: manual.info.tmp,  Node: replication-solutions-backups-rawdata,  Next: replication-solutions-backups-read-only,  Prev: replication-solutions-backups-mysqldump,  Up: replication-solutions-backups

17.3.1.2 Backing Up Raw Data from a Slave
.........................................

To guarantee the integrity of the files that are copied, backing up the
raw data files on your MySQL replication slave should take place while
your slave server is shut down.  If the MySQL server is still running,
background tasks may still be updating the database files, particularly
those involving storage engines with background processes such as
'InnoDB'.  With 'InnoDB', these problems should be resolved during crash
recovery, but since the slave server can be shut down during the backup
process without affecting the execution of the master it makes sense to
take advantage of this capability.

To shut down the server and back up the files:

  1. Shut down the slave MySQL server:

          shell> mysqladmin shutdown

  2. Copy the data files.  You can use any suitable copying or archive
     utility, including 'cp', 'tar' or 'WinZip'.  For example, assuming
     that the data directory is located under the current directory, you
     can archive the entire directory as follows:

          shell> tar cf /tmp/dbbackup.tar ./data

  3. Start the MySQL server again.  Under Unix:

          shell> mysqld_safe &

     Under Windows:

          C:\> "C:\Program Files\MySQL\MySQL Server 5.5\bin\mysqld"

Normally you should back up the entire data directory for the slave
MySQL server.  If you want to be able to restore the data and operate as
a slave (for example, in the event of failure of the slave), then in
addition to the slave's data, you should also back up the slave status
files, 'master.info' and 'relay-log.info', along with the relay log
files.  These files are needed to resume replication after you restore
the slave's data.

If you lose the relay logs but still have the 'relay-log.info' file, you
can check it to determine how far the SQL thread has executed in the
master binary logs.  Then you can use *note 'CHANGE MASTER TO':
change-master-to. with the 'MASTER_LOG_FILE' and 'MASTER_LOG_POS'
options to tell the slave to re-read the binary logs from that point.
This requires that the binary logs still exist on the master server.

If your slave is replicating *note 'LOAD DATA': load-data. statements,
you should also back up any 'SQL_LOAD-*' files that exist in the
directory that the slave uses for this purpose.  The slave needs these
files to resume replication of any interrupted *note 'LOAD DATA':
load-data. operations.  The location of this directory is the value of
the 'slave_load_tmpdir' system variable.  If the server was not started
with that variable set, the directory location is the value of the
'tmpdir' system variable.


File: manual.info.tmp,  Node: replication-solutions-backups-read-only,  Prev: replication-solutions-backups-rawdata,  Up: replication-solutions-backups

17.3.1.3 Backing Up a Master or Slave by Making It Read Only
............................................................

It is possible to back up either master or slave servers in a
replication setup by acquiring a global read lock and manipulating the
'read_only' system variable to change the read-only state of the server
to be backed up:

  1. Make the server read-only, so that it processes only retrievals and
     blocks updates.

  2. Perform the backup.

  3. Change the server back to its normal read/write state.

*Note*:

The instructions in this section place the server to be backed up in a
state that is safe for backup methods that get the data from the server,
such as *note 'mysqldump': mysqldump. (see *note mysqldump::).  You
should not attempt to use these instructions to make a binary backup by
copying files directly because the server may still have modified data
cached in memory and not flushed to disk.

The following instructions describe how to do this for a master server
and for a slave server.  For both scenarios discussed here, suppose that
you have the following replication setup:

   * A master server M1

   * A slave server S1 that has M1 as its master

   * A client C1 connected to M1

   * A client C2 connected to S1

In either scenario, the statements to acquire the global read lock and
manipulate the 'read_only' variable are performed on the server to be
backed up and do not propagate to any slaves of that server.

*Scenario 1: Backup with a Read-Only Master*

Put the master M1 in a read-only state by executing these statements on
it:

     mysql> FLUSH TABLES WITH READ LOCK;
     mysql> SET GLOBAL read_only = ON;

While M1 is in a read-only state, the following properties are true:

   * Requests for updates sent by C1 to M1 will block because the server
     is in read-only mode.

   * Requests for query results sent by C1 to M1 will succeed.

   * Making a backup on M1 is safe.

   * Making a backup on S1 is not safe.  This server is still running,
     and might be processing the binary log or update requests coming
     from client C2

While M1 is read only, perform the backup.  For example, you can use
*note 'mysqldump': mysqldump.

After the backup operation on M1 completes, restore M1 to its normal
operational state by executing these statements:

     mysql> SET GLOBAL read_only = OFF;
     mysql> UNLOCK TABLES;

Although performing the backup on M1 is safe (as far as the backup is
concerned), it is not optimal for performance because clients of M1 are
blocked from executing updates.

This strategy applies to backing up a master server in a replication
setup, but can also be used for a single server in a nonreplication
setting.

*Scenario 2: Backup with a Read-Only Slave*

Put the slave S1 in a read-only state by executing these statements on
it:

     mysql> FLUSH TABLES WITH READ LOCK;
     mysql> SET GLOBAL read_only = ON;

While S1 is in a read-only state, the following properties are true:

   * The master M1 will continue to operate, so making a backup on the
     master is not safe.

   * The slave S1 is stopped, so making a backup on the slave S1 is
     safe.

These properties provide the basis for a popular backup scenario: Having
one slave busy performing a backup for a while is not a problem because
it does not affect the entire network, and the system is still running
during the backup.  In particular, clients can still perform updates on
the master server, which remains unaffected by backup activity on the
slave.

While S1 is read only, perform the backup.  For example, you can use
*note 'mysqldump': mysqldump.

After the backup operation on S1 completes, restore S1 to its normal
operational state by executing these statements:

     mysql> SET GLOBAL read_only = OFF;
     mysql> UNLOCK TABLES;

After the slave is restored to normal operation, it again synchronizes
to the master by catching up with any outstanding updates from the
binary log of the master.


File: manual.info.tmp,  Node: replication-solutions-diffengines,  Next: replication-solutions-scaleout,  Prev: replication-solutions-backups,  Up: replication-solutions

17.3.2 Using Replication with Different Master and Slave Storage Engines
------------------------------------------------------------------------

It does not matter for the replication process whether the source table
on the master and the replicated table on the slave use different engine
types.  In fact, the 'default_storage_engine' and 'storage_engine'
system variables are not replicated.

This provides a number of benefits in the replication process in that
you can take advantage of different engine types for different
replication scenarios.  For example, in a typical scale-out scenario
(see *note replication-solutions-scaleout::), you want to use 'InnoDB'
tables on the master to take advantage of the transactional
functionality, but use 'MyISAM' on the slaves where transaction support
is not required because the data is only read.  When using replication
in a data-logging environment you may want to use the 'Archive' storage
engine on the slave.

Configuring different engines on the master and slave depends on how you
set up the initial replication process:

   * If you used *note 'mysqldump': mysqldump. to create the database
     snapshot on your master, you could edit the dump file text to
     change the engine type used on each table.

     Another alternative for *note 'mysqldump': mysqldump. is to disable
     engine types that you do not want to use on the slave before using
     the dump to build the data on the slave.  For example, you can add
     the '--skip-innodb' option on your slave to disable the 'InnoDB'
     engine.  If a specific engine does not exist for a table to be
     created, MySQL will use the default engine type, usually 'MyISAM'.
     (This requires that the 'NO_ENGINE_SUBSTITUTION' SQL mode is not
     enabled.)  If you want to disable additional engines in this way,
     you may want to consider building a special binary to be used on
     the slave that only supports the engines you want.

   * If you are using raw data files (a binary backup) to set up the
     slave, you will be unable to change the initial table format.
     Instead, use *note 'ALTER TABLE': alter-table. to change the table
     types after the slave has been started.

   * For new master/slave replication setups where there are currently
     no tables on the master, avoid specifying the engine type when
     creating new tables.

If you are already running a replication solution and want to convert
your existing tables to another engine type, follow these steps:

  1. Stop the slave from running replication updates:

          mysql> STOP SLAVE;

     This will enable you to change engine types without interruptions.

  2. Execute an 'ALTER TABLE ... ENGINE='ENGINE_TYPE'' for each table to
     be changed.

  3. Start the slave replication process again:

          mysql> START SLAVE;

Although the 'default_storage_engine' variable is not replicated, be
aware that *note 'CREATE TABLE': create-table. and *note 'ALTER TABLE':
alter-table. statements that include the engine specification will be
correctly replicated to the slave.  For example, if you have a CSV table
and you execute:

     mysql> ALTER TABLE csvtable Engine='MyISAM';

The above statement will be replicated to the slave and the engine type
on the slave will be converted to 'MyISAM', even if you have previously
changed the table type on the slave to an engine other than CSV. If you
want to retain engine differences on the master and slave, you should be
careful to use the 'default_storage_engine' variable on the master when
creating a new table.  For example, instead of:

     mysql> CREATE TABLE tablea (columna int) Engine=MyISAM;

Use this format:

     mysql> SET default_storage_engine=MyISAM;
     mysql> CREATE TABLE tablea (columna int);

When replicated, the 'default_storage_engine' variable will be ignored,
and the *note 'CREATE TABLE': create-table. statement will execute on
the slave using the slave's default engine.


File: manual.info.tmp,  Node: replication-solutions-scaleout,  Next: replication-solutions-partitioning,  Prev: replication-solutions-diffengines,  Up: replication-solutions

17.3.3 Using Replication for Scale-Out
--------------------------------------

You can use replication as a scale-out solution; that is, where you want
to split up the load of database queries across multiple database
servers, within some reasonable limitations.

Because replication works from the distribution of one master to one or
more slaves, using replication for scale-out works best in an
environment where you have a high number of reads and low number of
writes/updates.  Most websites fit into this category, where users are
browsing the website, reading articles, posts, or viewing products.
Updates only occur during session management, or when making a purchase
or adding a comment/message to a forum.

Replication in this situation enables you to distribute the reads over
the replication slaves, while still enabling your web servers to
communicate with the replication master when a write is required.  You
can see a sample replication layout for this scenario in *note
figure_replication-scaleout::.

FIGURE GOES HERE: Using Replication to Improve Performance During
Scale-Out

If the part of your code that is responsible for database access has
been properly abstracted/modularized, converting it to run with a
replicated setup should be very smooth and easy.  Change the
implementation of your database access to send all writes to the master,
and to send reads to either the master or a slave.  If your code does
not have this level of abstraction, setting up a replicated system gives
you the opportunity and motivation to clean it up.  Start by creating a
wrapper library or module that implements the following functions:

   * 'safe_writer_connect()'

   * 'safe_reader_connect()'

   * 'safe_reader_statement()'

   * 'safe_writer_statement()'

'safe_' in each function name means that the function takes care of
handling all error conditions.  You can use different names for the
functions.  The important thing is to have a unified interface for
connecting for reads, connecting for writes, doing a read, and doing a
write.

Then convert your client code to use the wrapper library.  This may be a
painful and scary process at first, but it pays off in the long run.
All applications that use the approach just described are able to take
advantage of a master/slave configuration, even one involving multiple
slaves.  The code is much easier to maintain, and adding troubleshooting
options is trivial.  You need modify only one or two functions (for
example, to log how long each statement took, or which statement among
those issued gave you an error).

If you have written a lot of code, you may want to automate the
conversion task by using the *note 'replace': replace-utility. utility
that comes with standard MySQL distributions, or write your own
conversion script.  Ideally, your code uses consistent programming style
conventions.  If not, then you are probably better off rewriting it
anyway, or at least going through and manually regularizing it to use a
consistent style.


File: manual.info.tmp,  Node: replication-solutions-partitioning,  Next: replication-solutions-performance,  Prev: replication-solutions-scaleout,  Up: replication-solutions

17.3.4 Replicating Different Databases to Different Slaves
----------------------------------------------------------

There may be situations where you have a single master and want to
replicate different databases to different slaves.  For example, you may
want to distribute different sales data to different departments to help
spread the load during data analysis.  A sample of this layout is shown
in *note figure_replication-multi-db::.

FIGURE GOES HERE: Using Replication to Replicate Databases to Separate
Replication Slaves

You can achieve this separation by configuring the master and slaves as
normal, and then limiting the binary log statements that each slave
processes by using the '--replicate-wild-do-table' configuration option
on each slave.

*Important*:

You should _not_ use '--replicate-do-db' for this purpose when using
statement-based replication, since statement-based replication causes
this option's affects to vary according to the database that is
currently selected.  This applies to mixed-format replication as well,
since this enables some updates to be replicated using the
statement-based format.

However, it should be safe to use '--replicate-do-db' for this purpose
if you are using row-based replication only, since in this case the
currently selected database has no effect on the option's operation.

For example, to support the separation as shown in *note
figure_replication-multi-db::, you should configure each replication
slave as follows, before executing *note 'START SLAVE': start-slave.:

   * Replication slave 1 should use
     '--replicate-wild-do-table=databaseA.%'.

   * Replication slave 2 should use
     '--replicate-wild-do-table=databaseB.%'.

   * Replication slave 3 should use
     '--replicate-wild-do-table=databaseC.%'.

Each slave in this configuration receives the entire binary log from the
master, but executes only those events from the binary log that apply to
the databases and tables included by the '--replicate-wild-do-table'
option in effect on that slave.

If you have data that must be synchronized to the slaves before
replication starts, you have a number of choices:

   * Synchronize all the data to each slave, and delete the databases,
     tables, or both that you do not want to keep.

   * Use *note 'mysqldump': mysqldump. to create a separate dump file
     for each database and load the appropriate dump file on each slave.

   * Use a raw data file dump and include only the specific files and
     databases that you need for each slave.

     *Note*:

     This does not work with *note 'InnoDB': innodb-storage-engine.
     databases unless you use 'innodb_file_per_table'.


File: manual.info.tmp,  Node: replication-solutions-performance,  Next: replication-solutions-switch,  Prev: replication-solutions-partitioning,  Up: replication-solutions

17.3.5 Improving Replication Performance
----------------------------------------

As the number of slaves connecting to a master increases, the load,
although minimal, also increases, as each slave uses a client connection
to the master.  Also, as each slave must receive a full copy of the
master binary log, the network load on the master may also increase and
create a bottleneck.

If you are using a large number of slaves connected to one master, and
that master is also busy processing requests (for example, as part of a
scale-out solution), then you may want to improve the performance of the
replication process.

One way to improve the performance of the replication process is to
create a deeper replication structure that enables the master to
replicate to only one slave, and for the remaining slaves to connect to
this primary slave for their individual replication requirements.  A
sample of this structure is shown in *note
figure_replication-performance::.

FIGURE GOES HERE: Using an Additional Replication Host to Improve
Performance

For this to work, you must configure the MySQL instances as follows:

   * Master 1 is the primary master where all changes and updates are
     written to the database.  Binary logging should be enabled on this
     machine.

   * Master 2 is the slave to the Master 1 that provides the replication
     functionality to the remainder of the slaves in the replication
     structure.  Master 2 is the only machine permitted to connect to
     Master 1.  Master 2 also has binary logging enabled, and the
     'log_slave_updates' system variable enabled so that replication
     instructions from Master 1 are also written to Master 2's binary
     log so that they can then be replicated to the true slaves.

   * Slave 1, Slave 2, and Slave 3 act as slaves to Master 2, and
     replicate the information from Master 2, which actually consists of
     the upgrades logged on Master 1.

The above solution reduces the client load and the network interface
load on the primary master, which should improve the overall performance
of the primary master when used as a direct database solution.

If your slaves are having trouble keeping up with the replication
process on the master, there are a number of options available:

   * If possible, put the relay logs and the data files on different
     physical drives.  To do this, set the 'relay_log' system variable
     to specify the location of the relay log.

   * If the slaves are significantly slower than the master, you may
     want to divide up the responsibility for replicating different
     databases to different slaves.  See *note
     replication-solutions-partitioning::.

   * If your master makes use of transactions and you are not concerned
     about transaction support on your slaves, use 'MyISAM' or another
     nontransactional engine on the slaves.  See *note
     replication-solutions-diffengines::.

   * If your slaves are not acting as masters, and you have a potential
     solution in place to ensure that you can bring up a master in the
     event of failure, then you can disable the 'log_slave_updates'
     system variable.  This prevents 'dumb' slaves from also logging
     events they have executed into their own binary log.


File: manual.info.tmp,  Node: replication-solutions-switch,  Next: replication-solutions-encrypted-connections,  Prev: replication-solutions-performance,  Up: replication-solutions

17.3.6 Switching Masters During Failover
----------------------------------------

There is in MySQL 5.5 no official solution for providing failover
between master and slaves in the event of a failure.  Instead, you must
set up a master and one or more slaves; then, you need to write an
application or script that monitors the master to check whether it is
up, and instructs the slaves and applications to change master in case
of failure.  This section discusses some of the issues encountered when
setting up failover in this fashion.

You can tell a slave to change to a new master using the *note 'CHANGE
MASTER TO': change-master-to. statement.  The slave does not check
whether the databases on the master are compatible with those on the
slave; it simply begins reading and executing events from the specified
coordinates in the new master's binary log.  In a failover situation,
all the servers in the group are typically executing the same events
from the same binary log file, so changing the source of the events
should not affect the structure or integrity of the database, provided
that you exercise care in making the change.

Slaves should be run with the '--log-bin' option and without enabling
the 'log_slave_updates' system variable.  In this way, the slave is
ready to become a master without restarting the slave *note 'mysqld':
mysqld.  Assume that you have the structure shown in *note
figure_replication-redundancy-before::.

Remember that you can tell a slave to change its master at any time,
using the *note 'CHANGE MASTER TO': change-master-to. statement.  The
slave will not check whether the databases on the master are compatible
with the slave, it will just start reading and executing events from the
specified binary log coordinates on the new master.  In a failover
situation, all the servers in the group are typically executing the same
events from the same binary log file, so changing the source of the
events should not affect the database structure or integrity providing
you are careful.

Run your slaves with the '--log-bin' option and without enabling the
'log_slave_updates' system variable.  In this way, the slave is ready to
become a master as soon as you issue *note 'STOP SLAVE': stop-slave.;
*note 'RESET MASTER': reset-master, and *note 'CHANGE MASTER TO':
change-master-to. statement on the other slaves.  For example, assume
that you have the structure shown in *note
figure_replication-redundancy-before::.

FIGURE GOES HERE: Redundancy Using Replication, Initial Structure

In this diagram, the 'MySQL Master' holds the master database, the
'MySQL Slave' hosts are replication slaves, and the 'Web Client'
machines are issuing database reads and writes.  Web clients that issue
only reads (and would normally be connected to the slaves) are not
shown, as they do not need to switch to a new server in the event of
failure.  For a more detailed example of a read/write scale-out
replication structure, see *note replication-solutions-scaleout::.

Each MySQL Slave ('Slave 1', 'Slave 2', and 'Slave 3') is a slave
running with '--log-bin' and without enabling the 'log_slave_updates'
system variable.  Because updates received by a slave from the master
are not logged in the binary log unless 'log_slave_updates' is enabled,
the binary log on each slave is empty initially.  If for some reason
'MySQL Master' becomes unavailable, you can pick one of the slaves to
become the new master.  For example, if you pick 'Slave 1', all 'Web
Clients' should be redirected to 'Slave 1', which writes the updates to
its binary log.  'Slave 2' and 'Slave 3' should then replicate from
'Slave 1'.

The reason for running the slave without 'log_slave_updates' enabled is
to prevent slaves from receiving updates twice in case you cause one of
the slaves to become the new master.  If 'Slave 1' has
'log_slave_updates' enabled, it writes any updates that it receives from
'Master' in its own binary log.  This means that, when 'Slave 2' changes
from 'Master' to 'Slave 1' as its master, it may receive updates from
'Slave 1' that it has already received from 'Master'.

Make sure that all slaves have processed any statements in their relay
log.  On each slave, issue 'STOP SLAVE IO_THREAD', then check the output
of *note 'SHOW PROCESSLIST': show-processlist. until you see 'Has read
all relay log'.  When this is true for all slaves, they can be
reconfigured to the new setup.  On the slave 'Slave 1' being promoted to
become the master, issue *note 'STOP SLAVE': stop-slave. and *note
'RESET MASTER': reset-master.

On the other slaves 'Slave 2' and 'Slave 3', use *note 'STOP SLAVE':
stop-slave. and 'CHANGE MASTER TO MASTER_HOST='Slave1'' (where
''Slave1'' represents the real host name of 'Slave 1').  To use 'CHANGE
MASTER TO', add all information about how to connect to 'Slave 1' from
'Slave 2' or 'Slave 3' (USER, PASSWORD, PORT).  When issuing the 'CHANGE
MASTER TO' statement in this, there is no need to specify the name of
the 'Slave 1' binary log file or log position to read from, since the
first binary log file and position 4, are the defaults.  Finally,
execute *note 'START SLAVE': start-slave. on 'Slave 2' and 'Slave 3'.

Once the new replication setup is in place, you need to tell each 'Web
Client' to direct its statements to 'Slave 1'.  From that point on, all
updates statements sent by 'Web Client' to 'Slave 1' are written to the
binary log of 'Slave 1', which then contains every update statement sent
to 'Slave 1' since 'Master' died.

The resulting server structure is shown in *note
figure_replication-redundancy-after::.

FIGURE GOES HERE: Redundancy Using Replication, After Master Failure

When 'Master' becomes available again, you should make it a slave of
'Slave 1'.  To do this, issue on 'Master' the same *note 'CHANGE MASTER
TO': change-master-to. statement as that issued on 'Slave 2' and 'Slave
3' previously.  'Master' then becomes a slave of 'S1ave 1' and picks up
the 'Web Client' writes that it missed while it was offline.

To make 'Master' a master again, use the preceding procedure as if
'Slave 1' was unavailable and 'Master' was to be the new master.  During
this procedure, do not forget to run *note 'RESET MASTER': reset-master.
on 'Master' before making 'Slave 1', 'Slave 2', and 'Slave 3' slaves of
'Master'.  If you fail to do this, the slaves may pick up stale writes
from the 'Web Client' applications dating from before the point at which
'Master' became unavailable.

You should be aware that there is no synchronization between slaves,
even when they share the same master, and thus some slaves might be
considerably ahead of others.  This means that in some cases the
procedure outlined in the previous example might not work as expected.
In practice, however, relay logs on all slaves should be relatively
close together.

One way to keep applications informed about the location of the master
is to have a dynamic DNS entry for the master.  With 'bind' you can use
'nsupdate' to update the DNS dynamically.


File: manual.info.tmp,  Node: replication-solutions-encrypted-connections,  Next: replication-semisync,  Prev: replication-solutions-switch,  Up: replication-solutions

17.3.7 Setting Up Replication to Use Encrypted Connections
----------------------------------------------------------

To use an encrypted connection for the transfer of the binary log
required during replication, both the master and the slave servers must
support encrypted network connections.  If either server does not
support encrypted connections (because it has not been compiled or
configured for them), replication through an encrypted connection is not
possible.

Setting up encrypted connections for replication is similar to doing so
for client/server connections.  You must obtain (or create) a suitable
security certificate that you can use on the master, and a similar
certificate (from the same certificate authority) on each slave.  You
must also obtain suitable key files.

For more information on setting up a server and client for encrypted
connections, see *note using-encrypted-connections::.

To enable encrypted connections on the master, you must create or obtain
suitable certificate and key files, and then add the following
configuration options to the master's configuration within the
'[mysqld]' section of the master's 'my.cnf' file, changing the file
names as necessary:

     [mysqld]
     ssl-ca=cacert.pem
     ssl-cert=server-cert.pem
     ssl-key=server-key.pem

The paths to the files may be relative or absolute; we recommend that
you always use complete paths for this purpose.

The options are as follows:

   * '--ssl-ca': The path name of the Certificate Authority (CA)
     certificate file.  ('--ssl-capath' is similar but specifies the
     path name of a directory of CA certificate files.)

   * '--ssl-cert': The path name of the server public key certificate
     file.  This certificate can be sent to the client and authenticated
     against the CA certificate that it has.

   * '--ssl-key': The path name of the server private key file.

On the slave, there are two ways to specify the information required for
connecting using encryption to the master.  You can either name the
slave certificate and key files in the '[client]' section of the slave's
'my.cnf' file, or you can explicitly specify that information using the
*note 'CHANGE MASTER TO': change-master-to. statement:

   * To name the slave certificate and key files using an option file,
     add the following lines to the '[client]' section of the slave's
     'my.cnf' file, changing the file names as necessary:

          [client]
          ssl-ca=cacert.pem
          ssl-cert=client-cert.pem
          ssl-key=client-key.pem

     Restart the slave server, using the '--skip-slave-start' option to
     prevent the slave from connecting to the master.  Use *note 'CHANGE
     MASTER TO': change-master-to. to specify the master configuration,
     using the 'MASTER_SSL' option to connect using encryption:

          mysql> CHANGE MASTER TO
              -> MASTER_HOST='master_hostname',
              -> MASTER_USER='replicate',
              -> MASTER_PASSWORD='PASSWORD',
              -> MASTER_SSL=1;

   * To specify the certificate and key names using the *note 'CHANGE
     MASTER TO': change-master-to. statement, append the appropriate
     'MASTER_SSL_XXX' options:

          mysql> CHANGE MASTER TO
              -> MASTER_HOST='master_hostname',
              -> MASTER_USER='replicate',
              -> MASTER_PASSWORD='PASSWORD',
              -> MASTER_SSL=1,
              -> MASTER_SSL_CA = 'ca_file_name',
              -> MASTER_SSL_CAPATH = 'ca_directory_name',
              -> MASTER_SSL_CERT = 'cert_file_name',
              -> MASTER_SSL_KEY = 'key_file_name';

After the master information has been updated, start the slave
replication process:

     mysql> START SLAVE;

You can use the *note 'SHOW SLAVE STATUS': show-slave-status. statement
to confirm that an encrypted connection was established successfully.

For more information on the *note 'CHANGE MASTER TO': change-master-to.
statement, see *note change-master-to::.

If you want to enforce the use of encrypted connections during
replication, create a user with the 'REPLICATION SLAVE' privilege and
use the 'REQUIRE SSL' option for that user.  For example:

     mysql> CREATE USER 'repl'@'%.example.com' IDENTIFIED BY 'PASSWORD';
     mysql> GRANT REPLICATION SLAVE ON *.*
         -> TO 'repl'@'%.example.com' REQUIRE SSL;

If the account already exists, you can add 'REQUIRE SSL' to it with this
statement:

     mysql> GRANT USAGE ON *.*
         -> TO 'repl'@'%.example.com' REQUIRE SSL;


File: manual.info.tmp,  Node: replication-semisync,  Prev: replication-solutions-encrypted-connections,  Up: replication-solutions

17.3.8 Semisynchronous Replication
----------------------------------

* Menu:

* replication-semisync-interface::  Semisynchronous Replication Administrative Interface
* replication-semisync-installation::  Semisynchronous Replication Installation and Configuration
* replication-semisync-monitoring::  Semisynchronous Replication Monitoring

In addition to the built-in asynchronous replication, MySQL 5.5 supports
an interface to semisynchronous replication that is implemented by
plugins.  This section discusses what semisynchronous replication is and
how it works.  The following sections cover the administrative interface
to semisynchronous replication and how to install, configure, and
monitor it.

MySQL replication by default is asynchronous.  The master writes events
to its binary log but does not know whether or when a slave has
retrieved and processed them.  With asynchronous replication, if the
master crashes, transactions that it has committed might not have been
transmitted to any slave.  Consequently, failover from master to slave
in this case may result in failover to a server that is missing
transactions relative to the master.

Semisynchronous replication can be used as an alternative to
asynchronous replication:

   * A slave indicates whether it is semisynchronous-capable when it
     connects to the master.

   * If semisynchronous replication is enabled on the master side and
     there is at least one semisynchronous slave, a thread that performs
     a transaction commit on the master blocks after the commit is done
     and waits until at least one semisynchronous slave acknowledges
     that it has received all events for the transaction, or until a
     timeout occurs.

   * The slave acknowledges receipt of a transaction's events only after
     the events have been written to its relay log and flushed to disk.

   * If a timeout occurs without any slave having acknowledged the
     transaction, the master reverts to asynchronous replication.  When
     at least one semisynchronous slave catches up, the master returns
     to semisynchronous replication.

   * Semisynchronous replication must be enabled on both the master and
     slave sides.  If semisynchronous replication is disabled on the
     master, or enabled on the master but on no slaves, the master uses
     asynchronous replication.

While the master is blocking (waiting for acknowledgment from a slave
after having performed a commit), it does not return to the session that
performed the transaction.  When the block ends, the master returns to
the session, which then can proceed to execute other statements.  At
this point, the transaction has committed on the master side, and
receipt of its events has been acknowledged by at least one slave.

Blocking also occurs after rollbacks that are written to the binary log,
which occurs when a transaction that modifies nontransactional tables is
rolled back.  The rolled-back transaction is logged even though it has
no effect for transactional tables because the modifications to the
nontransactional tables cannot be rolled back and must be sent to
slaves.

For statements that do not occur in transactional context (that is, when
no transaction has been started with *note 'START TRANSACTION': commit.
or *note 'SET autocommit = 0': set-variable.), autocommit is enabled and
each statement commits implicitly.  With semisynchronous replication,
the master blocks after committing each such statement, just as it does
for explicit transaction commits.

To understand what the 'semi' in 'semisynchronous replication' means,
compare it with asynchronous and fully synchronous replication:

   * With asynchronous replication, the master writes events to its
     binary log and slaves request them when they are ready.  There is
     no guarantee that any event will ever reach any slave.

   * With fully synchronous replication, when a master commits a
     transaction, all slaves also will have committed the transaction
     before the master returns to the session that performed the
     transaction.  The drawback of this is that there might be a lot of
     delay to complete a transaction.

   * Semisynchronous replication falls between asynchronous and fully
     synchronous replication.  The master waits after commit only until
     at least one slave has received and logged the events.  It does not
     wait for all slaves to acknowledge receipt, and it requires only
     receipt, not that the events have been fully executed and committed
     on the slave side.

Compared to asynchronous replication, semisynchronous replication
provides improved data integrity.  When a commit returns successfully,
it is known that the data exists in at least two places (on the master
and at least one slave).  If the master commits but a crash occurs while
the master is waiting for acknowledgment from a slave, it is possible
that the transaction may not have reached any slave.

Semisynchronous replication also places a rate limit on busy sessions by
constraining the speed at which binary log events can be sent from
master to slave.  When one user is too busy, this will slow it down,
which is useful in some deployment situations.

Semisynchronous replication does have some performance impact because
commits are slower due to the need to wait for slaves.  This is the
tradeoff for increased data integrity.  The amount of slowdown is at
least the TCP/IP roundtrip time to send the commit to the slave and wait
for the acknowledgment of receipt by the slave.  This means that
semisynchronous replication works best for close servers communicating
over fast networks, and worst for distant servers communicating over
slow networks.


File: manual.info.tmp,  Node: replication-semisync-interface,  Next: replication-semisync-installation,  Prev: replication-semisync,  Up: replication-semisync

17.3.8.1 Semisynchronous Replication Administrative Interface
.............................................................

The administrative interface to semisynchronous replication has several
components:

   * Two plugins implement semisynchronous capability.  There is one
     plugin for the master side and one for the slave side.

   * System variables control plugin behavior.  Some examples:

        * 'rpl_semi_sync_master_enabled'

          Controls whether semisynchronous replication is enabled on the
          master.  To enable or disable the plugin, set this variable to
          1 or 0, respectively.  The default is 0 (off).

        * 'rpl_semi_sync_master_timeout'

          A value in milliseconds that controls how long the master
          waits on a commit for acknowledgment from a slave before
          timing out and reverting to asynchronous replication.  The
          default value is 10000 (10 seconds).

        * 'rpl_semi_sync_slave_enabled'

          Similar to 'rpl_semi_sync_master_enabled', but controls the
          slave plugin.

     All 'rpl_semi_sync_XXX' system variables are described at *note
     server-system-variables::.

   * Status variables enable semisynchronous replication monitoring.
     Some examples:

        * 'Rpl_semi_sync_master_clients'

          The number of semisynchronous slaves.

        * 'Rpl_semi_sync_master_status'

          Whether semisynchronous replication currently is operational
          on the master.  The value is 1 if the plugin has been enabled
          and a commit acknowledgment has occurred.  It is 0 if the
          plugin is not enabled or the master has fallen back to
          asynchronous replication due to commit acknowledgment timeout.

        * 'Rpl_semi_sync_master_no_tx'

          The number of commits that were not acknowledged successfully
          by a slave.

        * 'Rpl_semi_sync_master_yes_tx'

          The number of commits that were acknowledged successfully by a
          slave.

        * 'Rpl_semi_sync_slave_status'

          Whether semisynchronous replication currently is operational
          on the slave.  This is 1 if the plugin has been enabled and
          the slave I/O thread is running, 0 otherwise.

     All 'Rpl_semi_sync_XXX' status variables are described at *note
     server-status-variables::.

The system and status variables are available only if the appropriate
master or slave plugin has been installed with *note 'INSTALL PLUGIN':
install-plugin.


File: manual.info.tmp,  Node: replication-semisync-installation,  Next: replication-semisync-monitoring,  Prev: replication-semisync-interface,  Up: replication-semisync

17.3.8.2 Semisynchronous Replication Installation and Configuration
...................................................................

Semisynchronous replication is implemented using plugins, so the plugins
must be installed into the server to make them available.  After a
plugin has been installed, you control it by means of the system
variables associated with it.  These system variables are unavailable
until the associated plugin has been installed.

This section describes how to install the semisynchronous replication
plugins.  For general information about installing plugins, see *note
plugin-loading::.

To use semisynchronous replication, the following requirements must be
satisfied:

   * MySQL 5.5 or higher must be installed.

   * The capability of installing plugins requires a MySQL server that
     supports dynamic loading.  To verify this, check that the value of
     the 'have_dynamic_loading' system variable is 'YES'.  Binary
     distributions should support dynamic loading.

   * Replication must already be working.  For information on creating a
     master/slave relationship, see *note replication-howto::.

To set up semisynchronous replication, use the following instructions.
The *note 'INSTALL PLUGIN': install-plugin, *note 'SET GLOBAL':
set-variable, *note 'STOP SLAVE': stop-slave, and *note 'START SLAVE':
start-slave. statements mentioned here require the 'SUPER' privilege.

MySQL distributions include semisynchronous replication plugin files for
the master side and the slave side.

To be usable by a master or slave server, the appropriate plugin library
file must be located in the MySQL plugin directory (the directory named
by the 'plugin_dir' system variable).  If necessary, configure the
plugin directory location by setting the value of 'plugin_dir' at server
startup.

The plugin library file base names are 'semisync_master' and
'semisync_slave'.  The file name suffix differs per platform (for
example, '.so' for Unix and Unix-like systems, '.dll' for Windows).

The master plugin library file must be present in the plugin directory
of the master server.  The slave plugin library file must be present in
the plugin directory of each slave server.

To load the plugins, use the *note 'INSTALL PLUGIN': install-plugin.
statement on the master and on each slave that is to be semisynchronous
(adjust the '.so' suffix for your platform as necessary).

On the master:

     INSTALL PLUGIN rpl_semi_sync_master SONAME 'semisync_master.so';

On each slave:

     INSTALL PLUGIN rpl_semi_sync_slave SONAME 'semisync_slave.so';

If an attempt to install a plugin results in an error on Linux similar
to that shown here, you must install 'libimf':

     mysql> INSTALL PLUGIN rpl_semi_sync_master SONAME 'semisync_master.so';
     ERROR 1126 (HY000): Can't open shared library
     '/usr/local/mysql/lib/plugin/semisync_master.so'
     (errno: 22 libimf.so: cannot open shared object file:
     No such file or directory)

You can obtain 'libimf' from
<https://dev.mysql.com/downloads/os-linux.html>.

To see which plugins are installed, use the *note 'SHOW PLUGINS':
show-plugins. statement, or query the *note
'INFORMATION_SCHEMA.PLUGINS': plugins-table. table.

To verify plugin installation, examine the *note
'INFORMATION_SCHEMA.PLUGINS': plugins-table. table or use the *note
'SHOW PLUGINS': show-plugins. statement (see *note
obtaining-plugin-information::).  For example:

     mysql> SELECT PLUGIN_NAME, PLUGIN_STATUS
            FROM INFORMATION_SCHEMA.PLUGINS
            WHERE PLUGIN_NAME LIKE '%semi%';
     +----------------------+---------------+
     | PLUGIN_NAME          | PLUGIN_STATUS |
     +----------------------+---------------+
     | rpl_semi_sync_master | ACTIVE        |
     +----------------------+---------------+

If the plugin failed to initialize, check the server error log for
diagnostic messages.

After a semisynchronous replication plugin has been installed, it is
disabled by default.  The plugins must be enabled both on the master
side and the slave side to enable semisynchronous replication.  If only
one side is enabled, replication will be asynchronous.

To control whether an installed plugin is enabled, set the appropriate
system variables.  You can set these variables at runtime using *note
'SET GLOBAL': set-variable, or at server startup on the command line or
in an option file.

At runtime, these master-side system variables are available:

     SET GLOBAL rpl_semi_sync_master_enabled = {0|1};
     SET GLOBAL rpl_semi_sync_master_timeout = N;

On the slave side, this system variable is available:

     SET GLOBAL rpl_semi_sync_slave_enabled = {0|1};

For 'rpl_semi_sync_master_enabled' or 'rpl_semi_sync_slave_enabled', the
value should be 1 to enable semisynchronous replication or 0 to disable
it.  By default, these variables are set to 0.

For 'rpl_semi_sync_master_timeout', the value N is given in
milliseconds.  The default value is 10000 (10 seconds).

If you enable semisynchronous replication on a slave at runtime, you
must also start the slave I/O thread (stopping it first if it is already
running) to cause the slave to connect to the master and register as a
semisynchronous slave:

     STOP SLAVE IO_THREAD;
     START SLAVE IO_THREAD;

If the I/O thread is already running and you do not restart it, the
slave continues to use asynchronous replication.

At server startup, the variables that control semisynchronous
replication can be set as command-line options or in an option file.  A
setting listed in an option file takes effect each time the server
starts.  For example, you can set the variables in 'my.cnf' files on the
master and slave sides as follows.

On the master:

     [mysqld]
     rpl_semi_sync_master_enabled=1
     rpl_semi_sync_master_timeout=1000 # 1 second

On each slave:

     [mysqld]
     rpl_semi_sync_slave_enabled=1


File: manual.info.tmp,  Node: replication-semisync-monitoring,  Prev: replication-semisync-installation,  Up: replication-semisync

17.3.8.3 Semisynchronous Replication Monitoring
...............................................

The plugins for the semisynchronous replication capability expose
several system and status variables that you can examine to determine
its configuration and operational state.

The system variable reflect how semisynchronous replication is
configured.  To check their values, use *note 'SHOW VARIABLES':
show-variables.:

     mysql> SHOW VARIABLES LIKE 'rpl_semi_sync%';

The status variables enable you to monitor the operation of
semisynchronous replication.  To check their values, use *note 'SHOW
STATUS': show-status.:

     mysql> SHOW STATUS LIKE 'Rpl_semi_sync%';

When the master switches between asynchronous or semisynchronous
replication due to commit-blocking timeout or a slave catching up, it
sets the value of the 'Rpl_semi_sync_master_status' status variable
appropriately.  Automatic fallback from semisynchronous to asynchronous
replication on the master means that it is possible for the
'rpl_semi_sync_master_enabled' system variable to have a value of 1 on
the master side even when semisynchronous replication is in fact not
operational at the moment.  You can monitor the
'Rpl_semi_sync_master_status' status variable to determine whether the
master currently is using asynchronous or semisynchronous replication.

To see how many semisynchronous slaves are connected, check
'Rpl_semi_sync_master_clients'.

The number of commits that have been acknowledged successfully or
unsuccessfully by slaves are indicated by the
'Rpl_semi_sync_master_yes_tx' and 'Rpl_semi_sync_master_no_tx'
variables.

On the slave side, 'Rpl_semi_sync_slave_status' indicates whether
semisynchronous replication currently is operational.


File: manual.info.tmp,  Node: replication-notes,  Prev: replication-solutions,  Up: replication

17.4 Replication Notes and Tips
===============================

* Menu:

* replication-features::         Replication Features and Issues
* replication-compatibility::    Replication Compatibility Between MySQL Versions
* replication-upgrade::          Upgrading a Replication Setup
* replication-problems::         Troubleshooting Replication
* replication-bugs::             How to Report Replication Bugs or Problems


File: manual.info.tmp,  Node: replication-features,  Next: replication-compatibility,  Prev: replication-notes,  Up: replication-notes

17.4.1 Replication Features and Issues
--------------------------------------

* Menu:

* replication-features-auto-increment::  Replication and AUTO_INCREMENT
* replication-features-blackhole::  Replication and BLACKHOLE Tables
* replication-features-charset::  Replication and Character Sets
* replication-features-checksum-table::  Replication and CHECKSUM TABLE
* replication-features-create-alter-drop-server::  Replication of CREATE SERVER, ALTER SERVER, and DROP SERVER
* replication-features-create-if-not-exists::  Replication of CREATE ... IF NOT EXISTS Statements
* replication-features-create-select::  Replication of CREATE TABLE ... SELECT Statements
* replication-features-current-user::  Replication of CURRENT_USER()
* replication-features-differing-tables::  Replication with Differing Table Definitions on Master and Slave
* replication-features-directory::  Replication and DIRECTORY Table Options
* replication-features-drop-if-exists::  Replication of DROP ... IF EXISTS Statements
* replication-features-floatvalues::  Replication and Floating-Point Values
* replication-features-flush::   Replication and FLUSH
* replication-features-functions::  Replication and System Functions
* replication-features-invoked::  Replication of Invoked Features
* replication-features-limit::   Replication and LIMIT
* replication-features-load-data::  Replication and LOAD DATA
* replication-features-logging::  Replication and the Slow Query Log
* replication-features-max-allowed-packet::  Replication and max_allowed_packet
* replication-features-memory::  Replication and MEMORY Tables
* replication-features-mysqldb::  Replication of the mysql System Database
* replication-features-optimizer::  Replication and the Query Optimizer
* replication-features-partitioning::  Replication and Partitioning
* replication-features-repair-table::  Replication and REPAIR TABLE
* replication-features-reserved-words::  Replication and Reserved Words
* replication-features-set-password::  SET PASSWORD and Row-Based Replication
* replication-features-shutdowns::  Replication and Master or Slave Shutdowns
* replication-features-slaveerrors::  Slave Errors During Replication
* replication-features-sql-mode::  Replication and Server SQL Mode
* replication-features-temptables::  Replication and Temporary Tables
* replication-features-timeout::  Replication Retries and Timeouts
* replication-features-timestamp::  Replication and TIMESTAMP
* replication-features-timezone::  Replication and Time Zones
* replication-features-transactions::  Replication and Transactions
* replication-features-triggers::  Replication and Triggers
* replication-features-truncate::  Replication and TRUNCATE TABLE
* replication-features-variables::  Replication and Variables
* replication-features-views::   Replication and Views

The following sections provide information about what is supported and
what is not in MySQL replication, and about specific issues and
situations that may occur when replicating certain statements.

Statement-based replication depends on compatibility at the SQL level
between the master and slave.  In others, successful SBR requires that
any SQL features used be supported by both the master and the slave
servers.  For example, if you use a feature on the master server that is
available only in MySQL 5.5 (or later), you cannot replicate to a slave
that uses MySQL 5.1 (or earlier).

Such incompatibilities also can occur within a release series when using
pre-production releases of MySQL. For example, the 'SLEEP()' function is
available beginning with MySQL 5.0.12.  If you use this function on the
master, you cannot replicate to a slave that uses MySQL 5.0.11 or
earlier.

For this reason, use Generally Available (GA) releases of MySQL for
statement-based replication in a production setting, since we do not
introduce new SQL statements or change their behavior within a given
release series once that series reaches GA release status.

If you are planning to use statement-based replication between MySQL 5.5
and a previous MySQL release series, it is also a good idea to consult
the edition of the 'MySQL Reference Manual' corresponding to the earlier
release series for information regarding the replication characteristics
of that series.

With MySQL's statement-based replication, there may be issues with
replicating stored routines or triggers.  You can avoid these issues by
using MySQL's row-based replication instead.  For a detailed list of
issues, see *note stored-programs-logging::.  For more information about
row-based logging and row-based replication, see *note
binary-log-formats::, and *note replication-formats::.

For additional information specific to replication and 'InnoDB', see
*note innodb-and-mysql-replication::.  For information relating to
replication with NDB Cluster, see *note mysql-cluster-replication::.


File: manual.info.tmp,  Node: replication-features-auto-increment,  Next: replication-features-blackhole,  Prev: replication-features,  Up: replication-features

17.4.1.1 Replication and AUTO_INCREMENT
.......................................

Statement-based replication of 'AUTO_INCREMENT', 'LAST_INSERT_ID()', and
*note 'TIMESTAMP': datetime. values is done correctly, subject to the
following exceptions:

   * When using statement-based replication prior to MySQL 5.5.30,
     'AUTO_INCREMENT' columns in tables on the slave must match the same
     columns on the master; that is, 'AUTO_INCREMENT' columns must be
     replicated to 'AUTO_INCREMENT' columns.  (Bug #12669186)

   * A statement invoking a trigger or function that causes an update to
     an 'AUTO_INCREMENT' column is not replicated correctly using
     statement-based replication.  In MySQL 5.5, such statements are
     marked as unsafe.  (Bug #45677)

   * An *note 'INSERT': insert. into a table that has a composite
     primary key that includes an 'AUTO_INCREMENT' column that is not
     the first column of this composite key is not safe for
     statement-based logging or replication.  Beginning with MySQL
     5.5.25, such statements are marked as unsafe.  (Bug #11754117, Bug
     #45670)

     This issue does not affect tables using the *note 'InnoDB':
     innodb-storage-engine. storage engine, since an 'InnoDB' table with
     an AUTO_INCREMENT column requires at least one key where the
     auto-increment column is the only or leftmost column.

   * Adding an 'AUTO_INCREMENT' column to a table with *note 'ALTER
     TABLE': alter-table. might not produce the same ordering of the
     rows on the slave and the master.  This occurs because the order in
     which the rows are numbered depends on the specific storage engine
     used for the table and the order in which the rows were inserted.
     If it is important to have the same order on the master and slave,
     the rows must be ordered before assigning an 'AUTO_INCREMENT'
     number.  Assuming that you want to add an 'AUTO_INCREMENT' column
     to a table 't1' that has columns 'col1' and 'col2', the following
     statements produce a new table 't2' identical to 't1' but with an
     'AUTO_INCREMENT' column:

          CREATE TABLE t2 LIKE t1;
          ALTER TABLE t2 ADD id INT AUTO_INCREMENT PRIMARY KEY;
          INSERT INTO t2 SELECT * FROM t1 ORDER BY col1, col2;

     *Important*:

     To guarantee the same ordering on both master and slave, the 'ORDER
     BY' clause must name _all_ columns of 't1'.

     The instructions just given are subject to the limitations of *note
     'CREATE TABLE ... LIKE': create-table-like.: Foreign key
     definitions are ignored, as are the 'DATA DIRECTORY' and 'INDEX
     DIRECTORY' table options.  If a table definition includes any of
     those characteristics, create 't2' using a *note 'CREATE TABLE':
     create-table. statement that is identical to the one used to create
     't1', but with the addition of the 'AUTO_INCREMENT' column.

     Regardless of the method used to create and populate the copy
     having the 'AUTO_INCREMENT' column, the final step is to drop the
     original table and then rename the copy:

          DROP t1;
          ALTER TABLE t2 RENAME t1;

     See also *note alter-table-problems::.


File: manual.info.tmp,  Node: replication-features-blackhole,  Next: replication-features-charset,  Prev: replication-features-auto-increment,  Up: replication-features

17.4.1.2 Replication and BLACKHOLE Tables
.........................................

The *note 'BLACKHOLE': blackhole-storage-engine. storage engine accepts
data but discards it and does not store it.  When performing binary
logging, all inserts to such tables are always logged, regardless of the
logging format in use.  Updates and deletes are handled differently
depending on whether statement based or row based logging is in use.
With the statement based logging format, all statements affecting
'BLACKHOLE' tables are logged, but their effects ignored.  When using
row-based logging, updates and deletes to such tables are simply
skipped--they are not written to the binary log.  In MySQL 5.5.32 and
later, a warning is logged whenever this occurs (Bug #13004581)

For this reason we recommend when you replicate to tables using the
*note 'BLACKHOLE': blackhole-storage-engine. storage engine that you
have the 'binlog_format' server variable set to 'STATEMENT', and not to
either 'ROW' or 'MIXED'.


File: manual.info.tmp,  Node: replication-features-charset,  Next: replication-features-checksum-table,  Prev: replication-features-blackhole,  Up: replication-features

17.4.1.3 Replication and Character Sets
.......................................

The following applies to replication between MySQL servers that use
different character sets:

   * If the master has databases with a character set different from the
     global 'character_set_server' value, you should design your *note
     'CREATE TABLE': create-table. statements so that they do not
     implicitly rely on the database default character set.  A good
     workaround is to state the character set and collation explicitly
     in *note 'CREATE TABLE': create-table. statements.


File: manual.info.tmp,  Node: replication-features-checksum-table,  Next: replication-features-create-alter-drop-server,  Prev: replication-features-charset,  Up: replication-features

17.4.1.4 Replication and CHECKSUM TABLE
.......................................

*note 'CHECKSUM TABLE': checksum-table. returns a checksum that is
calculated row by row, using a method that depends on the table row
storage format, which is not guaranteed to remain the same between MySQL
release series.  For example, the storage format for temporal types such
as *note 'TIME': time, *note 'DATETIME': datetime, and *note
'TIMESTAMP': datetime. changes in MySQL 5.6 prior to MySQL 5.6.5, so if
a 5.5 table is upgraded to MySQL 5.6, the checksum value may change.


File: manual.info.tmp,  Node: replication-features-create-alter-drop-server,  Next: replication-features-create-if-not-exists,  Prev: replication-features-checksum-table,  Up: replication-features

17.4.1.5 Replication of CREATE SERVER, ALTER SERVER, and DROP SERVER
....................................................................

In MySQL 5.5, the statements *note 'CREATE SERVER': create-server, *note
'ALTER SERVER': alter-server, and *note 'DROP SERVER': drop-server. are
not written to the binary log, regardless of the binary logging format
that is in use.


File: manual.info.tmp,  Node: replication-features-create-if-not-exists,  Next: replication-features-create-select,  Prev: replication-features-create-alter-drop-server,  Up: replication-features

17.4.1.6 Replication of CREATE ... IF NOT EXISTS Statements
...........................................................

MySQL applies these rules when various 'CREATE ... IF NOT EXISTS'
statements are replicated:

   * Every *note 'CREATE DATABASE IF NOT EXISTS': create-database.
     statement is replicated, whether or not the database already exists
     on the master.

   * Similarly, every *note 'CREATE TABLE IF NOT EXISTS': create-table.
     statement without a *note 'SELECT': select. is replicated, whether
     or not the table already exists on the master.  This includes *note
     'CREATE TABLE IF NOT EXISTS ... LIKE': create-table-like.
     Replication of *note 'CREATE TABLE IF NOT EXISTS ... SELECT':
     create-table-select. follows somewhat different rules; see *note
     replication-features-create-select::, for more information.

   * *note 'CREATE EVENT IF NOT EXISTS': create-event. is always
     replicated in MySQL 5.5, whether or not the event named in the
     statement already exists on the master.

See also Bug #45574.


File: manual.info.tmp,  Node: replication-features-create-select,  Next: replication-features-current-user,  Prev: replication-features-create-if-not-exists,  Up: replication-features

17.4.1.7 Replication of CREATE TABLE ... SELECT Statements
..........................................................

This section discusses how MySQL replicates *note 'CREATE TABLE ...
SELECT': create-table-select. statements.

These behaviors are not dependent on MySQL version:

   * *note 'CREATE TABLE ... SELECT': create-table-select. always
     performs an implicit commit (*note implicit-commit::).

   * If destination table does not exist, logging occurs as follows.  It
     does not matter whether 'IF NOT EXISTS' is present.

        * 'STATEMENT' or 'MIXED' format: The statement is logged as
          written.

        * 'ROW' format: The statement is logged as a *note 'CREATE
          TABLE': create-table. statement followed by a series of
          insert-row events.

   * If the statement fails, nothing is logged.  This includes the case
     that the destination table exists and 'IF NOT EXISTS' is not given.

When the destination table exists and 'IF NOT EXISTS' is given, MySQL
handles the statement in a version-dependent way.

In MySQL 5.1 before 5.1.51 and in MySQL 5.5 before 5.5.6 (this is the
original behavior):

   * 'STATEMENT' or 'MIXED' format: The statement is logged as written.

   * 'ROW' format: The statement is logged as a *note 'CREATE TABLE':
     create-table. statement followed by a series of insert-row events.

In MySQL 5.1 as of 5.1.51:

   * 'STATEMENT' or 'MIXED' format: The statement is logged as the
     equivalent pair of *note 'CREATE TABLE': create-table. and *note
     'INSERT INTO ... SELECT': insert-select. statements.

   * 'ROW' format: The statement is logged as a *note 'CREATE TABLE':
     create-table. statement followed by a series of insert-row events.

In MySQL 5.5 as of 5.5.6:

   * Nothing is inserted or logged.

These version dependencies arise due to a change in MySQL 5.5.6 in
handling of *note 'CREATE TABLE ... SELECT': create-table-select. not to
insert rows if the destination table already exists, and a change made
in MySQL 5.1.51 to preserve forward compatibility in replication of such
statements from a 5.1 master to a 5.5 slave.  For details, see *note
create-table-select::.

When using statement-based replication between a MySQL 5.6 or later
slave and a master running a previous version of MySQL, a *note 'CREATE
TABLE ... SELECT': create-table-select. statement causing changes in
other tables on the master fails on the slave, causing replication to
stop.  This is due to the fact that MySQL 5.6 does not allow a *note
'CREATE TABLE ... SELECT': create-table-select. statement to make any
changes in tables other than the table that is created by the
statement--a change in behavior from previous versions of MySQL, which
permitted these statements to do so.  To keep this from happening, you
should use row-based replication, rewrite the offending statement before
running it on the master, or upgrade the master to MySQL 5.6 (or later).
(If you choose to upgrade the master, keep in mind that such a *note
'CREATE TABLE ... SELECT': create-table-select. statement will fail
there as well, following the upgrade, unless the statement is rewritten
to remove any side effects on other tables.)  This is not an issue when
using row-based replication, because the statement is logged as a *note
'CREATE TABLE': create-table. statement with any changes to table data
logged as row-insert events (or possibly row-update events), rather than
as the entire *note 'CREATE TABLE ... SELECT': create-table-select.
statement.


File: manual.info.tmp,  Node: replication-features-current-user,  Next: replication-features-differing-tables,  Prev: replication-features-create-select,  Up: replication-features

17.4.1.8 Replication of CURRENT_USER()
......................................

The following statements support use of the 'CURRENT_USER()' function to
take the place of the name of (and, possibly, the host for) an affected
user or a definer; in such cases, 'CURRENT_USER()' is expanded where and
as needed:

   * *note 'DROP USER': drop-user.

   * *note 'RENAME USER': rename-user.

   * *note 'GRANT': grant.

   * *note 'REVOKE': revoke.

   * *note 'CREATE FUNCTION': create-function.

   * *note 'CREATE PROCEDURE': create-procedure.

   * *note 'CREATE TRIGGER': create-trigger.

   * *note 'CREATE EVENT': create-event.

   * *note 'CREATE VIEW': create-view.

   * *note 'ALTER EVENT': alter-event.

   * *note 'ALTER VIEW': alter-view.

   * *note 'SET PASSWORD': set-password.

When 'CURRENT_USER()' or 'CURRENT_USER' is used as the definer in any of
the statements *note 'CREATE FUNCTION': create-function, *note 'CREATE
PROCEDURE': create-procedure, *note 'CREATE TRIGGER': create-trigger,
*note 'CREATE EVENT': create-event, *note 'CREATE VIEW': create-view, or
*note 'ALTER VIEW': alter-view. when binary logging is enabled, the
function reference is expanded before it is written to the binary log,
so that the statement refers to the same user on both the master and the
slave when the statement is replicated.  'CURRENT_USER()' or
'CURRENT_USER' is also expanded prior to being written to the binary log
when used in *note 'DROP USER': drop-user, *note 'RENAME USER':
rename-user, *note 'GRANT': grant, *note 'REVOKE': revoke, or *note
'ALTER EVENT': alter-event.


File: manual.info.tmp,  Node: replication-features-differing-tables,  Next: replication-features-directory,  Prev: replication-features-current-user,  Up: replication-features

17.4.1.9 Replication with Differing Table Definitions on Master and Slave
.........................................................................

* Menu:

* replication-features-more-columns::  Replication with More Columns on Master or Slave
* replication-features-different-data-types::  Replication of Columns Having Different Data Types

Source and target tables for replication do not have to be identical.  A
table on the master can have more or fewer columns than the slave's copy
of the table.  In addition, corresponding table columns on the master
and the slave can use different data types, subject to certain
conditions.

*Note*:

Replication between tables which are partitioned differently from one
another is not supported.  See *note
replication-features-partitioning::.

In all cases where the source and target tables do not have identical
definitions, the database and table names must be the same on both the
master and the slave.  Additional conditions are discussed, with
examples, in the following two sections.


File: manual.info.tmp,  Node: replication-features-more-columns,  Next: replication-features-different-data-types,  Prev: replication-features-differing-tables,  Up: replication-features-differing-tables

17.4.1.10 Replication with More Columns on Master or Slave
..........................................................

You can replicate a table from the master to the slave such that the
master and slave copies of the table have differing numbers of columns,
subject to the following conditions:

   * Columns common to both versions of the table must be defined in the
     same order on the master and the slave.

     (This is true even if both tables have the same number of columns.)

   * Columns common to both versions of the table must be defined before
     any additional columns.

     This means that executing an *note 'ALTER TABLE': alter-table.
     statement on the slave where a new column is inserted into the
     table within the range of columns common to both tables causes
     replication to fail, as shown in the following example:

     Suppose that a table 't', existing on the master and the slave, is
     defined by the following *note 'CREATE TABLE': create-table.
     statement:

          CREATE TABLE t (
              c1 INT,
              c2 INT,
              c3 INT
          );

     Suppose that the *note 'ALTER TABLE': alter-table. statement shown
     here is executed on the slave:

          ALTER TABLE t ADD COLUMN cnew1 INT AFTER c3;

     The previous *note 'ALTER TABLE': alter-table. is permitted on the
     slave because the columns 'c1', 'c2', and 'c3' that are common to
     both versions of table 't' remain grouped together in both versions
     of the table, before any columns that differ.

     However, the following *note 'ALTER TABLE': alter-table. statement
     cannot be executed on the slave without causing replication to
     break:

          ALTER TABLE t ADD COLUMN cnew2 INT AFTER c2;

     Replication fails after execution on the slave of the *note 'ALTER
     TABLE': alter-table. statement just shown, because the new column
     'cnew2' comes between columns common to both versions of 't'.

   * Each 'extra' column in the version of the table having more columns
     must have a default value.

     *Note*:

     A column's default value is determined by a number of factors,
     including its type, whether it is defined with a 'DEFAULT' option,
     whether it is declared as 'NULL', and the server SQL mode in effect
     at the time of its creation; for more information, see *note
     data-type-defaults::).

In addition, when the slave's copy of the table has more columns than
the master's copy, each column common to the tables must use the same
data type in both tables.

Examples

The following examples illustrate some valid and invalid table
definitions:

More columns on the master

The following table definitions are valid and replicate correctly:

     master> CREATE TABLE t1 (c1 INT, c2 INT, c3 INT);
     slave>  CREATE TABLE t1 (c1 INT, c2 INT);

The following table definitions would raise an error because the
definitions of the columns common to both versions of the table are in a
different order on the slave than they are on the master:

     master> CREATE TABLE t1 (c1 INT, c2 INT, c3 INT);
     slave>  CREATE TABLE t1 (c2 INT, c1 INT);

The following table definitions would also raise an error because the
definition of the extra column on the master appears before the
definitions of the columns common to both versions of the table:

     master> CREATE TABLE t1 (c3 INT, c1 INT, c2 INT);
     slave>  CREATE TABLE t1 (c1 INT, c2 INT);

More columns on the slave

The following table definitions are valid and replicate correctly:

     master> CREATE TABLE t1 (c1 INT, c2 INT);
     slave>  CREATE TABLE t1 (c1 INT, c2 INT, c3 INT);

The following definitions raise an error because the columns common to
both versions of the table are not defined in the same order on both the
master and the slave:

     master> CREATE TABLE t1 (c1 INT, c2 INT);
     slave>  CREATE TABLE t1 (c2 INT, c1 INT, c3 INT);

The following table definitions also raise an error because the
definition for the extra column in the slave's version of the table
appears before the definitions for the columns which are common to both
versions of the table:

     master> CREATE TABLE t1 (c1 INT, c2 INT);
     slave>  CREATE TABLE t1 (c3 INT, c1 INT, c2 INT);

The following table definitions fail because the slave's version of the
table has additional columns compared to the master's version, and the
two versions of the table use different data types for the common column
'c2':

     master> CREATE TABLE t1 (c1 INT, c2 BIGINT);
     slave>  CREATE TABLE t1 (c1 INT, c2 INT, c3 INT);


File: manual.info.tmp,  Node: replication-features-different-data-types,  Prev: replication-features-more-columns,  Up: replication-features-differing-tables

17.4.1.11 Replication of Columns Having Different Data Types
............................................................

Corresponding columns on the master's and the slave's copies of the same
table ideally should have the same data type.  However, beginning with
MySQL 5.1.21, this is not always strictly enforced, as long as certain
conditions are met.

All other things being equal, it is always possible to replicate from a
column of a given data type to another column of the same type and same
size or width, where applicable, or larger.  For example, you can
replicate from a 'CHAR(10)' column to another 'CHAR(10)', or from a
'CHAR(10)' column to a 'CHAR(25)' column without any problems.  In
certain cases, it also possible to replicate from a column having one
data type (on the master) to a column having a different data type (on
the slave); when the data type of the master's version of the column is
promoted to a type that is the same size or larger on the slave, this is
known as _attribute promotion_.

Attribute promotion can be used with both statement-based and row-based
replication, and is not dependent on the storage engine used by either
the master or the slave.  However, the choice of logging format does
have an effect on the type conversions that are permitted; the
particulars are discussed later in this section.

*Important*:

Whether you use statement-based or row-based replication, the slave's
copy of the table cannot contain more columns than the master's copy if
you wish to employ attribute promotion.

Statement-based replication

When using statement-based replication, a simple rule of thumb to follow
is, 'If the statement run on the master would also execute successfully
on the slave, it should also replicate successfully'.  In other words,
if the statement uses a value that is compatible with the type of a
given column on the slave, the statement can be replicated.  For
example, you can insert any value that fits in a 'TINYINT' column into a
'BIGINT' column as well; it follows that, even if you change the type of
a 'TINYINT' column in the slave's copy of a table to 'BIGINT', any
insert into that column on the master that succeeds should also succeed
on the slave, since it is impossible to have a legal 'TINYINT' value
that is large enough to exceed a 'BIGINT' column.

Prior to MySQL 5.5.30, when using statement-based replication,
'AUTO_INCREMENT' columns were required to be the same on both the master
and the slave; otherwise, updates could be applied to the wrong table on
the slave.  (Bug #12669186)

Row-based replication: attribute promotion and demotion

Formerly, due to the fact that in row-based replication changes rather
than statements are replicated, and that these changes are transmitted
using formats that do not always map directly to MySQL server column
data types, you could not replicate between different subtypes of the
same general type (for example, from 'TINYINT' to 'BIGINT', both 'INT'
subtypes).  However, beginning with MySQL 5.5.3, MySQL Replication
supports attribute promotion and demotion between smaller data types and
larger types.  It is also possible to specify whether or not to permit
lossy (truncated) or non-lossy conversions of demoted column values, as
explained later in this section.

Lossy and non-lossy conversions

In the event that the target type cannot represent the value being
inserted, a decision must be made on how to handle the conversion.  If
we permit the conversion but truncate (or otherwise modify) the source
value to achieve a 'fit' in the target column, we make what is known as
a _lossy conversion_.  A conversion which does not require truncation or
similar modifications to fit the source column value in the target
column is a _non-lossy_ conversion.

Type conversion modes (slave_type_conversions variable)

The setting of the 'slave_type_conversions' global server variable
controls the type conversion mode used on the slave.  This variable
takes a set of values from the following table, which shows the effects
of each mode on the slave's type-conversion behavior:

Mode                      Effect
                          
'ALL_LOSSY'               
                          In this mode, type conversions that would
                          mean loss of information are permitted.
                          
                          This does not imply that non-lossy
                          conversions are permitted, merely that only
                          cases requiring either lossy conversions or
                          no conversion at all are permitted; for
                          example, enabling _only_ this mode permits an
                          'INT' column to be converted to 'TINYINT' (a
                          lossy conversion), but not a 'TINYINT' column
                          to an 'INT' column (non-lossy).  Attempting
                          the latter conversion in this case would
                          cause replication to stop with an error on
                          the slave.
                          
'ALL_NON_LOSSY'           
                          This mode permits conversions that do not
                          require truncation or other special handling
                          of the source value; that is, it permits
                          conversions where the target type has a wider
                          range than the source type.
                          
                          Setting this mode has no bearing on whether
                          lossy conversions are permitted; this is
                          controlled with the 'ALL_LOSSY' mode.  If
                          only 'ALL_NON_LOSSY' is set, but not
                          'ALL_LOSSY', then attempting a conversion
                          that would result in the loss of data (such
                          as 'INT' to 'TINYINT', or 'CHAR(25)' to
                          'VARCHAR(20)') causes the slave to stop with
                          an error.
                          
'ALL_LOSSY,ALL_NON_LOSSY' 
                          When this mode is set, all supported type
                          conversions are permitted, whether or not
                          they are lossy conversions.
                          
[_empty_]                 
                          When 'slave_type_conversions' is not set, no
                          attribute promotion or demotion is permitted;
                          this means that all columns in the source and
                          target tables must be of the same types.
                          
                          This mode is the default.

Changing the type conversion mode requires restarting the slave with the
new 'slave_type_conversions' setting.

Supported conversions

Supported conversions between different but similar data types are shown
in the following list:

   * Between any of the integer types *note 'TINYINT': integer-types,
     *note 'SMALLINT': integer-types, *note 'MEDIUMINT': integer-types,
     *note 'INT': integer-types, and *note 'BIGINT': integer-types.

     This includes conversions between the signed and unsigned versions
     of these types.

     Lossy conversions are made by truncating the source value to the
     maximum (or minimum) permitted by the target column.  For insuring
     non-lossy conversions when going from unsigned to signed types, the
     target column must be large enough to accommodate the range of
     values in the source column.  For example, you can demote 'TINYINT
     UNSIGNED' non-lossily to 'SMALLINT', but not to 'TINYINT'.

   * Between any of the decimal types *note 'DECIMAL':
     fixed-point-types, *note 'FLOAT': floating-point-types, *note
     'DOUBLE': floating-point-types, and *note 'NUMERIC':
     fixed-point-types.

     'FLOAT' to 'DOUBLE' is a non-lossy conversion; 'DOUBLE' to 'FLOAT'
     can only be handled lossily.  A conversion from 'DECIMAL(M,D)' to
     'DECIMAL(M',D')' where 'D' >= D' and '(M'-D') >= (M'-D) are
     non-lossy; for any case where 'M' < M', 'D' < D', or both, only a
     lossy conversion can be made.

     For any of the decimal types, if a value to be stored cannot be fit
     in the target type, the value is rounded down according to the
     rounding rules defined for the server elsewhere in the
     documentation.  See *note precision-math-rounding::, for
     information about how this is done for decimal types.

   * Between any of the string types *note 'CHAR': char, *note
     'VARCHAR': char, and *note 'TEXT': blob, including conversions
     between different widths.

     Conversion of a 'CHAR', 'VARCHAR', or 'TEXT' to a 'CHAR',
     'VARCHAR', or 'TEXT' column the same size or larger is never lossy.
     Lossy conversion is handled by inserting only the first N
     characters of the string on the slave, where N is the width of the
     target column.

     *Important*:

     Replication between columns using different character sets is not
     supported.

   * Between any of the binary data types *note 'BINARY':
     binary-varbinary, *note 'VARBINARY': binary-varbinary, and *note
     'BLOB': blob, including conversions between different widths.

     Conversion of a 'BINARY', 'VARBINARY', or 'BLOB' to a 'BINARY',
     'VARBINARY', or 'BLOB' column the same size or larger is never
     lossy.  Lossy conversion is handled by inserting only the first N
     bytes of the string on the slave, where N is the width of the
     target column.

   * Between any 2 *note 'BIT': bit-type. columns of any 2 sizes.

     When inserting a value from a 'BIT(M)' column into a 'BIT(M')'
     column, where 'M' > M', the most significant bits of the 'BIT(M')'
     columns are cleared (set to zero) and the M bits of the 'BIT(M)'
     value are set as the least significant bits of the 'BIT(M')'
     column.

     When inserting a value from a source 'BIT(M)' column into a target
     'BIT(M')' column, where 'M' < M', the maximum possible value for
     the 'BIT(M')' column is assigned; in other words, an 'all-set'
     value is assigned to the target column.

Conversions between types not in the previous list are not permitted.

Replication type conversions in MySQL 5.5.3 and earlier

Prior to MySQL 5.5.3, with row-based binary logging, you could not
replicate between different 'INT' subtypes, such as from 'TINYINT' to
'BIGINT', because changes to columns of these types were represented
differently from one another in the binary log when using row-based
logging.  (However, you could replicate from 'BLOB' to 'TEXT' using
row-based replication because changes to 'BLOB' and 'TEXT' columns were
represented using the same format in the binary log.)

Supported conversions for attribute promotion when using row-based
replication prior to MySQL 5.5.3 are shown in the following table:

From (Master)                        To (Slave)
                                     
*note 'BINARY': binary-varbinary.    *note 'CHAR': char.
                                     
*note 'BLOB': blob.                  *note 'TEXT': blob.
                                     
*note 'CHAR': char.                  *note 'BINARY': binary-varbinary.
                                     
*note 'DECIMAL': fixed-point-types.  *note 'NUMERIC': fixed-point-types.
                                     
*note 'NUMERIC': fixed-point-types.  *note 'DECIMAL': fixed-point-types.
                                     
*note 'TEXT': blob.                  *note 'BLOB': blob.
                                     
*note 'VARBINARY': binary-varbinary. *note 'VARCHAR': char.
                                     
*note 'VARCHAR': char.               *note 'VARBINARY': binary-varbinary.

*Note*:

In all cases, the size or width of the column on the slave must be equal
to or greater than that of the column on the master.  For example, you
could replicate from a 'CHAR(10)' column on the master to a column that
used 'BINARY(10)' or 'BINARY(25)' on the slave, but you could not
replicate from a 'CHAR(10)' column on the master to 'BINARY(5)' column
on the slave.

Any unique index (including primary keys) having a prefix must use a
prefix of the same length on both master and slave; in such cases,
differing prefix lengths are disallowed.  It is possible to use a
nonunique index whose prefix length differs between master and slave,
but this can cause serious performance issues, particularly when the
prefix used on the master is longer.  This is due to the fact that 2
unique prefixes of a given length may no longer be unique at a shorter
length; for example, the words _catalogue_ and _catamount_ have the
5-character prefixes _catal_ and _catam_, respectively, but share the
same 4-character prefix (_cata_).  This can lead to queries that use
such indexes executing less efficiently on the slave, when a shorter
prefix is employed in the slave' definition of the same index than on
the master.

For *note 'DECIMAL': fixed-point-types. and *note 'NUMERIC':
fixed-point-types. columns, both the mantissa (_M_) and the number of
decimals (_D_) must be the same size or larger on the slave as compared
with the master.  For example, replication from a 'NUMERIC(5,4)' to a
'DECIMAL(6,4)' worked, but not from a 'NUMERIC(5,4)' to a
'DECIMAL(5,3)'.

Prior to MySQL 5.5.3, MySQL replication did not support attribute
promotion of any of the following data types to or from any other data
type when using row-based replication:

   * *note 'INT': integer-types. (including 'TINYINT', 'SMALLINT',
     'MEDIUMINT', 'BIGINT').

     Promotion between *note 'INT': integer-types. subtypes--for
     example, from 'SMALLINT' to 'BIGINT'--was also not supported prior
     to MySQL 5.5.3.

   * *note 'SET': set. or *note 'ENUM': enum.

   * *note 'FLOAT': floating-point-types. or *note 'DOUBLE':
     floating-point-types.

   * All of the data types relating to dates, times, or both: *note
     'DATE': datetime, *note 'TIME': time, *note 'DATETIME': datetime,
     *note 'TIMESTAMP': datetime, and *note 'YEAR': year.


File: manual.info.tmp,  Node: replication-features-directory,  Next: replication-features-drop-if-exists,  Prev: replication-features-differing-tables,  Up: replication-features

17.4.1.12 Replication and DIRECTORY Table Options
.................................................

If a 'DATA DIRECTORY' or 'INDEX DIRECTORY' table option is used in a
*note 'CREATE TABLE': create-table. statement on the master server, the
table option is also used on the slave.  This can cause problems if no
corresponding directory exists in the slave host file system or if it
exists but is not accessible to the slave server.  This can be
overridden by using the 'NO_DIR_IN_CREATE' server SQL mode on the slave,
which causes the slave to ignore the 'DATA DIRECTORY' and 'INDEX
DIRECTORY' table options when replicating *note 'CREATE TABLE':
create-table. statements.  The result is that 'MyISAM' data and index
files are created in the table's database directory.

For more information, see *note sql-mode::.


File: manual.info.tmp,  Node: replication-features-drop-if-exists,  Next: replication-features-floatvalues,  Prev: replication-features-directory,  Up: replication-features

17.4.1.13 Replication of DROP ... IF EXISTS Statements
......................................................

The *note 'DROP DATABASE IF EXISTS': drop-database, *note 'DROP TABLE IF
EXISTS': drop-table, and *note 'DROP VIEW IF EXISTS': drop-view.
statements are always replicated, even if the database, table, or view
to be dropped does not exist on the master.  This is to ensure that the
object to be dropped no longer exists on either the master or the slave,
once the slave has caught up with the master.

'DROP ... IF EXISTS' statements for stored programs (stored procedures
and functions, triggers, and events) are also replicated, even if the
stored program to be dropped does not exist on the master.


File: manual.info.tmp,  Node: replication-features-floatvalues,  Next: replication-features-flush,  Prev: replication-features-drop-if-exists,  Up: replication-features

17.4.1.14 Replication and Floating-Point Values
...............................................

With statement-based replication, values are converted from decimal to
binary.  Because conversions between decimal and binary representations
of them may be approximate, comparisons involving floating-point values
are inexact.  This is true for operations that use floating-point values
explicitly, or that use values that are converted to floating-point
implicitly.  Comparisons of floating-point values might yield different
results on master and slave servers due to differences in computer
architecture, the compiler used to build MySQL, and so forth.  See *note
type-conversion::, and *note problems-with-float::.


File: manual.info.tmp,  Node: replication-features-flush,  Next: replication-features-functions,  Prev: replication-features-floatvalues,  Up: replication-features

17.4.1.15 Replication and FLUSH
...............................

Some forms of the *note 'FLUSH': flush. statement are not logged because
they could cause problems if replicated to a slave: 'FLUSH LOGS', 'FLUSH
MASTER', 'FLUSH SLAVE', and 'FLUSH TABLES WITH READ LOCK'.  For a syntax
example, see *note flush::.  The 'FLUSH TABLES', *note 'ANALYZE TABLE':
analyze-table, *note 'OPTIMIZE TABLE': optimize-table, and *note 'REPAIR
TABLE': repair-table. statements are written to the binary log and thus
replicated to slaves.  This is not normally a problem because these
statements do not modify table data.

However, this behavior can cause difficulties under certain
circumstances.  If you replicate the privilege tables in the 'mysql'
database and update those tables directly without using *note 'GRANT':
grant, you must issue a 'FLUSH PRIVILEGES' on the slaves to put the new
privileges into effect.  In addition, if you use 'FLUSH TABLES' when
renaming a 'MyISAM' table that is part of a 'MERGE' table, you must
issue 'FLUSH TABLES' manually on the slaves.  These statements are
written to the binary log unless you specify 'NO_WRITE_TO_BINLOG' or its
alias 'LOCAL'.


File: manual.info.tmp,  Node: replication-features-functions,  Next: replication-features-invoked,  Prev: replication-features-flush,  Up: replication-features

17.4.1.16 Replication and System Functions
..........................................

Certain functions do not replicate well under some conditions:

   * The 'USER()', 'CURRENT_USER()' (or 'CURRENT_USER'), 'UUID()',
     'VERSION()', and 'LOAD_FILE()' functions are replicated without
     change and thus do not work reliably on the slave unless row-based
     replication is enabled.  (See *note replication-formats::.)

     'USER()' and 'CURRENT_USER()' are automatically replicated using
     row-based replication when using 'MIXED' mode, and generate a
     warning in 'STATEMENT' mode.  (Bug #28086) (See also *note
     replication-features-current-user::.)  This is also true for
     'VERSION()' and 'RAND()'.

   * For 'NOW()', the binary log includes the timestamp.  This means
     that the value _as returned by the call to this function on the
     master_ is replicated to the slave.  This can lead to a possibly
     unexpected result when replicating between MySQL servers in
     different time zones.  Suppose that the master is located in New
     York, the slave is located in Stockholm, and both servers are using
     local time.  Suppose further that, on the master, you create a
     table 'mytable', perform an *note 'INSERT': insert. statement on
     this table, and then select from the table, as shown here:

          mysql> CREATE TABLE mytable (mycol TEXT);
          Query OK, 0 rows affected (0.06 sec)

          mysql> INSERT INTO mytable VALUES ( NOW() );
          Query OK, 1 row affected (0.00 sec)

          mysql> SELECT * FROM mytable;
          +---------------------+
          | mycol               |
          +---------------------+
          | 2009-09-01 12:00:00 |
          +---------------------+
          1 row in set (0.00 sec)

     Local time in Stockholm is 6 hours later than in New York; so, if
     you issue 'SELECT NOW()' on the slave at that exact same instant,
     the value '2009-09-01 18:00:00' is returned.  For this reason, if
     you select from the slave's copy of 'mytable' after the *note
     'CREATE TABLE': create-table. and *note 'INSERT': insert.
     statements just shown have been replicated, you might expect
     'mycol' to contain the value '2009-09-01 18:00:00'.  However, this
     is not the case; when you select from the slave's copy of
     'mytable', you obtain exactly the same result as on the master:

          mysql> SELECT * FROM mytable;
          +---------------------+
          | mycol               |
          +---------------------+
          | 2009-09-01 12:00:00 |
          +---------------------+
          1 row in set (0.00 sec)

     Unlike 'NOW()', the 'SYSDATE()' function is not replication-safe
     because it is not affected by 'SET TIMESTAMP' statements in the
     binary log and is nondeterministic if statement-based logging is
     used.  This is not a problem if row-based logging is used.

     An alternative is to use the '--sysdate-is-now' option to cause
     'SYSDATE()' to be an alias for 'NOW()'.  This must be done on the
     master and the slave to work correctly.  In such cases, a warning
     is still issued by this function, but can safely be ignored as long
     as '--sysdate-is-now' is used on both the master and the slave.

     Beginning with MySQL 5.5.1, 'SYSDATE()' is automatically replicated
     using row-based replication when using 'MIXED' mode, and generates
     a warning in 'STATEMENT' mode.  (Bug #47995)

     See also *note replication-features-timezone::.

   * _The following restriction applies to statement-based replication
     only, not to row-based replication._  The 'GET_LOCK()',
     'RELEASE_LOCK()', 'IS_FREE_LOCK()', and 'IS_USED_LOCK()' functions
     that handle user-level locks are replicated without the slave
     knowing the concurrency context on the master.  Therefore, these
     functions should not be used to insert into a master table because
     the content on the slave would differ.  For example, do not issue a
     statement such as 'INSERT INTO mytable VALUES(GET_LOCK(...))'.

     Beginning with MySQL 5.5.1, these functions are automatically
     replicated using row-based replication when using 'MIXED' mode, and
     generate a warning in 'STATEMENT' mode.  (Bug #47995)

As a workaround for the preceding limitations when statement-based
replication is in effect, you can use the strategy of saving the
problematic function result in a user variable and referring to the
variable in a later statement.  For example, the following single-row
*note 'INSERT': insert. is problematic due to the reference to the
'UUID()' function:

     INSERT INTO t VALUES(UUID());

To work around the problem, do this instead:

     SET @my_uuid = UUID();
     INSERT INTO t VALUES(@my_uuid);

That sequence of statements replicates because the value of '@my_uuid'
is stored in the binary log as a user-variable event prior to the *note
'INSERT': insert. statement and is available for use in the *note
'INSERT': insert.

The same idea applies to multiple-row inserts, but is more cumbersome to
use.  For a two-row insert, you can do this:

     SET @my_uuid1 = UUID(); @my_uuid2 = UUID();
     INSERT INTO t VALUES(@my_uuid1),(@my_uuid2);

However, if the number of rows is large or unknown, the workaround is
difficult or impracticable.  For example, you cannot convert the
following statement to one in which a given individual user variable is
associated with each row:

     INSERT INTO t2 SELECT UUID(), * FROM t1;

Within a stored function, 'RAND()' replicates correctly as long as it is
invoked only once during the execution of the function.  (You can
consider the function execution timestamp and random number seed as
implicit inputs that are identical on the master and slave.)

The 'FOUND_ROWS()' and 'ROW_COUNT()' functions are not replicated
reliably using statement-based replication.  A workaround is to store
the result of the function call in a user variable, and then use that in
the *note 'INSERT': insert. statement.  For example, if you wish to
store the result in a table named 'mytable', you might normally do so
like this:

     SELECT SQL_CALC_FOUND_ROWS FROM mytable LIMIT 1;
     INSERT INTO mytable VALUES( FOUND_ROWS() );

However, if you are replicating 'mytable', you should use *note 'SELECT
... INTO': select-into, and then store the variable in the table, like
this:

     SELECT SQL_CALC_FOUND_ROWS INTO @found_rows FROM mytable LIMIT 1;
     INSERT INTO mytable VALUES(@found_rows);

In this way, the user variable is replicated as part of the context, and
applied on the slave correctly.

These functions are automatically replicated using row-based replication
when using 'MIXED' mode, and generate a warning in 'STATEMENT' mode.
(Bug #12092, Bug #30244)

Prior to MySQL 5.5.35, the value of 'LAST_INSERT_ID()' was not
replicated correctly if any filtering options such as
'--replicate-ignore-db' and '--replicate-do-table' were enabled on the
slave.  (Bug #17234370, BUG# 69861)


File: manual.info.tmp,  Node: replication-features-invoked,  Next: replication-features-limit,  Prev: replication-features-functions,  Up: replication-features

17.4.1.17 Replication of Invoked Features
.........................................

Replication of invoked features such as user-defined functions (UDFs)
and stored programs (stored procedures and functions, triggers, and
events) provides the following characteristics:

   * The effects of the feature are always replicated.

   * The following statements are replicated using statement-based
     replication:

        * *note 'CREATE EVENT': create-event.

        * *note 'ALTER EVENT': alter-event.

        * *note 'DROP EVENT': drop-event.

        * *note 'CREATE PROCEDURE': create-procedure.

        * *note 'DROP PROCEDURE': drop-procedure.

        * *note 'CREATE FUNCTION': create-function.

        * *note 'DROP FUNCTION': drop-function.

        * *note 'CREATE TRIGGER': create-trigger.

        * *note 'DROP TRIGGER': drop-trigger.

     However, the _effects_ of features created, modified, or dropped
     using these statements are replicated using row-based replication.

     *Note*:

     Attempting to replicate invoked features using statement-based
     replication produces the warning 'Statement is not safe to log in
     statement format'.  For example, trying to replicate a UDF with
     statement-based replication generates this warning because it
     currently cannot be determined by the MySQL server whether the UDF
     is deterministic.  If you are absolutely certain that the invoked
     feature's effects are deterministic, you can safely disregard such
     warnings.

   * 
     In the case of *note 'CREATE EVENT': create-event. and *note 'ALTER
     EVENT': alter-event.:

        * The status of the event is set to 'SLAVESIDE_DISABLED' on the
          slave regardless of the state specified (this does not apply
          to *note 'DROP EVENT': drop-event.).

        * The master on which the event was created is identified on the
          slave by its server ID. The 'ORIGINATOR' column in *note
          'INFORMATION_SCHEMA.EVENTS': events-table. and the
          'originator' column in 'mysql.event' store this information.
          See *note events-table::, and *note show-events::, for more
          information.

   * The feature implementation resides on the slave in a renewable
     state so that if the master fails, the slave can be used as the
     master without loss of event processing.

To determine whether there are any scheduled events on a MySQL server
that were created on a different server (that was acting as a
replication master), query the *note 'INFORMATION_SCHEMA.EVENTS':
events-table. table in a manner similar to what is shown here:

     SELECT EVENT_SCHEMA, EVENT_NAME
         FROM INFORMATION_SCHEMA.EVENTS
         WHERE STATUS = 'SLAVESIDE_DISABLED';

Alternatively, you can use the *note 'SHOW EVENTS': show-events.
statement, like this:

     SHOW EVENTS
         WHERE STATUS = 'SLAVESIDE_DISABLED';

When promoting a replication slave having such events to a replication
master, you must enable each event using *note 'ALTER EVENT EVENT_NAME
ENABLE': alter-event, where EVENT_NAME is the name of the event.

If more than one master was involved in creating events on this slave,
and you wish to identify events that were created only on a given master
having the server ID MASTER_ID, modify the previous query on the *note
'EVENTS': events-table. table to include the 'ORIGINATOR' column, as
shown here:

     SELECT EVENT_SCHEMA, EVENT_NAME, ORIGINATOR
         FROM INFORMATION_SCHEMA.EVENTS
         WHERE STATUS = 'SLAVESIDE_DISABLED'
         AND   ORIGINATOR = 'MASTER_ID'

You can employ 'ORIGINATOR' with the *note 'SHOW EVENTS': show-events.
statement in a similar fashion:

     SHOW EVENTS
         WHERE STATUS = 'SLAVESIDE_DISABLED'
         AND   ORIGINATOR = 'MASTER_ID'

Before enabling events that were replicated from the master, you should
disable the MySQL Event Scheduler on the slave (using a statement such
as 'SET GLOBAL event_scheduler = OFF;'), run any necessary *note 'ALTER
EVENT': alter-event. statements, restart the server, then re-enable the
Event Scheduler on the slave afterward (using a statement such as 'SET
GLOBAL event_scheduler = ON;')-

If you later demote the new master back to being a replication slave,
you must disable manually all events enabled by the *note 'ALTER EVENT':
alter-event. statements.  You can do this by storing in a separate table
the event names from the *note 'SELECT': select. statement shown
previously, or using *note 'ALTER EVENT': alter-event. statements to
rename the events with a common prefix such as 'replicated_' to identify
them.

If you rename the events, then when demoting this server back to being a
replication slave, you can identify the events by querying the *note
'EVENTS': events-table. table, as shown here:

     SELECT CONCAT(EVENT_SCHEMA, '.', EVENT_NAME) AS 'Db.Event'
           FROM INFORMATION_SCHEMA.EVENTS
           WHERE INSTR(EVENT_NAME, 'replicated_') = 1;


File: manual.info.tmp,  Node: replication-features-limit,  Next: replication-features-load-data,  Prev: replication-features-invoked,  Up: replication-features

17.4.1.18 Replication and LIMIT
...............................

Statement-based replication of 'LIMIT' clauses in *note 'DELETE':
delete, *note 'UPDATE': update, and *note 'INSERT ... SELECT':
insert-select. statements is unsafe since the order of the rows affected
is not defined.  (Such statements can be replicated correctly with
statement-based replication only if they also contain an 'ORDER BY'
clause.)  When such a statement is encountered:

   * When using 'STATEMENT' mode, a warning that the statement is not
     safe for statement-based replication is now issued.

     When using 'STATEMENT' mode, warnings are issued for DML statements
     containing 'LIMIT' even when they also have an 'ORDER BY' clause
     (and so are made deterministic).  This is a known issue.  (Bug
     #42851)

   * When using 'MIXED' mode, the statement is now automatically
     replicated using row-based mode.


File: manual.info.tmp,  Node: replication-features-load-data,  Next: replication-features-logging,  Prev: replication-features-limit,  Up: replication-features

17.4.1.19 Replication and LOAD DATA
...................................

In MySQL 5.5.6 and later, *note 'LOAD DATA': load-data. is considered
unsafe for statement-based logging (see *note
replication-rbr-safe-unsafe::).  When 'binlog_format=MIXED' is set, the
statement is logged in row-based format.  When 'binlog_format=STATEMENT'
is set, note that *note 'LOAD DATA': load-data. does not generate a
warning, unlike other unsafe statements.

When *note 'mysqlbinlog': mysqlbinlog. reads log events for *note 'LOAD
DATA': load-data. statements logged in statement-based format, a
generated local file is created in a temporary directory.  These
temporary files are not automatically removed by *note 'mysqlbinlog':
mysqlbinlog. or any other MySQL program.  If you do use *note 'LOAD
DATA': load-data. statements with statement-based binary logging, you
should delete the temporary files yourself after you no longer need the
statement log.  For more information, see *note mysqlbinlog::.


File: manual.info.tmp,  Node: replication-features-logging,  Next: replication-features-max-allowed-packet,  Prev: replication-features-load-data,  Up: replication-features

17.4.1.20 Replication and the Slow Query Log
............................................

In older versions of MySQL, replication slaves did not write replicated
queries to the slow query log, even if the same queries were written to
the slow query log on the master.  This is no longer an issue in MySQL
5.5.  (Bug #23300)


File: manual.info.tmp,  Node: replication-features-max-allowed-packet,  Next: replication-features-memory,  Prev: replication-features-logging,  Up: replication-features

17.4.1.21 Replication and max_allowed_packet
............................................

'max_allowed_packet' sets an upper limit on the size of any single
message between the MySQL server and clients, including replication
slaves.  If you are replicating large column values (such as might be
found in *note 'TEXT': blob. or *note 'BLOB': blob. columns) and
'max_allowed_packet' is too small on the master, the master fails with
an error, and the slave shuts down the I/O thread.  If
'max_allowed_packet' is too small on the slave, this also causes the
slave to stop the I/O thread.

Row-based replication currently sends all columns and column values for
updated rows from the master to the slave, including values of columns
that were not actually changed by the update.  This means that, when you
are replicating large column values using row-based replication, you
must take care to set 'max_allowed_packet' large enough to accommodate
the largest row in any table to be replicated, even if you are
replicating updates only, or you are inserting only relatively small
values.


File: manual.info.tmp,  Node: replication-features-memory,  Next: replication-features-mysqldb,  Prev: replication-features-max-allowed-packet,  Up: replication-features

17.4.1.22 Replication and MEMORY Tables
.......................................

When a master server shuts down and restarts, its *note 'MEMORY':
memory-storage-engine. tables become empty.  To replicate this effect to
slaves, the first time that the master uses a given *note 'MEMORY':
memory-storage-engine. table after startup, it logs an event that
notifies slaves that the table must to be emptied by writing a *note
'DELETE': delete. statement for that table to the binary log.

When a slave server shuts down and restarts, its *note 'MEMORY':
memory-storage-engine. tables become empty.  This causes the slave to be
out of synchrony with the master and may lead to other failures or cause
the slave to stop:

   * Row-format updates and deletes received from the master may fail
     with 'Can't find record in 'MEMORY_TABLE''.

   * Statements such as *note 'INSERT INTO ... SELECT FROM
     MEMORY_TABLE': insert-select. may insert a different set of rows on
     the master and slave.

The safe way to restart a slave that is replicating *note 'MEMORY':
memory-storage-engine. tables is to first drop or delete all rows from
the *note 'MEMORY': memory-storage-engine. tables on the master and wait
until those changes have replicated to the slave.  Then it is safe to
restart the slave.

An alternative restart method may apply in some cases.  When
'binlog_format=ROW', you can prevent the slave from stopping if you set
'slave_exec_mode=IDEMPOTENT' before you start the slave again.  This
allows the slave to continue to replicate, but its *note 'MEMORY':
memory-storage-engine. tables will still be different from those on the
master.  This can be okay if the application logic is such that the
contents of *note 'MEMORY': memory-storage-engine. tables can be safely
lost (for example, if the *note 'MEMORY': memory-storage-engine. tables
are used for caching).  'slave_exec_mode=IDEMPOTENT' applies globally to
all tables, so it may hide other replication errors in non-*note
'MEMORY': memory-storage-engine. tables.

(The method just described is not applicable in NDB Cluster, where
'slave_exec_mode' is always 'IDEMPOTENT', and cannot be changed.)

The size of *note 'MEMORY': memory-storage-engine. tables is limited by
the value of the 'max_heap_table_size' system variable, which is not
replicated (see *note replication-features-variables::).  A change in
'max_heap_table_size' takes effect for 'MEMORY' tables that are created
or updated using *note 'ALTER TABLE ... ENGINE = MEMORY': alter-table.
or *note 'TRUNCATE TABLE': truncate-table. following the change, or for
all *note 'MEMORY': memory-storage-engine. tables following a server
restart.  If you increase the value of this variable on the master
without doing so on the slave, it becomes possible for a table on the
master to grow larger than its counterpart on the slave, leading to
inserts that succeed on the master but fail on the slave with 'Table is
full' errors.  This is a known issue (Bug #48666).  In such cases, you
must set the global value of 'max_heap_table_size' on the slave as well
as on the master, then restart replication.  It is also recommended that
you restart both the master and slave MySQL servers, to insure that the
new value takes complete (global) effect on each of them.

See *note memory-storage-engine::, for more information about *note
'MEMORY': memory-storage-engine. tables.


File: manual.info.tmp,  Node: replication-features-mysqldb,  Next: replication-features-optimizer,  Prev: replication-features-memory,  Up: replication-features

17.4.1.23 Replication of the mysql System Database
..................................................

Data modification statements made to tables in the 'mysql' database are
replicated according to the value of 'binlog_format'; if this value is
'MIXED', these statements are replicated using row-based format.
However, statements that would normally update this information
indirectly--such *note 'GRANT': grant, *note 'REVOKE': revoke, and
statements manipulating triggers, stored routines, and views--are
replicated to slaves using statement-based replication.


File: manual.info.tmp,  Node: replication-features-optimizer,  Next: replication-features-partitioning,  Prev: replication-features-mysqldb,  Up: replication-features

17.4.1.24 Replication and the Query Optimizer
.............................................

It is possible for the data on the master and slave to become different
if a statement is written in such a way that the data modification is
nondeterministic; that is, left up the query optimizer.  (In general,
this is not a good practice, even outside of replication.)  Examples of
nondeterministic statements include *note 'DELETE': delete. or *note
'UPDATE': update. statements that use 'LIMIT' with no 'ORDER BY' clause;
see *note replication-features-limit::, for a detailed discussion of
these.


File: manual.info.tmp,  Node: replication-features-partitioning,  Next: replication-features-repair-table,  Prev: replication-features-optimizer,  Up: replication-features

17.4.1.25 Replication and Partitioning
......................................

Replication is supported between partitioned tables as long as they use
the same partitioning scheme and otherwise have the same structure
except where an exception is specifically allowed (see *note
replication-features-differing-tables::).

Replication between tables having different partitioning is generally
not supported.  This because statements (such as *note 'ALTER TABLE ...
DROP PARTITION': alter-table-partition-operations.) acting directly on
partitions in such cases may produce different results on master and
slave.  In the case where a table is partitioned on the master but not
on the slave, any statements operating on partitions on the master's
copy of the slave fail on the slave.  When the slave's copy of the table
is partitioned but the master's copy is not, statements acting on
partitions cannot be run on the master without causing errors there.

Due to these dangers of causing replication to fail entirely (on account
of failed statements) and of inconsistencies (when the result of a
partition-level SQL statement produces different results on master and
slave), we recommend that insure that the partitioning of any tables to
be replicated from the master is matched by the slave's versions of
these tables.


File: manual.info.tmp,  Node: replication-features-repair-table,  Next: replication-features-reserved-words,  Prev: replication-features-partitioning,  Up: replication-features

17.4.1.26 Replication and REPAIR TABLE
......................................

When used on a corrupted or otherwise damaged table, it is possible for
the *note 'REPAIR TABLE': repair-table. statement to delete rows that
cannot be recovered.  However, any such modifications of table data
performed by this statement are not replicated, which can cause master
and slave to lose synchronization.  For this reason, in the event that a
table on the master becomes damaged and you use *note 'REPAIR TABLE':
repair-table. to repair it, you should first stop replication (if it is
still running) before using *note 'REPAIR TABLE': repair-table, then
afterward compare the master's and slave's copies of the table and be
prepared to correct any discrepancies manually, before restarting
replication.


File: manual.info.tmp,  Node: replication-features-reserved-words,  Next: replication-features-set-password,  Prev: replication-features-repair-table,  Up: replication-features

17.4.1.27 Replication and Reserved Words
........................................

You can encounter problems when you attempt to replicate from an older
master to a newer slave and you make use of identifiers on the master
that are reserved words in the newer MySQL version running on the slave.
An example of this is using a table column named 'range' on a 5.0 master
that is replicating to a 5.1 or higher slave because 'RANGE' is a
reserved word beginning in MySQL 5.1.  Replication can fail in such
cases with Error 1064 'You have an error in your SQL syntax...', _even
if a database or table named using the reserved word or a table having a
column named using the reserved word is excluded from replication_.
This is due to the fact that each SQL event must be parsed by the slave
prior to execution, so that the slave knows which database object or
objects would be affected; only after the event is parsed can the slave
apply any filtering rules defined by '--replicate-do-db',
'--replicate-do-table', '--replicate-ignore-db', and
'--replicate-ignore-table'.

To work around the problem of database, table, or column names on the
master which would be regarded as reserved words by the slave, do one of
the following:

   * Use one or more *note 'ALTER TABLE': alter-table. statements on the
     master to change the names of any database objects where these
     names would be considered reserved words on the slave, and change
     any SQL statements that use the old names to use the new names
     instead.

   * In any SQL statements using these database object names, write the
     names as quoted identifiers using backtick characters ('`').

For listings of reserved words by MySQL version, see Reserved Words
(http://dev.mysql.com/doc/mysqld-version-reference/en/mysqld-version-reference-optvar.html),
in the 'MySQL Server Version Reference'.  For identifier quoting rules,
see *note identifiers::.


File: manual.info.tmp,  Node: replication-features-set-password,  Next: replication-features-shutdowns,  Prev: replication-features-reserved-words,  Up: replication-features

17.4.1.28 SET PASSWORD and Row-Based Replication
................................................

Row-based replication of *note 'SET PASSWORD': set-password. statements
from a MySQL 5.1 master to a MySQL 5.5 slave did not work correctly
prior to MySQL 5.1.53 on the master and MySQL 5.5.7 on the slave (see
Bug #57098, Bug #57357).


File: manual.info.tmp,  Node: replication-features-shutdowns,  Next: replication-features-slaveerrors,  Prev: replication-features-set-password,  Up: replication-features

17.4.1.29 Replication and Master or Slave Shutdowns
...................................................

It is safe to shut down a master server and restart it later.  When a
slave loses its connection to the master, the slave tries to reconnect
immediately and retries periodically if that fails.  The default is to
retry every 60 seconds.  This may be changed with the *note 'CHANGE
MASTER TO': change-master-to. statement.  A slave also is able to deal
with network connectivity outages.  However, the slave notices the
network outage only after receiving no data from the master for
'slave_net_timeout' seconds.  If your outages are short, you may want to
decrease 'slave_net_timeout'.  See *note server-system-variables::.

An unclean shutdown (for example, a crash) on the master side can result
in the master binary log having a final position less than the most
recent position read by the slave, due to the master binary log file not
being flushed.  This can cause the slave not to be able to replicate
when the master comes back up.  Setting 'sync_binlog=1' in the master
'my.cnf' file helps to minimize this problem because it causes the
master to flush its binary log more frequently.

Shutting down a slave cleanly is safe because it keeps track of where it
left off.  However, be careful that the slave does not have temporary
tables open; see *note replication-features-temptables::.  Unclean
shutdowns might produce problems, especially if the disk cache was not
flushed to disk before the problem occurred:

   * For transactions, the slave commits and then updates
     'relay-log.info'.  If a crash occurs between these two operations,
     relay log processing will have proceeded further than the
     information file indicates and the slave will re-execute the events
     from the last transaction in the relay log after it has been
     restarted.

   * A similar problem can occur if the slave updates 'relay-log.info'
     but the server host crashes before the write has been flushed to
     disk.  To minimize the chance of this occurring, set
     'sync_relay_log_info=1' in the slave 'my.cnf' file.  The default
     value of 'sync_relay_log_info' is 0, which does not cause writes to
     be forced to disk; the server relies on the operating system to
     flush the file from time to time.

The fault tolerance of your system for these types of problems is
greatly increased if you have a good uninterruptible power supply.


File: manual.info.tmp,  Node: replication-features-slaveerrors,  Next: replication-features-sql-mode,  Prev: replication-features-shutdowns,  Up: replication-features

17.4.1.30 Slave Errors During Replication
.........................................

If a statement produces the same error (identical error code) on both
the master and the slave, the error is logged, but replication
continues.

If a statement produces different errors on the master and the slave,
the slave SQL thread terminates, and the slave writes a message to its
error log and waits for the database administrator to decide what to do
about the error.  This includes the case that a statement produces an
error on the master or the slave, but not both.  To address the issue,
connect to the slave manually and determine the cause of the problem.
*note 'SHOW SLAVE STATUS': show-slave-status. is useful for this.  Then
fix the problem and run *note 'START SLAVE': start-slave.  For example,
you might need to create a nonexistent table before you can start the
slave again.

*Note*:

If a temporary error is recorded in the slave's error log, you do not
necessarily have to take any action suggested in the quoted error
message.  Temporary errors should be handled by the client retrying the
transaction.  For example, if the slave SQL thread records a temporary
error relating to a deadlock, you do not need to restart the transaction
manually on the slave, unless the slave SQL thread subsequently
terminates with a nontemporary error message.

If this error code validation behavior is not desirable, some or all
errors can be masked out (ignored) with the '--slave-skip-errors'
option.

For nontransactional storage engines such as 'MyISAM', it is possible to
have a statement that only partially updates a table and returns an
error code.  This can happen, for example, on a multiple-row insert that
has one row violating a key constraint, or if a long update statement is
killed after updating some of the rows.  If that happens on the master,
the slave expects execution of the statement to result in the same error
code.  If it does not, the slave SQL thread stops as described
previously.

If you are replicating between tables that use different storage engines
on the master and slave, keep in mind that the same statement might
produce a different error when run against one version of the table, but
not the other, or might cause an error for one version of the table, but
not the other.  For example, since 'MyISAM' ignores foreign key
constraints, an *note 'INSERT': insert. or *note 'UPDATE': update.
statement accessing an 'InnoDB' table on the master might cause a
foreign key violation but the same statement performed on a 'MyISAM'
version of the same table on the slave would produce no such error,
causing replication to stop.


File: manual.info.tmp,  Node: replication-features-sql-mode,  Next: replication-features-temptables,  Prev: replication-features-slaveerrors,  Up: replication-features

17.4.1.31 Replication and Server SQL Mode
.........................................

Using different server SQL mode settings on the master and the slave may
cause the same *note 'INSERT': insert. statements to be handled
differently on the master and the slave, leading the master and slave to
diverge.  For best results, you should always use the same server SQL
mode on the master and on the slave.  This advice applies whether you
are using statement-based or row-based replication.

If you are replicating partitioned tables, using different SQL modes on
the master and the slave is likely to cause issues.  At a minimum, this
is likely to cause the distribution of data among partitions to be
different in the master's and slave's copies of a given table.  It may
also cause inserts into partitioned tables that succeed on the master to
fail on the slave.

For more information, see *note sql-mode::.


File: manual.info.tmp,  Node: replication-features-temptables,  Next: replication-features-timeout,  Prev: replication-features-sql-mode,  Up: replication-features

17.4.1.32 Replication and Temporary Tables
..........................................

The discussion in the following paragraphs does not apply when
'binlog_format=ROW' because, in that case, temporary tables are not
replicated; this means that there are never any temporary tables on the
slave to be lost in the event of an unplanned shutdown by the slave.
The remainder of this section applies only when using statement-based or
mixed-format replication.  Loss of replicated temporary tables on the
slave can be an issue, whenever 'binlog_format' is 'STATEMENT' or
'MIXED', for statements involving temporary tables that can be logged
safely using statement-based format.  For more information about
row-based replication and temporary tables, see *note
replication-rbr-usage-temptables::.

Safe slave shutdown when using temporary tables

Temporary tables are replicated except in the case where you stop the
slave server (not just the slave threads) and you have replicated
temporary tables that are open for use in updates that have not yet been
executed on the slave.  If you stop the slave server, the temporary
tables needed by those updates are no longer available when the slave is
restarted.  To avoid this problem, do not shut down the slave while it
has temporary tables open.  Instead, use the following procedure:

  1. Issue a 'STOP SLAVE SQL_THREAD' statement.

  2. Use *note 'SHOW STATUS': show-status. to check the value of the
     'Slave_open_temp_tables' variable.

  3. If the value is not 0, restart the slave SQL thread with 'START
     SLAVE SQL_THREAD' and repeat the procedure later.

  4. When the value is 0, issue a *note 'mysqladmin shutdown':
     mysqladmin. command to stop the slave.

Temporary tables and replication options

By default, all temporary tables are replicated; this happens whether or
not there are any matching '--replicate-do-db', '--replicate-do-table',
or '--replicate-wild-do-table' options in effect.  However, the
'--replicate-ignore-table' and '--replicate-wild-ignore-table' options
are honored for temporary tables.

A recommended practice when using statement-based or mixed-format
replication is to designate a prefix for exclusive use in naming
temporary tables that you do not want replicated, then employ a
'--replicate-wild-ignore-table' option to match that prefix.  For
example, you might give all such tables names beginning with 'norep'
(such as 'norepmytable', 'norepyourtable', and so on), then use
'--replicate-wild-ignore-table=norep%' to prevent them from being
replicated.


File: manual.info.tmp,  Node: replication-features-timeout,  Next: replication-features-timestamp,  Prev: replication-features-temptables,  Up: replication-features

17.4.1.33 Replication Retries and Timeouts
..........................................

The global system variable 'slave_transaction_retries' affects
replication as follows: If the slave SQL thread fails to execute a
transaction because of an 'InnoDB' deadlock or because it exceeded the
'InnoDB' 'innodb_lock_wait_timeout' value, or the *note 'NDBCLUSTER':
mysql-cluster. 'TransactionDeadlockDetectionTimeout' or
'TransactionInactiveTimeout' value, the slave automatically retries the
transaction 'slave_transaction_retries' times before stopping with an
error.  The default value is 10.  The total retry count can be seen in
the output of *note 'SHOW STATUS': show-status.; see *note
server-status-variables::.


File: manual.info.tmp,  Node: replication-features-timestamp,  Next: replication-features-timezone,  Prev: replication-features-timeout,  Up: replication-features

17.4.1.34 Replication and TIMESTAMP
...................................

Older versions of MySQL (prior to 4.1) differed significantly in several
ways in their handling of the *note 'TIMESTAMP': datetime. data type
from what is supported in MySQL versions 5.5 and newer; these include
syntax extensions which are deprecated in MySQL 5.1, and that no longer
supported in MySQL 5.5.  This can cause problems (including replication
failures) when replicating between MySQL Server versions, if you are
using columns that are defined using the old *note 'TIMESTAMP(N)':
datetime. syntax.  See *note upgrading-from-previous-series::, for more
information about the differences, how they can impact MySQL
replication, and what you can do if you encounter such problems.


File: manual.info.tmp,  Node: replication-features-timezone,  Next: replication-features-transactions,  Prev: replication-features-timestamp,  Up: replication-features

17.4.1.35 Replication and Time Zones
....................................

The same system time zone should be set for both master and slave.
Otherwise, statements depending on the local time on the master are not
replicated properly, such as statements that use the 'NOW()' or
'FROM_UNIXTIME()' functions.  You can set the time zone in which MySQL
server runs by using the '--timezone=TIMEZONE_NAME' option of the
'mysqld_safe' script or by setting the 'TZ' environment variable.  See
also *note replication-features-functions::.


File: manual.info.tmp,  Node: replication-features-transactions,  Next: replication-features-triggers,  Prev: replication-features-timezone,  Up: replication-features

17.4.1.36 Replication and Transactions
......................................

Mixing transactional and nontransactional statements within the same
transaction

In general, you should avoid transactions that update both transactional
and nontransactional tables in a replication environment.  You should
also avoid using any statement that accesses both transactional (or
temporary) and nontransactional tables and writes to any of them.

As of MySQL 5.5.2, the server uses these rules for binary logging:

   * If the initial statements in a transaction are nontransactional,
     they are written to the binary log immediately.  The remaining
     statements in the transaction are cached and not written to the
     binary log until the transaction is committed.  (If the transaction
     is rolled back, the cached statements are written to the binary log
     only if they make nontransactional changes that cannot be rolled
     back.  Otherwise, they are discarded.)

   * For statement-based logging, logging of nontransactional statements
     is affected by the 'binlog_direct_non_transactional_updates' system
     variable.  When this variable is 'OFF' (the default), logging is as
     just described.  When this variable is 'ON', logging occurs
     immediately for nontransactional statements occurring anywhere in
     the transaction (not just initial nontransactional statements).
     Other statements are kept in the transaction cache and logged when
     the transaction commits.  'binlog_direct_non_transactional_updates'
     has no effect for row-format or mixed-format binary logging.

Transactional, nontransactional, and mixed statements

To apply those rules, the server considers a statement nontransactional
if it changes only nontransactional tables, and transactional if it
changes only transactional tables.  Prior to MySQL 5.5.6, a statement
that changed both nontransactional and transactional tables was
considered 'mixed'.  Beginning with MySQL 5.5.6, a statement that
references both nontransactional and transactional tables and updates
_any_ of the tables involved, is considered a mixed statement.  Mixed
statements, like transactional statements, are cached and logged when
the transaction commits.

Beginning with MySQL 5.5.6, a mixed statement that updates a
transactional table is considered unsafe if the statement also performs
either of the following actions:

   * Updates or reads a temporary table

   * Reads a nontransactional table and the transaction isolation level
     is less than REPEATABLE_READ

Also beginning with MySQL 5.5.6, any mixed statement following the
update of a transactional table within a transaction is considered
unsafe if it performs either of the following actions:

   * Updates any table and reads from any temporary table

   * Updates a nontransactional table and
     'binlog_direct_non_transactional_updates' is OFF

For more information, see *note replication-rbr-safe-unsafe::.

*Note*:

A mixed statement is unrelated to mixed binary logging format.

Before MySQL 5.5.2, the rules for binary logging are similar to those
just described, except that there is no
'binlog_direct_non_transactional_updates' system variable to affect
logging of transactional statements.  Thus, the server immediately logs
only the initial nontransactional statements in a transaction and caches
the rest until commit time.

In situations where transactions mix updates to transactional and
nontransactional tables, the order of statements in the binary log is
correct, and all needed statements are written to the binary log even in
case of a *note 'ROLLBACK': commit.  However, when a second connection
updates the nontransactional table before the first connection
transaction is complete, statements can be logged out of order because
the second connection update is written immediately after it is
performed, regardless of the state of the transaction being performed by
the first connection.

Using different storage engines on master and slave

It is possible to replicate transactional tables on the master using
nontransactional tables on the slave.  For example, you can replicate an
'InnoDB' master table as a 'MyISAM' slave table.  However, if you do
this, there are problems if the slave is stopped in the middle of a
*note 'BEGIN': commit. ...  *note 'COMMIT': commit. block because the
slave restarts at the beginning of the *note 'BEGIN': commit. block.

Beginning with MySQL 5.5.0, it is also safe to replicate transactions
from *note 'MyISAM': myisam-storage-engine. tables on the master to
transactional tables--such as tables that use the *note 'InnoDB':
innodb-storage-engine. storage engine--on the slave.  In such cases
(beginning with MySQL 5.5.0), an 'AUTOCOMMIT=1' statement issued on the
master is replicated, thus enforcing 'AUTOCOMMIT' mode on the slave.

When the storage engine type of the slave is nontransactional,
transactions on the master that mix updates of transactional and
nontransactional tables should be avoided because they can cause
inconsistency of the data between the master transactional table and the
slave nontransactional table.  That is, such transactions can lead to
master storage engine-specific behavior with the possible effect of
replication going out of synchrony.  MySQL does not issue a warning
about this currently, so extra care should be taken when replicating
transactional tables from the master to nontransactional tables on the
slaves.

Changing the binary logging format within transactions

Beginning with MySQL 5.5.3, the 'binlog_format' system variable is
read-only as long as a transaction is in progress.  (Bug #47863)

Every transaction (including 'autocommit' transactions) is recorded in
the binary log as though it starts with a *note 'BEGIN': commit.
statement, and ends with either a *note 'COMMIT': commit. or a *note
'ROLLBACK': commit. statement.  In MySQL 5.5, this true is even for
statements affecting tables that use a nontransactional storage engine
(such as *note 'MyISAM': myisam-storage-engine.).


File: manual.info.tmp,  Node: replication-features-triggers,  Next: replication-features-truncate,  Prev: replication-features-transactions,  Up: replication-features

17.4.1.37 Replication and Triggers
..................................

With statement-based replication, triggers executed on the master also
execute on the slave.  With row-based replication, triggers executed on
the master do not execute on the slave.  Instead, the row changes on the
master resulting from trigger execution are replicated and applied on
the slave.

This behavior is by design.  If under row-based replication the slave
applied the triggers as well as the row changes caused by them, the
changes would in effect be applied twice on the slave, leading to
different data on the master and the slave.

If you want triggers to execute on both the master and the
slave--perhaps because you have different triggers on the master and
slave--you must use statement-based replication.  However, to enable
slave-side triggers, it is not necessary to use statement-based
replication exclusively.  It is sufficient to switch to statement-based
replication only for those statements where you want this effect, and to
use row-based replication the rest of the time.

A statement invoking a trigger (or function) that causes an update to an
'AUTO_INCREMENT' column is not replicated correctly using
statement-based replication.  MySQL 5.5 marks such statements as unsafe.
(Bug #45677)


File: manual.info.tmp,  Node: replication-features-truncate,  Next: replication-features-variables,  Prev: replication-features-triggers,  Up: replication-features

17.4.1.38 Replication and TRUNCATE TABLE
........................................

*note 'TRUNCATE TABLE': truncate-table. is normally regarded as a DML
statement, and so would be expected to be logged and replicated using
row-based format when the binary logging mode is 'ROW' or 'MIXED'.
However this caused issues when logging or replicating, in 'STATEMENT'
or 'MIXED' mode, tables that used transactional storage engines such as
*note 'InnoDB': innodb-storage-engine. when the transaction isolation
level was 'READ COMMITTED' or 'READ UNCOMMITTED', which precludes
statement-based logging.

*note 'TRUNCATE TABLE': truncate-table. is treated for purposes of
logging and replication as DDL rather than DML so that it can be logged
and replicated as a statement.  However, the effects of the statement as
applicable to *note 'InnoDB': innodb-storage-engine. and other
transactional tables on replication slaves still follow the rules
described in *note truncate-table:: governing such tables.  (Bug #36763)


File: manual.info.tmp,  Node: replication-features-variables,  Next: replication-features-views,  Prev: replication-features-truncate,  Up: replication-features

17.4.1.39 Replication and Variables
...................................

System variables are not replicated correctly when using 'STATEMENT'
mode, except for the following variables when they are used with session
scope:

   * 'auto_increment_increment'

   * 'auto_increment_offset'

   * 'character_set_client'

   * 'character_set_connection'

   * 'character_set_database'

   * 'character_set_server'

   * 'collation_connection'

   * 'collation_database'

   * 'collation_server'

   * 'foreign_key_checks'

   * 'identity'

   * 'last_insert_id'

   * 'lc_time_names'

   * 'pseudo_thread_id'

   * 'sql_auto_is_null'

   * 'time_zone'

   * 'timestamp'

   * 'unique_checks'

When 'MIXED' mode is used, the variables in the preceding list, when
used with session scope, cause a switch from statement-based to
row-based logging.  See *note binary-log-mixed::.

'sql_mode' is also replicated except for the 'NO_DIR_IN_CREATE' mode;
the slave always preserves its own value for 'NO_DIR_IN_CREATE',
regardless of changes to it on the master.  This is true for all
replication formats.

However, when *note 'mysqlbinlog': mysqlbinlog. parses a 'SET @@sql_mode
= MODE' statement, the full MODE value, including 'NO_DIR_IN_CREATE', is
passed to the receiving server.  For this reason, replication of such a
statement may not be safe when 'STATEMENT' mode is in use.

The 'default_storage_engine' and 'storage_engine' system variables are
not replicated, regardless of the logging mode; this is intended to
facilitate replication between different storage engines.

The 'read_only' system variable is not replicated.  In addition, the
enabling this variable has different effects with regard to temporary
tables, table locking, and the *note 'SET PASSWORD': set-password.
statement in different MySQL versions.

The 'max_heap_table_size' system variable is not replicated.  Increasing
the value of this variable on the master without doing so on the slave
can lead eventually to 'Table is full' errors on the slave when trying
to execute *note 'INSERT': insert. statements on a *note 'MEMORY':
memory-storage-engine. table on the master that is thus permitted to
grow larger than its counterpart on the slave.  For more information,
see *note replication-features-memory::.

In statement-based replication, session variables are not replicated
properly when used in statements that update tables.  For example, the
following sequence of statements will not insert the same data on the
master and the slave:

     SET max_join_size=1000;
     INSERT INTO mytable VALUES(@@max_join_size);

This does not apply to the common sequence:

     SET time_zone=...;
     INSERT INTO mytable VALUES(CONVERT_TZ(..., ..., @@time_zone));

Replication of session variables is not a problem when row-based
replication is being used, in which case, session variables are always
replicated safely.  See *note replication-formats::.

In MySQL 5.5, the following session variables are written to the binary
log and honored by the replication slave when parsing the binary log,
regardless of the logging format:

   * 'sql_mode'

   * 'foreign_key_checks'

   * 'unique_checks'

   * 'character_set_client'

   * 'collation_connection'

   * 'collation_database'

   * 'collation_server'

   * 'sql_auto_is_null'

*Important*:

Even though session variables relating to character sets and collations
are written to the binary log, replication between different character
sets is not supported.

It is strongly recommended that you always use the same setting for the
'lower_case_table_names' system variable on both master and slave.  In
particular, when a case-sensitive file system is used, setting this
variable to 1 on the slave, but to a different value on the master, can
cause two types of problems: Names of databases are not converted to
lowercase; in addition, when using row-based replication names of tables
are also not converted.  Either of these problems can cause replication
to fail.  This is a known issue, which is fixed in MySQL 5.6.


File: manual.info.tmp,  Node: replication-features-views,  Prev: replication-features-variables,  Up: replication-features

17.4.1.40 Replication and Views
...............................

Views are always replicated to slaves.  Views are filtered by their own
name, not by the tables they refer to.  This means that a view can be
replicated to the slave even if the view contains a table that would
normally be filtered out by 'replication-ignore-table' rules.  Care
should therefore be taken to ensure that views do not replicate table
data that would normally be filtered for security reasons.

Replication from a table to a samed-named view is supported using
statement-based logging, but not when using row-based logging.  In MySQL
5.5.31 and later, trying to do so when row-based logging is in effect
causes an error.  (Bug #11752707, Bug #43975)


File: manual.info.tmp,  Node: replication-compatibility,  Next: replication-upgrade,  Prev: replication-features,  Up: replication-notes

17.4.2 Replication Compatibility Between MySQL Versions
-------------------------------------------------------

MySQL supports replication from one release series to the next higher
release series.  For example, you can replicate from a master running
MySQL 5.1 to a slave running MySQL 5.5, from a master running MySQL 5.5
to a slave running MySQL 5.6, and so on.

However, one may encounter difficulties when replicating from an older
master to a newer slave if the master uses statements or relies on
behavior no longer supported in the version of MySQL used on the slave.
For example, in MySQL 5.5, *note 'CREATE TABLE ... SELECT':
create-table-select. statements are permitted to change tables other
than the one being created, but are no longer allowed to do so in MySQL
5.6 (see *note replication-features-create-select::).

The use of more than two MySQL Server versions is not supported in
replication setups involving multiple masters, regardless of the number
of master or slave MySQL servers.  This restriction applies not only to
release series, but to version numbers within the same release series as
well.  For example, if you are using a chained or circular replication
setup, you cannot use MySQL 5.5.1, MySQL 5.5.2, and MySQL 5.5.4
concurrently, although you could use any two of these releases together.

*Important*:

It is strongly recommended to use the most recent release available
within a given MySQL release series because replication (and other)
capabilities are continually being improved.  It is also recommended to
upgrade masters and slaves that use early releases of a release series
of MySQL to GA (production) releases when the latter become available
for that release series.

Replication from newer masters to older slaves may be possible, but is
generally not supported.  This is due to a number of factors:

   * Binary log format changes

     The binary log format can change between major releases.  While we
     attempt to maintain backward compatibility, this is not always
     possible.  For example, the binary log format implemented in MySQL
     5.0 changed considerably from that used in previous versions,
     especially with regard to handling of character sets, *note 'LOAD
     DATA': load-data, and time zones.  This means that replication from
     a MySQL 5.0 (or later) master to a MySQL 4.1 (or earlier) slave is
     generally not supported.

     This also has significant implications for upgrading replication
     servers; see *note replication-upgrade::, for more information.

   * Use of row-based replication

     Row-based replication was implemented in MySQL 5.1.5, so you cannot
     replicate using row-based replication from any MySQL 5.5 or later
     master to a slave older than MySQL 5.1.5.

     For more information about row-based replication, see *note
     replication-formats::.

   * SQL incompatibilities

     You cannot replicate from a newer master to an older slave using
     statement-based replication if the statements to be replicated use
     SQL features available on the master but not on the slave.

     However, if both the master and the slave support row-based
     replication, and there are no data definition statements to be
     replicated that depend on SQL features found on the master but not
     on the slave, you can use row-based replication to replicate the
     effects of data modification statements even if the DDL run on the
     master is not supported on the slave.

For more information on potential replication issues, see *note
replication-features::.


File: manual.info.tmp,  Node: replication-upgrade,  Next: replication-problems,  Prev: replication-compatibility,  Up: replication-notes

17.4.3 Upgrading a Replication Setup
------------------------------------

When you upgrade servers that participate in a replication setup, the
procedure for upgrading depends on the current server versions and the
version to which you are upgrading.  This section provides information
about how upgrading affects replication.  For general information about
upgrading MySQL, see *note upgrading::

When you upgrade a master to 5.5 from an earlier MySQL release series,
you should first ensure that all the slaves of this master are using the
same 5.5.x release.  If this is not the case, you should first upgrade
the slaves.  To upgrade each slave, shut it down, upgrade it to the
appropriate 5.5.x version, restart it, and restart replication.  The 5.5
slave is able to read the old relay logs written prior to the upgrade
and to execute the statements they contain.  Relay logs created by the
slave after the upgrade are in 5.5 format.

After the slaves have been upgraded, shut down the master, upgrade it to
the same 5.5.x release as the slaves, and restart it.  The 5.5 master is
able to read the old binary logs written prior to the upgrade and to
send them to the 5.5 slaves.  The slaves recognize the old format and
handle it properly.  Binary logs created by the master subsequent to the
upgrade are in 5.5 format.  These too are recognized by the 5.5 slaves.

In other words, when upgrading to MySQL 5.5, the slaves must be MySQL
5.5 before you can upgrade the master to 5.5.  Note that downgrading
from 5.5 to older versions does not work so simply: You must ensure that
any 5.5 binary log or relay log has been fully processed, so that you
can remove it before proceeding with the downgrade.

Some upgrades may require that you drop and re-create database objects
when you move from one MySQL series to the next.  For example, collation
changes might require that table indexes be rebuilt.  Such operations,
if necessary, are detailed at *note upgrading-from-previous-series::.
It is safest to perform these operations separately on the slaves and
the master, and to disable replication of these operations from the
master to the slave.  To achieve this, use the following procedure:

  1. Stop all the slaves and upgrade them.  Restart them with the
     '--skip-slave-start' option so that they do not connect to the
     master.  Perform any table repair or rebuilding operations needed
     to re-create database objects, such as use of 'REPAIR TABLE' or
     'ALTER TABLE', or dumping and reloading tables or triggers.

  2. Disable the binary log on the master.  To do this without
     restarting the master, execute a 'SET sql_log_bin = OFF' statement.
     Alternatively, stop the master and restart it without the
     '--log-bin' option.  If you restart the master, you might also want
     to disallow client connections.  For example, if all clients
     connect using TCP/IP, enable the 'skip_networking' system variable
     when you restart the master.

  3. With the binary log disabled, perform any table repair or
     rebuilding operations needed to re-create database objects.  The
     binary log must be disabled during this step to prevent these
     operations from being logged and sent to the slaves later.

  4. Re-enable the binary log on the master.  If you set 'sql_log_bin'
     to 'OFF' earlier, execute a 'SET sql_log_bin = ON' statement.  If
     you restarted the master to disable the binary log, restart it with
     '--log-bin', and without enabling the 'skip_networking' system
     variable so that clients and slaves can connect.

  5. Restart the slaves, this time without the '--skip-slave-start'
     option.


File: manual.info.tmp,  Node: replication-problems,  Next: replication-bugs,  Prev: replication-upgrade,  Up: replication-notes

17.4.4 Troubleshooting Replication
----------------------------------

If you have followed the instructions but your replication setup is not
working, the first thing to do is _check the error log for messages_.
Many users have lost time by not doing this soon enough after
encountering problems.

If you cannot tell from the error log what the problem was, try the
following techniques:

   * Verify that the master has binary logging enabled by issuing a
     *note 'SHOW MASTER STATUS': show-master-status. statement.  If
     logging is enabled, 'Position' is nonzero.  If binary logging is
     not enabled, verify that you are running the master with the
     '--log-bin' option.

   * Verify that the 'server_id' system variable was set at startup on
     both the master and slave and that the ID value is unique on each
     server.

   * Verify that the slave is running.  Use *note 'SHOW SLAVE STATUS':
     show-slave-status. to check whether the 'Slave_IO_Running' and
     'Slave_SQL_Running' values are both 'Yes'.  If not, verify the
     options that were used when starting the slave server.  For
     example, '--skip-slave-start' prevents the slave threads from
     starting until you issue a *note 'START SLAVE': start-slave.
     statement.

   * If the slave is running, check whether it established a connection
     to the master.  Use *note 'SHOW PROCESSLIST': show-processlist,
     find the I/O and SQL threads and check their 'State' column to see
     what they display.  See *note replication-implementation-details::.
     If the I/O thread state says 'Connecting to master', check the
     following:

        * Verify the privileges for the user being used for replication
          on the master.

        * Check that the host name of the master is correct and that you
          are using the correct port to connect to the master.  The port
          used for replication is the same as used for client network
          communication (the default is '3306').  For the host name,
          ensure that the name resolves to the correct IP address.

        * Check the configuration file to see whether the
          'skip_networking' system variable has been enabled on the
          master or slave to disable networking.  If so, comment the
          setting or remove it.

        * If the master has a firewall or IP filtering configuration,
          ensure that the network port being used for MySQL is not being
          filtered.

        * Check that you can reach the master by using 'ping' or
          'traceroute'/'tracert' to reach the host.

   * If the slave was running previously but has stopped, the reason
     usually is that some statement that succeeded on the master failed
     on the slave.  This should never happen if you have taken a proper
     snapshot of the master, and never modified the data on the slave
     outside of the slave thread.  If the slave stops unexpectedly, it
     is a bug or you have encountered one of the known replication
     limitations described in *note replication-features::.  If it is a
     bug, see *note replication-bugs::, for instructions on how to
     report it.

   * If a statement that succeeded on the master refuses to run on the
     slave, try the following procedure if it is not feasible to do a
     full database resynchronization by deleting the slave's databases
     and copying a new snapshot from the master:

       1. Determine whether the affected table on the slave is different
          from the master table.  Try to understand how this happened.
          Then make the slave's table identical to the master's and run
          *note 'START SLAVE': start-slave.

       2. If the preceding step does not work or does not apply, try to
          understand whether it would be safe to make the update
          manually (if needed) and then ignore the next statement from
          the master.

       3. If you decide that the slave can skip the next statement from
          the master, issue the following statements:

               mysql> SET GLOBAL sql_slave_skip_counter = N;
               mysql> START SLAVE;

          The value of N should be 1 if the next statement from the
          master does not use 'AUTO_INCREMENT' or 'LAST_INSERT_ID()'.
          Otherwise, the value should be 2.  The reason for using a
          value of 2 for statements that use 'AUTO_INCREMENT' or
          'LAST_INSERT_ID()' is that they take two events in the binary
          log of the master.

          See also *note set-global-sql-slave-skip-counter::.

       4. If you are sure that the slave started out perfectly
          synchronized with the master, and that no one has updated the
          tables involved outside of the slave thread, then presumably
          the discrepancy is the result of a bug.  If you are running
          the most recent version of MySQL, please report the problem.
          If you are running an older version, try upgrading to the
          latest production release to determine whether the problem
          persists.


File: manual.info.tmp,  Node: replication-bugs,  Prev: replication-problems,  Up: replication-notes

17.4.5 How to Report Replication Bugs or Problems
-------------------------------------------------

When you have determined that there is no user error involved, and
replication still either does not work at all or is unstable, it is time
to send us a bug report.  We need to obtain as much information as
possible from you to be able to track down the bug.  Please spend some
time and effort in preparing a good bug report.

If you have a repeatable test case that demonstrates the bug, please
enter it into our bugs database using the instructions given in *note
bug-reports::.  If you have a 'phantom' problem (one that you cannot
duplicate at will), use the following procedure:

  1. Verify that no user error is involved.  For example, if you update
     the slave outside of the slave thread, the data goes out of
     synchrony, and you can have unique key violations on updates.  In
     this case, the slave thread stops and waits for you to clean up the
     tables manually to bring them into synchrony.  _This is not a
     replication problem.  It is a problem of outside interference
     causing replication to fail._

  2. Run the slave with the '--log-slave-updates' and '--log-bin'
     options.  These options cause the slave to log the updates that it
     receives from the master into its own binary logs.

  3. Save all evidence before resetting the replication state.  If we
     have no information or only sketchy information, it becomes
     difficult or impossible for us to track down the problem.  The
     evidence you should collect is:

        * All binary log files from the master

        * All binary log files from the slave

        * The output of *note 'SHOW MASTER STATUS': show-master-status.
          from the master at the time you discovered the problem

        * The output of *note 'SHOW SLAVE STATUS': show-slave-status.
          from the slave at the time you discovered the problem

        * Error logs from the master and the slave

  4. Use *note 'mysqlbinlog': mysqlbinlog. to examine the binary logs.
     The following should be helpful to find the problem statement.
     LOG_FILE and LOG_POS are the 'Master_Log_File' and
     'Read_Master_Log_Pos' values from *note 'SHOW SLAVE STATUS':
     show-slave-status.

          shell> mysqlbinlog --start-position=LOG_POS LOG_FILE | head

After you have collected the evidence for the problem, try to isolate it
as a separate test case first.  Then enter the problem with as much
information as possible into our bugs database using the instructions at
*note bug-reports::.


File: manual.info.tmp,  Node: mysql-cluster,  Next: partitioning,  Prev: replication,  Up: Top

18 MySQL NDB Cluster 7.2
************************

* Menu:

* mysql-cluster-overview::       NDB Cluster Overview
* mysql-cluster-installation::   NDB Cluster Installation
* mysql-cluster-configuration::  Configuration of NDB Cluster
* mysql-cluster-programs::       NDB Cluster Programs
* mysql-cluster-management::     Management of NDB Cluster
* mysql-cluster-replication::    NDB Cluster Replication
* mysql-cluster-news::           NDB Cluster Release Notes

MySQL _NDB Cluster_ is a high-availability, high-redundancy version of
MySQL adapted for the distributed computing environment.  Recent
releases of NDB Cluster use version 7 of the *note 'NDBCLUSTER':
mysql-cluster. storage engine (also known as *note 'NDB':
mysql-cluster.) to enable running several computers with MySQL servers
and other software in a cluster.  NDB Cluster 7.6, now available as a
General Availability (GA) release beginning with version 7.6.6,
incorporates version 7.6 of the 'NDB' storage engine.  NDB Cluster 7.5,
still available as a GA release, uses version 7.5 of 'NDB'.  Previous GA
releases still available for use in production, NDB Cluster 7.3 and NDB
Cluster 7.4, incorporate 'NDB' versions 7.3 and 7.4, respectively.

Support for the *note 'NDBCLUSTER': mysql-cluster. storage engine is not
included in standard MySQL Server 5.5 binaries built by Oracle.
Instead, users of NDB Cluster binaries from Oracle should upgrade to the
most recent binary release of NDB Cluster for supported platforms--these
include RPMs that should work with most Linux distributions.  NDB
Cluster users who build from source should use the sources provided for
NDB Cluster.  (Locations where the sources can be obtained are listed
later in this section.)

This chapter contains information about NDB Cluster 7.2 releases through
5.5.65-ndb-7.2.39.  NDB Cluster 7.6 is now available as a General
Availability release, and recommended for new deployments; for
information about NDB Cluster 7.6, see What is New in NDB Cluster 7.6
(https://dev.mysql.com/doc/refman/5.7/en/mysql-cluster-what-is-new-7-6.html).
NDB Cluster 7.5, 7.4, and 7.3 are previous GA releases still supported
in production.  NDB Cluster 7.2 is a previous GA release series which is
still supported, although we recommend that new deployments for
production use NDB Cluster 7.6 (see MySQL NDB Cluster 7.5 and NDB
Cluster 7.6
(https://dev.mysql.com/doc/refman/5.7/en/mysql-cluster.html)).  For more
information about NDB Cluster 7.4 and NDB Cluster 7.3, see MySQL NDB
Cluster 7.3 and NDB Cluster 7.4
(https://dev.mysql.com/doc/refman/5.6/en/mysql-cluster.html).

NDB Cluster 8.0 is now available as a Developer Preview release for
evaluation and testing of new features in the 'NDBCLUSTER' storage
engine; for more information, see MySQL NDB Cluster 8.0
(https://dev.mysql.com/doc/refman/8.0/en/mysql-cluster.html).

Release notes for the changes in each release of NDB Cluster are located
at NDB Cluster 7.2 Release Notes
(https://dev.mysql.com/doc/relnotes/mysql-cluster/7.2/en/).

Supported Platforms

NDB Cluster is currently available and supported on a number of
platforms.  For exact levels of support available for on specific
combinations of operating system versions, operating system
distributions, and hardware platforms, please refer to
<https://www.mysql.com/support/supportedplatforms/cluster.html>.

Availability

NDB Cluster binary and source packages are available for supported
platforms from <https://dev.mysql.com/downloads/cluster/>.

NDB Cluster release numbers

NDB Cluster follows a somewhat different release pattern from the
mainline MySQL Server 5.5 series of releases.  In this 'Manual' and
other MySQL documentation, we identify these and later NDB Cluster
releases employing a version number that begins with 'NDB'.  This
version number is that of the *note 'NDBCLUSTER': mysql-cluster. storage
engine used in the release, and not of the MySQL server version on which
the NDB Cluster release is based.

Version strings used in NDB Cluster software

The version string displayed by NDB Cluster programs uses this format:

     mysql-MYSQL_SERVER_VERSION-ndb-NDB_ENGINE_VERSION

MYSQL_SERVER_VERSION represents the version of the MySQL Server on which
the NDB Cluster release is based.  For all NDB Cluster 6.x and 7.x
releases, this is '5.1'.  NDB_ENGINE_VERSION is the version of the *note
'NDB': mysql-cluster. storage engine used by this release of the NDB
Cluster software.  You can see this format used in the *note 'mysql':
mysql. client, as shown here:

     shell> mysql
     Welcome to the MySQL monitor.  Commands end with ; or \g.
     Your MySQL connection id is 2
     Server version: 5.5.65-ndb-7.2.39 Source distribution

     Type 'help;' or '\h' for help. Type '\c' to clear the buffer.

     mysql> SELECT VERSION()\G
     *************************** 1. row ***************************
     VERSION(): 5.5.65-ndb-7.2.39
     1 row in set (0.00 sec)

This version string is also displayed in the output of the 'SHOW'
command in the *note 'ndb_mgm': mysql-cluster-programs-ndb-mgm. client:

     ndb_mgm> SHOW
     Connected to Management Server at: localhost:1186
     Cluster Configuration
     ---------------------
     [ndbd(NDB)]     2 node(s)
     id=1    @10.0.10.6  (5.5.65-ndb-7.2.39, Nodegroup: 0, *)
     id=2    @10.0.10.8  (5.5.65-ndb-7.2.39, Nodegroup: 0)

     [ndb_mgmd(MGM)] 1 node(s)
     id=3    @10.0.10.2  (5.5.65-ndb-7.2.39)

     [mysqld(API)]   2 node(s)
     id=4    @10.0.10.10  (5.5.65-ndb-7.2.39)
     id=5 (not connected, accepting connect from any host)

The version string identifies the mainline MySQL version from which the
NDB Cluster release was branched and the version of the *note
'NDBCLUSTER': mysql-cluster. storage engine used.  For example, the full
version string for NDB 7.2.4 (the first NDB Cluster production release
based on MySQL Server 5.5) is 'mysql-5.5.19-ndb-7.2.4'.  From this we
can determine the following:

   * Since the portion of the version string preceding '-ndb-' is the
     base MySQL Server version, this means that NDB 7.2.4 derives from
     the MySQL 5.5.19, and contains all feature enhancements and
     bugfixes from MySQL 5.5 up to and including MySQL 5.5.19.

   * Since the portion of the version string following '-ndb-'
     represents the version number of the *note 'NDB': mysql-cluster.
     (or *note 'NDBCLUSTER': mysql-cluster.) storage engine, NDB 7.2.4
     uses version 7.2.4 of the *note 'NDBCLUSTER': mysql-cluster.
     storage engine.

New NDB Cluster releases are numbered according to updates in the 'NDB'
storage engine, and do not necessarily correspond in a one-to-one
fashion with mainline MySQL Server releases.  For example, NDB 7.2.4 (as
previously noted) is based on MySQL 5.5.19, while NDB 7.2.0 was based on
MySQL 5.1.51 (version string: 'mysql-5.1.51-ndb-7.2.0').

Compatibility with standard MySQL 5.5 releases

While many standard MySQL schemas and applications can work using NDB
Cluster, it is also true that unmodified applications and database
schemas may be slightly incompatible or have suboptimal performance when
run using NDB Cluster (see *note mysql-cluster-limitations::).  Most of
these issues can be overcome, but this also means that you are very
unlikely to be able to switch an existing application datastore--that
currently uses, for example, *note 'MyISAM': myisam-storage-engine. or
*note 'InnoDB': innodb-storage-engine.--to use the *note 'NDB':
mysql-cluster. storage engine without allowing for the possibility of
changes in schemas, queries, and applications.  In addition, the MySQL
Server and NDB Cluster codebases diverge considerably, so that the
standard *note 'mysqld': mysqld. cannot function as a drop-in
replacement for the version of *note 'mysqld': mysqld. supplied with NDB
Cluster.

NDB Cluster development source trees

NDB Cluster development trees can also be accessed from
<https://github.com/mysql/mysql-server>.

The NDB Cluster development sources maintained at
<https://github.com/mysql/mysql-server> are licensed under the GPL. For
information about obtaining MySQL sources using Git and building them
yourself, see *note installing-development-tree::.

*Note*:

As with MySQL Server 5.5, NDB Cluster 7.2 is built using 'CMake'.

NDB Cluster 7.2 is a previous General Availability (GA) release series
which is still maintained; NDB Cluster 7.1 and earlier versions are no
longer in active development.  We recommend that new deployments for
production use NDB Cluster 7.6 (see MySQL NDB Cluster 7.5 and NDB
Cluster 7.6
(https://dev.mysql.com/doc/refman/5.7/en/mysql-cluster.html)).  NDB
Cluster 7.5, 7.4, and 7.3 releases are previous General Availability
(GA) releases, still supported in production.  For an overview of major
features added in NDB Cluster 7.6, see What is New in NDB Cluster 7.6
(https://dev.mysql.com/doc/refman/5.7/en/mysql-cluster-what-is-new-7-6.html).
For similar information about NDB Cluster 7.5, see What is New in NDB
Cluster 7.5
(https://dev.mysql.com/doc/refman/5.7/en/mysql-cluster-what-is-new-7-5.html).
For information about NDB 7.4 and 7.3, see MySQL NDB Cluster 7.3 and NDB
Cluster 7.4
(https://dev.mysql.com/doc/refman/5.6/en/mysql-cluster.html).

This chapter represents a work in progress, and its contents are subject
to revision as NDB Cluster continues to evolve.  Additional information
regarding NDB Cluster can be found on the MySQL website at
<http://www.mysql.com/products/cluster/>.

Additional Resources

More information about NDB Cluster can be found in the following places:

   * For answers to some commonly asked questions about NDB Cluster, see
     *note faqs-mysql-cluster::.

   * The NDB Cluster Forum: <https://forums.mysql.com/list.php?25>.

   * Many NDB Cluster users and developers blog about their experiences
     with NDB Cluster, and make feeds of these available through
     PlanetMySQL (http://www.planetmysql.org/).


File: manual.info.tmp,  Node: mysql-cluster-overview,  Next: mysql-cluster-installation,  Prev: mysql-cluster,  Up: mysql-cluster

18.1 NDB Cluster Overview
=========================

* Menu:

* mysql-cluster-basics::         NDB Cluster Core Concepts
* mysql-cluster-nodes-groups::   NDB Cluster Nodes, Node Groups, Replicas, and Partitions
* mysql-cluster-overview-requirements::  NDB Cluster Hardware, Software, and Networking Requirements
* mysql-cluster-what-is-new::    What is New in MySQL NDB Cluster 7.2
* mysql-cluster-added-deprecated-removed::  Options, Variables, and Parameters Added, Deprecated or Removed in NDB 7.2
* mysql-cluster-compared::       MySQL Server Using InnoDB Compared with NDB Cluster
* mysql-cluster-limitations::    Known Limitations of NDB Cluster

_NDB Cluster_ is a technology that enables clustering of in-memory
databases in a shared-nothing system.  The shared-nothing architecture
enables the system to work with very inexpensive hardware, and with a
minimum of specific requirements for hardware or software.

NDB Cluster is designed not to have any single point of failure.  In a
shared-nothing system, each component is expected to have its own memory
and disk, and the use of shared storage mechanisms such as network
shares, network file systems, and SANs is not recommended or supported.

NDB Cluster integrates the standard MySQL server with an in-memory
clustered storage engine called *note 'NDB': mysql-cluster. (which
stands for '_N_etwork _D_ata_B_ase').  In our documentation, the term
*note 'NDB': mysql-cluster. refers to the part of the setup that is
specific to the storage engine, whereas 'MySQL NDB Cluster' refers to
the combination of one or more MySQL servers with the *note 'NDB':
mysql-cluster. storage engine.

An NDB Cluster consists of a set of computers, known as _hosts_, each
running one or more processes.  These processes, known as _nodes_, may
include MySQL servers (for access to NDB data), data nodes (for storage
of the data), one or more management servers, and possibly other
specialized data access programs.  The relationship of these components
in an NDB Cluster is shown here:

FIGURE GOES HERE: NDB Cluster Components

All these programs work together to form an NDB Cluster (see *note
mysql-cluster-programs::.  When data is stored by the *note 'NDB':
mysql-cluster. storage engine, the tables (and table data) are stored in
the data nodes.  Such tables are directly accessible from all other
MySQL servers (SQL nodes) in the cluster.  Thus, in a payroll
application storing data in a cluster, if one application updates the
salary of an employee, all other MySQL servers that query this data can
see this change immediately.

Although an NDB Cluster SQL node uses the *note 'mysqld': mysqld. server
daemon, it differs in a number of critical respects from the *note
'mysqld': mysqld. binary supplied with the MySQL 5.5 distributions, and
the two versions of *note 'mysqld': mysqld. are not interchangeable.

In addition, a MySQL server that is not connected to an NDB Cluster
cannot use the *note 'NDB': mysql-cluster. storage engine and cannot
access any NDB Cluster data.

The data stored in the data nodes for NDB Cluster can be mirrored; the
cluster can handle failures of individual data nodes with no other
impact than that a small number of transactions are aborted due to
losing the transaction state.  Because transactional applications are
expected to handle transaction failure, this should not be a source of
problems.

Individual nodes can be stopped and restarted, and can then rejoin the
system (cluster).  Rolling restarts (in which all nodes are restarted in
turn) are used in making configuration changes and software upgrades
(see *note mysql-cluster-rolling-restart::).  Rolling restarts are also
used as part of the process of adding new data nodes online (see *note
mysql-cluster-online-add-node::).  For more information about data
nodes, how they are organized in an NDB Cluster, and how they handle and
store NDB Cluster data, see *note mysql-cluster-nodes-groups::.

Backing up and restoring NDB Cluster databases can be done using the
'NDB'-native functionality found in the NDB Cluster management client
and the *note 'ndb_restore': mysql-cluster-programs-ndb-restore. program
included in the NDB Cluster distribution.  For more information, see
*note mysql-cluster-backup::, and *note
mysql-cluster-programs-ndb-restore::.  You can also use the standard
MySQL functionality provided for this purpose in *note 'mysqldump':
mysqldump. and the MySQL server.  See *note mysqldump::, for more
information.

NDB Cluster nodes can employ different transport mechanisms for
inter-node communications; TCP/IP over standard 100 Mbps or faster
Ethernet hardware is used in most real-world deployments.


File: manual.info.tmp,  Node: mysql-cluster-basics,  Next: mysql-cluster-nodes-groups,  Prev: mysql-cluster-overview,  Up: mysql-cluster-overview

18.1.1 NDB Cluster Core Concepts
--------------------------------

_*note 'NDBCLUSTER': mysql-cluster._ (also known as *note 'NDB':
mysql-cluster.) is an in-memory storage engine offering
high-availability and data-persistence features.

The *note 'NDBCLUSTER': mysql-cluster. storage engine can be configured
with a range of failover and load-balancing options, but it is easiest
to start with the storage engine at the cluster level.  NDB Cluster's
*note 'NDB': mysql-cluster. storage engine contains a complete set of
data, dependent only on other data within the cluster itself.

The 'Cluster' portion of NDB Cluster is configured independently of the
MySQL servers.  In an NDB Cluster, each part of the cluster is
considered to be a _node_.

*Note*:

In many contexts, the term 'node' is used to indicate a computer, but
when discussing NDB Cluster it means a _process_.  It is possible to run
multiple nodes on a single computer; for a computer on which one or more
cluster nodes are being run we use the term _cluster host_.

There are three types of cluster nodes, and in a minimal NDB Cluster
configuration, there will be at least three nodes, one of each of these
types:

   * _Management node_: The role of this type of node is to manage the
     other nodes within the NDB Cluster, performing such functions as
     providing configuration data, starting and stopping nodes, and
     running backups.  Because this node type manages the configuration
     of the other nodes, a node of this type should be started first,
     before any other node.  An MGM node is started with the command
     *note 'ndb_mgmd': mysql-cluster-programs-ndb-mgmd.

   * _Data node_: This type of node stores cluster data.  There are as
     many data nodes as there are replicas, times the number of
     fragments (see *note mysql-cluster-nodes-groups::).  For example,
     with two replicas, each having two fragments, you need four data
     nodes.  One replica is sufficient for data storage, but provides no
     redundancy; therefore, it is recommended to have 2 (or more)
     replicas to provide redundancy, and thus high availability.  A data
     node is started with the command *note 'ndbd':
     mysql-cluster-programs-ndbd. (see *note
     mysql-cluster-programs-ndbd::) or *note 'ndbmtd':
     mysql-cluster-programs-ndbmtd. (see *note
     mysql-cluster-programs-ndbmtd::).

     NDB Cluster tables are normally stored completely in memory rather
     than on disk (this is why we refer to NDB Cluster as an _in-memory_
     database).  However, some NDB Cluster data can be stored on disk;
     see *note mysql-cluster-disk-data::, for more information.

   * _SQL node_: This is a node that accesses the cluster data.  In the
     case of NDB Cluster, an SQL node is a traditional MySQL server that
     uses the *note 'NDBCLUSTER': mysql-cluster. storage engine.  An SQL
     node is a *note 'mysqld': mysqld. process started with the
     '--ndbcluster' and '--ndb-connectstring' options, which are
     explained elsewhere in this chapter, possibly with additional MySQL
     server options as well.

     An SQL node is actually just a specialized type of _API node_,
     which designates any application which accesses NDB Cluster data.
     Another example of an API node is the *note 'ndb_restore':
     mysql-cluster-programs-ndb-restore. utility that is used to restore
     a cluster backup.  It is possible to write such applications using
     the NDB API. For basic information about the NDB API, see Getting
     Started with the NDB API
     (https://dev.mysql.com/doc/ndbapi/en/ndb-getting-started.html).

*Important*:

It is not realistic to expect to employ a three-node setup in a
production environment.  Such a configuration provides no redundancy; to
benefit from NDB Cluster's high-availability features, you must use
multiple data and SQL nodes.  The use of multiple management nodes is
also highly recommended.

For a brief introduction to the relationships between nodes, node
groups, replicas, and partitions in NDB Cluster, see *note
mysql-cluster-nodes-groups::.

Configuration of a cluster involves configuring each individual node in
the cluster and setting up individual communication links between nodes.
NDB Cluster is currently designed with the intention that data nodes are
homogeneous in terms of processor power, memory space, and bandwidth.
In addition, to provide a single point of configuration, all
configuration data for the cluster as a whole is located in one
configuration file.

The management server manages the cluster configuration file and the
cluster log.  Each node in the cluster retrieves the configuration data
from the management server, and so requires a way to determine where the
management server resides.  When interesting events occur in the data
nodes, the nodes transfer information about these events to the
management server, which then writes the information to the cluster log.

In addition, there can be any number of cluster client processes or
applications.  These include standard MySQL clients, 'NDB'-specific API
programs, and management clients.  These are described in the next few
paragraphs.

Standard MySQL clients

NDB Cluster can be used with existing MySQL applications written in PHP,
Perl, C, C++, Java, Python, Ruby, and so on.  Such client applications
send SQL statements to and receive responses from MySQL servers acting
as NDB Cluster SQL nodes in much the same way that they interact with
standalone MySQL servers.

MySQL clients using an NDB Cluster as a data source can be modified to
take advantage of the ability to connect with multiple MySQL servers to
achieve load balancing and failover.  For example, Java clients using
Connector/J 5.0.6 and later can use 'jdbc:mysql:loadbalance://' URLs
(improved in Connector/J 5.1.7) to achieve load balancing transparently;
for more information about using Connector/J with NDB Cluster, see Using
Connector/J with NDB Cluster
(https://dev.mysql.com/doc/ndbapi/en/mccj-using-connectorj.html).

NDB client programs

Client programs can be written that access NDB Cluster data directly
from the 'NDBCLUSTER' storage engine, bypassing any MySQL Servers that
may be connected to the cluster, using the _NDB API_, a high-level C++
API. Such applications may be useful for specialized purposes where an
SQL interface to the data is not needed.  For more information, see The
NDB API (https://dev.mysql.com/doc/ndbapi/en/ndbapi.html).

'NDB'-specific Java applications can also be written for NDB Cluster
using the _NDB Cluster Connector for Java_.  This NDB Cluster Connector
includes _ClusterJ_, a high-level database API similar to
object-relational mapping persistence frameworks such as Hibernate and
JPA that connect directly to 'NDBCLUSTER', and so does not require
access to a MySQL Server.  Support is also provided in NDB Cluster for
_ClusterJPA_, an OpenJPA implementation for NDB Cluster that leverages
the strengths of ClusterJ and JDBC; ID lookups and other fast operations
are performed using ClusterJ (bypassing the MySQL Server), while more
complex queries that can benefit from MySQL's query optimizer are sent
through the MySQL Server, using JDBC. See Java and NDB Cluster
(https://dev.mysql.com/doc/ndbapi/en/mccj-overview-java.html), and The
ClusterJ API and Data Object Model
(https://dev.mysql.com/doc/ndbapi/en/mccj-overview-clusterj-object-models.html),
for more information.

The Memcache API for NDB Cluster, implemented as the loadable
_ndbmemcache_ storage engine for memcached version 1.6 and later, is
available beginning with NDB 7.2.2.  This API can be used to provide a
persistent NDB Cluster data store, accessed using the memcache protocol.

The standard 'memcached' caching engine is included in the NDB Cluster
7.2 distribution (7.2.2 and later).  Each 'memcached' server has direct
access to data stored in NDB Cluster, but is also able to cache data
locally and to serve (some) requests from this local cache.

For more information, see ndbmemcache--Memcache API for NDB Cluster
(https://dev.mysql.com/doc/ndbapi/en/ndbmemcache.html).

Management clients

These clients connect to the management server and provide commands for
starting and stopping nodes gracefully, starting and stopping message
tracing (debug versions only), showing node versions and status,
starting and stopping backups, and so on.  An example of this type of
program is the *note 'ndb_mgm': mysql-cluster-programs-ndb-mgm.
management client supplied with NDB Cluster (see *note
mysql-cluster-programs-ndb-mgm::).  Such applications can be written
using the _MGM API_, a C-language API that communicates directly with
one or more NDB Cluster management servers.  For more information, see
The MGM API (https://dev.mysql.com/doc/ndbapi/en/mgm-api.html).

Oracle also makes available MySQL Cluster Manager, which provides an
advanced command-line interface simplifying many complex NDB Cluster
management tasks, such restarting an NDB Cluster with a large number of
nodes.  The MySQL Cluster Manager client also supports commands for
getting and setting the values of most node configuration parameters as
well as *note 'mysqld': mysqld. server options and variables relating to
NDB Cluster.  See MySQL(tm) Cluster Manager 1.3.6 User Manual
(https://dev.mysql.com/doc/mysql-cluster-manager/1.3/en/), for more
information.

Event logs

NDB Cluster logs events by category (startup, shutdown, errors,
checkpoints, and so on), priority, and severity.  A complete listing of
all reportable events may be found in *note
mysql-cluster-event-reports::.  Event logs are of the two types listed
here:

   * _Cluster log_: Keeps a record of all desired reportable events for
     the cluster as a whole.

   * _Node log_: A separate log which is also kept for each individual
     node.

*Note*:

Under normal circumstances, it is necessary and sufficient to keep and
examine only the cluster log.  The node logs need be consulted only for
application development and debugging purposes.

Checkpoint

Generally speaking, when data is saved to disk, it is said that a
_checkpoint_ has been reached.  More specific to NDB Cluster, a
checkpoint is a point in time where all committed transactions are
stored on disk.  With regard to the *note 'NDB': mysql-cluster. storage
engine, there are two types of checkpoints which work together to ensure
that a consistent view of the cluster's data is maintained.  These are
shown in the following list:

   * _Local Checkpoint (LCP)_: This is a checkpoint that is specific to
     a single node; however, LCPs take place for all nodes in the
     cluster more or less concurrently.  An LCP involves saving all of a
     node's data to disk, and so usually occurs every few minutes.  The
     precise interval varies, and depends upon the amount of data stored
     by the node, the level of cluster activity, and other factors.

   * _Global Checkpoint (GCP)_: A GCP occurs every few seconds, when
     transactions for all nodes are synchronized and the redo-log is
     flushed to disk.

For more information about the files and directories created by local
checkpoints and global checkpoints, see NDB Cluster Data Node File
System Directory Files
(https://dev.mysql.com/doc/ndb-internals/en/ndb-internals-ndbd-filesystemdir-files.html).


File: manual.info.tmp,  Node: mysql-cluster-nodes-groups,  Next: mysql-cluster-overview-requirements,  Prev: mysql-cluster-basics,  Up: mysql-cluster-overview

18.1.2 NDB Cluster Nodes, Node Groups, Replicas, and Partitions
---------------------------------------------------------------

This section discusses the manner in which NDB Cluster divides and
duplicates data for storage.

A number of concepts central to an understanding of this topic are
discussed in the next few paragraphs.

Data node

An *note 'ndbd': mysql-cluster-programs-ndbd. or *note 'ndbmtd':
mysql-cluster-programs-ndbmtd. process, which stores one or more
_replicas_--that is, copies of the _partitions_ (discussed later in this
section) assigned to the node group of which the node is a member.

Each data node should be located on a separate computer.  While it is
also possible to host multiple data node processes on a single computer,
such a configuration is not usually recommended.

It is common for the terms 'node' and 'data node' to be used
interchangeably when referring to an *note 'ndbd':
mysql-cluster-programs-ndbd. or *note 'ndbmtd':
mysql-cluster-programs-ndbmtd. process; where mentioned, management
nodes (*note 'ndb_mgmd': mysql-cluster-programs-ndb-mgmd. processes) and
SQL nodes (*note 'mysqld': mysqld. processes) are specified as such in
this discussion.

Node group

A node group consists of one or more nodes, and stores partitions, or
sets of _replicas_ (see next item).

The number of node groups in an NDB Cluster is not directly
configurable; it is a function of the number of data nodes and of the
number of replicas ('NoOfReplicas' configuration parameter), as shown
here:

     [# of node groups] = [# of data nodes] / NoOfReplicas

Thus, an NDB Cluster with 4 data nodes has 4 node groups if
'NoOfReplicas' is set to 1 in the 'config.ini' file, 2 node groups if
'NoOfReplicas' is set to 2, and 1 node group if 'NoOfReplicas' is set to
4.  Replicas are discussed later in this section; for more information
about 'NoOfReplicas', see *note mysql-cluster-ndbd-definition::.

*Note*:

All node groups in an NDB Cluster must have the same number of data
nodes.

You can add new node groups (and thus new data nodes) online, to a
running NDB Cluster; see *note mysql-cluster-online-add-node::, for more
information.

Partition

This is a portion of the data stored by the cluster.  Each node is
responsible for keeping at least one copy of any partitions assigned to
it (that is, at least one replica) available to the cluster.

The number of partitions used by default by NDB Cluster depends on the
number of data nodes and the number of LDM threads in use by the data
nodes, as shown here:

     [# of partitions] = [# of data nodes] * [# of LDM threads]

When using data nodes running *note 'ndbmtd':
mysql-cluster-programs-ndbmtd, the number of LDM threads is controlled
by the setting for 'MaxNoOfExecutionThreads'.  When using *note 'ndbd':
mysql-cluster-programs-ndbd. there is a single LDM thread, which means
that there are as many cluster partitions as nodes participating in the
cluster.  This is also the case when using *note 'ndbmtd':
mysql-cluster-programs-ndbmtd. with 'MaxNoOfExecutionThreads' set to 3
or less.  (You should be aware that the number of LDM threads increases
with the value of this parameter, but not in a strictly linear fashion,
and that there are additional constraints on setting it; see the
description of 'MaxNoOfExecutionThreads' for more information.)

NDB and user-defined partitioning

NDB Cluster normally partitions *note 'NDBCLUSTER': mysql-cluster.
tables automatically.  However, it is also possible to employ
user-defined partitioning with *note 'NDBCLUSTER': mysql-cluster.
tables.  This is subject to the following limitations:

  1. Only the 'KEY' and 'LINEAR KEY' partitioning schemes are supported
     in production with *note 'NDB': mysql-cluster. tables.

  2. The maximum number of partitions that may be defined explicitly for
     any *note 'NDB': mysql-cluster. table is '8 * [NUMBER OF LDM
     THREADS] * [NUMBER OF NODE GROUPS]', the number of node groups in
     an NDB Cluster being determined as discussed previously in this
     section.  When running *note 'ndbd': mysql-cluster-programs-ndbd.
     for data node processes, setting the number of LDM threads has no
     effect (since 'ThreadConfig' applies only to *note 'ndbmtd':
     mysql-cluster-programs-ndbmtd.); in such cases, this value can be
     treated as though it were equal to 1 for purposes of performing
     this calculation.

     See *note mysql-cluster-programs-ndbmtd::, for more information.

For more information relating to NDB Cluster and user-defined
partitioning, see *note mysql-cluster-limitations::, and *note
partitioning-limitations-storage-engines::.

Replica

This is a copy of a cluster partition.  Each node in a node group stores
a replica.  Also sometimes known as a _partition replica_.  The number
of replicas is equal to the number of nodes per node group.

A replica belongs entirely to a single node; a node can (and usually
does) store several replicas.

The following diagram illustrates an NDB Cluster with four data nodes
running *note 'ndbd': mysql-cluster-programs-ndbd, arranged in two node
groups of two nodes each; nodes 1 and 2 belong to node group 0, and
nodes 3 and 4 belong to node group 1.

*Note*:

Only data nodes are shown here; although a working NDB Cluster requires
an *note 'ndb_mgmd': mysql-cluster-programs-ndb-mgmd. process for
cluster management and at least one SQL node to access the data stored
by the cluster, these have been omitted from the figure for clarity.

FIGURE GOES HERE: NDB Cluster with Two Node Groups

The data stored by the cluster is divided into four partitions, numbered
0, 1, 2, and 3.  Each partition is stored--in multiple copies--on the
same node group.  Partitions are stored on alternate node groups as
follows:

   * Partition 0 is stored on node group 0; a _primary replica_ (primary
     copy) is stored on node 1, and a _backup replica_ (backup copy of
     the partition) is stored on node 2.

   * Partition 1 is stored on the other node group (node group 1); this
     partition's primary replica is on node 3, and its backup replica is
     on node 4.

   * Partition 2 is stored on node group 0.  However, the placing of its
     two replicas is reversed from that of Partition 0; for Partition 2,
     the primary replica is stored on node 2, and the backup on node 1.

   * Partition 3 is stored on node group 1, and the placement of its two
     replicas are reversed from those of partition 1.  That is, its
     primary replica is located on node 4, with the backup on node 3.

What this means regarding the continued operation of an NDB Cluster is
this: so long as each node group participating in the cluster has at
least one node operating, the cluster has a complete copy of all data
and remains viable.  This is illustrated in the next diagram.

FIGURE GOES HERE: Nodes Required for a 2x2 NDB Cluster

In this example, the cluster consists of two node groups each consisting
of two data nodes.  Each data node is running an instance of *note
'ndbd': mysql-cluster-programs-ndbd.  Any combination of at least one
node from node group 0 and at least one node from node group 1 is
sufficient to keep the cluster 'alive'.  However, if both nodes from a
single node group fail, the combination consisting of the remaining two
nodes in the other node group is not sufficient.  In this situation, the
cluster has lost an entire partition and so can no longer provide access
to a complete set of all NDB Cluster data.


File: manual.info.tmp,  Node: mysql-cluster-overview-requirements,  Next: mysql-cluster-what-is-new,  Prev: mysql-cluster-nodes-groups,  Up: mysql-cluster-overview

18.1.3 NDB Cluster Hardware, Software, and Networking Requirements
------------------------------------------------------------------

One of the strengths of NDB Cluster is that it can be run on commodity
hardware and has no unusual requirements in this regard, other than for
large amounts of RAM, due to the fact that all live data storage is done
in memory.  (It is possible to reduce this requirement using Disk Data
tables--see *note mysql-cluster-disk-data::, for more information about
these.)  Naturally, multiple and faster CPUs can enhance performance.
Memory requirements for other NDB Cluster processes are relatively
small.

The software requirements for NDB Cluster are also modest.  Host
operating systems do not require any unusual modules, services,
applications, or configuration to support NDB Cluster.  For supported
operating systems, a standard installation should be sufficient.  The
MySQL software requirements are simple: all that is needed is a
production release of NDB Cluster.  It is not strictly necessary to
compile MySQL yourself merely to be able to use NDB Cluster.  We assume
that you are using the binaries appropriate to your platform, available
from the NDB Cluster software downloads page at
<https://dev.mysql.com/downloads/cluster/>.

For communication between nodes, NDB Cluster supports TCP/IP networking
in any standard topology, and the minimum expected for each host is a
standard 100 Mbps Ethernet card, plus a switch, hub, or router to
provide network connectivity for the cluster as a whole.  We strongly
recommend that an NDB Cluster be run on its own subnet which is not
shared with machines not forming part of the cluster for the following
reasons:

   * Security

     Communications between NDB Cluster nodes are not encrypted or
     shielded in any way.  The only means of protecting transmissions
     within an NDB Cluster is to run your NDB Cluster on a protected
     network.  If you intend to use NDB Cluster for Web applications,
     the cluster should definitely reside behind your firewall and not
     in your network's De-Militarized Zone (DMZ
     (http://compnetworking.about.com/cs/networksecurity/g/bldef_dmz.htm))
     or elsewhere.

     See *note mysql-cluster-security-networking-issues::, for more
     information.

   * Efficiency

     Setting up an NDB Cluster on a private or protected network enables
     the cluster to make exclusive use of bandwidth between cluster
     hosts.  Using a separate switch for your NDB Cluster not only helps
     protect against unauthorized access to NDB Cluster data, it also
     ensures that NDB Cluster nodes are shielded from interference
     caused by transmissions between other computers on the network.
     For enhanced reliability, you can use dual switches and dual cards
     to remove the network as a single point of failure; many device
     drivers support failover for such communication links.

Network communication and latency

NDB Cluster requires communication between data nodes and API nodes
(including SQL nodes), as well as between data nodes and other data
nodes, to execute queries and updates.  Communication latency between
these processes can directly affect the observed performance and latency
of user queries.  In addition, to maintain consistency and service
despite the silent failure of nodes, NDB Cluster uses heartbeating and
timeout mechanisms which treat an extended loss of communication from a
node as node failure.  This can lead to reduced redundancy.  Recall
that, to maintain data consistency, an NDB Cluster shuts down when the
last node in a node group fails.  Thus, to avoid increasing the risk of
a forced shutdown, breaks in communication between nodes should be
avoided wherever possible.

The failure of a data or API node results in the abort of all
uncommitted transactions involving the failed node.  Data node recovery
requires synchronization of the failed node's data from a surviving data
node, and re-establishment of disk-based redo and checkpoint logs,
before the data node returns to service.  This recovery can take some
time, during which the Cluster operates with reduced redundancy.

Heartbeating relies on timely generation of heartbeat signals by all
nodes.  This may not be possible if the node is overloaded, has
insufficient machine CPU due to sharing with other programs, or is
experiencing delays due to swapping.  If heartbeat generation is
sufficiently delayed, other nodes treat the node that is slow to respond
as failed.

This treatment of a slow node as a failed one may or may not be
desirable in some circumstances, depending on the impact of the node's
slowed operation on the rest of the cluster.  When setting timeout
values such as 'HeartbeatIntervalDbDb' and 'HeartbeatIntervalDbApi' for
NDB Cluster, care must be taken care to achieve quick detection,
failover, and return to service, while avoiding potentially expensive
false positives.

Where communication latencies between data nodes are expected to be
higher than would be expected in a LAN environment (on the order of 100
µs), timeout parameters must be increased to ensure that any allowed
periods of latency periods are well within configured timeouts.
Increasing timeouts in this way has a corresponding effect on the
worst-case time to detect failure and therefore time to service
recovery.

LAN environments can typically be configured with stable low latency,
and such that they can provide redundancy with fast failover.
Individual link failures can be recovered from with minimal and
controlled latency visible at the TCP level (where NDB Cluster normally
operates).  WAN environments may offer a range of latencies, as well as
redundancy with slower failover times.  Individual link failures may
require route changes to propagate before end-to-end connectivity is
restored.  At the TCP level this can appear as large latencies on
individual channels.  The worst-case observed TCP latency in these
scenarios is related to the worst-case time for the IP layer to reroute
around the failures.

SCI (Scalable Coherent Interface)

It may also be possible to use the high-speed Scalable Coherent
Interface (SCI) with NDB Cluster, but this is no longer officially
supported.  See *note mysql-cluster-interconnects::, for more
information.


File: manual.info.tmp,  Node: mysql-cluster-what-is-new,  Next: mysql-cluster-added-deprecated-removed,  Prev: mysql-cluster-overview-requirements,  Up: mysql-cluster-overview

18.1.4 What is New in MySQL NDB Cluster 7.2
-------------------------------------------

In this section, we discuss changes in the implementation of NDB Cluster
in MySQL NDB Cluster 7.2, as compared to NDB Cluster 7.1 and earlier
releases.  Changes and features most likely to be of interest are shown
in the following list:

   * NDB Cluster 7.2 is based on MySQL 5.5.  For more information about
     new features in MySQL Server 5.5, see *note mysql-nutshell::.

   * Version 2 binary log row events, to provide support for
     improvements in NDB Cluster Replication conflict detection (see
     next item).  A given *note 'mysqld': mysqld. can be made to use
     Version 1 or Version 2 binary logging row events by setting the
     'log_bin_use_v1_row_events' system variable.

   * Two new 'primary wins' conflict detection and resolution functions
     'NDB$EPOCH()' and 'NDB$EPOCH_TRANS()' for use in replication setups
     with 2 NDB Clusters.  For more information, see *note
     mysql-cluster-replication::.

   * Distribution of MySQL users and privileges across NDB Cluster SQL
     nodes is now supported--see *note
     mysql-cluster-privilege-distribution::.

   * Improved support for distributed pushed-down joins, which greatly
     improve performance for many joins that can be executed in parallel
     on the data nodes.

   * Default values for a number of data node configuration parameters
     such as 'HeartbeatIntervalDbDb' and 'ArbitrationTimeout' have been
     improved.

   * Support for the Memcache API using the loadable ndbmemcache storage
     engine.  See ndbmemcache--Memcache API for NDB Cluster
     (https://dev.mysql.com/doc/ndbapi/en/ndbmemcache.html).

This section contains information about NDB Cluster 7.2 releases through
5.5.65-ndb-7.2.39, which is a previous GA release but still supported.
NDB 7.1 and earlier releases series are no longer maintained or
supported in production.  We recommend that new deployments use NDB
Cluster 7.6, which is the most recent series of General Availability
releases; see MySQL NDB Cluster 7.5 and NDB Cluster 7.6
(https://dev.mysql.com/doc/refman/5.7/en/mysql-cluster.html).  NDB
Cluster 8.0, currently under development, is available for evaluation
and testing as a Developer Preview release; for more information, see
MySQL NDB Cluster 8.0
(https://dev.mysql.com/doc/refman/8.0/en/mysql-cluster.html).  NDB
Cluster 7.5 is a previous GA release still supported in production; see
MySQL NDB Cluster 7.5 and NDB Cluster 7.6
(https://dev.mysql.com/doc/refman/5.7/en/mysql-cluster.html), for more
information.  NDB 7.4 and 7.3 are older GA releases still supported in
production; see MySQL NDB Cluster 7.3 and NDB Cluster 7.4
(https://dev.mysql.com/doc/refman/5.6/en/mysql-cluster.html).

The following improvements to NDB Cluster have been made in NDB Cluster
7.2:

   * Based on MySQL Server 5.5

     Previous NDB Cluster release series, including NDB Cluster 7.1,
     used MySQL 5.1 as a base.  Beginning with NDB 7.2.1, NDB Cluster
     7.2 is based on MySQL Server 5.5, so that NDB Cluster users can
     benefit from MySQL 5.5's improvements in scalability and
     performance monitoring.  As with MySQL 5.5, NDB 7.2.1 and later use
     'CMake' for configuring and building from source in place of GNU
     Autotools (used in MySQL 5.1 and NDB Cluster releases based on
     MySQL 5.1).  For more information about changes and improvements in
     MySQL 5.5, see *note mysql-nutshell::.

   * Conflict detection using GCI Reflection

     NDB Cluster Replication implements a new 'primary wins' conflict
     detection and resolution mechanism.  _GCI Reflection_ applies in
     two-cluster circulation 'active-active' replication setups,
     tracking the order in which changes are applied on the NDB Cluster
     designated as primary relative to changes originating on the other
     NDB Cluster (referred to as the secondary).  This relative ordering
     is used to determine whether changes originating on the slave are
     concurrent with any changes that originate locally, and are
     therefore potentially in conflict.  Two new conflict detection
     functions are added: When using 'NDB$EPOCH()', rows that are out of
     sync on the secondary are realigned with those on the primary; with
     'NDB$EPOCH_TRANS()', this realignment is applied to transactions.
     For more information, see *note
     mysql-cluster-replication-conflict-resolution::.

   * Version 2 binary log row events

     A new format for binary log row events, known as Version 2 binary
     log row events, provides support for improvements in NDB Cluster
     Replication conflict detection (see previous item) and is intended
     to facilitate further improvements in MySQL Replication.  You can
     cause a given *note 'mysqld': mysqld. use Version 1 or Version 2
     binary logging row events by setting the
     'log_bin_use_v1_row_events' system variable.  For backward
     compatibility, Version 2 binary log row events are also available
     in NDB Cluster 7.0 (7.0.27 and later) and NDB Cluster 7.1 (7.1.16
     and later).  However, NDB Cluster 7.0 and NDB Cluster 7.1 continue
     to use Version 1 binary log row events as the default, whereas the
     default in NDB 7.2.1 and later is use Version 2 row events for
     binary logging.

   * Distribution of MySQL users and privileges

     Automatic distribution of MySQL users and privileges across all SQL
     nodes in a given NDB Cluster is now supported.  To enable this
     support, you must first import an SQL script
     'share/mysql/ndb_dist_priv.sql' that is included with the NDB
     Cluster 7.2 distribution.  This script creates several stored
     procedures which you can use to enable privilege distribution and
     perform related tasks.

     When a new MySQL Server joins an NDB Cluster where privilege
     distribution is in effect, it also participates in the privilege
     distribution automatically.

     Once privilege distribution is enabled, all changes to the grant
     tables made on any *note 'mysqld': mysqld. attached to the cluster
     are immediately available on any other attached MySQL Servers.
     This is true whether the changes are made using *note 'CREATE
     USER': create-user, *note 'GRANT': grant, or any of the other
     statements described elsewhere in this Manual (see *note
     account-management-statements::.)  This includes privileges
     relating to stored routines and views; however, automatic
     distribution of the views or stored routines themselves is not
     currently supported.

     For more information, see *note
     mysql-cluster-privilege-distribution::.

   * Distributed pushed-down joins

     Many joins can now be pushed down to the NDB kernel for processing
     on NDB Cluster data nodes.  Previously, a join was handled in NDB
     Cluster by means of repeated accesses of *note 'NDB':
     mysql-cluster. by the SQL node; however, when pushed-down joins are
     enabled, a pushable join is sent in its entirety to the data nodes,
     where it can be distributed among the data nodes and executed in
     parallel on multiple copies of the data, with a single, merged
     result being returned to *note 'mysqld': mysqld.  This can reduce
     greatly the number of round trips between an SQL node and the data
     nodes required to handle such a join, leading to greatly improved
     performance of join processing.

     It is possible to determine when joins can be pushed down to the
     data nodes by examining the join with *note 'EXPLAIN': explain.  A
     number of new system status variables
     ('Ndb_pushed_queries_defined', 'Ndb_pushed_queries_dropped',
     'Ndb_pushed_queries_executed', and 'Ndb_pushed_reads') and
     additions to the *note 'counters': mysql-cluster-ndbinfo-counters.
     table (in the *note 'ndbinfo': mysql-cluster-ndbinfo. information
     database) can also be helpful in determining when and how well
     joins are being pushed down.

     More information and examples are available in the description of
     the 'ndb_join_pushdown' server system variable.  See also the
     description of the status variables referenced in the previous
     paragraph, as well as *note mysql-cluster-ndbinfo-counters::.

   * Improved default values for data node configuration parameters

     In order to provide more resiliency to environmental issues and
     better handling of some potential failure scenarios, and to perform
     more reliably with increases in memory and other resource
     requirements brought about by recent improvements in join handling
     by *note 'NDB': mysql-cluster, the default values for a number of
     NDB Cluster data node configuration parameters have been changed.
     The parameters and changes are described in the following list:

        * 'HeartbeatIntervalDbDb': Default increased from 1500 ms to
          5000 ms.

        * 'ArbitrationTimeout': Default increased from 3000 ms to 7500
          ms.

        * 'TimeBetweenEpochsTimeout': Now effectively disabled by
          default (default changed from 4000 ms to 0).

        * 'SharedGlobalMemory': Default increased from 20 MB to 128 MB.

        * 'MaxParallelScansPerFragment': Default increased from 32 to
          256.

        * 'CrashOnCorruptedTuple' changed from 'FALSE' to 'TRUE'.

        * Beginning with NDB 7.2.10, 'DefaultOperationRedoProblemAction'
          changed from 'ABORT' to 'QUEUE'.

     In addition, the value computed for 'MaxNoOfLocalScans' when this
     parameter is not set in 'config.ini' has been increased by a factor
     of 4.

   * Fail-fast data nodes

     Beginning with NDB 7.2.1, data nodes handle corrupted tuples in a
     fail-fast manner by default.  This is a change from previous
     versions of NDB Cluster where this behavior had to be enabled
     explicitly by enabling the 'CrashOnCorruptedTuple' configuration
     parameter.  In NDB 7.2.1 and later, this parameter is enabled by
     default and must be explicitly disabled, in which case data nodes
     merely log a warning whenever they detect a corrupted tuple.

   * Memcache API support (ndbmemcache)

     The Memcached server is a distributed in-memory caching server that
     uses a simple text-based protocol.  It is often employed with
     key-value stores.  The Memcache API for NDB Cluster, available
     beginning with NDB 7.2.2, is implemented as a loadable storage
     engine for memcached version 1.6 and later.  This API can be used
     to access a persistent NDB Cluster data store employing the
     memcache protocol.  It is also possible for the 'memcached' server
     to provide a strictly defined interface to existing NDB Cluster
     tables.

     Each memcache server can both cache data locally and access data
     stored in NDB Cluster directly.  Caching policies are configurable.
     For more information, see ndbmemcache--Memcache API for NDB Cluster
     (https://dev.mysql.com/doc/ndbapi/en/ndbmemcache.html), in the 'NDB
     Cluster API Developers Guide'.

   * Rows per partition limit removed

     Previously it was possible to store a maximum of 46137488 rows in a
     single NDB Cluster partition--that is, per data node.  Beginning
     with NDB 7.2.9, this limitation has been lifted, and there is no
     longer any practical upper limit to this number.  (Bug #13844405,
     Bug #14000373)

NDB Cluster 7.2 is also supported by MySQL Cluster Manager, which
provides an advanced command-line interface that can simplify many
complex NDB Cluster management tasks.  See MySQL(tm) Cluster Manager
1.3.6 User Manual
(https://dev.mysql.com/doc/mysql-cluster-manager/1.3/en/), for more
information.


File: manual.info.tmp,  Node: mysql-cluster-added-deprecated-removed,  Next: mysql-cluster-compared,  Prev: mysql-cluster-what-is-new,  Up: mysql-cluster-overview

18.1.5 Options, Variables, and Parameters Added, Deprecated or Removed in NDB 7.2
---------------------------------------------------------------------------------

   * *note params-added-ndb-7.2::

   * *note params-deprecated-ndb-7.2::

   * *note optvars-removed-ndb-7.2::

   * *note optvars-added-ndb-7.2::

   * *note optvars-deprecated-ndb-7.2::

   * *note params-removed-ndb-7.2::

The next few sections contain information about 'NDB' configuration
parameters and NDB-specific *note 'mysqld': mysqld. options and
variables that have been added to, deprecated in, or removed from NDB
7.2.

*Node Configuration Parameters Introduced in NDB 7.2*

The following node configuration parameters have been added in NDB 7.2.

   * 'ConnectBackoffMaxTime': Specifies longest time in milliseconds
     (~100ms resolution) to allow between connection attempts to any
     given data node by this API node.  Excludes time elapsed while
     connection attempts are ongoing, which in worst case can take
     several seconds.  Disable by setting to 0.  If no data nodes are
     currently connected to this API node, StartConnectBackoffMaxTime is
     used instead.  Added in NDB 7.2.18.

   * 'CrashOnCorruptedTuple': When enabled, forces node to shut down
     whenever it detects a corrupted tuple.  Added in NDB 7.2.1.

   * 'DefaultHashMapSize': Set size (in buckets) to use for table hash
     maps.  Three values are supported: 0, 240, and 3840.  Intended
     primarily for upgrades and downgrades within NDB 7.2.  Added in NDB
     7.2.11.

   * 'DefaultHashMapSize': Set size (in buckets) to use for table hash
     maps.  Three values are supported: 0, 240, and 3840.  Intended
     primarily for upgrades and downgrades within NDB 7.2.  Added in NDB
     7.2.11.

   * 'DiskPageBufferEntries': Number of 32 KB page entries to allocate
     in DiskPageBufferMemory.  Very large disk transactions may require
     increasing this value.  Added in NDB 7.2.19.

   * 'ExtraSendBufferMemory': Memory to use for send buffers in addition
     to any allocated by TotalSendBufferMemory or SendBufferMemory.
     Default (0) allows up to 16MB. Added in NDB 7.2.14.

   * 'ExtraSendBufferMemory': Memory to use for send buffers in addition
     to any allocated by TotalSendBufferMemory or SendBufferMemory.
     Default (0) allows up to 16MB. Added in NDB 7.2.14.

   * 'ExtraSendBufferMemory': Memory to use for send buffers in addition
     to any allocated by TotalSendBufferMemory or SendBufferMemory.
     Default (0) allows up to 16MB. Added in NDB 7.2.5.

   * 'HeartbeatIntervalMgmdMgmd': Time between
     management-node-to-management-node heartbeats; connection between
     management nodes is considered lost after 3 missed heartbeats.
     Added in NDB 7.2.12.

   * 'IndexStatAutoCreate': Enable/disable automatic statistics
     collection when indexes are created.  Added in NDB 7.2.1.

   * 'IndexStatAutoUpdate': Monitor indexes for changes and trigger
     automatic statistics updates.  Added in NDB 7.2.1.

   * 'IndexStatSaveScale': Scaling factor used in determining size of
     stored index statistics.  Added in NDB 7.2.1.

   * 'IndexStatSaveSize': Maximum size in bytes for saved statistics per
     index.  Added in NDB 7.2.1.

   * 'IndexStatTriggerPct': Threshold percent change in DML operations
     for index statistics updates.  Value is scaled down by
     IndexStatTriggerScale.  Added in NDB 7.2.1.

   * 'IndexStatTriggerScale': Scale down IndexStatTriggerPct by this
     amount, multiplied by base 2 logarithm of index size, for a large
     index.  Set to 0 to disable scaling.  Added in NDB 7.2.1.

   * 'IndexStatUpdateDelay': Minimum delay between automatic index
     statistics updates for a given index.  0 means no delay.  Added in
     NDB 7.2.1.

   * 'LcpScanProgressTimeout': Maximum time that local checkpoint
     fragment scan can be stalled before node is shut down to ensure
     systemwide LCP progress.  Use 0 to disable.  Added in NDB 7.2.14.

   * 'MaxBufferedEpochBytes': Total number of bytes allocated for
     buffering epochs.  Added in NDB 7.2.13.

   * 'MinFreePct': Percentage of memory resources to keep in reserve for
     restarts.  Added in NDB 7.2.3.

   * 'NoOfFragmentLogParts': Number of redo log file groups belonging to
     this data node; value must be an even multiple of 4.  Added in NDB
     7.2.5.

   * 'RestartSubscriberConnectTimeout': Amount of time for data node to
     wait for subscribing API nodes to connect.  Set to 0 to disable
     timeout, which is always resolved to nearest full second.  Added in
     NDB 7.2.17.

   * 'StartConnectBackoffMaxTime': Same as ConnectBackoffMaxTime except
     that this parameter is used in its place if no data nodes are
     connected to this API node.  Added in NDB 7.2.18.

   * 'ThreadConfig': Used for configuration of multithreaded data nodes
     (ndbmtd).  Default is an empty string; see documentation for syntax
     and other information.  Added in NDB 7.2.3.

   * 'TimeBetweenGlobalCheckpointsTimeout': Minimum timeout for group
     commit of transactions to disk.  Added in NDB 7.2.20.

*Node Configuration Parameters Deprecated in NDB 7.2*

The following node configuration parameters have been deprecated in NDB
7.2.

   * 'ReservedSendBufferMemory': This parameter is present in NDB code
     but is not enabled, and is now deprecated.  Deprecated as of NDB
     7.2.5.

*MySQL Server Options and Variables Removed in NDB 7.2*

The following *note 'mysqld': mysqld. system variables, status
variables, and options have been removed in NDB 7.2.

   * 'ndb_index_stat_cache_entries': Sets the granularity of the
     statistics by determining the number of starting and ending keys.
     Removed in NDB 7.2.16.

   * 'ndb_index_stat_update_freq': How often to query data nodes instead
     of the statistics cache.  Removed in NDB 7.2.16.

*MySQL Server Options and Variables Introduced in NDB 7.2*

The following *note 'mysqld': mysqld. system variables, status
variables, and options have been added in NDB 7.2.

   * 'Ndb_conflict_fn_epoch': Number of rows that have been found in
     conflict by the NDB$EPOCH() conflict detection function.  Added in
     NDB 7.2.1.

   * 'Ndb_conflict_fn_epoch_trans': Number of rows that have been found
     in conflict by the NDB$EPOCH_TRANS() conflict detection function.
     Added in NDB 7.2.1.

   * 'Ndb_conflict_trans_conflict_commit_count': Number of epoch
     transactions committed after requiring transactional conflict
     handling.  Added in NDB 7.2.1.

   * 'Ndb_conflict_trans_detect_iter_count': Number of internal
     iterations required to commit an epoch transaction.  Should be
     (slightly) greater than or equal to
     Ndb_conflict_trans_conflict_commit_count.  Added in NDB 7.2.1.

   * 'Ndb_conflict_trans_reject_count': Number of transactions rejected
     after being found in conflict by a transactional conflict function.
     Added in NDB 7.2.1.

   * 'Ndb_conflict_trans_row_reject_count': Total number of rows
     realigned after being found in conflict by a transactional conflict
     function.  Includes Ndb_conflict_trans_row_conflict_count and any
     rows included in or dependent on conflicting transactions.  Added
     in NDB 7.2.1.

   * 'Ndb_pushed_queries_defined': Number of joins that API nodes have
     attempted to push down to the data nodes.  Added in NDB 7.2.0.

   * 'Ndb_pushed_queries_dropped': Number of joins that API nodes have
     tried to push down, but failed.  Added in NDB 7.2.0.

   * 'Ndb_pushed_queries_executed': Number of joins successfully pushed
     down and executed on the data nodes.  Added in NDB 7.2.0.

   * 'Ndb_pushed_reads': Number of reads executed on the data nodes by
     pushed-down joins.  Added in NDB 7.2.0.

   * 'ndb-log-transaction-id': Write NDB transaction IDs in the binary
     log.  Requires -log-bin-v1-events=OFF. Added in NDB 7.2.1.

   * 'ndb-transid-mysql-connection-map': Enable or disable the
     ndb_transid_mysql_connection_map plugin; that is, enable or disable
     the INFORMATION_SCHEMA table having that name.  Added in NDB 7.2.2.

   * 'ndb_eventbuffer_max_alloc': Maximum memory that can be allocated
     for buffering events by the NDB API. Defaults to 0 (no limit).
     Added in NDB 7.2.14.

   * 'ndb_index_stat_option': Comma-separated list of tunable options
     for NDB index statistics; the list should contain no spaces.  Added
     in NDB 7.2.1.

   * 'ndb_join_pushdown': Enables pushing down of joins to data nodes.
     Added in NDB 7.2.0.

   * 'ndb_log_transaction_id': Whether NDB transaction IDs are written
     into the binary log (Read-only.).  Added in NDB 7.2.1.

*MySQL Server Options and Variables Deprecated in NDB 7.2*

No system variables, status variables, or options have been deprecated
in NDB 7.2.

*Node Configuration Parameters Removed in NDB 7.2*

No node configuration parameters have been removed from NDB 7.2.


File: manual.info.tmp,  Node: mysql-cluster-compared,  Next: mysql-cluster-limitations,  Prev: mysql-cluster-added-deprecated-removed,  Up: mysql-cluster-overview

18.1.6 MySQL Server Using InnoDB Compared with NDB Cluster
----------------------------------------------------------

* Menu:

* mysql-cluster-ndb-innodb-engines::  Differences Between the NDB and InnoDB Storage Engines
* mysql-cluster-ndb-innodb-workloads::  NDB and InnoDB Workloads
* mysql-cluster-ndb-innodb-usage::  NDB and InnoDB Feature Usage Summary

MySQL Server offers a number of choices in storage engines.  Since both
*note 'NDB': mysql-cluster. and *note 'InnoDB': innodb-storage-engine.
can serve as transactional MySQL storage engines, users of MySQL Server
sometimes become interested in NDB Cluster.  They see *note 'NDB':
mysql-cluster. as a possible alternative or upgrade to the default *note
'InnoDB': innodb-storage-engine. storage engine in MySQL 5.5.  While
*note 'NDB': mysql-cluster. and *note 'InnoDB': innodb-storage-engine.
share common characteristics, there are differences in architecture and
implementation, so that some existing MySQL Server applications and
usage scenarios can be a good fit for NDB Cluster, but not all of them.

In this section, we discuss and compare some characteristics of the
*note 'NDB': mysql-cluster. storage engine used by NDB Cluster 7.2 with
*note 'InnoDB': innodb-storage-engine. used in MySQL 5.5.  The next few
sections provide a technical comparison.  In many instances, decisions
about when and where to use NDB Cluster must be made on a case-by-case
basis, taking all factors into consideration.  While it is beyond the
scope of this documentation to provide specifics for every conceivable
usage scenario, we also attempt to offer some very general guidance on
the relative suitability of some common types of applications for *note
'NDB': mysql-cluster. as opposed to *note 'InnoDB':
innodb-storage-engine. back ends.

Recent NDB Cluster 7.2 releases use a *note 'mysqld': mysqld. based on
MySQL 5.5, including support for *note 'InnoDB': innodb-storage-engine.
1.1.  While it is possible to use 'InnoDB' tables with NDB Cluster, such
tables are not clustered.  It is also not possible to use programs or
libraries from an NDB Cluster 7.2 distribution with MySQL Server 5.5, or
the reverse.

While it is also true that some types of common business applications
can be run either on NDB Cluster or on MySQL Server (most likely using
the *note 'InnoDB': innodb-storage-engine. storage engine), there are
some important architectural and implementation differences.  *note
mysql-cluster-ndb-innodb-engines::, provides a summary of the these
differences.  Due to the differences, some usage scenarios are clearly
more suitable for one engine or the other; see *note
mysql-cluster-ndb-innodb-workloads::.  This in turn has an impact on the
types of applications that better suited for use with *note 'NDB':
mysql-cluster. or *note 'InnoDB': innodb-storage-engine.  See *note
mysql-cluster-ndb-innodb-usage::, for a comparison of the relative
suitability of each for use in common types of database applications.

For information about the relative characteristics of the *note 'NDB':
mysql-cluster. and *note 'MEMORY': memory-storage-engine. storage
engines, see *note memory-storage-engine-compared-cluster::.

See *note storage-engines::, for additional information about MySQL
storage engines.


File: manual.info.tmp,  Node: mysql-cluster-ndb-innodb-engines,  Next: mysql-cluster-ndb-innodb-workloads,  Prev: mysql-cluster-compared,  Up: mysql-cluster-compared

18.1.6.1 Differences Between the NDB and InnoDB Storage Engines
...............................................................

The *note 'NDB': mysql-cluster. storage engine is implemented using a
distributed, shared-nothing architecture, which causes it to behave
differently from *note 'InnoDB': innodb-storage-engine. in a number of
ways.  For those unaccustomed to working with *note 'NDB':
mysql-cluster, unexpected behaviors can arise due to its distributed
nature with regard to transactions, foreign keys, table limits, and
other characteristics.  These are shown in the following table:

*Feature differences between the InnoDB and NDB storage engines.*

Feature                  'InnoDB' 1.1             *note 'NDB': mysql-cluster.
                                                  7.2
                                                  
_MySQL Server Version_   5.5                      5.5
                                                  
_*note 'InnoDB': innodb-storage-engine.*note 'InnoDB': innodb-storage-engine.*note 'InnoDB': innodb-storage-engine.
Version _                1.1                      1.1
                                                  
_NDB Cluster Version_    N/A                      *note 'NDB': mysql-cluster.
                                                  7.2.39
                                                  
_Storage Limits_         64TB                     3TB (Practical upper
                                                  limit based on 48 data
                                                  nodes with 64GB RAM
                                                  each; can be increased
                                                  with disk-based data
                                                  and BLOBs)
                                                  
_Foreign Keys_           Yes                      Available in NDB
                                                  Cluster 7.3 and later
                                                  (NDB 7.2 ignores them,
                                                  as with
                                                  *note 'MyISAM': myisam-storage-engine.)
                                                  
_Transactions_           All standard types       'READ COMMITTED'
                                                  
_MVCC_                   Yes                      No
                                                  
_Data Compression_       Yes                      No (NDB checkpoint and
                                                  backup files can be
                                                  compressed)
                                                  
_Large Row Support (>    Supported for            Supported for
14K)_                    *note 'VARBINARY': binary-varbinary,*note 'BLOB': blob.
                         *note 'VARCHAR': char,   and
                         *note 'BLOB': blob,      *note 'TEXT': blob.
                         and                      columns only (Using
                         *note 'TEXT': blob.      these types to store
                         columns                  very large amounts of
                                                  data can lower NDB
                                                  performance)
                                                  
_Replication Support_    Asynchronous and         Automatic synchronous
                         semisynchronous          replication within an
                         replication using        NDB Cluster;
                         MySQL Replication        asynchronous
                                                  replication between
                                                  NDB Clusters, using
                                                  MySQL Replication
                                                  (Semisynchronous
                                                  replication is not
                                                  supported)
                                                  
_Scaleout for Read       Yes (MySQL               Yes (Automatic
Operations_              Replication)             partitioning in NDB
                                                  Cluster; NDB Cluster
                                                  Replication)
                                                  
_Scaleout for Write      Requires                 Yes (Automatic
Operations_              application-level        partitioning in NDB
                         partitioning             Cluster is transparent
                         (sharding)               to applications)
                                                  
_High Availability       Requires additional      Yes (Designed for
(HA)_                    software                 99.999% uptime)
                                                  
_Node Failure Recovery   Requires additional      Automatic (Key element
and Failover_            software                 in NDB architecture)
                                                  
_Time for Node Failure   30 seconds or longer     Typically < 1 second
Recovery_                                         

_Real-Time               No                       Yes
Performance_                                      

_In-Memory Tables_       No                       Yes (Some data can
                                                  optionally be stored
                                                  on disk; both
                                                  in-memory and disk
                                                  data storage are
                                                  durable)
                                                  
_NoSQL Access to         Yes                      Yes (Multiple APIs,
Storage Engine_                                   including Memcached,
                                                  Node.js/JavaScript,
                                                  Java, JPA, C++, and
                                                  HTTP/REST)
                                                  
_Concurrent and          Not supported            Up to 48 writers,
Parallel Writes_                                  optimized for
                                                  concurrent writes
                                                  
_Conflict Detection      No                       Yes
and Resolution                                    
(Multiple Replication
Masters)_

_Hash Indexes_           No                       Yes
                                                  
_Online Addition of      Read-only replicas       Yes (all node types)
Nodes_                   using MySQL              
                         Replication
                         
_Online Upgrades_        No                       Yes
                                                  
_Online Schema           No                       Yes
Modifications_           


File: manual.info.tmp,  Node: mysql-cluster-ndb-innodb-workloads,  Next: mysql-cluster-ndb-innodb-usage,  Prev: mysql-cluster-ndb-innodb-engines,  Up: mysql-cluster-compared

18.1.6.2 NDB and InnoDB Workloads
.................................

NDB Cluster has a range of unique attributes that make it ideal to serve
applications requiring high availability, fast failover, high
throughput, and low latency.  Due to its distributed architecture and
multi-node implementation, NDB Cluster also has specific constraints
that may keep some workloads from performing well.  A number of major
differences in behavior between the *note 'NDB': mysql-cluster. and
*note 'InnoDB': innodb-storage-engine. storage engines with regard to
some common types of database-driven application workloads are shown in
the following table::

*Major differences between the InnoDB and NDB storage engines, common
types of database-driven application workloads.*

Workload                 *note 'InnoDB': innodb-storage-engine.NDB Cluster
                                                  (*note 'NDB': mysql-cluster.)
                                                  
_High-Volume OLTP        Yes                      Yes
Applications_                                     

_DSS Applications        Yes                      Limited (Join
(data marts,                                      operations across OLTP
analytics)_                                       datasets not exceeding
                                                  3TB in size)
                                                  
_Custom Applications_    Yes                      Yes
                                                  
_Packaged                Yes                      Limited (should be
Applications_                                     mostly primary key
                                                  access); NDB Cluster
                                                  7.3 adds support for
                                                  foreign keys
                                                  
_In-Network Telecoms     No                       Yes
Applications (HLR,                                
HSS, SDP)_

_Session Management      Yes                      Yes
and Caching_                                      

_E-Commerce              Yes                      Yes
Applications_                                     

_User Profile            Yes                      Yes
Management, AAA          
Protocol_


File: manual.info.tmp,  Node: mysql-cluster-ndb-innodb-usage,  Prev: mysql-cluster-ndb-innodb-workloads,  Up: mysql-cluster-compared

18.1.6.3 NDB and InnoDB Feature Usage Summary
.............................................

When comparing application feature requirements to the capabilities of
*note 'InnoDB': innodb-storage-engine. with *note 'NDB': mysql-cluster,
some are clearly more compatible with one storage engine than the other.

The following table lists supported application features according to
the storage engine to which each feature is typically better suited.

*Supported application features according to the storage engine to which
each feature is typically better suited*

Preferred application requirements   Preferred application requirements
for                                  for *note 'NDB': mysql-cluster.
*note 'InnoDB': innodb-storage-engine.

   * Foreign keys                       * Write scaling
                                     
     *Note*:                            * 99.999% uptime
                                     
     NDB Cluster 7.3 adds support       * Online addition of nodes and
     for foreign keys.                    online schema operations
                                     
   * Full table scans                   * Multiple SQL and NoSQL APIs
                                          (see NDB Cluster APIs:
   * Very large databases, rows,          Overview and Concepts
     or transactions                      (https://dev.mysql.com/doc/ndbapi/en/mysql-cluster-api-overview.html))
                                     
   * Transactions other than 'READ      * Real-time performance
     COMMITTED'                      
                                        * Limited use of
                                          *note 'BLOB': blob. columns


File: manual.info.tmp,  Node: mysql-cluster-limitations,  Prev: mysql-cluster-compared,  Up: mysql-cluster-overview

18.1.7 Known Limitations of NDB Cluster
---------------------------------------

* Menu:

* mysql-cluster-limitations-syntax::  Noncompliance with SQL Syntax in NDB Cluster
* mysql-cluster-limitations-limits::  Limits and Differences of NDB Cluster from Standard MySQL Limits
* mysql-cluster-limitations-transactions::  Limits Relating to Transaction Handling in NDB Cluster
* mysql-cluster-limitations-error-handling::  NDB Cluster Error Handling
* mysql-cluster-limitations-database-objects::  Limits Associated with Database Objects in NDB Cluster
* mysql-cluster-limitations-unsupported::  Unsupported or Missing Features in NDB Cluster
* mysql-cluster-limitations-performance::  Limitations Relating to Performance in NDB Cluster
* mysql-cluster-limitations-exclusive-to-cluster::  Issues Exclusive to NDB Cluster
* mysql-cluster-limitations-disk-data::  Limitations Relating to NDB Cluster Disk Data Storage
* mysql-cluster-limitations-multiple-nodes::  Limitations Relating to Multiple NDB Cluster Nodes
* mysql-cluster-limitations-resolved::  Previous NDB Cluster Issues Resolved in MySQL 5.1, NDB Cluster 6.x, and NDB Cluster 7.x

In the sections that follow, we discuss known limitations in current
releases of NDB Cluster as compared with the features available when
using the 'MyISAM' and 'InnoDB' storage engines.  If you check the
'Cluster' category in the MySQL bugs database at
<http://bugs.mysql.com>, you can find known bugs in the following
categories under 'MySQL Server:' in the MySQL bugs database at
<http://bugs.mysql.com>, which we intend to correct in upcoming releases
of NDB Cluster:

   * NDB Cluster

   * Cluster Direct API (NDBAPI)

   * Cluster Disk Data

   * Cluster Replication

   * ClusterJ

This information is intended to be complete with respect to the
conditions just set forth.  You can report any discrepancies that you
encounter to the MySQL bugs database using the instructions given in
*note bug-reports::.  If we do not plan to fix the problem in NDB
Cluster 7.2, we will add it to the list.

See *note mysql-cluster-limitations-resolved:: for a list of issues in
NDB Cluster in MySQL 5.1 that have been resolved in the current version.

*Note*:

Limitations and other issues specific to NDB Cluster Replication are
described in *note mysql-cluster-replication-issues::.


File: manual.info.tmp,  Node: mysql-cluster-limitations-syntax,  Next: mysql-cluster-limitations-limits,  Prev: mysql-cluster-limitations,  Up: mysql-cluster-limitations

18.1.7.1 Noncompliance with SQL Syntax in NDB Cluster
.....................................................

Some SQL statements relating to certain MySQL features produce errors
when used with *note 'NDB': mysql-cluster. tables, as described in the
following list:

   * Temporary tables

     Temporary tables are not supported.  Trying either to create a
     temporary table that uses the *note 'NDB': mysql-cluster. storage
     engine or to alter an existing temporary table to use *note 'NDB':
     mysql-cluster. fails with the error 'Table storage engine
     'ndbcluster' does not support the create option 'TEMPORARY''.

   * Indexes and keys in NDB tables

     Keys and indexes on NDB Cluster tables are subject to the following
     limitations:

        * Column width

          Attempting to create an index on an 'NDB' table column whose
          width is greater than 3072 bytes succeeds, but only the first
          3072 bytes are actually used for the index.  In such cases, a
          warning 'Specified key was too long; max key length is 3072
          bytes' is issued, and a *note 'SHOW CREATE TABLE':
          show-create-table. statement shows the length of the index as
          3072.

        * TEXT and BLOB columns

          You cannot create indexes on *note 'NDB': mysql-cluster. table
          columns that use any of the *note 'TEXT': blob. or *note
          'BLOB': blob. data types.

        * FULLTEXT indexes

          The *note 'NDB': mysql-cluster. storage engine does not
          support 'FULLTEXT' indexes, which are possible for 'MyISAM'
          tables only.

          However, you can create indexes on *note 'VARCHAR': char.
          columns of *note 'NDB': mysql-cluster. tables.

        * USING HASH keys and NULL

          Using nullable columns in unique keys and primary keys means
          that queries using these columns are handled as full table
          scans.  To work around this issue, make the column 'NOT NULL',
          or re-create the index without the 'USING HASH' option.

        * Prefixes

          There are no prefix indexes; only entire columns can be
          indexed.  (The size of an 'NDB' column index is always the
          same as the width of the column in bytes, up to and including
          3072 bytes, as described earlier in this section.  Also see
          *note mysql-cluster-limitations-unsupported::, for additional
          information.)

        * BIT columns

          A *note 'BIT': bit-type. column cannot be a primary key,
          unique key, or index, nor can it be part of a composite
          primary key, unique key, or index.

        * AUTO_INCREMENT columns

          Like other MySQL storage engines, the *note 'NDB':
          mysql-cluster. storage engine can handle a maximum of one
          'AUTO_INCREMENT' column per table, and this column must be
          indexed.  However, in the case of an NDB table with no
          explicit primary key, an 'AUTO_INCREMENT' column is
          automatically defined and used as a 'hidden' primary key.  For
          this reason, you cannot create an 'NDB' table having an
          'AUTO_INCREMENT' column and no explicit primary key.

   * NDB Cluster and geometry data types

     Geometry data types ('WKT' and 'WKB') are supported for *note
     'NDB': mysql-cluster. tables.  However, spatial indexes are not
     supported.

   * Character sets and binary log files

     Currently, the 'ndb_apply_status' and 'ndb_binlog_index' tables are
     created using the 'latin1' (ASCII) character set.  Because names of
     binary logs are recorded in this table, binary log files named
     using non-Latin characters are not referenced correctly in these
     tables.  This is a known issue, which we are working to fix.  (Bug
     #50226)

     To work around this problem, use only Latin-1 characters when
     naming binary log files or setting any the '--basedir',
     '--log-bin', or '--log-bin-index' options.

   * Creating NDB tables with user-defined partitioning

     Support for user-defined partitioning in NDB Cluster is restricted
     to ['LINEAR'] 'KEY' partitioning.  Using any other partitioning
     type with 'ENGINE=NDB' or 'ENGINE=NDBCLUSTER' in a *note 'CREATE
     TABLE': create-table. statement results in an error.

     It is possible to override this restriction, but doing so is not
     supported for use in production settings.  For details, see *note
     partitioning-limitations-ndb::.

     Default partitioning scheme

     All NDB Cluster tables are by default partitioned by 'KEY' using
     the table's primary key as the partitioning key.  If no primary key
     is explicitly set for the table, the 'hidden' primary key
     automatically created by the *note 'NDB': mysql-cluster. storage
     engine is used instead.  For additional discussion of these and
     related issues, see *note partitioning-key::.

     *note 'CREATE TABLE': create-table. and *note 'ALTER TABLE':
     alter-table. statements that would cause a user-partitioned *note
     'NDBCLUSTER': mysql-cluster. table not to meet either or both of
     the following two requirements are not permitted, and fail with an
     error:

       1. The table must have an explicit primary key.

       2. All columns listed in the table's partitioning expression must
          be part of the primary key.

     Exception

     If a user-partitioned *note 'NDBCLUSTER': mysql-cluster. table is
     created using an empty column-list (that is, using 'PARTITION BY
     [LINEAR] KEY()'), then no explicit primary key is required.

     Maximum number of partitions for NDBCLUSTER tables

     The maximum number of partitions that can defined for a *note
     'NDBCLUSTER': mysql-cluster. table when employing user-defined
     partitioning is 8 per node group.  (See *note
     mysql-cluster-nodes-groups::, for more information about NDB
     Cluster node groups.

     DROP PARTITION not supported

     It is not possible to drop partitions from *note 'NDB':
     mysql-cluster. tables using 'ALTER TABLE ... DROP PARTITION'.  The
     other partitioning extensions to *note 'ALTER TABLE':
     alter-table.--'ADD PARTITION', 'REORGANIZE PARTITION', and
     'COALESCE PARTITION'--are supported for NDB tables, but use copying
     and so are not optimized.  See *note
     partitioning-management-range-list:: and *note alter-table::.

   * Row-based replication

     When using row-based replication with NDB Cluster, binary logging
     cannot be disabled.  That is, the *note 'NDB': mysql-cluster.
     storage engine ignores the value of 'sql_log_bin'.  (Bug #16680)


File: manual.info.tmp,  Node: mysql-cluster-limitations-limits,  Next: mysql-cluster-limitations-transactions,  Prev: mysql-cluster-limitations-syntax,  Up: mysql-cluster-limitations

18.1.7.2 Limits and Differences of NDB Cluster from Standard MySQL Limits
.........................................................................

In this section, we list limits found in NDB Cluster that either differ
from limits found in, or that are not found in, standard MySQL.

Memory usage and recovery

Memory consumed when data is inserted into an *note 'NDB':
mysql-cluster. table is not automatically recovered when deleted, as it
is with other storage engines.  Instead, the following rules hold true:

   * A *note 'DELETE': delete. statement on an *note 'NDB':
     mysql-cluster. table makes the memory formerly used by the deleted
     rows available for re-use by inserts on the same table only.
     However, this memory can be made available for general re-use by
     performing *note 'OPTIMIZE TABLE': optimize-table.

     A rolling restart of the cluster also frees any memory used by
     deleted rows.  See *note mysql-cluster-rolling-restart::.

   * A *note 'DROP TABLE': drop-table. or *note 'TRUNCATE TABLE':
     truncate-table. operation on an *note 'NDB': mysql-cluster. table
     frees the memory that was used by this table for re-use by any
     *note 'NDB': mysql-cluster. table, either by the same table or by
     another *note 'NDB': mysql-cluster. table.

     *Note*:

     Recall that *note 'TRUNCATE TABLE': truncate-table. drops and
     re-creates the table.  See *note truncate-table::.

   * Limits imposed by the cluster's configuration

     A number of hard limits exist which are configurable, but available
     main memory in the cluster sets limits.  See the complete list of
     configuration parameters in *note mysql-cluster-config-file::.
     Most configuration parameters can be upgraded online.  These hard
     limits include:

        * Database memory size and index memory size ('DataMemory' and
          'IndexMemory', respectively).

          'DataMemory' is allocated as 32KB pages.  As each 'DataMemory'
          page is used, it is assigned to a specific table; once
          allocated, this memory cannot be freed except by dropping the
          table.

          See *note mysql-cluster-ndbd-definition::, for more
          information.

        * The maximum number of operations that can be performed per
          transaction is set using the configuration parameters
          'MaxNoOfConcurrentOperations' and 'MaxNoOfLocalOperations'.

          *Note*:

          Bulk loading, *note 'TRUNCATE TABLE': truncate-table, and
          *note 'ALTER TABLE': alter-table. are handled as special cases
          by running multiple transactions, and so are not subject to
          this limitation.

        * Different limits related to tables and indexes.  For example,
          the maximum number of ordered indexes in the cluster is
          determined by 'MaxNoOfOrderedIndexes', and the maximum number
          of ordered indexes per table is 16.

   * Node and data object maximums

     The following limits apply to numbers of cluster nodes and metadata
     objects:

        * The maximum number of data nodes is 48.

          A data node must have a node ID in the range of 1 to 48,
          inclusive.  (Management and API nodes may use node IDs in the
          range 1 to 255, inclusive.)

        * The total maximum number of nodes in an NDB Cluster is 255.
          This number includes all SQL nodes (MySQL Servers), API nodes
          (applications accessing the cluster other than MySQL servers),
          data nodes, and management servers.

        * The maximum number of metadata objects in current versions of
          NDB Cluster is 20320.  This limit is hard-coded.

     See *note mysql-cluster-limitations-resolved::, for more
     information.


File: manual.info.tmp,  Node: mysql-cluster-limitations-transactions,  Next: mysql-cluster-limitations-error-handling,  Prev: mysql-cluster-limitations-limits,  Up: mysql-cluster-limitations

18.1.7.3 Limits Relating to Transaction Handling in NDB Cluster
...............................................................

A number of limitations exist in NDB Cluster with regard to the handling
of transactions.  These include the following:

   * Transaction isolation level

     The *note 'NDBCLUSTER': mysql-cluster. storage engine supports only
     the 'READ COMMITTED' transaction isolation level.  ('InnoDB', for
     example, supports 'READ COMMITTED', 'READ UNCOMMITTED', 'REPEATABLE
     READ', and 'SERIALIZABLE'.)  You should keep in mind that 'NDB'
     implements 'READ COMMITTED' on a per-row basis; when a read request
     arrives at the data node storing the row, what is returned is the
     last committed version of the row at that time.

     Uncommitted data is never returned, but when a transaction
     modifying a number of rows commits concurrently with a transaction
     reading the same rows, the transaction performing the read can
     observe 'before' values, 'after' values, or both, for different
     rows among these, due to the fact that a given row read request can
     be processed either before or after the commit of the other
     transaction.

     To ensure that a given transaction reads only before or after
     values, you can impose row locks using *note 'SELECT ... LOCK IN
     SHARE MODE': select.  In such cases, the lock is held until the
     owning transaction is committed.  Using row locks can also cause
     the following issues:

        * Increased frequency of lock wait timeout errors, and reduced
          concurrency

        * Increased transaction processing overhead due to reads
          requiring a commit phase

        * Possibility of exhausting the available number of concurrent
          locks, which is limited by 'MaxNoOfConcurrentOperations'

     'NDB' uses 'READ COMMITTED' for all reads unless a modifier such as
     'LOCK IN SHARE MODE' or 'FOR UPDATE' is used.  'LOCK IN SHARE MODE'
     causes shared row locks to be used; 'FOR UPDATE' causes exclusive
     row locks to be used.  Unique key reads have their locks upgraded
     automatically by 'NDB' to ensure a self-consistent read; 'BLOB'
     reads also employ extra locking for consistency.

     See *note mysql-cluster-backup-troubleshooting::, for information
     on how NDB Cluster's implementation of transaction isolation level
     can affect backup and restoration of 'NDB' databases.

   * Transactions and BLOB or TEXT columns

     *note 'NDBCLUSTER': mysql-cluster. stores only part of a column
     value that uses any of MySQL's *note 'BLOB': blob. or *note 'TEXT':
     blob. data types in the table visible to MySQL; the remainder of
     the *note 'BLOB': blob. or *note 'TEXT': blob. is stored in a
     separate internal table that is not accessible to MySQL. This gives
     rise to two related issues of which you should be aware whenever
     executing *note 'SELECT': select. statements on tables that contain
     columns of these types:

       1. For any *note 'SELECT': select. from an NDB Cluster table: If
          the *note 'SELECT': select. includes a *note 'BLOB': blob. or
          *note 'TEXT': blob. column, the 'READ COMMITTED' transaction
          isolation level is converted to a read with read lock.  This
          is done to guarantee consistency.

       2. For any *note 'SELECT': select. which uses a unique key lookup
          to retrieve any columns that use any of the *note 'BLOB':
          blob. or *note 'TEXT': blob. data types and that is executed
          within a transaction, a shared read lock is held on the table
          for the duration of the transaction--that is, until the
          transaction is either committed or aborted.

          This issue does not occur for queries that use index or table
          scans, even against *note 'NDB': mysql-cluster. tables having
          *note 'BLOB': blob. or *note 'TEXT': blob. columns.

          For example, consider the table 't' defined by the following
          *note 'CREATE TABLE': create-table. statement:

               CREATE TABLE t (
                   a INT NOT NULL AUTO_INCREMENT PRIMARY KEY,
                   b INT NOT NULL,
                   c INT NOT NULL,
                   d TEXT,
                   INDEX i(b),
                   UNIQUE KEY u(c)
               ) ENGINE = NDB,

          Either of the following queries on 't' causes a shared read
          lock, because the first query uses a primary key lookup and
          the second uses a unique key lookup:

               SELECT * FROM t WHERE a = 1;

               SELECT * FROM t WHERE c = 1;

          However, none of the four queries shown here causes a shared
          read lock:

               SELECT * FROM t WHERE b = 1;

               SELECT * FROM t WHERE d = '1';

               SELECT * FROM t;

               SELECT b,c WHERE a = 1;

          This is because, of these four queries, the first uses an
          index scan, the second and third use table scans, and the
          fourth, while using a primary key lookup, does not retrieve
          the value of any *note 'BLOB': blob. or *note 'TEXT': blob.
          columns.

          You can help minimize issues with shared read locks by
          avoiding queries that use unique key lookups that retrieve
          *note 'BLOB': blob. or *note 'TEXT': blob. columns, or, in
          cases where such queries are not avoidable, by committing
          transactions as soon as possible afterward.

   * Rollbacks

     There are no partial transactions, and no partial rollbacks of
     transactions.  A duplicate key or similar error causes the entire
     transaction to be rolled back.

     This behavior differs from that of other transactional storage
     engines such as *note 'InnoDB': innodb-storage-engine. that may
     roll back individual statements.

   * Transactions and memory usage

     As noted elsewhere in this chapter, NDB Cluster does not handle
     large transactions well; it is better to perform a number of small
     transactions with a few operations each than to attempt a single
     large transaction containing a great many operations.  Among other
     considerations, large transactions require very large amounts of
     memory.  Because of this, the transactional behavior of a number of
     MySQL statements is affected as described in the following list:

        * *note 'TRUNCATE TABLE': truncate-table. is not transactional
          when used on *note 'NDB': mysql-cluster. tables.  If a *note
          'TRUNCATE TABLE': truncate-table. fails to empty the table,
          then it must be re-run until it is successful.

        * 'DELETE FROM' (even with no 'WHERE' clause) _is_
          transactional.  For tables containing a great many rows, you
          may find that performance is improved by using several 'DELETE
          FROM ... LIMIT ...' statements to 'chunk' the delete
          operation.  If your objective is to empty the table, then you
          may wish to use *note 'TRUNCATE TABLE': truncate-table.
          instead.

        * LOAD DATA statements

          *note 'LOAD DATA': load-data. is not transactional when used
          on *note 'NDB': mysql-cluster. tables.

          *Important*:

          When executing a *note 'LOAD DATA': load-data. statement, the
          *note 'NDB': mysql-cluster. engine performs commits at
          irregular intervals that enable better utilization of the
          communication network.  It is not possible to know ahead of
          time when such commits take place.

        * ALTER TABLE and transactions

          When copying an *note 'NDB': mysql-cluster. table as part of
          an *note 'ALTER TABLE': alter-table, the creation of the copy
          is nontransactional.  (In any case, this operation is rolled
          back when the copy is deleted.)

   * Transactions and the COUNT() function

     When using NDB Cluster Replication, it is not possible to guarantee
     the transactional consistency of the 'COUNT()' function on the
     slave.  In other words, when performing on the master a series of
     statements (*note 'INSERT': insert, *note 'DELETE': delete, or
     both) that changes the number of rows in a table within a single
     transaction, executing 'SELECT COUNT(*) FROM TABLE' queries on the
     slave may yield intermediate results.  This is due to the fact that
     'SELECT COUNT(...)' may perform dirty reads, and is not a bug in
     the *note 'NDB': mysql-cluster. storage engine.  (See Bug #31321
     for more information.)


File: manual.info.tmp,  Node: mysql-cluster-limitations-error-handling,  Next: mysql-cluster-limitations-database-objects,  Prev: mysql-cluster-limitations-transactions,  Up: mysql-cluster-limitations

18.1.7.4 NDB Cluster Error Handling
...................................

Starting, stopping, or restarting a node may give rise to temporary
errors causing some transactions to fail.  These include the following
cases:

   * Temporary errors

     When first starting a node, it is possible that you may see Error
     1204 'Temporary failure, distribution changed' and similar
     temporary errors.

   * Errors due to node failure

     The stopping or failure of any data node can result in a number of
     different node failure errors.  (However, there should be no
     aborted transactions when performing a planned shutdown of the
     cluster.)

In either of these cases, any errors that are generated must be handled
within the application.  This should be done by retrying the
transaction.

See also *note mysql-cluster-limitations-limits::.


File: manual.info.tmp,  Node: mysql-cluster-limitations-database-objects,  Next: mysql-cluster-limitations-unsupported,  Prev: mysql-cluster-limitations-error-handling,  Up: mysql-cluster-limitations

18.1.7.5 Limits Associated with Database Objects in NDB Cluster
...............................................................

Some database objects such as tables and indexes have different
limitations when using the *note 'NDBCLUSTER': mysql-cluster. storage
engine:

   * Database and table names

     When using the 'NDB' storage engine, the maximum allowed length
     both for database names and for table names is 63 characters.

   * Number of database objects

     The maximum number of _all_ *note 'NDB': mysql-cluster. database
     objects in a single NDB Cluster--including databases, tables, and
     indexes--is limited to 20320.

   * Attributes per table

     The maximum number of attributes (that is, columns and indexes)
     that can belong to a given table is 512.

   * Attributes per key

     The maximum number of attributes per key is 32.

   * Row size

     The maximum permitted size of any one row is 14000 bytes.

     Each *note 'BLOB': blob. or *note 'TEXT': blob. column contributes
     256 + 8 = 264 bytes to this total; see *note
     data-types-storage-reqs-strings::, for more information relating to
     these types.

     In addition, the maximum offset for a fixed-width column of an
     'NDB' table is 8188 bytes; attempting to create a table that
     violates this limitation fails with NDB error 851 'Maximum offset
     for fixed-size columns exceeded'.  For memory-based columns, you
     can work around this limitation by using a variable-width column
     type such as *note 'VARCHAR': char. or defining the column as
     'COLUMN_FORMAT=DYNAMIC'; this does not work with columns stored on
     disk.  For disk-based columns, you may be able to do so by
     reordering one or more of the table's disk-based columns such that
     the combined width of all but the disk-based column defined last in
     the *note 'CREATE TABLE': create-table. statement used to create
     the table does not exceed 8188 bytes, less any possible rounding
     performed for some data types such as *note 'CHAR': char. or
     'VARCHAR'; otherwise it is necessary to use memory-based storage
     for one or more of the offending column or columns instead.

   * BIT column storage per table

     The maximum combined width for all *note 'BIT': bit-type. columns
     used in a given 'NDB' table is 4096.

   * FIXED column storage

     NDB Cluster supports a maximum of 16 GB per fragment of data in
     'FIXED' columns.


File: manual.info.tmp,  Node: mysql-cluster-limitations-unsupported,  Next: mysql-cluster-limitations-performance,  Prev: mysql-cluster-limitations-database-objects,  Up: mysql-cluster-limitations

18.1.7.6 Unsupported or Missing Features in NDB Cluster
.......................................................

A number of features supported by other storage engines are not
supported for *note 'NDB': mysql-cluster. tables.  Trying to use any of
these features in NDB Cluster does not cause errors in or of itself;
however, errors may occur in applications that expects the features to
be supported or enforced.  Statements referencing such features, even if
effectively ignored by 'NDB', must be syntactically and otherwise valid.

   * Foreign key constraints

     Prior to NDB Cluster 7.3, the foreign key construct is ignored,
     just as it is by 'MyISAM' tables.  Foreign keys are supported in
     NDB Cluster 7.3 and later.

   * Index prefixes

     Prefixes on indexes are not supported for 'NDB' tables.  If a
     prefix is used as part of an index specification in a statement
     such as *note 'CREATE TABLE': create-table, *note 'ALTER TABLE':
     alter-table, or *note 'CREATE INDEX': create-index, the prefix is
     not created by 'NDB'.

     A statement containing an index prefix, and creating or modifying
     an 'NDB' table, must still be syntactically valid.  For example,
     the following statement always fails with Error 1089 'Incorrect
     prefix key; the used key part isn't a string, the used length is
     longer than the key part, or the storage engine doesn't support
     unique prefix keys', regardless of storage engine:

          *note CREATE TABLE: create-table. t1 (
              c1 INT NOT NULL,
              _c2 VARCHAR(100),
              INDEX i1 (c2(500))_
          );

     This happens on account of the SQL syntax rule that no index may
     have a prefix larger than itself.

   * Savepoints and rollbacks

     Savepoints and rollbacks to savepoints are ignored as in *note
     'MyISAM': myisam-storage-engine.

   * Durability of commits

     There are no durable commits on disk.  Commits are replicated, but
     there is no guarantee that logs are flushed to disk on commit.

   * Replication

     Statement-based replication is not supported.  Use
     '--binlog-format=ROW' (or '--binlog-format=MIXED') when setting up
     cluster replication.  See *note mysql-cluster-replication::, for
     more information.

     Semisynchronous replication is not supported in NDB Cluster.

*Note*:

See *note mysql-cluster-limitations-transactions::, for more information
relating to limitations on transaction handling in *note 'NDB':
mysql-cluster.


File: manual.info.tmp,  Node: mysql-cluster-limitations-performance,  Next: mysql-cluster-limitations-exclusive-to-cluster,  Prev: mysql-cluster-limitations-unsupported,  Up: mysql-cluster-limitations

18.1.7.7 Limitations Relating to Performance in NDB Cluster
...........................................................

The following performance issues are specific to or especially
pronounced in NDB Cluster:

   * Range scans

     There are query performance issues due to sequential access to the
     *note 'NDB': mysql-cluster. storage engine; it is also relatively
     more expensive to do many range scans than it is with either
     'MyISAM' or 'InnoDB'.

   * Reliability of Records in range

     The 'Records in range' statistic is available but is not completely
     tested or officially supported.  This may result in nonoptimal
     query plans in some cases.  If necessary, you can employ 'USE
     INDEX' or 'FORCE INDEX' to alter the execution plan.  See *note
     index-hints::, for more information on how to do this.

   * Unique hash indexes

     Unique hash indexes created with 'USING HASH' cannot be used for
     accessing a table if 'NULL' is given as part of the key.


File: manual.info.tmp,  Node: mysql-cluster-limitations-exclusive-to-cluster,  Next: mysql-cluster-limitations-disk-data,  Prev: mysql-cluster-limitations-performance,  Up: mysql-cluster-limitations

18.1.7.8 Issues Exclusive to NDB Cluster
........................................

The following are limitations specific to the *note 'NDB':
mysql-cluster. storage engine:

   * Machine architecture

     All machines used in the cluster must have the same architecture.
     That is, all machines hosting nodes must be either big-endian or
     little-endian, and you cannot use a mixture of both.  For example,
     you cannot have a management node running on a PowerPC which
     directs a data node that is running on an x86 machine.  This
     restriction does not apply to machines simply running *note
     'mysql': mysql. or other clients that may be accessing the
     cluster's SQL nodes.

   * Binary logging

     NDB Cluster has the following limitations or restrictions with
     regard to binary logging:

        * 'sql_log_bin' has no effect on data operations; however, it is
          supported for schema operations.

        * NDB Cluster cannot produce a binary log for tables having
          *note 'BLOB': blob. columns but no primary key.

        * Only the following schema operations are logged in a cluster
          binary log which is _not_ on the *note 'mysqld': mysqld.
          executing the statement:

             * *note 'CREATE TABLE': create-table.

             * *note 'ALTER TABLE': alter-table.

             * *note 'DROP TABLE': drop-table.

             * *note 'CREATE DATABASE': create-database. / *note 'CREATE
               SCHEMA': create-database.

             * *note 'DROP DATABASE': drop-database. / *note 'DROP
               SCHEMA': drop-database.

             * *note 'CREATE TABLESPACE': create-tablespace.

             * *note 'ALTER TABLESPACE': alter-tablespace.

             * *note 'DROP TABLESPACE': drop-tablespace.

             * *note 'CREATE LOGFILE GROUP': create-logfile-group.

             * *note 'ALTER LOGFILE GROUP': alter-logfile-group.

             * *note 'DROP LOGFILE GROUP': drop-logfile-group.

   * Schema operations

     Schema operations (DDL statements) are rejected while any data node
     restarts.  Schema operations are also not supported while
     performing an online upgrade or downgrade.

   * Number of replicas

     The number of replicas, as determined by the 'NoOfReplicas' data
     node configuration parameter, is the number of copies of all data
     stored by NDB Cluster.  Setting this parameter to 1 means there is
     only a single copy; in this case, no redundancy is provided, and
     the loss of a data node entails loss of data.  To guarantee
     redundancy, and thus preservation of data even if a data node
     fails, set this parameter to 2, which is the default and
     recommended value in production.

     Setting 'NoOfReplicas' to a value greater than 2 is possible (to a
     maximum of 4) but unnecessary to guard against loss of data.  In
     addition, _values greater than 2 for this parameter are not
     supported in production_.

See also *note mysql-cluster-limitations-multiple-nodes::.


File: manual.info.tmp,  Node: mysql-cluster-limitations-disk-data,  Next: mysql-cluster-limitations-multiple-nodes,  Prev: mysql-cluster-limitations-exclusive-to-cluster,  Up: mysql-cluster-limitations

18.1.7.9 Limitations Relating to NDB Cluster Disk Data Storage
..............................................................

Disk Data object maximums and minimums

Disk data objects are subject to the following maximums and minimums:

   * Maximum number of tablespaces: 2^32 (4294967296)

   * Maximum number of data files per tablespace: 2^16 (65536)

   * The minimum and maximum possible sizes of extents for tablespace
     data files are 32K and 2G, respectively.  See *note
     create-tablespace::, for more information.

In addition, when working with NDB Disk Data tables, you should be aware
of the following issues regarding data files and extents:

   * Data files use 'DataMemory'.  Usage is the same as for in-memory
     data.

   * Data files use file descriptors.  It is important to keep in mind
     that data files are always open, which means the file descriptors
     are always in use and cannot be re-used for other system tasks.

   * Extents require sufficient 'DiskPageBufferMemory'; you must reserve
     enough for this parameter to account for all memory used by all
     extents (number of extents times size of extents).

Disk Data tables and diskless mode

Use of Disk Data tables is not supported when running the cluster in
diskless mode.  Beginning with MySQL 5.1.12, it is prohibited
altogether.  (Bug #20008)


File: manual.info.tmp,  Node: mysql-cluster-limitations-multiple-nodes,  Next: mysql-cluster-limitations-resolved,  Prev: mysql-cluster-limitations-disk-data,  Up: mysql-cluster-limitations

18.1.7.10 Limitations Relating to Multiple NDB Cluster Nodes
............................................................

Multiple SQL nodes

The following are issues relating to the use of multiple MySQL servers
as NDB Cluster SQL nodes, and are specific to the *note 'NDBCLUSTER':
mysql-cluster. storage engine:

   * No distributed table locks

     A *note 'LOCK TABLES': lock-tables. works only for the SQL node on
     which the lock is issued; no other SQL node in the cluster 'sees'
     this lock.  This is also true for a lock issued by any statement
     that locks tables as part of its operations.  (See next item for an
     example.)

   * ALTER TABLE operations

     *note 'ALTER TABLE': alter-table. is not fully locking when running
     multiple MySQL servers (SQL nodes).  (As discussed in the previous
     item, NDB Cluster does not support distributed table locks.)

Multiple management nodes

When using multiple management servers:

   * If any of the management servers are running on the same host, you
     must give nodes explicit IDs in connection strings because
     automatic allocation of node IDs does not work across multiple
     management servers on the same host.  This is not required if every
     management server resides on a different host.

   * When a management server starts, it first checks for any other
     management server in the same NDB Cluster, and upon successful
     connection to the other management server uses its configuration
     data.  This means that the management server '--reload' and
     '--initial' startup options are ignored unless the management
     server is the only one running.  It also means that, when
     performing a rolling restart of an NDB Cluster with multiple
     management nodes, the management server reads its own configuration
     file if (and only if) it is the only management server running in
     this NDB Cluster.  See *note mysql-cluster-rolling-restart::, for
     more information.

Multiple network addresses

Multiple network addresses per data node are not supported.  Use of
these is liable to cause problems: In the event of a data node failure,
an SQL node waits for confirmation that the data node went down but
never receives it because another route to that data node remains open.
This can effectively make the cluster inoperable.

*Note*:

It is possible to use multiple network hardware _interfaces_ (such as
Ethernet cards) for a single data node, but these must be bound to the
same address.  This also means that it not possible to use more than one
'[tcp]' section per connection in the 'config.ini' file.  See *note
mysql-cluster-tcp-definition::, for more information.


File: manual.info.tmp,  Node: mysql-cluster-limitations-resolved,  Prev: mysql-cluster-limitations-multiple-nodes,  Up: mysql-cluster-limitations

18.1.7.11 Previous NDB Cluster Issues Resolved in MySQL 5.1, NDB Cluster 6.x, and NDB Cluster 7.x
.................................................................................................

A number of limitations and related issues existing in earlier versions
of NDB Cluster have been resolved:

   * Variable-length column support

     The *note 'NDBCLUSTER': mysql-cluster. storage engine now supports
     variable-length column types for in-memory tables.

     Previously, for example, any Cluster table having one or more *note
     'VARCHAR': char. fields which contained only relatively small
     values, much more memory and disk space were required when using
     the *note 'NDBCLUSTER': mysql-cluster. storage engine than would
     have been the case for the same table and data using the 'MyISAM'
     engine.  In other words, in the case of a *note 'VARCHAR': char.
     column, such a column required the same amount of storage as a
     *note 'CHAR': char. column of the same size.  In MySQL 5.1, this is
     no longer the case for in-memory tables, where storage requirements
     for variable-length column types such as *note 'VARCHAR': char. and
     'BINARY' are comparable to those for these column types when used
     in 'MyISAM' tables (see *note storage-requirements::).

     *Important*:

     For NDB Cluster Disk Data tables, the fixed-width limitation
     continues to apply.  See *note mysql-cluster-disk-data::.

   * Replication with NDB Cluster

     It is now possible to use MySQL replication with Cluster databases.
     For details, see *note mysql-cluster-replication::.

     Circular Replication

     Circular replication is also supported with NDB Cluster, beginning
     with MySQL 5.1.18.  See *note
     mysql-cluster-replication-multi-master::.

   * auto_increment_increment and auto_increment_offset

     The 'auto_increment_increment' and 'auto_increment_offset' server
     system variables are supported for NDB Cluster Replication.

   * Backup and restore between architectures

     It is possible to perform a Cluster backup and restore between
     different architectures.  Previously--for example--you could not
     back up a cluster running on a big-endian platform and then restore
     from that backup to a cluster running on a little-endian system.
     (Bug #19255)

   * Multiple data nodes, multithreaded data nodes

     NDB Cluster 7.2 supports multiple data node processes on a single
     host as well as multithreaded data node processes.  See *note
     mysql-cluster-programs-ndbmtd::, for more information.

   * Identifiers

     Formerly (in MySQL 5.0 and earlier), database names, table names
     and attribute names could not be as long for *note 'NDB':
     mysql-cluster. tables as tables using other storage engines,
     because attribute names were truncated internally.  In MySQL 5.1
     and later, names of NDB Cluster databases, tables, and table
     columns follow the same rules regarding length as they do for any
     other storage engine.

   * Length of CREATE TABLE statements

     *note 'CREATE TABLE': create-table. statements may be no more than
     4096 characters in length.  _This limitation affects MySQL 5.1.6,
     5.1.7, and 5.1.8 only_.  (See Bug #17813)

   * IGNORE and REPLACE functionality

     In MySQL 5.1.7 and earlier, *note 'INSERT IGNORE': insert, *note
     'UPDATE IGNORE': update, and *note 'REPLACE': replace. were
     supported only for primary keys, but not for unique keys.  It was
     possible to work around this issue by removing the constraint, then
     dropping the unique index, performing any inserts, and then adding
     the unique index again.

     This limitation was removed for *note 'INSERT IGNORE': insert. and
     *note 'REPLACE': replace. in MySQL 5.1.8.  (See Bug #17431.)

   * AUTO_INCREMENT columns

     In MySQL 5.1.10 and earlier versions, the maximum number of tables
     having 'AUTO_INCREMENT' columns--including those belonging to
     hidden primary keys--was 2048.

     This limitation was lifted in MySQL 5.1.11.

   * Maximum number of cluster nodes

     The total maximum number of nodes in an NDB Cluster is 255,
     including all SQL nodes (MySQL Servers), API nodes (applications
     accessing the cluster other than MySQL servers), data nodes, and
     management servers.  The total number of data nodes and management
     nodes is 63, of which up to 48 can be data nodes.

     *Note*:

     A data node cannot have a node ID greater than 49.

   * Recovery of memory from deleted rows

     Memory can be reclaimed from an *note 'NDB': mysql-cluster. table
     for reuse with any *note 'NDB': mysql-cluster. table by employing
     *note 'OPTIMIZE TABLE': optimize-table, subject to the following
     limitations:

        * Only in-memory tables are supported; the *note 'OPTIMIZE
          TABLE': optimize-table. statement has no effect on NDB Cluster
          Disk Data tables.

        * Only variable-length columns (such as those declared as *note
          'VARCHAR': char, *note 'TEXT': blob, or *note 'BLOB': blob.)
          are supported.

          However, you can force columns defined using fixed-length data
          types (such as *note 'CHAR': char.) to be dynamic using the
          'ROW_FORMAT' or 'COLUMN_FORMAT' option with a *note 'CREATE
          TABLE': create-table. or *note 'ALTER TABLE': alter-table.
          statement.

          See *note create-table::, and *note alter-table::, for
          information on these options.

     You can regulate the effects of 'OPTIMIZE' on performance by
     adjusting the value of the global system variable
     'ndb_optimization_delay', which sets the number of milliseconds to
     wait between batches of rows being processed by 'OPTIMIZE'.  The
     default value is 10 milliseconds.  It is possible to set a lower
     value (to a minimum of '0'), but not recommended.  The maximum is
     100000 milliseconds (that is, 100 seconds).

   * Number of tables

     The maximum number of *note 'NDBCLUSTER': mysql-cluster. tables in
     a single NDB Cluster is included in the total maximum number of
     *note 'NDBCLUSTER': mysql-cluster. database objects (20320).  (See
     *note mysql-cluster-limitations-database-objects::.)

   * Adding and dropping of data nodes

     In NDB Cluster 7.2 (NDB Cluster 7.0 and later), it is possible to
     add new data nodes to a running NDB Cluster by performing a rolling
     restart, so that the cluster and the data stored in it remain
     available to applications.

     When planning to increase the number of data nodes in the cluster
     online, you should be aware of and take into account the following
     issues:

        * New data nodes can be added online to an NDB Cluster only as
          part of a new node group.

        * New data nodes can be added online, but cannot be dropped
          online.  Reducing the number of data nodes requires a system
          restart of the cluster.

        * As in previous NDB Cluster releases, it is not possible to
          change online either the number of replicas ('NoOfReplicas'
          configuration parameter) or the number of data nodes per node
          group.  These changes require a system restart.

        * Redistribution of existing cluster data using the new data
          nodes is not automatic; however, this can be accomplished
          using simple SQL statements in the *note 'mysql': mysql.
          client or other MySQL client application once the nodes have
          been added.  During this procedure, it is not possible to
          perform DDL operations, although DML operations can continue
          as normal.

          The distribution of new cluster data (that is, data stored in
          the cluster _after_ the new nodes have been added) uses the
          new nodes without manual intervention.

     For more information, see *note mysql-cluster-online-add-node::.

   * Native support for default column values

     Starting with NDB 7.1.0, default values for table columns are
     stored by *note 'NDBCLUSTER': mysql-cluster, rather than by the
     MySQL server as was previously the case.  Because less data must be
     sent from an SQL node to the data nodes, inserts on tables having
     column value defaults can be performed more efficiently than
     before.

     Tables created using previous NDB Cluster releases can still be
     used in NDB 7.1.0 and later, although they do not support native
     default values and continue to use defaults supplied by the MySQL
     server until they are upgraded.  This can be done by means of an
     offline *note 'ALTER TABLE': alter-table. statement.

     *Important*:

     You cannot set or change a table column's default value using an
     online *note 'ALTER TABLE': alter-table. operation

   * Distribution of MySQL users and privileges

     Previously, MySQL users and privileges created on one SQL node were
     unique to that SQL node, due to the fact that the MySQL grant
     tables were restricted to using the *note 'MyISAM':
     myisam-storage-engine. storage engine.  Beginning with NDB 7.2.0,
     it is possible, following installation of the NDB Cluster software
     and setup of the desired users and privileges on one SQL node, to
     convert the grant tables to use *note 'NDB': mysql-cluster. and
     thus to distribute the users and privileges across all SQL nodes
     connected to the cluster.  You can do this by loading and making
     use of a set of stored procedures defined in an SQL script supplied
     with the NDB Cluster distribution.  For more information, see *note
     mysql-cluster-privilege-distribution::.

   * Number of rows per partition

     Previously, a single NDB Cluster partition could hold a maximum of
     46137488 rows.  This limitation was removed in NDB 7.2.9.  (Bug
     #13844405, Bug #14000373)

     If you are still using a previous NDB Cluster release, you can work
     around this limitation by taking advantage of the fact that the
     number of partitions is the same as the number of data nodes in the
     cluster (see *note mysql-cluster-nodes-groups::).  This means that,
     by increasing the number of data nodes, you can increase the
     available space for storing data.

     NDB Cluster 7.2 also supports increasing the number of data nodes
     in the cluster while the cluster remains in operation.  See *note
     mysql-cluster-online-add-node::, for more information.

     It is also possible to increase the number of partitions for *note
     'NDB': mysql-cluster. tables by using explicit 'KEY' or 'LINEAR
     KEY' partitioning (see *note partitioning-key::).


File: manual.info.tmp,  Node: mysql-cluster-installation,  Next: mysql-cluster-configuration,  Prev: mysql-cluster-overview,  Up: mysql-cluster

18.2 NDB Cluster Installation
=============================

* Menu:

* mysql-cluster-install-linux::  Installing NDB Cluster on Linux
* mysql-cluster-install-windows::  Installing NDB Cluster on Windows
* mysql-cluster-install-configuration::  Initial Configuration of NDB Cluster
* mysql-cluster-install-first-start::  Initial Startup of NDB Cluster
* mysql-cluster-install-example-data::  NDB Cluster Example with Tables and Data
* mysql-cluster-install-shutdown-restart::  Safe Shutdown and Restart of NDB Cluster
* mysql-cluster-upgrade-downgrade::  Upgrading and Downgrading NDB Cluster

This section describes the basics for planning, installing, configuring,
and running an NDB Cluster.  Whereas the examples in *note
mysql-cluster-configuration:: provide more in-depth information on a
variety of clustering options and configuration, the result of following
the guidelines and procedures outlined here should be a usable NDB
Cluster which meets the _minimum_ requirements for availability and
safeguarding of data.

For information about upgrading or downgrading an NDB Cluster between
release versions, see *note mysql-cluster-upgrade-downgrade::.

This section covers hardware and software requirements; networking
issues; installation of NDB Cluster; basic configuration issues;
starting, stopping, and restarting the cluster; loading of a sample
database; and performing queries.

Assumptions

The following sections make a number of assumptions regarding the
cluster's physical and network configuration.  These assumptions are
discussed in the next few paragraphs.

Cluster nodes and host computers

The cluster consists of four nodes, each on a separate host computer,
and each with a fixed network address on a typical Ethernet network as
shown here:

*Network addresses of nodes in example cluster*

Node                                 IP Address
                                     
Management node ('mgmd')             198.51.100.10
                                     
SQL node (*note 'mysqld': mysqld.)   198.51.100.20
                                     
Data node "A"                        198.51.100.30
(*note 'ndbd': mysql-cluster-programs-ndbd.)

Data node "B"                        198.51.100.40
(*note 'ndbd': mysql-cluster-programs-ndbd.)

This may be made clearer by the following diagram:

FIGURE GOES HERE: NDB Cluster Multi-Computer Setup

Network addressing

In the interest of simplicity (and reliability), this 'How-To' uses only
numeric IP addresses.  However, if DNS resolution is available on your
network, it is possible to use host names in lieu of IP addresses in
configuring Cluster.  Alternatively, you can use the 'hosts' file
(typically '/etc/hosts' for Linux and other Unix-like operating systems,
'C:\WINDOWS\system32\drivers\etc\hosts' on Windows, or your operating
system's equivalent) for providing a means to do host lookup if such is
available.

Potential hosts file issues

A common problem when trying to use host names for Cluster nodes arises
because of the way in which some operating systems (including some Linux
distributions) set up the system's own host name in the '/etc/hosts'
during installation.  Consider two machines with the host names 'ndb1'
and 'ndb2', both in the 'cluster' network domain.  Red Hat Linux
(including some derivatives such as CentOS and Fedora) places the
following entries in these machines' '/etc/hosts' files:

     #  ndb1 /etc/hosts:
     127.0.0.1   ndb1.cluster ndb1 localhost.localdomain localhost

     #  ndb2 /etc/hosts:
     127.0.0.1   ndb2.cluster ndb2 localhost.localdomain localhost

SUSE Linux (including OpenSUSE) places these entries in the machines'
'/etc/hosts' files:

     #  ndb1 /etc/hosts:
     127.0.0.1       localhost
     127.0.0.2       ndb1.cluster ndb1

     #  ndb2 /etc/hosts:
     127.0.0.1       localhost
     127.0.0.2       ndb2.cluster ndb2

In both instances, 'ndb1' routes 'ndb1.cluster' to a loopback IP
address, but gets a public IP address from DNS for 'ndb2.cluster', while
'ndb2' routes 'ndb2.cluster' to a loopback address and obtains a public
address for 'ndb1.cluster'.  The result is that each data node connects
to the management server, but cannot tell when any other data nodes have
connected, and so the data nodes appear to hang while starting.

*Caution*:

You cannot mix 'localhost' and other host names or IP addresses in
'config.ini'.  For these reasons, the solution in such cases (other than
to use IP addresses for _all_ 'config.ini' 'HostName' entries) is to
remove the fully qualified host names from '/etc/hosts' and use these in
'config.ini' for all cluster hosts.

Host computer type

Each host computer in our installation scenario is an Intel-based
desktop PC running a supported operating system installed to disk in a
standard configuration, and running no unnecessary services.  The core
operating system with standard TCP/IP networking capabilities should be
sufficient.  Also for the sake of simplicity, we also assume that the
file systems on all hosts are set up identically.  In the event that
they are not, you should adapt these instructions accordingly.

Network hardware

Standard 100 Mbps or 1 gigabit Ethernet cards are installed on each
machine, along with the proper drivers for the cards, and that all four
hosts are connected through a standard-issue Ethernet networking
appliance such as a switch.  (All machines should use network cards with
the same throughput.  That is, all four machines in the cluster should
have 100 Mbps cards _or_ all four machines should have 1 Gbps cards.)
NDB Cluster works in a 100 Mbps network; however, gigabit Ethernet
provides better performance.

*Important*:

NDB Cluster is _not_ intended for use in a network for which throughput
is less than 100 Mbps or which experiences a high degree of latency.
For this reason (among others), attempting to run an NDB Cluster over a
wide area network such as the Internet is not likely to be successful,
and is not supported in production.

Sample data

We use the 'world' database which is available for download from the
MySQL website (see <https://dev.mysql.com/doc/index-other.html>).  We
assume that each machine has sufficient memory for running the operating
system, required NDB Cluster processes, and (on the data nodes) storing
the database.

For general information about installing MySQL, see *note installing::.
For information about installation of NDB Cluster on Linux and other
Unix-like operating systems, see *note mysql-cluster-install-linux::.
For information about installation of NDB Cluster on Windows operating
systems, see *note mysql-cluster-install-windows::.

For general information about NDB Cluster hardware, software, and
networking requirements, see *note
mysql-cluster-overview-requirements::.


File: manual.info.tmp,  Node: mysql-cluster-install-linux,  Next: mysql-cluster-install-windows,  Prev: mysql-cluster-installation,  Up: mysql-cluster-installation

18.2.1 Installing NDB Cluster on Linux
--------------------------------------

* Menu:

* mysql-cluster-install-linux-binary::  Installing an NDB Cluster Binary Release on Linux
* mysql-cluster-install-linux-rpm::  Installing NDB Cluster from RPM
* mysql-cluster-install-debian::  Installing NDB Cluster Using .deb Files
* mysql-cluster-install-linux-source::  Building NDB Cluster from Source on Linux

This section covers installation of NDB Cluster on Linux and other
Unix-like operating systems.  While the next few sections refer to a
Linux operating system, the instructions and procedures given there
should be easily adaptable to other supported Unix-like platforms.

NDB Cluster 7.2 is also available for Windows operating systems; for
installation and setup instructions specific to Windows, see *note
mysql-cluster-install-windows::.

Each NDB Cluster host computer must have the correct executable programs
installed.  A host running an SQL node must have installed on it a MySQL
Server binary (*note 'mysqld': mysqld.).  Management nodes require the
management server daemon (*note 'ndb_mgmd':
mysql-cluster-programs-ndb-mgmd.); data nodes require the data node
daemon (*note 'ndbd': mysql-cluster-programs-ndbd. or *note 'ndbmtd':
mysql-cluster-programs-ndbmtd.).  It is not necessary to install the
MySQL Server binary on management node hosts and data node hosts.  It is
recommended that you also install the management client (*note
'ndb_mgm': mysql-cluster-programs-ndb-mgm.) on the management server
host.

Installation of NDB Cluster on Linux can be done using precompiled
binaries from Oracle (downloaded as a .tar.gz archive), with RPM
packages (also available from Oracle), or from source code.  All three
of these installation methods are described in the section that follow.

Regardless of the method used, it is still necessary following
installation of the NDB Cluster binaries to create configuration files
for all cluster nodes, before you can start the cluster.  See *note
mysql-cluster-install-configuration::.


File: manual.info.tmp,  Node: mysql-cluster-install-linux-binary,  Next: mysql-cluster-install-linux-rpm,  Prev: mysql-cluster-install-linux,  Up: mysql-cluster-install-linux

18.2.1.1 Installing an NDB Cluster Binary Release on Linux
..........................................................

This section covers the steps necessary to install the correct
executables for each type of Cluster node from precompiled binaries
supplied by Oracle.

For setting up a cluster using precompiled binaries, the first step in
the installation process for each cluster host is to download the binary
archive from the NDB Cluster downloads page
(https://dev.mysql.com/downloads/cluster/).  (For the most recent 64-bit
NDB 7.2 release, this is
'mysql-cluster-gpl-7.2.39-linux-glibc2.12-x86_64x86_64glibc2.124.tar.gz'.)
We assume that you have placed this file in each machine's '/var/tmp'
directory.

If you require a custom binary, see *note installing-development-tree::.

*Note*:

After completing the installation, do not yet start any of the binaries.
We show you how to do so following the configuration of the nodes (see
*note mysql-cluster-install-configuration::).

SQL nodes

On each of the machines designated to host SQL nodes, perform the
following steps as the system 'root' user:

  1. Check your '/etc/passwd' and '/etc/group' files (or use whatever
     tools are provided by your operating system for managing users and
     groups) to see whether there is already a 'mysql' group and 'mysql'
     user on the system.  Some OS distributions create these as part of
     the operating system installation process.  If they are not already
     present, create a new 'mysql' user group, and then add a 'mysql'
     user to this group:

          shell> groupadd mysql
          shell> useradd -g mysql -s /bin/false mysql

     The syntax for 'useradd' and 'groupadd' may differ slightly on
     different versions of Unix, or they may have different names such
     as 'adduser' and 'addgroup'.

  2. Change location to the directory containing the downloaded file,
     unpack the archive, and create a symbolic link named 'mysql' to the
     'mysql' directory.

     *Note*:

     The actual file and directory names vary according to the NDB
     Cluster version number.

          shell> cd /var/tmp
          shell> tar -C /usr/local -xzvf mysql-cluster-gpl-7.2.39-linux-glibc2.12-x86_64.tar.gz
          shell> ln -s /usr/local/mysql-cluster-gpl-7.2.39-linux-glibc2.12-x86_64 /usr/local/mysql

  3. Change location to the 'mysql' directory and run the supplied
     script for creating the system databases:

          shell> cd mysql
          shell> scripts/mysql_install_db --user=mysql

  4. Set the necessary permissions for the MySQL server and data
     directories:

          shell> chown -R root .
          shell> chown -R mysql data
          shell> chgrp -R mysql .

  5. Copy the MySQL startup script to the appropriate directory, make it
     executable, and set it to start when the operating system is booted
     up:

          shell> cp support-files/mysql.server /etc/rc.d/init.d/
          shell> chmod +x /etc/rc.d/init.d/mysql.server
          shell> chkconfig --add mysql.server

     (The startup scripts directory may vary depending on your operating
     system and version--for example, in some Linux distributions, it is
     '/etc/init.d'.)

     Here we use Red Hat's 'chkconfig' for creating links to the startup
     scripts; use whatever means is appropriate for this purpose on your
     platform, such as 'update-rc.d' on Debian.

Remember that the preceding steps must be repeated on each machine where
an SQL node is to reside.

Data nodes

Installation of the data nodes does not require the *note 'mysqld':
mysqld. binary.  Only the NDB Cluster data node executable *note 'ndbd':
mysql-cluster-programs-ndbd. (single-threaded) or *note 'ndbmtd':
mysql-cluster-programs-ndbmtd. (multithreaded) is required.  These
binaries can also be found in the '.tar.gz' archive.  Again, we assume
that you have placed this archive in '/var/tmp'.

As system 'root' (that is, after using 'sudo', 'su root', or your
system's equivalent for temporarily assuming the system administrator
account's privileges), perform the following steps to install the data
node binaries on the data node hosts:

  1. Change location to the '/var/tmp' directory, and extract the *note
     'ndbd': mysql-cluster-programs-ndbd. and *note 'ndbmtd':
     mysql-cluster-programs-ndbmtd. binaries from the archive into a
     suitable directory such as '/usr/local/bin':

          shell> cd /var/tmp
          shell> tar -zxvf mysql-cluster-gpl-7.2.39-linux-glibc2.12-x86_64.tar.gz
          shell> cd mysql-cluster-gpl-7.2.39-linux-glibc2.12-x86_64
          shell> cp bin/ndbd /usr/local/bin/ndbd
          shell> cp bin/ndbmtd /usr/local/bin/ndbmtd

     (You can safely delete the directory created by unpacking the
     downloaded archive, and the files it contains, from '/var/tmp' once
     *note 'ndb_mgm': mysql-cluster-programs-ndb-mgm. and *note
     'ndb_mgmd': mysql-cluster-programs-ndb-mgmd. have been copied to
     the executables directory.)

  2. Change location to the directory into which you copied the files,
     and then make both of them executable:

          shell> cd /usr/local/bin
          shell> chmod +x ndb*

The preceding steps should be repeated on each data node host.

Although only one of the data node executables is required to run an NDB
Cluster data node, we have shown you how to install both *note 'ndbd':
mysql-cluster-programs-ndbd. and *note 'ndbmtd':
mysql-cluster-programs-ndbmtd. in the preceding instructions.  We
recommend that you do this when installing or upgrading NDB Cluster,
even if you plan to use only one of them, since this will save time and
trouble in the event that you later decide to change from one to the
other.

*Note*:

The data directory on each machine hosting a data node is
'/usr/local/mysql/data'.  This piece of information is essential when
configuring the management node.  (See *note
mysql-cluster-install-configuration::.)

Management nodes

Installation of the management node does not require the *note 'mysqld':
mysqld. binary.  Only the NDB Cluster management server (*note
'ndb_mgmd': mysql-cluster-programs-ndb-mgmd.) is required; you most
likely want to install the management client (*note 'ndb_mgm':
mysql-cluster-programs-ndb-mgm.) as well.  Both of these binaries also
be found in the '.tar.gz' archive.  Again, we assume that you have
placed this archive in '/var/tmp'.

As system 'root', perform the following steps to install *note
'ndb_mgmd': mysql-cluster-programs-ndb-mgmd. and *note 'ndb_mgm':
mysql-cluster-programs-ndb-mgm. on the management node host:

  1. Change location to the '/var/tmp' directory, and extract the *note
     'ndb_mgm': mysql-cluster-programs-ndb-mgm. and *note 'ndb_mgmd':
     mysql-cluster-programs-ndb-mgmd. from the archive into a suitable
     directory such as '/usr/local/bin':

          shell> cd /var/tmp
          shell> tar -zxvf mysql-cluster-gpl-7.2.39-linux-glibc2.12-x86_64.tar.gz
          shell> cd mysql-cluster-gpl-7.2.39-linux-glibc2.12-x86_64
          shell> cp bin/ndb_mgm* /usr/local/bin

     (You can safely delete the directory created by unpacking the
     downloaded archive, and the files it contains, from '/var/tmp' once
     *note 'ndb_mgm': mysql-cluster-programs-ndb-mgm. and *note
     'ndb_mgmd': mysql-cluster-programs-ndb-mgmd. have been copied to
     the executables directory.)

  2. Change location to the directory into which you copied the files,
     and then make both of them executable:

          shell> cd /usr/local/bin
          shell> chmod +x ndb_mgm*

In *note mysql-cluster-install-configuration::, we create configuration
files for all of the nodes in our example NDB Cluster.


File: manual.info.tmp,  Node: mysql-cluster-install-linux-rpm,  Next: mysql-cluster-install-debian,  Prev: mysql-cluster-install-linux-binary,  Up: mysql-cluster-install-linux

18.2.1.2 Installing NDB Cluster from RPM
........................................

This section covers the steps necessary to install the correct
executables for each type of NDB Cluster node using RPM packages
supplied by Oracle.

RPMs are available for both 32-bit and 64-bit Linux platforms.  The
filenames for these RPMs use the following pattern:

     MySQL-Cluster-COMPONENT-PRODUCTTYPE-NDBVERSION.DISTRIBUTION.ARCHITECTURE.rpm

     COMPONENT:= {server | client [| OTHER]}

     PRODUCTTYPE:= {gpl | advanced}

     NDBVERSION:= MAJOR.MINOR.RELEASE

     DISTRIBUTION:= {sles10 | rhel5 | el6}

     ARCHITECTURE:= {i386 | x86_64}

The COMPONENT can be 'server' or 'client'.  (Other values are possible,
but since only the 'server' and 'client' components are required for a
working NDB Cluster installation, we do not discuss them here.)  The
PRODUCTTYPE for Community RPMs downloaded from
<https://dev.mysql.com/downloads/cluster/> is always 'gpl'; 'advanced'
is used to indicate commercial releases.  NDBVERSION represents the
three-part 'NDB' storage engine version number in 7.2.X format.  The
DISTRIBUTION can be one of 'sles11' (SUSE Enterprise Linux 11), 'rhel5'
(Oracle Linux 5, Red Hat Enterprise Linux 4 and 5), or 'el6' (Oracle
Linux 6, Red Hat Enterprise Linux 6) The ARCHITECTURE is 'i386' for
32-bit RPMs and 'x86_64' for 64-bit versions.

For an NDB Cluster, one and possibly two RPMs are required:

   * The 'server' RPM (for example,
     'MySQL-Cluster-server-gpl-7.2.39-1.sles11.i386.rpm'), which
     supplies the core files needed to run a MySQL Server with *note
     'NDBCLUSTER': mysql-cluster. storage engine support (that is, as an
     NDB Cluster SQL node) as well as all NDB Cluster executables,
     including the management node, data node, and *note 'ndb_mgm':
     mysql-cluster-programs-ndb-mgm. client binaries.  This RPM is
     always required for installing NDB Cluster.

   * If you do not have your own client application capable of
     administering a MySQL server, you should also obtain and install
     the 'client' RPM (for example,
     'MySQL-Cluster-client-gpl-7.2.39-1.sles11.i386.rpm'), which
     supplies the *note 'mysql': mysql. client

The NDB Cluster version number in the RPM file names (shown here as
'7.2.39') can vary according to the version which you are actually
using.  _It is very important that all of the Cluster RPMs to be
installed have the same version number_.  The ARCHITECTURE designation
should be appropriate to the machine on which the RPM is to be
installed; in particular, you should keep in mind that 64-bit RPMs
cannot be used with 32-bit operating systems.

Data nodes

On a computer that is to host a cluster data node it is necessary to
install only the 'server' RPM. To do so, copy this RPM to the data node
host, and run the following command as the system root user, replacing
the name shown for the RPM as necessary to match that of the RPM
downloaded from the MySQL website:

     shell> rpm -Uhv MySQL-Cluster-server-gpl-7.2.39-1.sles11.i386.rpm

Although this installs all NDB Cluster binaries, only the program *note
'ndbd': mysql-cluster-programs-ndbd. or *note 'ndbmtd':
mysql-cluster-programs-ndbmtd. (both in '/usr/sbin') is actually needed
to run an NDB Cluster data node.

SQL nodes

On each machine to be used for hosting a cluster SQL node, install the
'server' RPM by executing the following command as the system root user,
replacing the name shown for the RPM as necessary to match the name of
the RPM downloaded from the MySQL website:

     shell> rpm -Uhv MySQL-Cluster-server-gpl-7.2.39-1.sles11.i386.rpm

This installs the MySQL server binary (*note 'mysqld': mysqld.) with
*note 'NDB': mysql-cluster. storage engine support in the '/usr/sbin'
directory, as well as all needed MySQL Server support files.  It also
installs the *note 'mysql.server': mysql-server. and *note
'mysqld_safe': mysqld-safe. startup scripts (in '/usr/share/mysql' and
'/usr/bin', respectively).  The RPM installer should take care of
general configuration issues (such as creating the 'mysql' user and
group, if needed) automatically.

To administer the SQL node (MySQL server), you should also install the
'client' RPM, as shown here:

     shell> rpm -Uhv MySQL-Cluster-client-gpl-7.2.39-1.sles11.i386.rpm

This installs the *note 'mysql': mysql. client program.

Management nodes

To install the NDB Cluster management server, it is necessary only to
use the 'server' RPM. Copy this RPM to the computer intended to host the
management node, and then install it by running the following command as
the system root user (replace the name shown for the RPM as necessary to
match that of the 'server' RPM downloaded from the MySQL website):

     shell> rpm -Uhv MySQL-Cluster-server-gpl-7.2.39-1.sles11.i386.rpm

Although this RPM installs many other files, only the management server
binary *note 'ndb_mgmd': mysql-cluster-programs-ndb-mgmd. (in the
'/usr/sbin' directory) is actually required for running a management
node.  The 'server' RPM also installs *note 'ndb_mgm':
mysql-cluster-programs-ndb-mgm, the *note 'NDB': mysql-cluster.
management client.

See *note linux-installation-rpm::, for general information about
installing MySQL using RPMs supplied by Oracle.

After installing from RPM, you still need to configure the cluster as
discussed in *note mysql-cluster-install-configuration::.

*Note*:

A number of RPMs used by NDB Cluster 7.1 were made obsolete and
discontinued in NDB Cluster 7.2.  These include the former
'MySQL-Cluster-clusterj', 'MySQL-Cluster-extra',
'MySQL-Cluster-management', 'MySQL-Cluster-storage', and 'NDB
Cluster-tools' RPMs; all of these have been merged into the
'MySQL-Cluster-server' RPM. When upgrading from an NDB Cluster 7.1 RPM
installation to NDB 7.2.3 or an earlier NDB Cluster 7.2 release, it was
necessary to remove these packages manually before installing the NDB
Cluster 7.2 'MySQL-Cluster-server' RPM. This issue is fixed in NDB 7.2.4
and later, where the 'MySQL-Cluster-server' package specifically
obsoletes the discontinued packages (BUG #13545589).


File: manual.info.tmp,  Node: mysql-cluster-install-debian,  Next: mysql-cluster-install-linux-source,  Prev: mysql-cluster-install-linux-rpm,  Up: mysql-cluster-install-linux

18.2.1.3 Installing NDB Cluster Using .deb Files
................................................

The section provides information about installing NDB Cluster on Debian
and related Linux distributions such Ubuntu using the .deb files
supplied by Oracle for this purpose.

Oracle provides '.deb' installer files for NDB Cluster 7.2 for 32-bit
and 64-bit platforms.  For a Debian-based system, only a single
installer file is necessary.  This file is named using the pattern shown
here, according to the applicable NDB Cluster version, Debian version,
and architecture:

     mysql-cluster-gpl-NDBVER-debianDEBIANVER-ARCH.deb

Here, NDBVER is the 3-part 'NDB' engine version number, DEBIANVER is the
major version of Debian (for NDB Cluster 7.2, this is always '6.0'), and
ARCH is one of 'i686' or 'x86_64'.  In the examples that follow, we
assume you wish to install NDB 7.2.21 on a 64-bit Debian 6 system; in
this case, the installer file is named
'mysql-cluster-gpl-7.2.21-debian6.0-x86_64.deb'.

Once you have downloaded the appropriate '.deb' file, you can install it
from the command line using 'dpkg', like this:

     shell> dpkg -i mysql-cluster-gpl-7.2.21-debian6.0-i686.deb

You can also remove it using 'dpkg' as shown here:

     shell> dpkg -r mysql

The installer file should also be compatible with most graphical package
managers that work with '.deb' files, such as 'GDebi' for the Gnome
desktop.

The '.deb' file installs NDB Cluster under '/opt/mysql/server-VERSION/',
where VERSION is the 2-part release series version for the included
MySQL server.  For NDB Cluster 7.2, this is always '5.5'.  The directory
layout is the same as that for the generic Linux binary distribution
(see *note binary-installation-layout::), with the exception that
startup scripts and configuration files are found in 'support-files'
instead of 'share'.  All NDB Cluster executables, such as *note
'ndb_mgm': mysql-cluster-programs-ndb-mgm, *note 'ndbd':
mysql-cluster-programs-ndbd, and *note 'ndb_mgmd':
mysql-cluster-programs-ndb-mgmd, are placed in the 'bin' directory.


File: manual.info.tmp,  Node: mysql-cluster-install-linux-source,  Prev: mysql-cluster-install-debian,  Up: mysql-cluster-install-linux

18.2.1.4 Building NDB Cluster from Source on Linux
..................................................

This section provides information about compiling NDB Cluster on Linux
and other Unix-like platforms.  Building NDB Cluster from source is
similar to building the standard MySQL Server, although it differs in a
few key respects discussed here.  For general information about building
MySQL from source, see *note source-installation::.  For information
about compiling NDB Cluster on Windows platforms, see *note
mysql-cluster-install-windows-source::.

Building NDB Cluster requires using the NDB Cluster sources.  These are
available from the NDB Cluster downloads page at
<https://dev.mysql.com/downloads/cluster/>.  The archived source file
should have a name similar to 'mysql-cluster-gpl-7.2.39.tar.gz'.  You
can also obtain NDB Cluster 7.2 sources from GitHub at
<https://github.com/mysql/mysql-server/tree/cluster-7.2>.  _Building NDB
Cluster 7.2 from standard MySQL Server 5.5 sources is not supported_.

For NDB Cluster 7.2 on Linux and similar platforms, GCC 4 (4.2.1 or
later) or GCC 5 is required to compile from source; GCC 6 and later
versions of GCC are not supported.

The 'WITH_NDBCLUSTER_STORAGE_ENGINE' option for 'CMake' causes the
binaries for the management nodes, data nodes, and other NDB Cluster
programs to be built; it also causes *note 'mysqld': mysqld. to be
compiled with *note 'NDB': mysql-cluster. storage engine support.  This
option (or its alias 'WITH_NDBCLUSTER') is required when building NDB
Cluster.

*Important*:

Beginning with NDB 7.2.9, the 'WITH_NDB_JAVA' option is enabled by
default.  This means that, by default, if 'CMake' cannot find the
location of Java on your system, the configuration process fails; if you
do not wish to enable Java and ClusterJ support, you must indicate this
explicitly by configuring the build using '-DWITH_NDB_JAVA=OFF'.  (Bug
#12379735) Use 'WITH_CLASSPATH' to provide the Java classpath if needed.

For more information about 'CMake' options specific to building NDB
Cluster, see Options for Compiling NDB Cluster.

After you have run 'make && make install' (or your system's equivalent),
the result is similar to what is obtained by unpacking a precompiled
binary to the same location.

Management nodes

When building from source and running the default 'make install', the
management server and management client binaries (*note 'ndb_mgmd':
mysql-cluster-programs-ndb-mgmd. and *note 'ndb_mgm':
mysql-cluster-programs-ndb-mgm.) can be found in '/usr/local/mysql/bin'.
Only *note 'ndb_mgmd': mysql-cluster-programs-ndb-mgmd. is required to
be present on a management node host; however, it is also a good idea to
have *note 'ndb_mgm': mysql-cluster-programs-ndb-mgm. present on the
same host machine.  Neither of these executables requires a specific
location on the host machine's file system.

Data nodes

The only executable required on a data node host is the data node binary
*note 'ndbd': mysql-cluster-programs-ndbd. or *note 'ndbmtd':
mysql-cluster-programs-ndbmtd.  (*note 'mysqld': mysqld, for example,
does not have to be present on the host machine.)  By default, when
building from source, this file is placed in the directory
'/usr/local/mysql/bin'.  For installing on multiple data node hosts,
only *note 'ndbd': mysql-cluster-programs-ndbd. or *note 'ndbmtd':
mysql-cluster-programs-ndbmtd. need be copied to the other host machine
or machines.  (This assumes that all data node hosts use the same
architecture and operating system; otherwise you may need to compile
separately for each different platform.)  The data node binary need not
be in any particular location on the host's file system, as long as the
location is known.

When compiling NDB Cluster from source, no special options are required
for building multithreaded data node binaries.  Configuring the build
with *note 'NDB': mysql-cluster. storage engine support causes *note
'ndbmtd': mysql-cluster-programs-ndbmtd. to be built automatically;
'make install' places the *note 'ndbmtd': mysql-cluster-programs-ndbmtd.
binary in the installation 'bin' directory along with *note 'mysqld':
mysqld, *note 'ndbd': mysql-cluster-programs-ndbd, and *note 'ndb_mgm':
mysql-cluster-programs-ndb-mgm.

SQL nodes

If you compile MySQL with clustering support, and perform the default
installation (using 'make install' as the system 'root' user), *note
'mysqld': mysqld. is placed in '/usr/local/mysql/bin'.  Follow the steps
given in *note source-installation:: to make *note 'mysqld': mysqld.
ready for use.  If you want to run multiple SQL nodes, you can use a
copy of the same *note 'mysqld': mysqld. executable and its associated
support files on several machines.  The easiest way to do this is to
copy the entire '/usr/local/mysql' directory and all directories and
files contained within it to the other SQL node host or hosts, then
repeat the steps from *note source-installation:: on each machine.  If
you configure the build with a nondefault 'PREFIX' option, you must
adjust the directory accordingly.

In *note mysql-cluster-install-configuration::, we create configuration
files for all of the nodes in our example NDB Cluster.


File: manual.info.tmp,  Node: mysql-cluster-install-windows,  Next: mysql-cluster-install-configuration,  Prev: mysql-cluster-install-linux,  Up: mysql-cluster-installation

18.2.2 Installing NDB Cluster on Windows
----------------------------------------

* Menu:

* mysql-cluster-install-windows-binary::  Installing NDB Cluster on Windows from a Binary Release
* mysql-cluster-install-windows-source::  Compiling and Installing NDB Cluster from Source on Windows
* mysql-cluster-install-windows-initial-start::  Initial Startup of NDB Cluster on Windows
* mysql-cluster-install-windows-service::  Installing NDB Cluster Processes as Windows Services

NDB Cluster 7.2 binaries for Windows can be obtained from
<https://dev.mysql.com/downloads/cluster/>.  For information about
installing NDB Cluster on Windows from a binary release provided by
Oracle, see *note mysql-cluster-install-windows-binary::.

It is also possible to compile and install NDB Cluster from source on
Windows using Microsoft Visual Studio.  For more information, see *note
mysql-cluster-install-windows-source::.


File: manual.info.tmp,  Node: mysql-cluster-install-windows-binary,  Next: mysql-cluster-install-windows-source,  Prev: mysql-cluster-install-windows,  Up: mysql-cluster-install-windows

18.2.2.1 Installing NDB Cluster on Windows from a Binary Release
................................................................

This section describes a basic installation of NDB Cluster on Windows
using a binary 'no-install' NDB Cluster release provided by Oracle,
using the same 4-node setup outlined in the beginning of this section
(see *note mysql-cluster-installation::), as shown in the following
table:

*Network addresses of nodes in example cluster*

Node                                 IP Address
                                     
Management node ('mgmd')             198.51.100.10
                                     
SQL node (*note 'mysqld': mysqld.)   198.51.100.20
                                     
Data node "A"                        198.51.100.30
(*note 'ndbd': mysql-cluster-programs-ndbd.)

Data node "B"                        198.51.100.40
(*note 'ndbd': mysql-cluster-programs-ndbd.)

As on other platforms, the NDB Cluster host computer running an SQL node
must have installed on it a MySQL Server binary (*note 'mysqld.exe':
mysqld.).  You should also have the MySQL client (*note 'mysql.exe':
mysql.) on this host.  For management nodes and data nodes, it is not
necessary to install the MySQL Server binary; however, each management
node requires the management server daemon (*note 'ndb_mgmd.exe':
mysql-cluster-programs-ndb-mgmd.); each data node requires the data node
daemon (*note 'ndbd.exe': mysql-cluster-programs-ndbd. or *note
'ndbmtd.exe': mysql-cluster-programs-ndbmtd.).  For this example, we
refer to *note 'ndbd.exe': mysql-cluster-programs-ndbd. as the data node
executable, but you can install *note 'ndbmtd.exe':
mysql-cluster-programs-ndbmtd, the multithreaded version of this
program, instead, in exactly the same way.  You should also install the
management client (*note 'ndb_mgm.exe': mysql-cluster-programs-ndb-mgm.)
on the management server host.  This section covers the steps necessary
to install the correct Windows binaries for each type of NDB Cluster
node.

*Note*:

As with other Windows programs, NDB Cluster executables are named with
the '.exe' file extension.  However, it is not necessary to include the
'.exe' extension when invoking these programs from the command line.
Therefore, we often simply refer to these programs in this documentation
as *note 'mysqld': mysqld, *note 'mysql': mysql, *note 'ndb_mgmd':
mysql-cluster-programs-ndb-mgmd, and so on.  You should understand that,
whether we refer (for example) to *note 'mysqld': mysqld. or *note
'mysqld.exe': mysqld, either name means the same thing (the MySQL Server
program).

For setting up an NDB Cluster using Oracles's 'no-install' binaries, the
first step in the installation process is to download the latest NDB
Cluster Windows ZIP binary archive from
<https://dev.mysql.com/downloads/cluster/>.  This archive has a filename
of the 'mysql-cluster-gpl-VER-winARCH.zip', where VER is the 'NDB'
storage engine version (such as '7.2.39'), and ARCH is the architecture
('32' for 32-bit binaries, and '64' for 64-bit binaries).  For example,
the NDB Cluster 7.2.39 archive for 64-bit Windows systems is named
'mysql-cluster-gpl-7.2.39-win64.zip'.

You can run 32-bit NDB Cluster binaries on both 32-bit and 64-bit
versions of Windows; however, 64-bit NDB Cluster binaries can be used
only on 64-bit versions of Windows.  If you are using a 32-bit version
of Windows on a computer that has a 64-bit CPU, then you must use the
32-bit NDB Cluster binaries.

To minimize the number of files that need to be downloaded from the
Internet or copied between machines, we start with the computer where
you intend to run the SQL node.

SQL node

We assume that you have placed a copy of the archive in the directory
'C:\Documents and Settings\USERNAME\My Documents\Downloads' on the
computer having the IP address 198.51.100.20, where USERNAME is the name
of the current user.  (You can obtain this name using 'ECHO %USERNAME%'
on the command line.)  To install and run NDB Cluster executables as
Windows services, this user should be a member of the 'Administrators'
group.

Extract all the files from the archive.  The Extraction Wizard
integrated with Windows Explorer is adequate for this task.  (If you use
a different archive program, be sure that it extracts all files and
directories from the archive, and that it preserves the archive's
directory structure.)  When you are asked for a destination directory,
enter 'C:\', which causes the Extraction Wizard to extract the archive
to the directory 'C:\mysql-cluster-gpl-VER-winARCH'.  Rename this
directory to 'C:\mysql'.

It is possible to install the NDB Cluster binaries to directories other
than 'C:\mysql\bin'; however, if you do so, you must modify the paths
shown in this procedure accordingly.  In particular, if the MySQL Server
(SQL node) binary is installed to a location other than 'C:\mysql' or
'C:\Program Files\MySQL\MySQL Server 5.5', or if the SQL node's data
directory is in a location other than 'C:\mysql\data' or 'C:\Program
Files\MySQL\MySQL Server 5.5\data', extra configuration options must be
used on the command line or added to the 'my.ini' or 'my.cnf' file when
starting the SQL node.  For more information about configuring a MySQL
Server to run in a nonstandard location, see *note
windows-install-archive::.

For a MySQL Server with NDB Cluster support to run as part of an NDB
Cluster, it must be started with the options '--ndbcluster' and
'--ndb-connectstring'.  While you can specify these options on the
command line, it is usually more convenient to place them in an option
file.  To do this, create a new text file in Notepad or another text
editor.  Enter the following configuration information into this file:

     [mysqld]
     # Options for mysqld process:
     ndbcluster                       # run NDB storage engine
     ndb-connectstring=198.51.100.10  # location of management server

You can add other options used by this MySQL Server if desired (see
*note windows-create-option-file::), but the file must contain the
options shown, at a minimum.  Save this file as 'C:\mysql\my.ini'.  This
completes the installation and setup for the SQL node.

Data nodes

An NDB Cluster data node on a Windows host requires only a single
executable, one of either *note 'ndbd.exe': mysql-cluster-programs-ndbd.
or *note 'ndbmtd.exe': mysql-cluster-programs-ndbmtd.  For this example,
we assume that you are using *note 'ndbd.exe':
mysql-cluster-programs-ndbd, but the same instructions apply when using
*note 'ndbmtd.exe': mysql-cluster-programs-ndbmtd.  On each computer
where you wish to run a data node (the computers having the IP addresses
198.51.100.30 and 198.51.100.40), create the directories 'C:\mysql',
'C:\mysql\bin', and 'C:\mysql\cluster-data'; then, on the computer where
you downloaded and extracted the 'no-install' archive, locate 'ndbd.exe'
in the 'C:\mysql\bin' directory.  Copy this file to the 'C:\mysql\bin'
directory on each of the two data node hosts.

To function as part of an NDB Cluster, each data node must be given the
address or hostname of the management server.  You can supply this
information on the command line using the '--ndb-connectstring' or '-c'
option when starting each data node process.  However, it is usually
preferable to put this information in an option file.  To do this,
create a new text file in Notepad or another text editor and enter the
following text:

     [mysql_cluster]
     # Options for data node process:
     ndb-connectstring=198.51.100.10  # location of management server

Save this file as 'C:\mysql\my.ini' on the data node host.  Create
another text file containing the same information and save it on as
'C:mysql\my.ini' on the other data node host, or copy the my.ini file
from the first data node host to the second one, making sure to place
the copy in the second data node's 'C:\mysql' directory.  Both data node
hosts are now ready to be used in the NDB Cluster, which leaves only the
management node to be installed and configured.

Management node

The only executable program required on a computer used for hosting an
NDB Cluster management node is the management server program *note
'ndb_mgmd.exe': mysql-cluster-programs-ndb-mgmd.  However, in order to
administer the NDB Cluster once it has been started, you should also
install the NDB Cluster management client program *note 'ndb_mgm.exe':
mysql-cluster-programs-ndb-mgm. on the same machine as the management
server.  Locate these two programs on the machine where you downloaded
and extracted the 'no-install' archive; this should be the directory
'C:\mysql\bin' on the SQL node host.  Create the directory
'C:\mysql\bin' on the computer having the IP address 198.51.100.10, then
copy both programs to this directory.

You should now create two configuration files for use by 'ndb_mgmd.exe':

  1. A local configuration file to supply configuration data specific to
     the management node itself.  Typically, this file needs only to
     supply the location of the NDB Cluster global configuration file
     (see item 2).

     To create this file, start a new text file in Notepad or another
     text editor, and enter the following information:

          [mysql_cluster]
          # Options for management node process
          config-file=C:/mysql/bin/config.ini

     Save this file as the text file 'C:\mysql\bin\my.ini'.

  2. A global configuration file from which the management node can
     obtain configuration information governing the NDB Cluster as a
     whole.  At a minimum, this file must contain a section for each
     node in the NDB Cluster, and the IP addresses or hostnames for the
     management node and all data nodes ('HostName' configuration
     parameter).  It is also advisable to include the following
     additional information:

        * The IP address or hostname of any SQL nodes

        * The data memory and index memory allocated to each data node
          ('DataMemory' and 'IndexMemory' configuration parameters)

        * The number of replicas, using the 'NoOfReplicas' configuration
          parameter (see *note mysql-cluster-nodes-groups::)

        * The directory where each data node stores it data and log
          file, and the directory where the management node keeps its
          log files (in both cases, the 'DataDir' configuration
          parameter)

     Create a new text file using a text editor such as Notepad, and
     input the following information:

          [ndbd default]
          # Options affecting ndbd processes on all data nodes:
          NoOfReplicas=2                      # Number of replicas
          DataDir=C:/mysql/cluster-data       # Directory for each data node's data files
                                              # Forward slashes used in directory path,
                                              # rather than backslashes. This is correct;
                                              # see *Important* note in text
          DataMemory=80M    # Memory allocated to data storage
          IndexMemory=18M   # Memory allocated to index storage
                            # For DataMemory and IndexMemory, we have used the
                            # default values. Since the "world" database takes up
                            # only about 500KB, this should be more than enough for
                            # this example Cluster setup.

          [ndb_mgmd]
          # Management process options:
          HostName=198.51.100.10              # Hostname or IP address of management node
          DataDir=C:/mysql/bin/cluster-logs   # Directory for management node log files

          [ndbd]
          # Options for data node "A":
                                          # (one [ndbd] section per data node)
          HostName=198.51.100.30          # Hostname or IP address

          [ndbd]
          # Options for data node "B":
          HostName=198.51.100.40          # Hostname or IP address

          [mysqld]
          # SQL node options:
          HostName=198.51.100.20          # Hostname or IP address

     Save this file as the text file 'C:\mysql\bin\config.ini'.

*Important*:

A single backslash character ('\') cannot be used when specifying
directory paths in program options or configuration files used by NDB
Cluster on Windows.  Instead, you must either escape each backslash
character with a second backslash ('\\'), or replace the backslash with
a forward slash character ('/').  For example, the following line from
the '[ndb_mgmd]' section of an NDB Cluster 'config.ini' file does not
work:

     DataDir=C:\mysql\bin\cluster-logs

Instead, you may use either of the following:

     DataDir=C:\\mysql\\bin\\cluster-logs  # Escaped backslashes

     DataDir=C:/mysql/bin/cluster-logs     # Forward slashes

For reasons of brevity and legibility, we recommend that you use forward
slashes in directory paths used in NDB Cluster program options and
configuration files on Windows.


File: manual.info.tmp,  Node: mysql-cluster-install-windows-source,  Next: mysql-cluster-install-windows-initial-start,  Prev: mysql-cluster-install-windows-binary,  Up: mysql-cluster-install-windows

18.2.2.2 Compiling and Installing NDB Cluster from Source on Windows
....................................................................

Oracle provides precompiled NDB Cluster binaries for Windows which
should be adequate for most users.  However, if you wish, it is also
possible to compile NDB Cluster for Windows from source code.  The
procedure for doing this is almost identical to the procedure used to
compile the standard MySQL Server binaries for Windows, and uses the
same tools.  However, there are two major differences:

   * Building NDB Cluster requires using the NDB Cluster sources.  These
     are available from the NDB Cluster downloads page at
     <https://dev.mysql.com/downloads/cluster/>.  The archived source
     file should have a name similar to
     'mysql-cluster-gpl-7.2.39.tar.gz'.  You can also obtain NDB Cluster
     7.2 sources from GitHub at
     <https://github.com/mysql/mysql-server/tree/cluster-7.2>.
     _Building NDB Cluster 7.2 from standard MySQL Server 5.5 sources is
     not supported_.

   * You must configure the build using the
     'WITH_NDBCLUSTER_STORAGE_ENGINE' or 'WITH_NDBCLUSTER' option in
     addition to any other build options you wish to use with 'CMake'.
     ('WITH_NDBCLUSTER' is supported as an alias for
     'WITH_NDBCLUSTER_STORAGE_ENGINE', and works in exactly the same
     way.)

*Important*:

Beginning with NDB 7.2.9, the 'WITH_NDB_JAVA' option is enabled by
default.  This means that, by default, if 'CMake' cannot find the
location of Java on your system, the configuration process fails; if you
do not wish to enable Java and ClusterJ support, you must indicate this
explicitly by configuring the build using '-DWITH_NDB_JAVA=OFF'.  (Bug
#12379735) Use 'WITH_CLASSPATH' to provide the Java classpath if needed.

For more information about 'CMake' options specific to building NDB
Cluster, see Options for Compiling NDB Cluster.

Once the build process is complete, you can create a Zip archive
containing the compiled binaries; *note installing-source-distribution::
provides the commands needed to perform this task on Windows systems.
The NDB Cluster binaries can be found in the 'bin' directory of the
resulting archive, which is equivalent to the 'no-install' archive, and
which can be installed and configured in the same manner.  For more
information, see *note mysql-cluster-install-windows-binary::.


File: manual.info.tmp,  Node: mysql-cluster-install-windows-initial-start,  Next: mysql-cluster-install-windows-service,  Prev: mysql-cluster-install-windows-source,  Up: mysql-cluster-install-windows

18.2.2.3 Initial Startup of NDB Cluster on Windows
..................................................

Once the NDB Cluster executables and needed configuration files are in
place, performing an initial start of the cluster is simply a matter of
starting the NDB Cluster executables for all nodes in the cluster.  Each
cluster node process must be started separately, and on the host
computer where it resides.  The management node should be started first,
followed by the data nodes, and then finally by any SQL nodes.

  1. On the management node host, issue the following command from the
     command line to start the management node process.  The output
     should appear similar to what is shown here:

          C:\mysql\bin> ndb_mgmd
          2010-06-23 07:53:34 [MgmtSrvr] INFO -- NDB Cluster Management Server. mysql-5.5.65-ndb-7.2.39
          2010-06-23 07:53:34 [MgmtSrvr] INFO -- Reading cluster configuration from 'config.ini'

     The management node process continues to print logging output to
     the console.  This is normal, because the management node is not
     running as a Windows service.  (If you have used NDB Cluster on a
     Unix-like platform such as Linux, you may notice that the
     management node's default behavior in this regard on Windows is
     effectively the opposite of its behavior on Unix systems, where it
     runs by default as a Unix daemon process.  This behavior is also
     true of NDB Cluster data node processes running on Windows.)  For
     this reason, do not close the window in which *note 'ndb_mgmd.exe':
     mysql-cluster-programs-ndb-mgmd. is running; doing so kills the
     management node process.  (See *note
     mysql-cluster-install-windows-service::, where we show how to
     install and run NDB Cluster processes as Windows services.)

     The required '-f' option tells the management node where to find
     the global configuration file ('config.ini').  The long form of
     this option is '--config-file'.

     *Important*:

     An NDB Cluster management node caches the configuration data that
     it reads from 'config.ini'; once it has created a configuration
     cache, it ignores the 'config.ini' file on subsequent starts unless
     forced to do otherwise.  This means that, if the management node
     fails to start due to an error in this file, you must make the
     management node re-read 'config.ini' after you have corrected any
     errors in it.  You can do this by starting *note 'ndb_mgmd.exe':
     mysql-cluster-programs-ndb-mgmd. with the '--reload' or '--initial'
     option on the command line.  Either of these options works to
     refresh the configuration cache.

     It is not necessary or advisable to use either of these options in
     the management node's 'my.ini' file.

     For additional information about options which can be used with
     *note 'ndb_mgmd': mysql-cluster-programs-ndb-mgmd, see *note
     mysql-cluster-programs-ndb-mgmd::, as well as *note
     mysql-cluster-program-options-common::.

  2. On each of the data node hosts, run the command shown here to start
     the data node processes:

          C:\mysql\bin> ndbd
          2010-06-23 07:53:46 [ndbd] INFO -- Configuration fetched from 'localhost:1186', generation: 1

     In each case, the first line of output from the data node process
     should resemble what is shown in the preceding example, and is
     followed by additional lines of logging output.  As with the
     management node process, this is normal, because the data node is
     not running as a Windows service.  For this reason, do not close
     the console window in which the data node process is running; doing
     so kills *note 'ndbd.exe': mysql-cluster-programs-ndbd.  (For more
     information, see *note mysql-cluster-install-windows-service::.)

  3. Do not start the SQL node yet; it cannot connect to the cluster
     until the data nodes have finished starting, which may take some
     time.  Instead, in a new console window on the management node
     host, start the NDB Cluster management client *note 'ndb_mgm.exe':
     mysql-cluster-programs-ndb-mgm, which should be in 'C:\mysql\bin'
     on the management node host.  (Do not try to re-use the console
     window where *note 'ndb_mgmd.exe': mysql-cluster-programs-ndb-mgmd.
     is running by typing 'CTRL'+'C', as this kills the management
     node.)  The resulting output should look like this:

          C:\mysql\bin> ndb_mgm
          -- NDB Cluster -- Management Client --
          ndb_mgm>

     When the prompt 'ndb_mgm>' appears, this indicates that the
     management client is ready to receive NDB Cluster management
     commands.  You can observe the status of the data nodes as they
     start by entering 'ALL STATUS' at the management client prompt.
     This command causes a running report of the data nodes's startup
     sequence, which should look something like this:

          ndb_mgm> ALL STATUS
          Connected to Management Server at: localhost:1186
          Node 2: starting (Last completed phase 3) (mysql-5.5.65-ndb-7.2.39)
          Node 3: starting (Last completed phase 3) (mysql-5.5.65-ndb-7.2.39)

          Node 2: starting (Last completed phase 4) (mysql-5.5.65-ndb-7.2.39)
          Node 3: starting (Last completed phase 4) (mysql-5.5.65-ndb-7.2.39)

          Node 2: Started (version 7.2.39)
          Node 3: Started (version 7.2.39)

          ndb_mgm>

     *Note*:

     Commands issued in the management client are not case-sensitive; we
     use uppercase as the canonical form of these commands, but you are
     not required to observe this convention when inputting them into
     the *note 'ndb_mgm': mysql-cluster-programs-ndb-mgm. client.  For
     more information, see *note mysql-cluster-mgm-client-commands::.

     The output produced by 'ALL STATUS' is likely to vary from what is
     shown here, according to the speed at which the data nodes are able
     to start, the release version number of the NDB Cluster software
     you are using, and other factors.  What is significant is that,
     when you see that both data nodes have started, you are ready to
     start the SQL node.

     You can leave *note 'ndb_mgm.exe': mysql-cluster-programs-ndb-mgm.
     running; it has no negative impact on the performance of the NDB
     Cluster, and we use it in the next step to verify that the SQL node
     is connected to the cluster after you have started it.

  4. On the computer designated as the SQL node host, open a console
     window and navigate to the directory where you unpacked the NDB
     Cluster binaries (if you are following our example, this is
     'C:\mysql\bin').

     Start the SQL node by invoking *note 'mysqld.exe': mysqld. from the
     command line, as shown here:

          C:\mysql\bin> mysqld --console

     The '--console' option causes logging information to be written to
     the console, which can be helpful in the event of problems.  (Once
     you are satisfied that the SQL node is running in a satisfactory
     manner, you can stop it and restart it out without the '--console'
     option, so that logging is performed normally.)

     In the console window where the management client (*note
     'ndb_mgm.exe': mysql-cluster-programs-ndb-mgm.) is running on the
     management node host, enter the 'SHOW' command, which should
     produce output similar to what is shown here:

          ndb_mgm> SHOW
          Connected to Management Server at: localhost:1186
          Cluster Configuration
          ---------------------
          [ndbd(NDB)]     2 node(s)
          id=2    @198.51.100.30  (Version: 5.5.65-ndb-7.2.39, Nodegroup: 0, *)
          id=3    @198.51.100.40  (Version: 5.5.65-ndb-7.2.39, Nodegroup: 0)

          [ndb_mgmd(MGM)] 1 node(s)
          id=1    @198.51.100.10  (Version: 5.5.65-ndb-7.2.39)

          [mysqld(API)]   1 node(s)
          id=4    @198.51.100.20  (Version: 5.5.65-ndb-7.2.39)

     You can also verify that the SQL node is connected to the NDB
     Cluster in the *note 'mysql': mysql. client (*note 'mysql.exe':
     mysql.) using the 'SHOW ENGINE NDB STATUS' statement.

You should now be ready to work with database objects and data using NDB
Cluster's *note 'NDBCLUSTER': mysql-cluster. storage engine.  See *note
mysql-cluster-install-example-data::, for more information and examples.

You can also install *note 'ndb_mgmd.exe':
mysql-cluster-programs-ndb-mgmd, *note 'ndbd.exe':
mysql-cluster-programs-ndbd, and *note 'ndbmtd.exe':
mysql-cluster-programs-ndbmtd. as Windows services.  For information on
how to do this, see *note mysql-cluster-install-windows-service::).


File: manual.info.tmp,  Node: mysql-cluster-install-windows-service,  Prev: mysql-cluster-install-windows-initial-start,  Up: mysql-cluster-install-windows

18.2.2.4 Installing NDB Cluster Processes as Windows Services
.............................................................

Once you are satisfied that NDB Cluster is running as desired, you can
install the management nodes and data nodes as Windows services, so that
these processes are started and stopped automatically whenever Windows
is started or stopped.  This also makes it possible to control these
processes from the command line with the appropriate 'SC START' and 'SC
STOP' commands, or using the Windows graphical 'Services' utility.  'NET
START' and 'NET STOP' commands can also be used.

Installing programs as Windows services usually must be done using an
account that has Administrator rights on the system.

To install the management node as a service on Windows, invoke *note
'ndb_mgmd.exe': mysql-cluster-programs-ndb-mgmd. from the command line
on the machine hosting the management node, using the '--install'
option, as shown here:

     C:\> C:\mysql\bin\ndb_mgmd.exe --install
     Installing service 'NDB Cluster Management Server'
       as '"C:\mysql\bin\ndbd.exe" "--service=ndb_mgmd"'
     Service successfully installed.

*Important*:

When installing an NDB Cluster program as a Windows service, you should
always specify the complete path; otherwise the service installation may
fail with the error 'The system cannot find the file specified'.

The '--install' option must be used first, ahead of any other options
that might be specified for *note 'ndb_mgmd.exe':
mysql-cluster-programs-ndb-mgmd.  However, it is preferable to specify
such options in an options file instead.  If your options file is not in
one of the default locations as shown in the output of *note
'ndb_mgmd.exe': mysql-cluster-programs-ndb-mgmd. '--help', you can
specify the location using the '--config-file' option.

Now you should be able to start and stop the management server like
this:

     C:\> SC START ndb_mgmd

     C:\> SC STOP ndb_mgmd

*Note*:

If using 'NET' commands, you can also start or stop the management
server as a Windows service using the descriptive name, as shown here:

     C:\> NET START 'NDB Cluster Management Server'
     The NDB Cluster Management Server service is starting.
     The NDB Cluster Management Server service was started successfully.

     C:\> NET STOP  'NDB Cluster Management Server'
     The NDB Cluster Management Server service is stopping..
     The NDB Cluster Management Server service was stopped successfully.

It is usually simpler to specify a short service name or to permit the
default service name to be used when installing the service, and then
reference that name when starting or stopping the service.  To specify a
service name other than 'ndb_mgmd', append it to the '--install' option,
as shown in this example:

     C:\> C:\mysql\bin\ndb_mgmd.exe --install=mgmd1
     Installing service 'NDB Cluster Management Server'
       as '"C:\mysql\bin\ndb_mgmd.exe" "--service=mgmd1"'
     Service successfully installed.

Now you should be able to start or stop the service using the name you
have specified, like this:

     C:\> SC START mgmd1

     C:\> SC STOP mgmd1

To remove the management node service, use 'SC DELETE SERVICE_NAME':

     C:\> SC DELETE mgmd1

Alternatively, invoke *note 'ndb_mgmd.exe':
mysql-cluster-programs-ndb-mgmd. with the '--remove' option, as shown
here:

     C:\> C:\mysql\bin\ndb_mgmd.exe --remove
     Removing service 'NDB Cluster Management Server'
     Service successfully removed.

If you installed the service using a service name other than the
default, pass the service name as the value of the *note 'ndb_mgmd.exe':
mysql-cluster-programs-ndb-mgmd. '--remove' option, like this:

     C:\> C:\mysql\bin\ndb_mgmd.exe --remove=mgmd1
     Removing service 'mgmd1'
     Service successfully removed.

Installation of an NDB Cluster data node process as a Windows service
can be done in a similar fashion, using the '--install' option for *note
'ndbd.exe': mysql-cluster-programs-ndbd. (or *note 'ndbmtd.exe':
mysql-cluster-programs-ndbmtd.), as shown here:

     C:\> C:\mysql\bin\ndbd.exe --install
     Installing service 'NDB Cluster Data Node Daemon' as '"C:\mysql\bin\ndbd.exe" "--service=ndbd"'
     Service successfully installed.

Now you can start or stop the data node as shown in the following
example:

     C:\> SC START ndbd

     C:\> SC STOP ndbd

To remove the data node service, use 'SC DELETE SERVICE_NAME':

     C:\> SC DELETE ndbd

Alternatively, invoke *note 'ndbd.exe': mysql-cluster-programs-ndbd.
with the '--remove' option, as shown here:

     C:\> C:\mysql\bin\ndbd.exe --remove
     Removing service 'NDB Cluster Data Node Daemon'
     Service successfully removed.

As with *note 'ndb_mgmd.exe': mysql-cluster-programs-ndb-mgmd. (and
*note 'mysqld.exe': mysqld.), when installing *note 'ndbd.exe':
mysql-cluster-programs-ndbd. as a Windows service, you can also specify
a name for the service as the value of '--install', and then use it when
starting or stopping the service, like this:

     C:\> C:\mysql\bin\ndbd.exe --install=dnode1
     Installing service 'dnode1' as '"C:\mysql\bin\ndbd.exe" "--service=dnode1"'
     Service successfully installed.

     C:\> SC START dnode1

     C:\> SC STOP dnode1

If you specified a service name when installing the data node service,
you can use this name when removing it as well, as shown here:

     C:\> SC DELETE dnode1

Alternatively, you can pass the service name as the value of the
'ndbd.exe' '--remove' option, as shown here:

     C:\> C:\mysql\bin\ndbd.exe --remove=dnode1
     Removing service 'dnode1'
     Service successfully removed.

Installation of the SQL node as a Windows service, starting the service,
stopping the service, and removing the service are done in a similar
fashion, using *note 'mysqld': mysqld. '--install', 'SC START', 'SC
STOP', and 'SC DELETE' (or *note 'mysqld': mysqld. '--remove').  'NET'
commands can also be used to start or stop a service.  For additional
information, see *note windows-start-service::.


File: manual.info.tmp,  Node: mysql-cluster-install-configuration,  Next: mysql-cluster-install-first-start,  Prev: mysql-cluster-install-windows,  Up: mysql-cluster-installation

18.2.3 Initial Configuration of NDB Cluster
-------------------------------------------

For our four-node, four-host NDB Cluster, it is necessary to write four
configuration files, one per node host.

   * Each data node or SQL node requires a 'my.cnf' file that provides
     two pieces of information: a _connection string_ that tells the
     node where to find the management node, and a line telling the
     MySQL server on this host (the machine hosting the data node) to
     enable the *note 'NDBCLUSTER': mysql-cluster. storage engine.

     For more information on connection strings, see *note
     mysql-cluster-connection-strings::.

   * The management node needs a 'config.ini' file telling it how many
     replicas to maintain, how much memory to allocate for data and
     indexes on each data node, where to find the data nodes, where to
     save data to disk on each data node, and where to find any SQL
     nodes.

Configuring the data nodes and SQL nodes

The 'my.cnf' file needed for the data nodes is fairly simple.  The
configuration file should be located in the '/etc' directory and can be
edited using any text editor.  (Create the file if it does not exist.)
For example:

     shell> vi /etc/my.cnf

*Note*:

We show 'vi' being used here to create the file, but any text editor
should work just as well.

For each data node and SQL node in our example setup, 'my.cnf' should
look like this:

     [mysqld]
     # Options for mysqld process:
     ndbcluster                      # run NDB storage engine

     [mysql_cluster]
     # Options for NDB Cluster processes:
     ndb-connectstring=198.51.100.10  # location of management server

After entering the preceding information, save this file and exit the
text editor.  Do this for the machines hosting data node 'A', data node
'B', and the SQL node.

*Important*:

Once you have started a *note 'mysqld': mysqld. process with the
'ndbcluster' and 'ndb-connectstring' parameters in the '[mysqld]' and
'[mysql_cluster]' sections of the 'my.cnf' file as shown previously, you
cannot execute any *note 'CREATE TABLE': create-table. or *note 'ALTER
TABLE': alter-table. statements without having actually started the
cluster.  Otherwise, these statements will fail with an error.  This is
by design.

Configuring the management node

The first step in configuring the management node is to create the
directory in which the configuration file can be found and then to
create the file itself.  For example (running as 'root'):

     shell> mkdir /var/lib/mysql-cluster
     shell> cd /var/lib/mysql-cluster
     shell> vi config.ini

For our representative setup, the 'config.ini' file should read as
follows:

     [ndbd default]
     # Options affecting ndbd processes on all data nodes:
     NoOfReplicas=2    # Number of replicas
     DataMemory=80M    # How much memory to allocate for data storage
     IndexMemory=18M   # How much memory to allocate for index storage
                       # For DataMemory and IndexMemory, we have used the
                       # default values. Since the "world" database takes up
                       # only about 500KB, this should be more than enough for
                       # this example Cluster setup.

     [tcp default]
     # TCP/IP options:
     PortNumber=2202   # This the default; however, you can use any
                       # port that is free for all the hosts in the cluster
                       # Note: It is recommended that you do not specify the port
                       # number at all and simply allow the default value to be used
                       # instead

     [ndb_mgmd]
     # Management process options:
     HostName=198.51.100.10          # Hostname or IP address of MGM node
     DataDir=/var/lib/mysql-cluster  # Directory for MGM node log files

     [ndbd]
     # Options for data node "A":
                                     # (one [ndbd] section per data node)
     HostName=198.51.100.30          # Hostname or IP address
     NodeId=2                        # Node ID for this data node
     DataDir=/usr/local/mysql/data   # Directory for this data node's data files

     [ndbd]
     # Options for data node "B":
     HostName=198.51.100.40          # Hostname or IP address
     NodeId=3                        # Node ID for this data node
     DataDir=/usr/local/mysql/data   # Directory for this data node's data files

     [mysqld]
     # SQL node options:
     HostName=198.51.100.20          # Hostname or IP address
                                     # (additional mysqld connections can be
                                     # specified for this node for various
                                     # purposes such as running ndb_restore)

*Note*:

The 'world' database can be downloaded from
<https://dev.mysql.com/doc/index-other.html>.

After all the configuration files have been created and these minimal
options have been specified, you are ready to proceed with starting the
cluster and verifying that all processes are running.  We discuss how
this is done in *note mysql-cluster-install-first-start::.

For more detailed information about the available NDB Cluster
configuration parameters and their uses, see *note
mysql-cluster-config-file::, and *note mysql-cluster-configuration::.
For configuration of NDB Cluster as relates to making backups, see *note
mysql-cluster-backup-configuration::.

*Note*:

The default port for Cluster management nodes is 1186; the default port
for data nodes is 2202.  However, the cluster can automatically allocate
ports for data nodes from those that are already free.


File: manual.info.tmp,  Node: mysql-cluster-install-first-start,  Next: mysql-cluster-install-example-data,  Prev: mysql-cluster-install-configuration,  Up: mysql-cluster-installation

18.2.4 Initial Startup of NDB Cluster
-------------------------------------

Starting the cluster is not very difficult after it has been configured.
Each cluster node process must be started separately, and on the host
where it resides.  The management node should be started first, followed
by the data nodes, and then finally by any SQL nodes:

  1. On the management host, issue the following command from the system
     shell to start the management node process:

          shell> ndb_mgmd -f /var/lib/mysql-cluster/config.ini

     The first time that it is started, *note 'ndb_mgmd':
     mysql-cluster-programs-ndb-mgmd. must be told where to find its
     configuration file, using the '-f' or '--config-file' option.  (See
     *note mysql-cluster-programs-ndb-mgmd::, for details.)

     For additional options which can be used with *note 'ndb_mgmd':
     mysql-cluster-programs-ndb-mgmd, see *note
     mysql-cluster-program-options-common::.

  2. On each of the data node hosts, run this command to start the *note
     'ndbd': mysql-cluster-programs-ndbd. process:

          shell> ndbd

  3. If you used RPM files to install MySQL on the cluster host where
     the SQL node is to reside, you can (and should) use the supplied
     startup script to start the MySQL server process on the SQL node.

If all has gone well, and the cluster has been set up correctly, the
cluster should now be operational.  You can test this by invoking the
*note 'ndb_mgm': mysql-cluster-programs-ndb-mgm. management node client.
The output should look like that shown here, although you might see some
slight differences in the output depending upon the exact version of
MySQL that you are using:

     shell> ndb_mgm
     -- NDB Cluster -- Management Client --
     ndb_mgm> SHOW
     Connected to Management Server at: localhost:1186
     Cluster Configuration
     ---------------------
     [ndbd(NDB)]     2 node(s)
     id=2    @198.51.100.30  (Version: 5.5.65-ndb-7.2.39, Nodegroup: 0, *)
     id=3    @198.51.100.40  (Version: 5.5.65-ndb-7.2.39, Nodegroup: 0)

     [ndb_mgmd(MGM)] 1 node(s)
     id=1    @198.51.100.10  (Version: 5.5.65-ndb-7.2.39)

     [mysqld(API)]   1 node(s)
     id=4    @198.51.100.20  (Version: 5.5.65-ndb-7.2.39)

The SQL node is referenced here as '[mysqld(API)]', which reflects the
fact that the *note 'mysqld': mysqld. process is acting as an NDB
Cluster API node.

*Note*:

The IP address shown for a given NDB Cluster SQL or other API node in
the output of 'SHOW' is the address used by the SQL or API node to
connect to the cluster data nodes, and not to any management node.

You should now be ready to work with databases, tables, and data in NDB
Cluster.  See *note mysql-cluster-install-example-data::, for a brief
discussion.


File: manual.info.tmp,  Node: mysql-cluster-install-example-data,  Next: mysql-cluster-install-shutdown-restart,  Prev: mysql-cluster-install-first-start,  Up: mysql-cluster-installation

18.2.5 NDB Cluster Example with Tables and Data
-----------------------------------------------

*Note*:

The information in this section applies to NDB Cluster running on both
Unix and Windows platforms.

Working with database tables and data in NDB Cluster is not much
different from doing so in standard MySQL. There are two key points to
keep in mind:

   * For a table to be replicated in the cluster, it must use the *note
     'NDBCLUSTER': mysql-cluster. storage engine.  To specify this, use
     the 'ENGINE=NDBCLUSTER' or 'ENGINE=NDB' option when creating the
     table:

          CREATE TABLE TBL_NAME (COL_NAME COLUMN_DEFINITIONS) ENGINE=NDBCLUSTER;

     Alternatively, for an existing table that uses a different storage
     engine, use *note 'ALTER TABLE': alter-table. to change the table
     to use *note 'NDBCLUSTER': mysql-cluster.:

          ALTER TABLE TBL_NAME ENGINE=NDBCLUSTER;

   * Every *note 'NDBCLUSTER': mysql-cluster. table has a primary key.
     If no primary key is defined by the user when a table is created,
     the *note 'NDBCLUSTER': mysql-cluster. storage engine automatically
     generates a hidden one.  Such a key takes up space just as does any
     other table index.  (It is not uncommon to encounter problems due
     to insufficient memory for accommodating these automatically
     created indexes.)

If you are importing tables from an existing database using the output
of *note 'mysqldump': mysqldump, you can open the SQL script in a text
editor and add the 'ENGINE' option to any table creation statements, or
replace any existing 'ENGINE' options.  Suppose that you have the
'world' sample database on another MySQL server that does not support
NDB Cluster, and you want to export the 'City' table:

     shell> mysqldump --add-drop-table world City > city_table.sql

The resulting 'city_table.sql' file will contain this table creation
statement (and the *note 'INSERT': insert. statements necessary to
import the table data):

     DROP TABLE IF EXISTS `City`;
     CREATE TABLE `City` (
       `ID` int(11) NOT NULL auto_increment,
       `Name` char(35) NOT NULL default '',
       `CountryCode` char(3) NOT NULL default '',
       `District` char(20) NOT NULL default '',
       `Population` int(11) NOT NULL default '0',
       PRIMARY KEY  (`ID`)
     ) ENGINE=MyISAM DEFAULT CHARSET=latin1;

     INSERT INTO `City` VALUES (1,'Kabul','AFG','Kabol',1780000);
     INSERT INTO `City` VALUES (2,'Qandahar','AFG','Qandahar',237500);
     INSERT INTO `City` VALUES (3,'Herat','AFG','Herat',186800);(REMAINING INSERT STATEMENTS OMITTED)

You need to make sure that MySQL uses the *note 'NDBCLUSTER':
mysql-cluster. storage engine for this table.  There are two ways that
this can be accomplished.  One of these is to modify the table
definition _before_ importing it into the Cluster database.  Using the
'City' table as an example, modify the 'ENGINE' option of the definition
as follows:

     DROP TABLE IF EXISTS `City`;
     CREATE TABLE `City` (
       `ID` int(11) NOT NULL auto_increment,
       `Name` char(35) NOT NULL default '',
       `CountryCode` char(3) NOT NULL default '',
       `District` char(20) NOT NULL default '',
       `Population` int(11) NOT NULL default '0',
       PRIMARY KEY  (`ID`)
     ) *ENGINE=NDBCLUSTER* DEFAULT CHARSET=latin1;

     INSERT INTO `City` VALUES (1,'Kabul','AFG','Kabol',1780000);
     INSERT INTO `City` VALUES (2,'Qandahar','AFG','Qandahar',237500);
     INSERT INTO `City` VALUES (3,'Herat','AFG','Herat',186800);
     (REMAINING INSERT STATEMENTS OMITTED)

This must be done for the definition of each table that is to be part of
the clustered database.  The easiest way to accomplish this is to do a
search-and-replace on the file that contains the definitions and replace
all instances of 'TYPE=ENGINE_NAME' or 'ENGINE=ENGINE_NAME' with
'ENGINE=NDBCLUSTER'.  If you do not want to modify the file, you can use
the unmodified file to create the tables, and then use *note 'ALTER
TABLE': alter-table. to change their storage engine.  The particulars
are given later in this section.

Assuming that you have already created a database named 'world' on the
SQL node of the cluster, you can then use the *note 'mysql': mysql.
command-line client to read 'city_table.sql', and create and populate
the corresponding table in the usual manner:

     shell> mysql world < city_table.sql

It is very important to keep in mind that the preceding command must be
executed on the host where the SQL node is running (in this case, on the
machine with the IP address '198.51.100.20').

To create a copy of the entire 'world' database on the SQL node, use
*note 'mysqldump': mysqldump. on the noncluster server to export the
database to a file named 'world.sql' (for example, in the '/tmp'
directory).  Then modify the table definitions as just described and
import the file into the SQL node of the cluster like this:

     shell> mysql world < /tmp/world.sql

If you save the file to a different location, adjust the preceding
instructions accordingly.

Running *note 'SELECT': select. queries on the SQL node is no different
from running them on any other instance of a MySQL server.  To run
queries from the command line, you first need to log in to the MySQL
Monitor in the usual way (specify the 'root' password at the 'Enter
password:' prompt):

     shell> mysql -u root -p
     Enter password:
     Welcome to the MySQL monitor.  Commands end with ; or \g.
     Your MySQL connection id is 1 to server version: 5.5.65-ndb-7.2.39

     Type 'help;' or '\h' for help. Type '\c' to clear the buffer.

     mysql>

We simply use the MySQL server's 'root' account and assume that you have
followed the standard security precautions for installing a MySQL
server, including setting a strong 'root' password.  For more
information, see *note default-privileges::.

It is worth taking into account that Cluster nodes do _not_ make use of
the MySQL privilege system when accessing one another.  Setting or
changing MySQL user accounts (including the 'root' account) effects only
applications that access the SQL node, not interaction between nodes.
See *note mysql-cluster-security-mysql-privileges::, for more
information.

If you did not modify the 'ENGINE' clauses in the table definitions
prior to importing the SQL script, you should run the following
statements at this point:

     mysql> USE world;
     mysql> ALTER TABLE city ENGINE=NDBCLUSTER;
     mysql> ALTER TABLE country ENGINE=NDBCLUSTER;
     mysql> ALTER TABLE countrylanguage ENGINE=NDBCLUSTER;

Selecting a database and running a 'SELECT' query against a table in
that database is also accomplished in the usual manner, as is exiting
the MySQL Monitor:

     mysql> USE world;
     mysql> SELECT Name, Population FROM city ORDER BY Population DESC LIMIT 5;
     +-----------+------------+
     | Name      | Population |
     +-----------+------------+
     | Bombay    |   10500000 |
     | Seoul     |    9981619 |
     | Sa~o Paulo |    9968485 |
     | Shanghai  |    9696300 |
     | Jakarta   |    9604900 |
     +-----------+------------+
     5 rows in set (0.34 sec)

     mysql> \q
     Bye

     shell>

Applications that use MySQL can employ standard APIs to access *note
'NDB': mysql-cluster. tables.  It is important to remember that your
application must access the SQL node, and not the management or data
nodes.  This brief example shows how we might execute the *note
'SELECT': select. statement just shown by using the PHP 5.X 'mysqli'
extension running on a Web server elsewhere on the network:

     <!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
       "http://www.w3.org/TR/html4/loose.dtd">
     <html>
     <head>
       <meta http-equiv="Content-Type"
                content="text/html; charset=iso-8859-1">
       <title>SIMPLE mysqli SELECT</title>
     </head>
     <body>
     <?php
       # connect to SQL node:
       $link = new mysqli('198.51.100.20', 'root', 'ROOT_PASSWORD', 'world');
       # parameters for mysqli constructor are:
       #   host, user, password, database

       if( mysqli_connect_errno() )
         die("Connect failed: " . mysqli_connect_error());

       $query = "SELECT Name, Population
                 FROM City
                 ORDER BY Population DESC
                 LIMIT 5";

       # if no errors...
       if( $result = $link->query($query) )
       {
     ?>
     <table border="1" width="40%" cellpadding="4" cellspacing ="1">
       <tbody>
       <tr>
         <th width="10%">City</th>
         <th>Population</th>
       </tr>
     <?
         # then display the results...
         while($row = $result->fetch_object())
           printf("<tr>\n  <td align=\"center\">%s</td><td>%d</td>\n</tr>\n",
                   $row->Name, $row->Population);
     ?>
       </tbody
     </table>
     <?
       # ...and verify the number of rows that were retrieved
         printf("<p>Affected rows: %d</p>\n", $link->affected_rows);
       }
       else
         # otherwise, tell us what went wrong
         echo mysqli_error();

       # free the result set and the mysqli connection object
       $result->close();
       $link->close();
     ?>
     </body>
     </html>

We assume that the process running on the Web server can reach the IP
address of the SQL node.

In a similar fashion, you can use the MySQL C API, Perl-DBI,
Python-mysql, or MySQL Connectors to perform the tasks of data
definition and manipulation just as you would normally with MySQL.


File: manual.info.tmp,  Node: mysql-cluster-install-shutdown-restart,  Next: mysql-cluster-upgrade-downgrade,  Prev: mysql-cluster-install-example-data,  Up: mysql-cluster-installation

18.2.6 Safe Shutdown and Restart of NDB Cluster
-----------------------------------------------

To shut down the cluster, enter the following command in a shell on the
machine hosting the management node:

     shell> ndb_mgm -e shutdown

The '-e' option here is used to pass a command to the *note 'ndb_mgm':
mysql-cluster-programs-ndb-mgm. client from the shell.  (See *note
mysql-cluster-program-options-common::, for more information about this
option.)  The command causes the *note 'ndb_mgm':
mysql-cluster-programs-ndb-mgm, *note 'ndb_mgmd':
mysql-cluster-programs-ndb-mgmd, and any *note 'ndbd':
mysql-cluster-programs-ndbd. or *note 'ndbmtd':
mysql-cluster-programs-ndbmtd. processes to terminate gracefully.  Any
SQL nodes can be terminated using *note 'mysqladmin shutdown':
mysqladmin. and other means.  On Windows platforms, assuming that you
have installed the SQL node as a Windows service, you can use 'SC STOP
SERVICE_NAME' or 'NET STOP SERVICE_NAME'.

To restart the cluster on Unix platforms, run these commands:

   * On the management host ('198.51.100.10' in our example setup):

          shell> ndb_mgmd -f /var/lib/mysql-cluster/config.ini

   * On each of the data node hosts ('198.51.100.30' and
     '198.51.100.40'):

          shell> ndbd

   * Use the *note 'ndb_mgm': mysql-cluster-programs-ndb-mgm. client to
     verify that both data nodes have started successfully.

   * On the SQL host ('198.51.100.20'):

          shell> mysqld_safe &

On Windows platforms, assuming that you have installed all NDB Cluster
processes as Windows services using the default service names (see *note
mysql-cluster-install-windows-service::), you can restart the cluster as
follows:

   * On the management host ('198.51.100.10' in our example setup),
     execute the following command:

          C:\> SC START ndb_mgmd

   * On each of the data node hosts ('198.51.100.30' and
     '198.51.100.40'), execute the following command:

          C:\> SC START ndbd

   * On the management node host, use the *note 'ndb_mgm':
     mysql-cluster-programs-ndb-mgm. client to verify that the
     management node and both data nodes have started successfully (see
     *note mysql-cluster-install-windows-initial-start::).

   * On the SQL node host ('198.51.100.20'), execute the following
     command:

          C:\> SC START mysql

In a production setting, it is usually not desirable to shut down the
cluster completely.  In many cases, even when making configuration
changes, or performing upgrades to the cluster hardware or software (or
both), which require shutting down individual host machines, it is
possible to do so without shutting down the cluster as a whole by
performing a _rolling restart_ of the cluster.  For more information
about doing this, see *note mysql-cluster-rolling-restart::.


File: manual.info.tmp,  Node: mysql-cluster-upgrade-downgrade,  Prev: mysql-cluster-install-shutdown-restart,  Up: mysql-cluster-installation

18.2.7 Upgrading and Downgrading NDB Cluster
--------------------------------------------

This section provides information about NDB Cluster software and table
file compatibility between different NDB Cluster 7.2 releases with
regard to performing upgrades and downgrades as well as compatibility
matrices and notes.  You should already be familiar with installing and
configuring NDB Cluster prior to attempting an upgrade or downgrade.
See *note mysql-cluster-configuration::.

Schema operations, including SQL DDL statements, cannot be performed
while any data nodes are restarting, and thus during an online upgrade
or downgrade of the cluster.  For other information regarding the
rolling restart procedure used to perform an online upgrade, see *note
mysql-cluster-rolling-restart::.

*Important*:

Only compatibility between MySQL versions with regard to *note
'NDBCLUSTER': mysql-cluster. is taken into account in this section, and
there are likely other issues to be considered.  _As with any other
MySQL software upgrade or downgrade, you are strongly encouraged to
review the relevant portions of the MySQL Manual for the MySQL versions
from which and to which you intend to migrate, before attempting an
upgrade or downgrade of the NDB Cluster software_.  This is especially
true when planning a migration from NDB Cluster 7.1 (or earlier) to NDB
Cluster 7.2, since the version of the underlying MySQL Server also
changes from MySQL 5.1 to MySQL 5.5.  See *note upgrading::.

The table shown here provides information on NDB Cluster upgrade and
downgrade compatibility among different releases of NDB Cluster 7.2.
Additional notes about upgrades and downgrades to, from, or within the
NDB Cluster 7.2 release series can be found immediately following the
table.

FIGURE GOES HERE: NDB Cluster Upgrade and Downgrade Compatibility, MySQL
NDB Cluster 7.2

*Notes: NDB Cluster 7.2*

Versions supported

The following versions of NDB Cluster are supported for upgrades to NDB
Cluster 7.2 (7.2.4 and later):

   * NDB Cluster 7.1 GA releases (7.1.3 and later)

   * NDB Cluster 7.0 GA releases (7.0.5 and later)

   * NDB Cluster 6.3 GA releases (6.3.8 and later) that can be upgraded
     to NDB Cluster 7.1

For information about upgrades to and downgrades from NDB Cluster 7.3,
see Upgrading and Downgrading NDB Cluster
(https://dev.mysql.com/doc/refman/5.6/en/mysql-cluster-upgrade-downgrade.html).
For information about upgrades and downgrades in previous NDB Cluster
release series, see the 'MySQL 5.1 Reference Manual'.

NDB API (https://dev.mysql.com/doc/ndbapi/en/ndbapi.html), ClusterJ
(https://dev.mysql.com/doc/ndbapi/en/mccj.html), and other applications
used with recent releases of NDB Cluster 6.3 and later should continue
to work with NDB 7.2.4 and later without rewriting or recompiling.

In NDB Cluster 7.2, the default values for a number of node
configuration parameters have changed.  See *note
mysql-cluster-what-is-new-defaults-changes::, for a listing of these.

Other known issues include the following:

   * In NDB 7.2.7 and later, the size of the hash map is 3840 LDM
     threads, an increase from 240 in previous versions.  When upgrading
     an NDB Cluster from NDB 7.2.6 and earlier to NDB 7.2.9 or later,
     you can modify existing tables online to take advantage of the new
     size: following the upgrade, increase the number of fragments by
     (for example) adding new data nodes to the cluster, and then
     execute *note 'ALTER ONLINE TABLE ... REORGANIZE PARTITION':
     alter-table-partition-operations. on any tables that were created
     in the older version.  Following this, these tables can use the
     larger hash map size.  (Bug #14645319)

     Due to this change, it was not possible to downgrade online to NDB
     7.2.6 or earlier.  This issue was resolved in NDB 7.2.11, where the
     size is made configurable using the 'DefaultHashMapSize' parameter.
     (Bug #14800539) See the description of this parameter for more
     information.

   * It is not possible to downgrade online to NDB 7.2.13 or earlier
     from NDB 7.2.14 or later.  Online upgrades from NDB 7.2.13 to later
     NDB Cluster 7.2 releases are supported.


File: manual.info.tmp,  Node: mysql-cluster-configuration,  Next: mysql-cluster-programs,  Prev: mysql-cluster-installation,  Up: mysql-cluster

18.3 Configuration of NDB Cluster
=================================

* Menu:

* mysql-cluster-quick::          Quick Test Setup of NDB Cluster
* mysql-cluster-configuration-overview::  Overview of NDB Cluster Configuration Parameters, Options, and Variables
* mysql-cluster-config-file::    NDB Cluster Configuration Files
* mysql-cluster-interconnects::  Using High-Speed Interconnects with NDB Cluster

A MySQL server that is part of an NDB Cluster differs in one chief
respect from a normal (nonclustered) MySQL server, in that it employs
the *note 'NDB': mysql-cluster. storage engine.  This engine is also
referred to sometimes as *note 'NDBCLUSTER': mysql-cluster, although
'NDB' is preferred.

To avoid unnecessary allocation of resources, the server is configured
by default with the *note 'NDB': mysql-cluster. storage engine disabled.
To enable *note 'NDB': mysql-cluster, you must modify the server's
'my.cnf' configuration file, or start the server with the '--ndbcluster'
option.

This MySQL server is a part of the cluster, so it also must know how to
access a management node to obtain the cluster configuration data.  The
default behavior is to look for the management node on 'localhost'.
However, should you need to specify that its location is elsewhere, this
can be done in 'my.cnf', or with the *note 'mysql': mysql. client.
Before the *note 'NDB': mysql-cluster. storage engine can be used, at
least one management node must be operational, as well as any desired
data nodes.

For more information about '--ndbcluster' and other *note 'mysqld':
mysqld. options specific to NDB Cluster, see *note
mysql-cluster-program-options-mysqld::.

For information about installing NDB Cluster, see *note
mysql-cluster-installation::.


File: manual.info.tmp,  Node: mysql-cluster-quick,  Next: mysql-cluster-configuration-overview,  Prev: mysql-cluster-configuration,  Up: mysql-cluster-configuration

18.3.1 Quick Test Setup of NDB Cluster
--------------------------------------

To familiarize you with the basics, we will describe the simplest
possible configuration for a functional NDB Cluster.  After this, you
should be able to design your desired setup from the information
provided in the other relevant sections of this chapter.

First, you need to create a configuration directory such as
'/var/lib/mysql-cluster', by executing the following command as the
system 'root' user:

     shell> mkdir /var/lib/mysql-cluster

In this directory, create a file named 'config.ini' that contains the
following information.  Substitute appropriate values for 'HostName' and
'DataDir' as necessary for your system.

     # file "config.ini" - showing minimal setup consisting of 1 data node,
     # 1 management server, and 3 MySQL servers.
     # The empty default sections are not required, and are shown only for
     # the sake of completeness.
     # Data nodes must provide a hostname but MySQL Servers are not required
     # to do so.
     # If you don't know the hostname for your machine, use localhost.
     # The DataDir parameter also has a default value, but it is recommended to
     # set it explicitly.
     # Note: [db], [api], and [mgm] are aliases for [ndbd], [mysqld], and [ndb_mgmd],
     # respectively. [db] is deprecated and should not be used in new installations.

     [ndbd default]
     NoOfReplicas= 1

     [mysqld  default]
     [ndb_mgmd default]
     [tcp default]

     [ndb_mgmd]
     HostName= myhost.example.com

     [ndbd]
     HostName= myhost.example.com
     DataDir= /var/lib/mysql-cluster

     [mysqld]
     [mysqld]
     [mysqld]

You can now start the *note 'ndb_mgmd': mysql-cluster-programs-ndb-mgmd.
management server.  By default, it attempts to read the 'config.ini'
file in its current working directory, so change location into the
directory where the file is located and then invoke *note 'ndb_mgmd':
mysql-cluster-programs-ndb-mgmd.:

     shell> cd /var/lib/mysql-cluster
     shell> ndb_mgmd

Then start a single data node by running *note 'ndbd':
mysql-cluster-programs-ndbd.:

     shell> ndbd

For command-line options which can be used when starting *note 'ndbd':
mysql-cluster-programs-ndbd, see *note
mysql-cluster-program-options-common::.

By default, *note 'ndbd': mysql-cluster-programs-ndbd. looks for the
management server at 'localhost' on port 1186.

*Note*:

If you have installed MySQL from a binary tarball, you will need to
specify the path of the *note 'ndb_mgmd':
mysql-cluster-programs-ndb-mgmd. and *note 'ndbd':
mysql-cluster-programs-ndbd. servers explicitly.  (Normally, these will
be found in '/usr/local/mysql/bin'.)

Finally, change location to the MySQL data directory (usually
'/var/lib/mysql' or '/usr/local/mysql/data'), and make sure that the
'my.cnf' file contains the option necessary to enable the NDB storage
engine:

     [mysqld]
     ndbcluster

You can now start the MySQL server as usual:

     shell> mysqld_safe --user=mysql &

Wait a moment to make sure the MySQL server is running properly.  If you
see the notice 'mysql ended', check the server's '.err' file to find out
what went wrong.

If all has gone well so far, you now can start using the cluster.
Connect to the server and verify that the *note 'NDBCLUSTER':
mysql-cluster. storage engine is enabled:

     shell> mysql
     Welcome to the MySQL monitor.  Commands end with ; or \g.
     Your MySQL connection id is 1 to server version: 5.5.62

     Type 'help;' or '\h' for help. Type '\c' to clear the buffer.

     mysql> SHOW ENGINES\G
     ...
     *************************** 12. row ***************************
     Engine: NDBCLUSTER
     Support: YES
     Comment: Clustered, fault-tolerant, memory-based tables
     *************************** 13. row ***************************
     Engine: NDB
     Support: YES
     Comment: Alias for NDBCLUSTER
     ...

The row numbers shown in the preceding example output may be different
from those shown on your system, depending upon how your server is
configured.

Try to create an *note 'NDBCLUSTER': mysql-cluster. table:

     shell> mysql
     mysql> USE test;
     Database changed

     mysql> CREATE TABLE ctest (i INT) ENGINE=NDBCLUSTER;
     Query OK, 0 rows affected (0.09 sec)

     mysql> SHOW CREATE TABLE ctest \G
     *************************** 1. row ***************************
            Table: ctest
     Create Table: CREATE TABLE `ctest` (
       `i` int(11) default NULL
     ) ENGINE=ndbcluster DEFAULT CHARSET=latin1
     1 row in set (0.00 sec)

To check that your nodes were set up properly, start the management
client:

     shell> ndb_mgm

Use the 'SHOW' command from within the management client to obtain a
report on the cluster's status:

     ndb_mgm> SHOW
     Cluster Configuration
     ---------------------
     [ndbd(NDB)]     1 node(s)
     id=2    @127.0.0.1  (Version: 5.5.65-ndb-7.2.39, Nodegroup: 0, *)

     [ndb_mgmd(MGM)] 1 node(s)
     id=1    @127.0.0.1  (Version: 5.5.65-ndb-7.2.39)

     [mysqld(API)]   3 node(s)
     id=3    @127.0.0.1  (Version: 5.5.65-ndb-7.2.39)
     id=4 (not connected, accepting connect from any host)
     id=5 (not connected, accepting connect from any host)

At this point, you have successfully set up a working NDB Cluster.  You
can now store data in the cluster by using any table created with
'ENGINE=NDBCLUSTER' or its alias 'ENGINE=NDB'.


File: manual.info.tmp,  Node: mysql-cluster-configuration-overview,  Next: mysql-cluster-config-file,  Prev: mysql-cluster-quick,  Up: mysql-cluster-configuration

18.3.2 Overview of NDB Cluster Configuration Parameters, Options, and Variables
-------------------------------------------------------------------------------

* Menu:

* mysql-cluster-params-ndbd::    NDB Cluster Data Node Configuration Parameters
* mysql-cluster-params-mgmd::    NDB Cluster Management Node Configuration Parameters
* mysql-cluster-params-api::     NDB Cluster SQL Node and API Node Configuration Parameters
* mysql-cluster-params-other::   Other NDB Cluster Configuration Parameters
* mysql-cluster-option-tables::  NDB Cluster mysqld Option and Variable Reference

The next several sections provide summary tables of NDB Cluster node
configuration parameters used in the 'config.ini' file to govern various
aspects of node behavior, as well as of options and variables read by
*note 'mysqld': mysqld. from a 'my.cnf' file or from the command line
when run as an NDB Cluster process.  Each of the node parameter tables
lists the parameters for a given type ('ndbd', 'ndb_mgmd', 'mysqld',
'computer', 'tcp', 'shm', or 'sci').  All tables include the data type
for the parameter, option, or variable, as well as its default, mimimum,
and maximum values as applicable.

Considerations when restarting nodes

For node parameters, these tables also indicate what type of restart is
required (node restart or system restart)--and whether the restart must
be done with '--initial'--to change the value of a given configuration
parameter.  When performing a node restart or an initial node restart,
all of the cluster's data nodes must be restarted in turn (also referred
to as a _rolling restart_).  It is possible to update cluster
configuration parameters marked as 'node' online--that is, without
shutting down the cluster--in this fashion.  An initial node restart
requires restarting each *note 'ndbd': mysql-cluster-programs-ndbd.
process with the '--initial' option.

A system restart requires a complete shutdown and restart of the entire
cluster.  An initial system restart requires taking a backup of the
cluster, wiping the cluster file system after shutdown, and then
restoring from the backup following the restart.

In any cluster restart, all of the cluster's management servers must be
restarted for them to read the updated configuration parameter values.

*Important*:

Values for numeric cluster parameters can generally be increased without
any problems, although it is advisable to do so progressively, making
such adjustments in relatively small increments.  Many of these can be
increased online, using a rolling restart.

However, decreasing the values of such parameters--whether this is done
using a node restart, node initial restart, or even a complete system
restart of the cluster--is not to be undertaken lightly; it is
recommended that you do so only after careful planning and testing.
This is especially true with regard to those parameters that relate to
memory usage and disk space, such as 'MaxNoOfTables',
'MaxNoOfOrderedIndexes', and 'MaxNoOfUniqueHashIndexes'.  In addition,
it is the generally the case that configuration parameters relating to
memory and disk usage can be raised using a simple node restart, but
they require an initial node restart to be lowered.

Because some of these parameters can be used for configuring more than
one type of cluster node, they may appear in more than one of the
tables.

*Note*:

'4294967039' often appears as a maximum value in these tables.  This
value is defined in the *note 'NDBCLUSTER': mysql-cluster. sources as
'MAX_INT_RNIL' and is equal to '0xFFFFFEFF', or '2^32 − 2^8 − 1'.


File: manual.info.tmp,  Node: mysql-cluster-params-ndbd,  Next: mysql-cluster-params-mgmd,  Prev: mysql-cluster-configuration-overview,  Up: mysql-cluster-configuration-overview

18.3.2.1 NDB Cluster Data Node Configuration Parameters
.......................................................

The listings in this section provide information about parameters used
in the '[ndbd]' or '[ndbd default]' sections of a 'config.ini' file for
configuring NDB Cluster data nodes.  For detailed descriptions and other
additional information about each of these parameters, see *note
mysql-cluster-ndbd-definition::.

These parameters also apply to *note 'ndbmtd':
mysql-cluster-programs-ndbmtd, the multithreaded version of *note
'ndbd': mysql-cluster-programs-ndbd.  For more information, see *note
mysql-cluster-programs-ndbmtd::.

   * 'Arbitration': How arbitration should be performed to avoid
     split-brain issues in event of node failure

   * 'ArbitrationTimeout': Maximum time (milliseconds) database
     partition waits for arbitration signal

   * 'BackupDataBufferSize': Default size of databuffer for a backup (in
     bytes)

   * 'BackupDataDir': Path to where to store backups.  Note that string
     '/BACKUP' is always appended to this setting, so that *effective*
     default is FileSystemPath/BACKUP

   * 'BackupLogBufferSize': Default size of log buffer for a backup (in
     bytes)

   * 'BackupMaxWriteSize': Maximum size of file system writes made by
     backup (in bytes)

   * 'BackupMemory': Total memory allocated for backups per node (in
     bytes)

   * 'BackupReportFrequency': Frequency of backup status reports during
     backup in seconds

   * 'BackupWriteSize': Default size of file system writes made by
     backup (in bytes)

   * 'BatchSizePerLocalScan': Used to calculate number of lock records
     for scan with hold lock

   * 'BuildIndexThreads': Number of threads to use for building ordered
     indexes during a system or node restart.  Also applies when running
     ndb_restore -rebuild-indexes.  Setting this parameter to 0 disables
     multithreaded building of ordered indexes

   * 'CompressedBackup': Use zlib to compress backups as they are
     written

   * 'CompressedLCP': Write compressed LCPs using zlib

   * 'ConnectCheckIntervalDelay': Time between data node connectivity
     check stages.  Data node is considered suspect after 1 interval and
     dead after 2 intervals with no response

   * 'CrashOnCorruptedTuple': When enabled, forces node to shut down
     whenever it detects a corrupted tuple

   * 'DataDir': Data directory for this node

   * 'DataMemory': Number of bytes on each data node allocated for
     storing data; subject to available system RAM and size of
     IndexMemory

   * 'DefaultHashMapSize': Set size (in buckets) to use for table hash
     maps.  Three values are supported: 0, 240, and 3840.  Intended
     primarily for upgrades and downgrades within NDB 7.2

   * 'DictTrace': Enable DBDICT debugging; for NDB development

   * 'DiskCheckpointSpeed': Bytes allowed to be written by checkpoint,
     per second

   * 'DiskCheckpointSpeedInRestart': Bytes allowed to be written by
     checkpoint during restart, per second

   * 'DiskIOThreadPool': Number of unbound threads for file access,
     applies to disk data only

   * 'Diskless': Run without using disk

   * 'DiskPageBufferEntries': Number of 32 KB page entries to allocate
     in DiskPageBufferMemory.  Very large disk transactions may require
     increasing this value

   * 'DiskPageBufferMemory': Number of bytes on each data node allocated
     for disk page buffer cache

   * 'DiskSyncSize': Amount of data written to file before a synch is
     forced

   * 'EventLogBufferSize': Size of circular buffer for NDB log events
     within data nodes

   * 'ExecuteOnComputer': String referencing an earlier defined COMPUTER

   * 'ExtraSendBufferMemory': Memory to use for send buffers in addition
     to any allocated by TotalSendBufferMemory or SendBufferMemory.
     Default (0) allows up to 16MB

   * 'FileSystemPath': Path to directory where data node stores its data
     (directory must exist)

   * 'FileSystemPathDataFiles': Path to directory where data node stores
     its Disk Data files.  The default value is FilesystemPathDD, if
     set; otherwise, FilesystemPath is used if it is set; otherwise,
     value of DataDir is used

   * 'FileSystemPathDD': Path to directory where data node stores its
     Disk Data and undo files.  Default value is FileSystemPath, if set;
     otherwise, value of DataDir is used

   * 'FileSystemPathUndoFiles': Path to directory where data node stores
     its undo files for Disk Data.  Default value is FilesystemPathDD,
     if set; otherwise, FilesystemPath is used if it is set; otherwise,
     value of DataDir is used

   * 'FragmentLogFileSize': Size of each redo log file

   * 'HeartbeatIntervalDbApi': Time between API node-data node
     heartbeats.  (API connection closed after 3 missed heartbeats)

   * 'HeartbeatIntervalDbDb': Time between data node-to-data node
     heartbeats; data node considered dead after 3 missed heartbeats

   * 'HeartbeatOrder': Sets order in which data nodes check each others'
     heartbeats for determining whether given node is still active and
     connected to cluster.  Must be zero for all data nodes or distinct
     nonzero values for all data nodes; see documentation for further
     guidance

   * 'HostName': Host name or IP address for this data node

   * 'Id': Number identifying data node.  Now deprecated; use NodeId
     instead

   * 'IndexMemory': Number of bytes on each data node allocated for
     storing indexes; subject to available system RAM and size of
     DataMemory

   * 'IndexStatAutoCreate': Enable/disable automatic statistics
     collection when indexes are created

   * 'IndexStatAutoUpdate': Monitor indexes for changes and trigger
     automatic statistics updates

   * 'IndexStatSaveScale': Scaling factor used in determining size of
     stored index statistics

   * 'IndexStatSaveSize': Maximum size in bytes for saved statistics per
     index

   * 'IndexStatTriggerPct': Threshold percent change in DML operations
     for index statistics updates.  Value is scaled down by
     IndexStatTriggerScale

   * 'IndexStatTriggerScale': Scale down IndexStatTriggerPct by this
     amount, multiplied by base 2 logarithm of index size, for a large
     index.  Set to 0 to disable scaling

   * 'IndexStatUpdateDelay': Minimum delay between automatic index
     statistics updates for a given index.  0 means no delay

   * 'InitFragmentLogFiles': Initialize fragment logfiles (sparse/full)

   * 'InitialLogFileGroup': Describes a log file group that is created
     during an initial start.  See documentation for format

   * 'InitialNoOfOpenFiles': Initial number of files open per data node.
     (One thread is created per file)

   * 'InitialTablespace': Describes a tablespace that is created during
     an initial start.  See documentation for format

   * 'LateAlloc': Allocate memory after connection to management server
     has been established

   * 'LcpScanProgressTimeout': Maximum time that local checkpoint
     fragment scan can be stalled before node is shut down to ensure
     systemwide LCP progress.  Use 0 to disable

   * 'LockExecuteThreadToCPU': A comma-delimited list of CPU IDs

   * 'LockMaintThreadsToCPU': CPU ID indicating which CPU runs
     maintenance threads

   * 'LockPagesInMainMemory': 0=disable locking, 1=lock after memory
     allocation, 2=lock before memory allocation

   * 'LogLevelCheckpoint': Log level of local and global checkpoint
     information printed to stdout

   * 'LogLevelCongestion': Level of congestion information printed to
     stdout

   * 'LogLevelConnection': Level of node connect/disconnect information
     printed to stdout

   * 'LogLevelError': Transporter, heartbeat errors printed to stdout

   * 'LogLevelInfo': Heartbeat and log information printed to stdout

   * 'LogLevelNodeRestart': Level of node restart and node failure
     information printed to stdout

   * 'LogLevelShutdown': Level of node shutdown information printed to
     stdout

   * 'LogLevelStartup': Level of node startup information printed to
     stdout

   * 'LogLevelStatistic': Level of transaction, operation, and
     transporter information printed to stdout

   * 'LongMessageBuffer': Number of bytes allocated on each data node
     for internal long messages

   * 'MaxAllocate': Maximum size of allocation to use when allocating
     memory for tables

   * 'MaxBufferedEpochs': Allowed numbered of epochs that a subscribing
     node can lag behind (unprocessed epochs).  Exceeding will cause
     lagging subscribers to be disconnected

   * 'MaxBufferedEpochBytes': Total number of bytes allocated for
     buffering epochs

   * 'MaxDMLOperationsPerTransaction': Limit size of a transaction;
     aborts transaction if it requires more than this many DML
     operations.  Set to 0 to disable

   * 'MaxLCPStartDelay': Time in seconds that LCP polls for checkpoint
     mutex (to allow other data nodes to complete metadata
     synchronization), before putting itself in lock queue for parallel
     recovery of table data

   * 'MaxNoOfAttributes': Suggests a total number of attributes stored
     in database (sum over all tables)

   * 'MaxNoOfConcurrentIndexOperations': Total number of index
     operations that can execute simultaneously on one data node

   * 'MaxNoOfConcurrentOperations': Maximum number of operation records
     in transaction coordinator

   * 'MaxNoOfConcurrentScans': Maximum number of scans executing
     concurrently on data node

   * 'MaxNoOfConcurrentSubOperations': Maximum number of concurrent
     subscriber operations

   * 'MaxNoOfConcurrentTransactions': Maximum number of transactions
     executing concurrently on this data node, total number of
     transactions that can be executed concurrently is this value times
     number of data nodes in cluster

   * 'MaxNoOfFiredTriggers': Total number of triggers that can fire
     simultaneously on one data node

   * 'MaxNoOfLocalOperations': Maximum number of operation records
     defined on this data node

   * 'MaxNoOfLocalScans': Maximum number of fragment scans in parallel
     on this data node

   * 'MaxNoOfOpenFiles': Maximum number of files open per data node.(One
     thread is created per file)

   * 'MaxNoOfOrderedIndexes': Total number of ordered indexes that can
     be defined in system

   * 'MaxNoOfSavedMessages': Maximum number of error messages to write
     in error log and maximum number of trace files to retain

   * 'MaxNoOfSubscribers': Maximum number of subscribers (default 0 =
     MaxNoOfTables * 2)

   * 'MaxNoOfSubscriptions': Maximum number of subscriptions (default 0
     = MaxNoOfTables)

   * 'MaxNoOfTables': Suggests a total number of NDB tables stored in
     database

   * 'MaxNoOfTriggers': Total number of triggers that can be defined in
     system

   * 'MaxNoOfUniqueHashIndexes': Total number of unique hash indexes
     that can be defined in the system

   * 'MaxParallelScansPerFragment': Maximum number of parallel scans per
     fragment.  Once this limit is reached, scans are serialized

   * 'MaxStartFailRetries': Maximum retries when data node fails on
     startup, requires StopOnError = 0.  Setting to 0 causes start
     attempts to continue indefinitely

   * 'MemReportFrequency': Frequency of memory reports in seconds; 0 =
     report only when exceeding percentage limits

   * 'MinFreePct': Percentage of memory resources to keep in reserve for
     restarts

   * 'NodeGroup': Node group to which data node belongs; used only
     during initial start of cluster

   * 'NodeId': Number uniquely identifying data node among all nodes in
     cluster

   * 'NoOfFragmentLogFiles': Number of 16 MB redo log files in each of 4
     file sets belonging to data node

   * 'NoOfReplicas': Number of copies of all data in database

   * 'Numa': (Linux only; requires libnuma) Controls NUMA support.
     Setting to 0 permits system to determine use of interleaving by
     data node process; 1 means that it is determined by data node

   * 'ODirect': Use O_DIRECT file reads and writes when possible

   * 'RealtimeScheduler': When true, data node threads are scheduled as
     real-time threads.  Default is false

   * 'RedoBuffer': Number of bytes on each data node allocated for
     writing redo logs

   * 'RedoOverCommitCounter': When RedoOverCommitLimit has been exceeded
     this many times, transactions are aborted, and operations are
     handled as specified by DefaultOperationRedoProblemAction

   * 'RedoOverCommitLimit': Each time that flushing current redo buffer
     takes longer than this many seconds, number of times that this has
     happened is compared to RedoOverCommitCounter

   * 'ReservedSendBufferMemory': This parameter is present in NDB code
     but is not enabled, and is now deprecated

   * 'RestartOnErrorInsert': Control type of restart caused by inserting
     an error (when StopOnError is enabled)

   * 'SchedulerExecutionTimer': Number of microseconds to execute in
     scheduler before sending

   * 'SchedulerSpinTimer': Number of microseconds to execute in
     scheduler before sleeping

   * 'ServerPort': Port used to set up transporter for incoming
     connections from API nodes

   * 'SharedGlobalMemory': Total number of bytes on each data node
     allocated for any use

   * 'StartFailRetryDelay': Delay in seconds after start failure prior
     to retry; requires StopOnError = 0

   * 'StartFailureTimeout': Milliseconds to wait before terminating.
     (0=Wait forever)

   * 'StartNoNodeGroupTimeout': Time to wait for nodes without a
     nodegroup before trying to start (0=forever)

   * 'StartPartialTimeout': Milliseconds to wait before trying to start
     without all nodes.  (0=Wait forever)

   * 'StartPartitionedTimeout': Milliseconds to wait before trying to
     start partitioned.  (0=Wait forever)

   * 'StartupStatusReportFrequency': Frequency of status reports during
     startup

   * 'StopOnError': When set to 0, data node automatically restarts and
     recovers following node failures

   * 'StringMemory': Default size of string memory (0 to 100 = % of
     maximum, 101+ = actual bytes)

   * 'TcpBind_INADDR_ANY': Bind IP_ADDR_ANY so that connections can be
     made from anywhere (for autogenerated connections)

   * 'TimeBetweenEpochs': Time between epochs (synchronization used for
     replication)

   * 'TimeBetweenEpochsTimeout': Timeout for time between epochs.
     Exceeding will cause node shutdown

   * 'TimeBetweenGlobalCheckpoints': Time between group commits of
     transactions to disk

   * 'TimeBetweenGlobalCheckpointsTimeout': Minimum timeout for group
     commit of transactions to disk

   * 'TimeBetweenInactiveTransactionAbortCheck': Time between checks for
     inactive transactions

   * 'TimeBetweenLocalCheckpoints': Time between taking snapshots of
     database (expressed in base-2 logarithm of bytes)

   * 'TimeBetweenWatchDogCheck': Time between execution checks inside a
     data node

   * 'TimeBetweenWatchDogCheckInitial': Time between execution checks
     inside a data node (early start phases when memory is allocated)

   * 'TotalSendBufferMemory': Total memory to use for all transporter
     send buffers.

   * 'TransactionBufferMemory': Dynamic buffer space (in bytes) for key
     and attribute data allocated for each data node

   * 'TransactionDeadlockDetectionTimeout': Time transaction can spend
     executing within a data node.  This is time that transaction
     coordinator waits for each data node participating in transaction
     to execute a request.  If data node takes more than this amount of
     time, transaction is aborted.

   * 'TransactionInactiveTimeout': Milliseconds that application waits
     before executing another part of transaction.  This is time
     transaction coordinator waits for application to execute or send
     another part (query, statement) of transaction.  If application
     takes too much time, then transaction is aborted.  Timeout = 0
     means that application never times out

   * 'TwoPassInitialNodeRestartCopy': Copy data in 2 passes during
     initial node restart, which enables multithreaded building of
     ordered indexes for such restarts

   * 'UndoDataBuffer': Number of bytes on each data node allocated for
     writing data undo logs

   * 'UndoIndexBuffer': Number of bytes on each data node allocated for
     writing index undo logs

The following parameters are specific to *note 'ndbmtd':
mysql-cluster-programs-ndbmtd.:

   * 'MaxNoOfExecutionThreads': For ndbmtd only, specify maximum number
     of execution threads

   * 'NoOfFragmentLogParts': Number of redo log file groups belonging to
     this data node; value must be an even multiple of 4

   * 'ThreadConfig': Used for configuration of multithreaded data nodes
     (ndbmtd).  Default is an empty string; see documentation for syntax
     and other information


File: manual.info.tmp,  Node: mysql-cluster-params-mgmd,  Next: mysql-cluster-params-api,  Prev: mysql-cluster-params-ndbd,  Up: mysql-cluster-configuration-overview

18.3.2.2 NDB Cluster Management Node Configuration Parameters
.............................................................

The lsting in this section provides information about parameters used in
the '[ndb_mgmd]' or '[mgm]' section of a 'config.ini' file for
configuring NDB Cluster management nodes.  For detailed descriptions and
other additional information about each of these parameters, see *note
mysql-cluster-mgm-definition::.

   * 'ArbitrationDelay': When asked to arbitrate, arbitrator waits this
     long before voting (milliseconds)

   * 'ArbitrationRank': If 0, then management node is not arbitrator.
     Kernel selects arbitrators in order 1, 2

   * 'DataDir': Data directory for this node

   * 'ExecuteOnComputer': String referencing an earlier defined COMPUTER

   * 'ExtraSendBufferMemory': Memory to use for send buffers in addition
     to any allocated by TotalSendBufferMemory or SendBufferMemory.
     Default (0) allows up to 16MB

   * 'HeartbeatIntervalMgmdMgmd': Time between
     management-node-to-management-node heartbeats; connection between
     management nodes is considered lost after 3 missed heartbeats

   * 'HeartbeatThreadPriority': Set heartbeat thread policy and priority
     for management nodes; see manual for allowed values

   * 'HostName': Host name or IP address for this management node

   * 'Id': Number identifying management node.  Now deprecated; use
     NodeId instead

   * 'LogDestination': Where to send log messages: console, system log,
     or specified log file

   * 'MaxNoOfSavedEvents': Not used

   * 'NodeId': Number uniquely identifying management node among all
     nodes in cluster

   * 'PortNumber': Port number to send commands to and fetch
     configuration from management server

   * 'PortNumberStats': Port number used to get statistical information
     from a management server

   * 'TotalSendBufferMemory': Total memory to use for all transporter
     send buffers

   * 'wan': Use WAN TCP setting as default

*Note*:

After making changes in a management node's configuration, it is
necessary to perform a rolling restart of the cluster for the new
configuration to take effect.  See *note mysql-cluster-mgm-definition::,
for more information.

To add new management servers to a running NDB Cluster, it is also
necessary perform a rolling restart of all cluster nodes after modifying
any existing 'config.ini' files.  For more information about issues
arising when using multiple management nodes, see *note
mysql-cluster-limitations-multiple-nodes::.


File: manual.info.tmp,  Node: mysql-cluster-params-api,  Next: mysql-cluster-params-other,  Prev: mysql-cluster-params-mgmd,  Up: mysql-cluster-configuration-overview

18.3.2.3 NDB Cluster SQL Node and API Node Configuration Parameters
...................................................................

The listing in this section provides information about parameters used
in the '[mysqld]' and '[api]' sections of a 'config.ini' file for
configuring NDB Cluster SQL nodes and API nodes.  For detailed
descriptions and other additional information about each of these
parameters, see *note mysql-cluster-api-definition::.

   * 'ArbitrationDelay': When asked to arbitrate, arbitrator waits this
     many milliseconds before voting

   * 'ArbitrationRank': If 0, then API node is not arbitrator.  Kernel
     selects arbitrators in order 1, 2

   * 'AutoReconnect': Specifies whether an API node should reconnect
     fully when disconnected from cluster

   * 'BatchByteSize': Default batch size in bytes

   * 'BatchSize': Default batch size in number of records

   * 'ConnectBackoffMaxTime': Specifies longest time in milliseconds
     (~100ms resolution) to allow between connection attempts to any
     given data node by this API node.  Excludes time elapsed while
     connection attempts are ongoing, which in worst case can take
     several seconds.  Disable by setting to 0.  If no data nodes are
     currently connected to this API node, StartConnectBackoffMaxTime is
     used instead

   * 'ConnectionMap': Specifies which data nodes to connect

   * 'DefaultHashMapSize': Set size (in buckets) to use for table hash
     maps.  Three values are supported: 0, 240, and 3840.  Intended
     primarily for upgrades and downgrades within NDB 7.2

   * 'DefaultOperationRedoProblemAction': How operations are handled in
     event that RedoOverCommitCounter is exceeded

   * 'ExecuteOnComputer': String referencing an earlier defined COMPUTER

   * 'ExtraSendBufferMemory': Memory to use for send buffers in addition
     to any allocated by TotalSendBufferMemory or SendBufferMemory.
     Default (0) allows up to 16MB

   * 'HeartbeatThreadPriority': Set heartbeat thread policy and priority
     for API nodes; see manual for allowed values

   * 'HostName': Host name or IP address for this SQL or API node

   * 'Id': Number identifying MySQL server or API node (Id).  Now
     deprecated; use NodeId instead

   * 'MaxScanBatchSize': Maximum collective batch size for one scan

   * 'NodeId': Number uniquely identifying SQL node or API node among
     all nodes in cluster

   * 'StartConnectBackoffMaxTime': Same as ConnectBackoffMaxTime except
     that this parameter is used in its place if no data nodes are
     connected to this API node

   * 'TotalSendBufferMemory': Total memory to use for all transporter
     send buffers

   * 'wan': Use WAN TCP setting as default

For a discussion of MySQL server options for NDB Cluster, see *note
mysql-cluster-program-options-mysqld::.  For information about MySQL
server system variables relating to NDB Cluster, see *note
mysql-cluster-system-variables::.

*Note*:

To add new SQL or API nodes to the configuration of a running NDB
Cluster, it is necessary to perform a rolling restart of all cluster
nodes after adding new '[mysqld]' or '[api]' sections to the
'config.ini' file (or files, if you are using more than one management
server).  This must be done before the new SQL or API nodes can connect
to the cluster.

It is _not_ necessary to perform any restart of the cluster if new SQL
or API nodes can employ previously unused API slots in the cluster
configuration to connect to the cluster.


File: manual.info.tmp,  Node: mysql-cluster-params-other,  Next: mysql-cluster-option-tables,  Prev: mysql-cluster-params-api,  Up: mysql-cluster-configuration-overview

18.3.2.4 Other NDB Cluster Configuration Parameters
...................................................

The listings in this section provide information about parameters used
in the '[computer]', '[tcp]', '[shm]', and '[sci]' sections of a
'config.ini' file for configuring NDB Cluster.  For detailed
descriptions and additional information about individual parameters, see
*note mysql-cluster-tcp-definition::, *note
mysql-cluster-shm-definition::, or *note mysql-cluster-sci-definition::,
as appropriate.

The following parameters apply to the 'config.ini' file's '[computer]'
section:

   * 'HostName': Host name or IP address of this computer

   * 'Id': A unique identifier for this computer

The following parameters apply to the 'config.ini' file's '[tcp]'
section:

   * 'Checksum': If checksum is enabled, all signals between nodes are
     checked for errors

   * 'Group': Used for group proximity; smaller value is interpreted as
     being closer

   * 'NodeId1': ID of node (data node, API node, or management node) on
     one side of connection

   * 'NodeId2': ID of node (data node, API node, or management node) on
     one side of connection

   * 'NodeIdServer': Set server side of TCP connection

   * 'OverloadLimit': When more than this many unsent bytes are in send
     buffer, connection is considered overloaded

   * 'PortNumber': Port used for this TCP transporter (DEPRECATED)

   * 'Proxy':

   * 'ReceiveBufferMemory': Bytes of buffer for signals received by this
     node

   * 'SendBufferMemory': Bytes of TCP buffer for signals sent from this
     node

   * 'SendSignalId': Sends ID in each signal.  Used in trace files.
     Defaults to true in debug builds

   * 'TCP_MAXSEG_SIZE': Value used for TCP_MAXSEG

   * 'TCP_RCV_BUF_SIZE': Value used for SO_RCVBUF

   * 'TCP_SND_BUF_SIZE': Value used for SO_SNDBUF

   * 'TcpBind_INADDR_ANY': Bind InAddrAny instead of host name for
     server part of connection

The following parameters apply to the 'config.ini' file's '[shm]'
section:

   * 'Checksum': If checksum is enabled, all signals between nodes are
     checked for errors

   * 'Group': Used for group proximity; smaller value is interpreted as
     being closer

   * 'NodeId1': ID of node (data node, API node, or management node) on
     one side of connection

   * 'NodeId2': ID of node (data node, API node, or management node) on
     one side of connection

   * 'NodeIdServer': Set server side of SHM connection

   * 'OverloadLimit': When more than this many unsent bytes are in send
     buffer, connection is considered overloaded

   * 'PortNumber': Port used for this SHM transporter (DEPRECATED)

   * 'SendSignalId': Sends ID in each signal.  Used in trace files

   * 'ShmKey': A shared memory key; when set to 1, this is calculated by
     NDB

   * 'ShmSize': Size of shared memory segment

   * 'Signum': Signal number to be used for signalling

The following parameters apply to the 'config.ini' file's '[sci]'
section:

   * 'Checksum': If checksum is enabled, all signals between nodes are
     checked for errors

   * 'Group':

   * 'Host1SciId0': SCI-node ID for adapter 0 on Host1 (a computer can
     have two adapters)

   * 'Host1SciId1': SCI-node ID for adapter 1 on Host1 (a computer can
     have two adapters)

   * 'Host2SciId0': SCI-node ID for adapter 0 on Host2 (a computer can
     have two adapters)

   * 'Host2SciId1': SCI-node ID for adapter 1 on Host2 (a computer can
     have two adapters)

   * 'NodeId1': ID of node (data node, API node, or management node) on
     one side of connection

   * 'NodeId2': ID of node (data node, API node, or management node) on
     one side of connection

   * 'NodeIdServer': Set server side of SCI connection

   * 'OverloadLimit': When more than this many unsent bytes are in send
     buffer, connection is considered overloaded

   * 'PortNumber': Port used for this SCI transporter (DEPRECATED)

   * 'SendLimit': Transporter send buffer contents are sent when this
     number of bytes is buffered

   * 'SendSignalId': Sends ID in each signal.  Used in trace files

   * 'SharedBufferSize': Size of shared memory segment


File: manual.info.tmp,  Node: mysql-cluster-option-tables,  Prev: mysql-cluster-params-other,  Up: mysql-cluster-configuration-overview

18.3.2.5 NDB Cluster mysqld Option and Variable Reference
.........................................................

The following table provides a list of the command-line options, server
and status variables applicable within 'mysqld' when it is running as an
SQL node in an NDB Cluster.  For a table showing _all_ command-line
options, server and status variables available for use with *note
'mysqld': mysqld, see *note server-option-variable-reference::.

   * 'Com_show_ndb_status': Count of SHOW NDB STATUS statements

   * 'Handler_discover': Number of times that tables have been
     discovered

   * 'have_ndbcluster': Whether mysqld supports NDB Cluster tables (set
     by -ndbcluster option)

   * 'ndb-batch-size': Size (in bytes) to use for NDB transaction
     batches

   * 'ndb-blob-read-batch-bytes': Specifies size in bytes that large
     BLOB reads should be batched into.  0 = no limit.

   * 'ndb-blob-write-batch-bytes': Specifies size in bytes that large
     BLOB writes should be batched into.  0 = no limit.

   * 'ndb-cluster-connection-pool': Number of connections to the cluster
     used by MySQL

   * 'ndb-connectstring': Point to the management server that
     distributes the cluster configuration

   * 'ndb-deferred-constraints': Specifies that constraint checks on
     unique indexes (where these are supported) should be deferred until
     commit time.  Not normally needed or used; for testing purposes
     only.

   * 'ndb-distribution': Default distribution for new tables in
     NDBCLUSTER (KEYHASH or LINHASH, default is KEYHASH)

   * 'ndb-log-apply-status': Cause a MySQL server acting as a slave to
     log mysql.ndb_apply_status updates received from its immediate
     master in its own binary log, using its own server ID. Effective
     only if the server is started with the -ndbcluster option.

   * 'ndb-log-empty-epochs': When enabled, causes epochs in which there
     were no changes to be written to the ndb_apply_status and
     ndb_binlog_index tables, even when 'log_slave_updates' is enabled.

   * 'ndb-log-empty-update': When enabled, causes updates that produced
     no changes to be written to the ndb_apply_status and
     ndb_binlog_index tables, even when 'log_slave_updates' is enabled.

   * 'ndb-log-orig': Log originating server id and epoch in
     mysql.ndb_binlog_index table

   * 'ndb-log-transaction-id': Write NDB transaction IDs in the binary
     log.  Requires -log-bin-v1-events=OFF.

   * 'ndb-log-update-as-write': Toggles logging of updates on the master
     between updates (OFF) and writes (ON)

   * 'ndb-mgmd-host': Set the host (and port, if desired) for connecting
     to management server

   * 'ndb-nodeid': NDB Cluster node ID for this MySQL server

   * 'ndb-transid-mysql-connection-map': Enable or disable the
     ndb_transid_mysql_connection_map plugin; that is, enable or disable
     the INFORMATION_SCHEMA table having that name

   * 'ndb-wait-connected': Time (in seconds) for the MySQL server to
     wait for connection to cluster management and data nodes before
     accepting MySQL client connections

   * 'ndb-wait-setup': Time (in seconds) for the MySQL server to wait
     for NDB engine setup to complete

   * 'Ndb_api_bytes_received_count': Amount of data (in bytes) received
     from the data nodes by this MySQL Server (SQL node)

   * 'Ndb_api_bytes_received_count_session': Amount of data (in bytes)
     received from the data nodes in this client session

   * 'Ndb_api_bytes_received_count_slave': Amount of data (in bytes)
     received from the data nodes by this slave

   * 'Ndb_api_bytes_sent_count': Amount of data (in bytes) sent to the
     data nodes by this MySQL Server (SQL node)

   * 'Ndb_api_bytes_sent_count_session': Amount of data (in bytes) sent
     to the data nodes in this client session

   * 'Ndb_api_bytes_sent_count_slave': Amount of data (in bytes) sent to
     the data nodes by this slave

   * 'Ndb_api_event_bytes_count': Number of bytes of events received by
     this MySQL Server (SQL node)

   * 'Ndb_api_event_bytes_count_injector': Number of bytes of events
     received by the NDB binary log injector thread

   * 'Ndb_api_event_data_count': Number of row change events received by
     this MySQL Server (SQL node)

   * 'Ndb_api_event_data_count_injector': Number of row change events
     received by the NDB binary log injector thread

   * 'Ndb_api_event_nondata_count': Number of events received, other
     than row change events, by this MySQL Server (SQL node)

   * 'Ndb_api_event_nondata_count_injector': Number of events received,
     other than row change events, by the NDB binary log injector thread

   * 'Ndb_api_pk_op_count': Number of operations based on or using
     primary keys by this MySQL Server (SQL node)

   * 'Ndb_api_pk_op_count_session': Number of operations based on or
     using primary keys in this client session

   * 'Ndb_api_pk_op_count_slave': Number of operations based on or using
     primary keys by this slave

   * 'Ndb_api_pruned_scan_count': Number of scans that have been pruned
     to a single partition by this MySQL Server (SQL node)

   * 'Ndb_api_pruned_scan_count_session': Number of scans that have been
     pruned to a single partition in this client session

   * 'Ndb_api_pruned_scan_count_slave': Number of scans that have been
     pruned to a single partition by this slave

   * 'Ndb_api_range_scan_count': Number of range scans that have been
     started by this MySQL Server (SQL node)

   * 'Ndb_api_range_scan_count_session': Number of range scans that have
     been started in this client session

   * 'Ndb_api_range_scan_count_slave': Number of range scans that have
     been started by this slave

   * 'Ndb_api_read_row_count': Total number of rows that have been read
     by this MySQL Server (SQL node)

   * 'Ndb_api_read_row_count_session': Total number of rows that have
     been read in this client session

   * 'Ndb_api_read_row_count_slave': Total number of rows that have been
     read by this slave

   * 'Ndb_api_scan_batch_count': Number of batches of rows received by
     this MySQL Server (SQL node)

   * 'Ndb_api_scan_batch_count_session': Number of batches of rows
     received in this client session

   * 'Ndb_api_scan_batch_count_slave': Number of batches of rows
     received by this slave

   * 'Ndb_api_table_scan_count': Number of table scans that have been
     started, including scans of internal tables, by this MySQL Server
     (SQL node)

   * 'Ndb_api_table_scan_count_session': Number of table scans that have
     been started, including scans of internal tables, in this client
     session

   * 'Ndb_api_table_scan_count_slave': Number of table scans that have
     been started, including scans of internal tables, by this slave

   * 'Ndb_api_trans_abort_count': Number of transactions aborted by this
     MySQL Server (SQL node)

   * 'Ndb_api_trans_abort_count_session': Number of transactions aborted
     in this client session

   * 'Ndb_api_trans_abort_count_slave': Number of transactions aborted
     by this slave

   * 'Ndb_api_trans_close_count': Number of transactions aborted (may be
     greater than the sum of TransCommitCount and TransAbortCount) by
     this MySQL Server (SQL node)

   * 'Ndb_api_trans_close_count_session': Number of transactions aborted
     (may be greater than the sum of TransCommitCount and
     TransAbortCount) in this client session

   * 'Ndb_api_trans_close_count_slave': Number of transactions aborted
     (may be greater than the sum of TransCommitCount and
     TransAbortCount) by this slave

   * 'Ndb_api_trans_commit_count': Number of transactions committed by
     this MySQL Server (SQL node)

   * 'Ndb_api_trans_commit_count_session': Number of transactions
     committed in this client session

   * 'Ndb_api_trans_commit_count_slave': Number of transactions
     committed by this slave

   * 'Ndb_api_trans_local_read_row_count': Total number of rows that
     have been read by this MySQL Server (SQL node)

   * 'Ndb_api_trans_local_read_row_count_session': Total number of rows
     that have been read in this client session

   * 'Ndb_api_trans_local_read_row_count_slave': Total number of rows
     that have been read by this slave

   * 'Ndb_api_trans_start_count': Number of transactions started by this
     MySQL Server (SQL node)

   * 'Ndb_api_trans_start_count_session': Number of transactions started
     in this client session

   * 'Ndb_api_trans_start_count_slave': Number of transactions started
     by this slave

   * 'Ndb_api_uk_op_count': Number of operations based on or using
     unique keys by this MySQL Server (SQL node)

   * 'Ndb_api_uk_op_count_session': Number of operations based on or
     using unique keys in this client session

   * 'Ndb_api_uk_op_count_slave': Number of operations based on or using
     unique keys by this slave

   * 'Ndb_api_wait_exec_complete_count': Number of times thread has been
     blocked while waiting for execution of an operation to complete by
     this MySQL Server (SQL node)

   * 'Ndb_api_wait_exec_complete_count_session': Number of times thread
     has been blocked while waiting for execution of an operation to
     complete in this client session

   * 'Ndb_api_wait_exec_complete_count_slave': Number of times thread
     has been blocked while waiting for execution of an operation to
     complete by this slave

   * 'Ndb_api_wait_meta_request_count': Number of times thread has been
     blocked waiting for a metadata-based signal by this MySQL Server
     (SQL node)

   * 'Ndb_api_wait_meta_request_count_session': Number of times thread
     has been blocked waiting for a metadata-based signal in this client
     session

   * 'Ndb_api_wait_meta_request_count_slave': Number of times thread has
     been blocked waiting for a metadata-based signal by this slave

   * 'Ndb_api_wait_nanos_count': Total time (in nanoseconds) spent
     waiting for some type of signal from the data nodes by this MySQL
     Server (SQL node)

   * 'Ndb_api_wait_nanos_count_session': Total time (in nanoseconds)
     spent waiting for some type of signal from the data nodes in this
     client session

   * 'Ndb_api_wait_nanos_count_slave': Total time (in nanoseconds) spent
     waiting for some type of signal from the data nodes by this slave

   * 'Ndb_api_wait_scan_result_count': Number of times thread has been
     blocked while waiting for a scan-based signal by this MySQL Server
     (SQL node)

   * 'Ndb_api_wait_scan_result_count_session': Number of times thread
     has been blocked while waiting for a scan-based signal in this
     client session

   * 'Ndb_api_wait_scan_result_count_slave': Number of times thread has
     been blocked while waiting for a scan-based signal by this slave

   * 'ndb_autoincrement_prefetch_sz': NDB auto-increment prefetch size

   * 'ndb_cache_check_time': Number of milliseconds between checks of
     cluster SQL nodes made by the MySQL query cache

   * 'Ndb_cluster_node_id': If the server is acting as an NDB Cluster
     node, then the value of this variable its node ID in the cluster

   * 'Ndb_config_from_host': The host name or IP address of the Cluster
     management server Formerly Ndb_connected_host

   * 'Ndb_config_from_port': The port for connecting to Cluster
     management server.  Formerly Ndb_connected_port

   * 'Ndb_conflict_fn_epoch': Number of rows that have been found in
     conflict by the NDB$EPOCH() conflict detection function

   * 'Ndb_conflict_fn_epoch_trans': Number of rows that have been found
     in conflict by the NDB$EPOCH_TRANS() conflict detection function

   * 'Ndb_conflict_fn_max': If the server is part of an NDB Cluster
     involved in cluster replication, the value of this variable
     indicates the number of times that conflict resolution based on
     "greater timestamp wins" has been applied

   * 'Ndb_conflict_fn_old': If the server is part of an NDB Cluster
     involved in cluster replication, the value of this variable
     indicates the number of times that "same timestamp wins" conflict
     resolution has been applied

   * 'Ndb_conflict_trans_conflict_commit_count': Number of epoch
     transactions committed after requiring transactional conflict
     handling

   * 'Ndb_conflict_trans_detect_iter_count': Number of internal
     iterations required to commit an epoch transaction.  Should be
     (slightly) greater than or equal to
     Ndb_conflict_trans_conflict_commit_count

   * 'Ndb_conflict_trans_reject_count': Number of transactions rejected
     after being found in conflict by a transactional conflict function

   * 'Ndb_conflict_trans_row_reject_count': Total number of rows
     realigned after being found in conflict by a transactional conflict
     function.  Includes Ndb_conflict_trans_row_conflict_count and any
     rows included in or dependent on conflicting transactions.

   * 'ndb_deferred_constraints': Specifies that constraint checks should
     be deferred (where these are supported).  Not normally needed or
     used; for testing purposes only.

   * 'ndb_distribution': Default distribution for new tables in
     NDBCLUSTER (KEYHASH or LINHASH, default is KEYHASH)

   * 'ndb_eventbuffer_max_alloc': Maximum memory that can be allocated
     for buffering events by the NDB API. Defaults to 0 (no limit).

   * 'Ndb_execute_count': Provides the number of round trips to the NDB
     kernel made by operations

   * 'ndb_extra_logging': Controls logging of NDB Cluster schema,
     connection, and data distribution events in the MySQL error log

   * 'ndb_force_send': Forces sending of buffers to NDB immediately,
     without waiting for other threads

   * 'ndb_index_stat_cache_entries': Sets the granularity of the
     statistics by determining the number of starting and ending keys

   * 'ndb_index_stat_enable': Use NDB index statistics in query
     optimization

   * 'ndb_index_stat_option': Comma-separated list of tunable options
     for NDB index statistics; the list should contain no spaces

   * 'ndb_index_stat_update_freq': How often to query data nodes instead
     of the statistics cache

   * 'ndb_join_pushdown': Enables pushing down of joins to data nodes

   * 'ndb_log_apply_status': Whether or not a MySQL server acting as a
     slave logs mysql.ndb_apply_status updates received from its
     immediate master in its own binary log, using its own server ID

   * 'ndb_log_bin': Write updates to NDB tables in the binary log.
     Effective only if binary logging is enabled with -log-bin.

   * 'ndb_log_binlog_index': Insert mapping between epochs and binary
     log positions into the ndb_binlog_index table.  Defaults to ON.
     Effective only if binary logging is enabled on the server.

   * 'ndb_log_empty_epochs': When enabled, epochs in which there were no
     changes are written to the ndb_apply_status and ndb_binlog_index
     tables, even when 'log_slave_updates' is enabled.

   * 'ndb_log_empty_update': When enabled, updates which produce no
     changes are written to the ndb_apply_status and ndb_binlog_index
     tables, even when 'log_slave_updates' is enabled.

   * 'ndb_log_orig': Whether the id and epoch of the originating server
     are recorded in the mysql.ndb_binlog_index table.  Set using the
     -ndb-log-orig option when starting mysqld.

   * 'ndb_log_transaction_id': Whether NDB transaction IDs are written
     into the binary log (Read-only.)

   * 'ndb_log_updated_only': Log complete rows (ON) or updates only
     (OFF)

   * 'Ndb_number_of_data_nodes': If the server is part of an NDB
     Cluster, the value of this variable is the number of data nodes in
     the cluster

   * 'ndb-optimization-delay': Sets the number of milliseconds to wait
     between processing sets of rows by OPTIMIZE TABLE on NDB tables

   * 'ndb_optimized_node_selection': Determines how an SQL node chooses
     a cluster data node to use as transaction coordinator

   * 'Ndb_pruned_scan_count': Number of scans executed by NDB since the
     cluster was last started where partition pruning could be used

   * 'Ndb_pushed_queries_defined': Number of joins that API nodes have
     attempted to push down to the data nodes

   * 'Ndb_pushed_queries_dropped': Number of joins that API nodes have
     tried to push down, but failed

   * 'Ndb_pushed_queries_executed': Number of joins successfully pushed
     down and executed on the data nodes

   * 'Ndb_pushed_reads': Number of reads executed on the data nodes by
     pushed-down joins

   * 'ndb_report_thresh_binlog_epoch_slip': NDB 7.5.4 and later:
     Threshold for number of epochs completely buffered, but not yet
     consumed by binlog injector thread which when exceeded generates
     BUFFERED_EPOCHS_OVER_THRESHOLD event buffer status message; prior
     to NDB 7.5.4: Threshold for number of epochs to lag behind before
     reporting binary log status

   * 'ndb_report_thresh_binlog_mem_usage': This is a threshold on the
     percentage of free memory remaining before reporting binary log
     status

   * 'Ndb_scan_count': The total number of scans executed by NDB since
     the cluster was last started

   * 'ndb_table_no_logging': NDB tables created when this setting is
     enabled are not checkpointed to disk (although table schema files
     are created).  The setting in effect when the table is created with
     or altered to use NDBCLUSTER persists for the lifetime of the
     table.

   * 'ndb_table_temporary': NDB tables are not persistent on disk: no
     schema files are created and the tables are not logged

   * 'ndb_use_exact_count': Use exact row count when planning queries

   * 'ndb_use_transactions': Forces NDB to use a count of records during
     SELECT COUNT(*) query planning to speed up this type of query

   * 'ndb_version': Shows build and NDB engine version as an integer

   * 'ndb_version_string': Shows build information including NDB engine
     version in ndb-x.y.z format

   * 'ndbcluster': Enable NDB Cluster (if this version of MySQL supports
     it) Disabled by '--skip-ndbcluster'

   * 'ndbinfo_database': The name used for the NDB information database;
     read only

   * 'ndbinfo_max_bytes': Used for debugging only

   * 'ndbinfo_max_rows': Used for debugging only

   * 'ndbinfo_offline': Put the ndbinfo database into offline mode, in
     which no rows are returned from tables or views

   * 'ndbinfo_show_hidden': Whether to show ndbinfo internal base tables
     in the mysql client.  The default is OFF.

   * 'ndbinfo_table_prefix': The prefix to use for naming ndbinfo
     internal base tables

   * 'ndbinfo_version': The version of the ndbinfo engine; read only

   * 'server-id-bits': Sets the number of least significant bits in the
     server_id actually used for identifying the server, permitting NDB
     API applications to store application data in the most significant
     bits.  server_id must be less than 2 to the power of this value.

   * 'slave_allow_batching': Turns update batching on and off for a
     replication slave

   * 'transaction_allow_batching': Allows batching of statements within
     a transaction.  Disable AUTOCOMMIT to use.


File: manual.info.tmp,  Node: mysql-cluster-config-file,  Next: mysql-cluster-interconnects,  Prev: mysql-cluster-configuration-overview,  Up: mysql-cluster-configuration

18.3.3 NDB Cluster Configuration Files
--------------------------------------

* Menu:

* mysql-cluster-config-example::  NDB Cluster Configuration: Basic Example
* mysql-cluster-config-starting::  Recommended Starting Configuration for NDB Cluster
* mysql-cluster-connection-strings::  NDB Cluster Connection Strings
* mysql-cluster-computer-definition::  Defining Computers in an NDB Cluster
* mysql-cluster-mgm-definition::  Defining an NDB Cluster Management Server
* mysql-cluster-ndbd-definition::  Defining NDB Cluster Data Nodes
* mysql-cluster-api-definition::  Defining SQL and Other API Nodes in an NDB Cluster
* mysql-cluster-options-variables::  MySQL Server Options and Variables for NDB Cluster
* mysql-cluster-tcp-definition::  NDB Cluster TCP/IP Connections
* mysql-cluster-tcp-definition-direct::  NDB Cluster TCP/IP Connections Using Direct Connections
* mysql-cluster-shm-definition::  NDB Cluster Shared-Memory Connections
* mysql-cluster-sci-definition::  SCI Transport Connections in NDB Cluster
* mysql-cluster-config-send-buffers::  Configuring NDB Cluster Send Buffer Parameters

Configuring NDB Cluster requires working with two files:

   * 'my.cnf': Specifies options for all NDB Cluster executables.  This
     file, with which you should be familiar with from previous work
     with MySQL, must be accessible by each executable running in the
     cluster.

   * 'config.ini': This file, sometimes known as the _global
     configuration file_, is read only by the NDB Cluster management
     server, which then distributes the information contained therein to
     all processes participating in the cluster.  'config.ini' contains
     a description of each node involved in the cluster.  This includes
     configuration parameters for data nodes and configuration
     parameters for connections between all nodes in the cluster.  For a
     quick reference to the sections that can appear in this file, and
     what sorts of configuration parameters may be placed in each
     section, see Sections of the 'config.ini' File.

Caching of configuration data

In NDB Cluster 7.2, NDB Cluster uses _stateful configuration_.  Rather
than reading the global configuration file every time the management
server is restarted, the management server caches the configuration the
first time it is started, and thereafter, the global configuration file
is read only when one of the following conditions is true:

   * The management server is started using the -initial option

     When '--initial' is used, the global configuration file is re-read,
     any existing cache files are deleted, and the management server
     creates a new configuration cache.

   * The management server is started using the -reload option

     The '--reload' option causes the management server to compare its
     cache with the global configuration file.  If they differ, the
     management server creates a new configuration cache; any existing
     configuration cache is preserved, but not used.  If the management
     server's cache and the global configuration file contain the same
     configuration data, then the existing cache is used, and no new
     cache is created.

   * The management server is started using -config-cache=FALSE

     This disables '--config-cache' (enabled by default), and can be
     used to force the management server to bypass configuration caching
     altogether.  In this case, the management server ignores any
     configuration files that may be present, always reading its
     configuration data from the 'config.ini' file instead.

   * No configuration cache is found

     In this case, the management server reads the global configuration
     file and creates a cache containing the same configuration data as
     found in the file.

Configuration cache files

The management server by default creates configuration cache files in a
directory named 'mysql-cluster' in the MySQL installation directory.
(If you build NDB Cluster from source on a Unix system, the default
location is '/usr/local/mysql-cluster'.)  This can be overridden at
runtime by starting the management server with the '--configdir' option.
Configuration cache files are binary files named according to the
pattern 'ndb_NODE_ID_config.bin.SEQ_ID', where NODE_ID is the management
server's node ID in the cluster, and SEQ_ID is a cache idenitifer.
Cache files are numbered sequentially using SEQ_ID, in the order in
which they are created.  The management server uses the latest cache
file as determined by the SEQ_ID.

*Note*:

It is possible to roll back to a previous configuration by deleting
later configuration cache files, or by renaming an earlier cache file so
that it has a higher SEQ_ID.  However, since configuration cache files
are written in a binary format, you should not attempt to edit their
contents by hand.

For more information about the '--configdir', '--config-cache',
'--initial', and '--reload' options for the NDB Cluster management
server, see *note mysql-cluster-programs-ndb-mgmd::.

We are continuously making improvements in Cluster configuration and
attempting to simplify this process.  Although we strive to maintain
backward compatibility, there may be times when introduce an
incompatible change.  In such cases we will try to let Cluster users
know in advance if a change is not backward compatible.  If you find
such a change and we have not documented it, please report it in the
MySQL bugs database using the instructions given in *note bug-reports::.


File: manual.info.tmp,  Node: mysql-cluster-config-example,  Next: mysql-cluster-config-starting,  Prev: mysql-cluster-config-file,  Up: mysql-cluster-config-file

18.3.3.1 NDB Cluster Configuration: Basic Example
.................................................

To support NDB Cluster, you will need to update 'my.cnf' as shown in the
following example.  You may also specify these parameters on the command
line when invoking the executables.

*Note*:

The options shown here should not be confused with those that are used
in 'config.ini' global configuration files.  Global configuration
options are discussed later in this section.

     # my.cnf
     # example additions to my.cnf for NDB Cluster
     # (valid in MySQL 5.5)

     # enable ndbcluster storage engine, and provide connection string for
     # management server host (default port is 1186)
     [mysqld]
     ndbcluster
     ndb-connectstring=ndb_mgmd.mysql.com

     # provide connection string for management server host (default port: 1186)
     [ndbd]
     connect-string=ndb_mgmd.mysql.com

     # provide connection string for management server host (default port: 1186)
     [ndb_mgm]
     connect-string=ndb_mgmd.mysql.com

     # provide location of cluster configuration file
     [ndb_mgmd]
     config-file=/etc/config.ini

(For more information on connection strings, see *note
mysql-cluster-connection-strings::.)

     # my.cnf
     # example additions to my.cnf for NDB Cluster
     # (will work on all versions)

     # enable ndbcluster storage engine, and provide connection string for management
     # server host to the default port 1186
     [mysqld]
     ndbcluster
     ndb-connectstring=ndb_mgmd.mysql.com:1186

*Important*:

Once you have started a *note 'mysqld': mysqld. process with the *note
'NDBCLUSTER': mysql-cluster. and 'ndb-connectstring' parameters in the
'[mysqld]' in the 'my.cnf' file as shown previously, you cannot execute
any *note 'CREATE TABLE': create-table. or *note 'ALTER TABLE':
alter-table. statements without having actually started the cluster.
Otherwise, these statements will fail with an error.  _This is by
design_.

You may also use a separate '[mysql_cluster]' section in the cluster
'my.cnf' file for settings to be read and used by all executables:

     # cluster-specific settings
     [mysql_cluster]
     ndb-connectstring=ndb_mgmd.mysql.com:1186

For additional *note 'NDB': mysql-cluster. variables that can be set in
the 'my.cnf' file, see *note mysql-cluster-system-variables::.

The NDB Cluster global configuration file is by convention named
'config.ini' (but this is not required).  If needed, it is read by *note
'ndb_mgmd': mysql-cluster-programs-ndb-mgmd. at startup and can be
placed in any location that can be read by it.  The location and name of
the configuration are specified using '--config-file=PATH_NAME' with
*note 'ndb_mgmd': mysql-cluster-programs-ndb-mgmd. on the command line.
This option has no default value, and is ignored if *note 'ndb_mgmd':
mysql-cluster-programs-ndb-mgmd. uses the configuration cache.

The global configuration file for NDB Cluster uses INI format, which
consists of sections preceded by section headings (surrounded by square
brackets), followed by the appropriate parameter names and values.  One
deviation from the standard INI format is that the parameter name and
value can be separated by a colon (':') as well as the equal sign ('=');
however, the equal sign is preferred.  Another deviation is that
sections are not uniquely identified by section name.  Instead, unique
sections (such as two different nodes of the same type) are identified
by a unique ID specified as a parameter within the section.

Default values are defined for most parameters, and can also be
specified in 'config.ini'.  To create a default value section, simply
add the word 'default' to the section name.  For example, an '[ndbd]'
section contains parameters that apply to a particular data node,
whereas an '[ndbd default]' section contains parameters that apply to
all data nodes.  Suppose that all data nodes should use the same data
memory size.  To configure them all, create an '[ndbd default]' section
that contains a 'DataMemory' line to specify the data memory size.

If used, the '[ndbd default]' section must precede any '[ndbd]' sections
in the configuration file.  This is also true for 'default' sections of
any other type.

*Note*:

In some older releases of NDB Cluster, there was no default value for
'NoOfReplicas', which always had to be specified explicitly in the
'[ndbd default]' section.  Although this parameter now has a default
value of 2, which is the recommended setting in most common usage
scenarios, it is still recommended practice to set this parameter
explicitly.

The global configuration file must define the computers and nodes
involved in the cluster and on which computers these nodes are located.
An example of a simple configuration file for a cluster consisting of
one management server, two data nodes and two MySQL servers is shown
here:

     # file "config.ini" - 2 data nodes and 2 SQL nodes
     # This file is placed in the startup directory of ndb_mgmd (the
     # management server)
     # The first MySQL Server can be started from any host. The second
     # can be started only on the host mysqld_5.mysql.com

     [ndbd default]
     NoOfReplicas= 2
     DataDir= /var/lib/mysql-cluster

     [ndb_mgmd]
     Hostname= ndb_mgmd.mysql.com
     DataDir= /var/lib/mysql-cluster

     [ndbd]
     HostName= ndbd_2.mysql.com

     [ndbd]
     HostName= ndbd_3.mysql.com

     [mysqld]
     [mysqld]
     HostName= mysqld_5.mysql.com

*Note*:

The preceding example is intended as a minimal starting configuration
for purposes of familiarization with NDB Cluster, and is almost certain
not to be sufficient for production settings.  See *note
mysql-cluster-config-starting::, which provides a more complete example
starting configuration.

Each node has its own section in the 'config.ini' file.  For example,
this cluster has two data nodes, so the preceding configuration file
contains two '[ndbd]' sections defining these nodes.

*Note*:

Do not place comments on the same line as a section heading in the
'config.ini' file; this causes the management server not to start
because it cannot parse the configuration file in such cases.

*Sections of the config.ini File*

There are six different sections that you can use in the 'config.ini'
configuration file, as described in the following list:

   * '[computer]': Defines cluster hosts.  This is not required to
     configure a viable NDB Cluster, but be may used as a convenience
     when setting up a large cluster.  See *note
     mysql-cluster-computer-definition::, for more information.

   * '[ndbd]': Defines a cluster data node (*note 'ndbd':
     mysql-cluster-programs-ndbd. process).  See *note
     mysql-cluster-ndbd-definition::, for details.

   * '[mysqld]': Defines the cluster's MySQL server nodes (also called
     SQL or API nodes).  For a discussion of SQL node configuration, see
     *note mysql-cluster-api-definition::.

   * '[mgm]' or '[ndb_mgmd]': Defines a cluster management server (MGM)
     node.  For information concerning the configuration of management
     nodes, see *note mysql-cluster-mgm-definition::.

   * '[tcp]': Defines a TCP/IP connection between cluster nodes, with
     TCP/IP being the default connection protocol.  Normally, '[tcp]' or
     '[tcp default]' sections are not required to set up an NDB Cluster,
     as the cluster handles this automatically; however, it may be
     necessary in some situations to override the defaults provided by
     the cluster.  See *note mysql-cluster-tcp-definition::, for
     information about available TCP/IP configuration parameters and how
     to use them.  (You may also find *note
     mysql-cluster-tcp-definition-direct:: to be of interest in some
     cases.)

   * '[shm]': Defines shared-memory connections between nodes.  In MySQL
     5.5, it is enabled by default, but should still be considered
     experimental.  For a discussion of SHM interconnects, see *note
     mysql-cluster-shm-definition::.

   * '[sci]': Defines Scalable Coherent Interface connections between
     cluster data nodes.  Not supported in NDB 7.2 or later.

You can define 'default' values for each section.  If used, a 'default'
section should come before any other sections of that type.  For
example, an '[ndbd default]' section should appear in the configuration
file before any '[ndbd]' sections.

NDB Cluster parameter names are case-insensitive, unless specified in
MySQL Server 'my.cnf' or 'my.ini' files.


File: manual.info.tmp,  Node: mysql-cluster-config-starting,  Next: mysql-cluster-connection-strings,  Prev: mysql-cluster-config-example,  Up: mysql-cluster-config-file

18.3.3.2 Recommended Starting Configuration for NDB Cluster
...........................................................

Achieving the best performance from an NDB Cluster depends on a number
of factors including the following:

   * NDB Cluster software version

   * Numbers of data nodes and SQL nodes

   * Hardware

   * Operating system

   * Amount of data to be stored

   * Size and type of load under which the cluster is to operate

Therefore, obtaining an optimum configuration is likely to be an
iterative process, the outcome of which can vary widely with the
specifics of each NDB Cluster deployment.  Changes in configuration are
also likely to be indicated when changes are made in the platform on
which the cluster is run, or in applications that use the NDB Cluster's
data.  For these reasons, it is not possible to offer a single
configuration that is ideal for all usage scenarios.  However, in this
section, we provide a recommended base configuration.

Starting config.ini file

The following 'config.ini' file is a recommended starting point for
configuring a cluster running NDB Cluster 7.2:

     # TCP PARAMETERS

     [tcp default]SendBufferMemory=2M
     ReceiveBufferMemory=2M

     # Increasing the sizes of these 2 buffers beyond the default values
     # helps prevent bottlenecks due to slow disk I/O.

     # MANAGEMENT NODE PARAMETERS

     [ndb_mgmd default]
     DataDir=PATH/TO/MANAGEMENT/SERVER/DATA/DIRECTORY

     # It is possible to use a different data directory for each management
     # server, but for ease of administration it is preferable to be
     # consistent.

     [ndb_mgmd]
     HostName=MANAGEMENT-SERVER-A-HOSTNAME
     # NodeId=MANAGEMENT-SERVER-A-NODEID

     [ndb_mgmd]
     HostName=MANAGEMENT-SERVER-B-HOSTNAME
     # NodeId=MANAGEMENT-SERVER-B-NODEID

     # Using 2 management servers helps guarantee that there is always an
     # arbitrator in the event of network partitioning, and so is
     # recommended for high availability. Each management server must be
     # identified by a HostName. You may for the sake of convenience specify
     # a NodeId for any management server, although one will be allocated
     # for it automatically; if you do so, it must be in the range 1-255
     # inclusive and must be unique among all IDs specified for cluster
     # nodes.

     # DATA NODE PARAMETERS

     [ndbd default]
     NoOfReplicas=2

     # Using 2 replicas is recommended to guarantee availability of data;
     # using only 1 replica does not provide any redundancy, which means
     # that the failure of a single data node causes the entire cluster to
     # shut down. We do not recommend using more than 2 replicas, since 2 is
     # sufficient to provide high availability, and we do not currently test
     # with greater values for this parameter.

     LockPagesInMainMemory=1

     # On Linux and Solaris systems, setting this parameter locks data node
     # processes into memory. Doing so prevents them from swapping to disk,
     # which can severely degrade cluster performance.

     DataMemory=3072M
     IndexMemory=384M

     # The values provided for DataMemory and IndexMemory assume 4 GB RAM
     # per data node. However, for best results, you should first calculate
     # the memory that would be used based on the data you actually plan to
     # store (you may find the *note ndb_size.pl: mysql-cluster-programs-ndb-size-pl. utility helpful in estimating
     # this), then allow an extra 20% over the calculated values. Naturally,
     # you should ensure that each data node host has at least as much
     # physical memory as the sum of these two values.

     # ODirect=1

     # Enabling this parameter causes NDBCLUSTER to try using O_DIRECT
     # writes for local checkpoints and redo logs; this can reduce load on
     # CPUs. We recommend doing so when using NDB Cluster on systems running
     # Linux kernel 2.6 or later.

     NoOfFragmentLogFiles=300
     DataDir=PATH/TO/DATA/NODE/DATA/DIRECTORY
     MaxNoOfConcurrentOperations=100000

     SchedulerSpinTimer=400
     SchedulerExecutionTimer=100
     RealTimeScheduler=1
     # Setting these parameters allows you to take advantage of real-time scheduling
     # of NDB threads to achieve increased throughput when using *note ndbd: mysql-cluster-programs-ndbd. They
     # are not needed when using *note ndbmtd: mysql-cluster-programs-ndbmtd.; in particular, you should not set
     # RealTimeScheduler for *note ndbmtd: mysql-cluster-programs-ndbmtd. data nodes.

     TimeBetweenGlobalCheckpoints=1000
     TimeBetweenEpochs=200
     DiskCheckpointSpeed=10M
     DiskCheckpointSpeedInRestart=100M
     RedoBuffer=32M

     # CompressedLCP=1
     # CompressedBackup=1
     # Enabling CompressedLCP and CompressedBackup causes, respectively, local
     checkpoint files and backup files to be compressed, which can result in a space
     savings of up to 50% over noncompressed LCPs and backups.

     # MaxNoOfLocalScans=64
     MaxNoOfTables=1024
     MaxNoOfOrderedIndexes=256

     [ndbd]
     HostName=DATA-NODE-A-HOSTNAME
     # NodeId=DATA-NODE-A-NODEID

     LockExecuteThreadToCPU=1
     LockMaintThreadsToCPU=0
     # On systems with multiple CPUs, these parameters can be used to lock NDBCLUSTER
     # threads to specific CPUs

     [ndbd]
     HostName=DATA-NODE-B-HOSTNAME
     # NodeId=DATA-NODE-B-NODEID

     LockExecuteThreadToCPU=1
     LockMaintThreadsToCPU=0

     # You must have an [ndbd] section for every data node in the cluster;
     # each of these sections must include a HostName. Each section may
     # optionally include a NodeId for convenience, but in most cases, it is
     # sufficient to allow the cluster to allocate node IDs dynamically. If
     # you do specify the node ID for a data node, it must be in the range 1
     # to 48 inclusive and must be unique among all IDs specified for
     # cluster nodes.

     # SQL NODE / API NODE PARAMETERS

     [mysqld]
     # HostName=SQL-NODE-A-HOSTNAME
     # NodeId=SQL-NODE-A-NODEID

     [mysqld]

     [mysqld]

     # Each API or SQL node that connects to the cluster requires a [mysqld]
     # or [api] section of its own. Each such section defines a connection
     # 'slot'; you should have at least as many of these sections in the
     # config.ini file as the total number of API nodes and SQL nodes that
     # you wish to have connected to the cluster at any given time. There is
     # no performance or other penalty for having extra slots available in
     # case you find later that you want or need more API or SQL nodes to
     # connect to the cluster at the same time.
     # If no HostName is specified for a given [mysqld] or [api] section,
     # then _any_ API or SQL node may use that slot to connect to the
     # cluster. You may wish to use an explicit HostName for one connection slot
     # to guarantee that an API or SQL node from that host can always
     # connect to the cluster. If you wish to prevent API or SQL nodes from
     # connecting from other than a desired host or hosts, then use a
     # HostName for every [mysqld] or [api] section in the config.ini file.
     # You can if you wish define a node ID (NodeId parameter) for any API or
     # SQL node, but this is not necessary; if you do so, it must be in the
     # range 1 to 255 inclusive and must be unique among all IDs specified
     # for cluster nodes.

Recommended my.cnf options for SQL nodes

MySQL Servers acting as NDB Cluster SQL nodes must always be started
with the '--ndbcluster' and '--ndb-connectstring' options, either on the
command line or in 'my.cnf'.  In addition, set the following options for
all *note 'mysqld': mysqld. processes in the cluster, unless your setup
requires otherwise:

   * '--ndb-use-exact-count=0'

   * '--ndb-index-stat-enable=0'

   * '--ndb-force-send=1'

   * '--engine-condition-pushdown=1'


File: manual.info.tmp,  Node: mysql-cluster-connection-strings,  Next: mysql-cluster-computer-definition,  Prev: mysql-cluster-config-starting,  Up: mysql-cluster-config-file

18.3.3.3 NDB Cluster Connection Strings
.......................................

With the exception of the NDB Cluster management server (*note
'ndb_mgmd': mysql-cluster-programs-ndb-mgmd.), each node that is part of
an NDB Cluster requires a _connection string_ that points to the
management server's location.  This connection string is used in
establishing a connection to the management server as well as in
performing other tasks depending on the node's role in the cluster.  The
syntax for a connection string is as follows:

     [nodeid=NODE_ID, ]HOST-DEFINITION[, HOST-DEFINITION[, ...]]

     HOST-DEFINITION:
         HOST_NAME[:PORT_NUMBER]

'node_id' is an integer greater than or equal to 1 which identifies a
node in 'config.ini'.  HOST_NAME is a string representing a valid
Internet host name or IP address.  PORT_NUMBER is an integer referring
to a TCP/IP port number.

     example 1 (long):    "nodeid=2,myhost1:1100,myhost2:1100,198.51.100.3:1200"
     example 2 (short):   "myhost1"

'localhost:1186' is used as the default connection string value if none
is provided.  If PORT_NUM is omitted from the connection string, the
default port is 1186.  This port should always be available on the
network because it has been assigned by IANA for this purpose (see
<http://www.iana.org/assignments/port-numbers> for details).

By listing multiple host definitions, it is possible to designate
several redundant management servers.  An NDB Cluster data or API node
attempts to contact successive management servers on each host in the
order specified, until a successful connection has been established.

It is also possible to specify in a connection string one or more bind
addresses to be used by nodes having multiple network interfaces for
connecting to management servers.  A bind address consists of a hostname
or network address and an optional port number.  This enhanced syntax
for connection strings is shown here:

     [nodeid=NODE_ID, ]
         [bind-address=HOST-DEFINITION, ]
         HOST-DEFINITION[; bind-address=HOST-DEFINITION]
         HOST-DEFINITION[; bind-address=HOST-DEFINITION]
         [, ...]]

     HOST-DEFINITION:
         HOST_NAME[:PORT_NUMBER]

If a single bind address is used in the connection string _prior_ to
specifying any management hosts, then this address is used as the
default for connecting to any of them (unless overridden for a given
management server; see later in this section for an example).  For
example, the following connection string causes the node to use
'198.51.100.242' regardless of the management server to which it
connects:

     bind-address=198.51.100.242, poseidon:1186, perch:1186

If a bind address is specified _following_ a management host definition,
then it is used only for connecting to that management node.  Consider
the following connection string:

     poseidon:1186;bind-address=localhost, perch:1186;bind-address=198.51.100.242

In this case, the node uses 'localhost' to connect to the management
server running on the host named 'poseidon' and '198.51.100.242' to
connect to the management server running on the host named 'perch'.

You can specify a default bind address and then override this default
for one or more specific management hosts.  In the following example,
'localhost' is used for connecting to the management server running on
host 'poseidon'; since '198.51.100.242' is specified first (before any
management server definitions), it is the default bind address and so is
used for connecting to the management servers on hosts 'perch' and
'orca':

     bind-address=198.51.100.242,poseidon:1186;bind-address=localhost,perch:1186,orca:2200

There are a number of different ways to specify the connection string:

   * Each executable has its own command-line option which enables
     specifying the management server at startup.  (See the
     documentation for the respective executable.)

   * It is also possible to set the connection string for all nodes in
     the cluster at once by placing it in a '[mysql_cluster]' section in
     the management server's 'my.cnf' file.

   * For backward compatibility, two other options are available, using
     the same syntax:

       1. Set the 'NDB_CONNECTSTRING' environment variable to contain
          the connection string.

       2. Write the connection string for each executable into a text
          file named 'Ndb.cfg' and place this file in the executable's
          startup directory.

     However, these are now deprecated and should not be used for new
     installations.

The recommended method for specifying the connection string is to set it
on the command line or in the 'my.cnf' file for each executable.


File: manual.info.tmp,  Node: mysql-cluster-computer-definition,  Next: mysql-cluster-mgm-definition,  Prev: mysql-cluster-connection-strings,  Up: mysql-cluster-config-file

18.3.3.4 Defining Computers in an NDB Cluster
.............................................

The '[computer]' section has no real significance other than serving as
a way to avoid the need of defining host names for each node in the
system.  All parameters mentioned here are required.

Restart types

Information about the restart types used by the parameter descriptions
in this section is shown in the following table:

*NDB Cluster restart types*

Symbol  Restart Type           Description
                               
*N*     Node                   The parameter can be updated using a
                               rolling restart (see
                               *note mysql-cluster-rolling-restart::)
                               
*S*     System                 All cluster nodes must be shut down
                               completely, then restarted, to effect a
                               change in this parameter
                               
*I*     Initial                Data nodes must be restarted using the
                               '--initial' option

   * 'Id'

     *This table provides type and value information for the Id computer
     configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      string
                                          
     *Default*                            [none]
                                          
     *Range*                              ...
                                          
     *Restart Type*                       IS

     This is a unique identifier, used to refer to the host computer
     elsewhere in the configuration file.

     *Important*:

     The computer ID is _not_ the same as the node ID used for a
     management, API, or data node.  Unlike the case with node IDs, you
     cannot use 'NodeId' in place of 'Id' in the '[computer]' section of
     the 'config.ini' file.

   * 'HostName'

     *This table provides type and value information for the HostName
     computer configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      name or IP address
                                          
     *Default*                            [none]
                                          
     *Range*                              ...
                                          
     *Restart Type*                       N

     This is the computer's hostname or IP address.


File: manual.info.tmp,  Node: mysql-cluster-mgm-definition,  Next: mysql-cluster-ndbd-definition,  Prev: mysql-cluster-computer-definition,  Up: mysql-cluster-config-file

18.3.3.5 Defining an NDB Cluster Management Server
..................................................

The '[ndb_mgmd]' section is used to configure the behavior of the
management server.  If multiple management servers are employed, you can
specify parameters common to all of them in an '[ndb_mgmd default]'
section.  '[mgm]' and '[mgm default]' are older aliases for these,
supported for backward compatibility.

All parameters in the following list are optional and assume their
default values if omitted.

*Note*:

If neither the 'ExecuteOnComputer' nor the 'HostName' parameter is
present, the default value 'localhost' will be assumed for both.

Restart types

Information about the restart types used by the parameter descriptions
in this section is shown in the following table:

*NDB Cluster restart types*

Symbol  Restart Type           Description
                               
*N*     Node                   The parameter can be updated using a
                               rolling restart (see
                               *note mysql-cluster-rolling-restart::)
                               
*S*     System                 All cluster nodes must be shut down
                               completely, then restarted, to effect a
                               change in this parameter
                               
*I*     Initial                Data nodes must be restarted using the
                               '--initial' option

   * 
     'Id'

     *This table provides type and value information for the Id
     management node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      unsigned
                                          
     *Default*                            [none]
                                          
     *Range*                              1 - 255
                                          
     *Restart Type*                       IS

     Each node in the cluster has a unique identity.  For a management
     node, this is represented by an integer value in the range 1 to
     255, inclusive.  This ID is used by all internal cluster messages
     for addressing the node, and so must be unique for each NDB Cluster
     node, regardless of the type of node.

     *Note*:

     Data node IDs must be less than 49.  If you plan to deploy a large
     number of data nodes, it is a good idea to limit the node IDs for
     management nodes (and API nodes) to values greater than 48.

     The use of the 'Id' parameter for identifying management nodes is
     deprecated in favor of 'NodeId'.  Although 'Id' continues to be
     supported for backward compatibility, it now generates a warning
     and is subject to removal in a future version of NDB Cluster.

   * 
     'NodeId'

     *This table provides type and value information for the NodeId
     management node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      unsigned
                                          
     *Default*                            [none]
                                          
     *Range*                              1 - 255
                                          
     *Restart Type*                       IS

     Each node in the cluster has a unique identity.  For a management
     node, this is represented by an integer value in the range 1 to 255
     inclusive.  This ID is used by all internal cluster messages for
     addressing the node, and so must be unique for each NDB Cluster
     node, regardless of the type of node.

     *Note*:

     Data node IDs must be less than 49.  If you plan to deploy a large
     number of data nodes, it is a good idea to limit the node IDs for
     management nodes (and API nodes) to values greater than 48.

     'NodeId' is the preferred parameter name to use when identifying
     management nodes.  Although the older 'Id' continues to be
     supported for backward compatibility, it is now deprecated and
     generates a warning when used; it is also subject to removal in a
     future NDB Cluster release.

   * 
     'ExecuteOnComputer'

     *This table provides type and value information for the
     ExecuteOnComputer management node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      name
                                          
     *Default*                            [none]
                                          
     *Range*                              ...
                                          
     *Restart Type*                       S

     This refers to the 'Id' set for one of the computers defined in a
     '[computer]' section of the 'config.ini' file.

   * 
     'PortNumber'

     *This table provides type and value information for the PortNumber
     management node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      unsigned
                                          
     *Default*                            1186
                                          
     *Range*                              0 - 64K
                                          
     *Restart Type*                       S

     This is the port number on which the management server listens for
     configuration requests and management commands.

   * 
     'HostName'

     *This table provides type and value information for the HostName
     management node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      name or IP address
                                          
     *Default*                            [none]
                                          
     *Range*                              ...
                                          
     *Restart Type*                       N

     Specifying this parameter defines the hostname of the computer on
     which the management node is to reside.  To specify a hostname
     other than 'localhost', either this parameter or
     'ExecuteOnComputer' is required.

   * 
     'LogDestination'

     *This table provides type and value information for the
     LogDestination management node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      {CONSOLE|SYSLOG|FILE}
                                          
     *Default*                            [see text]
                                          
     *Range*                              ...
                                          
     *Restart Type*                       N

     This parameter specifies where to send cluster logging information.
     There are three options in this regard--'CONSOLE', 'SYSLOG', and
     'FILE'--with 'FILE' being the default:

        * 'CONSOLE' outputs the log to 'stdout':

               CONSOLE

        * 'SYSLOG' sends the log to a 'syslog' facility, possible values
          being one of 'auth', 'authpriv', 'cron', 'daemon', 'ftp',
          'kern', 'lpr', 'mail', 'news', 'syslog', 'user', 'uucp',
          'local0', 'local1', 'local2', 'local3', 'local4', 'local5',
          'local6', or 'local7'.

          *Note*:

          Not every facility is necessarily supported by every operating
          system.

               SYSLOG:facility=syslog

        * 'FILE' pipes the cluster log output to a regular file on the
          same machine.  The following values can be specified:

             * 'filename': The name of the log file.

               In NDB 7.2.6 and earlier, the log file's default name,
               used if 'FILE' was specified without also setting
               'filename', was 'logger.log'.  Beginning with NDB 7.2.7,
               the default log file name used in such cases is
               'ndb_NODEID_cluster.log'.

             * 'maxsize': The maximum size (in bytes) to which the file
               can grow before logging rolls over to a new file.  When
               this occurs, the old log file is renamed by appending .N
               to the file name, where N is the next number not yet used
               with this name.

             * 'maxfiles': The maximum number of log files.

               FILE:filename=cluster.log,maxsize=1000000,maxfiles=6

          The default value for the 'FILE' parameter is
          'FILE:filename=ndb_NODE_ID_cluster.log,maxsize=1000000,maxfiles=6',
          where NODE_ID is the ID of the node.

     It is possible to specify multiple log destinations separated by
     semicolons as shown here:

          CONSOLE;SYSLOG:facility=local0;FILE:filename=/var/log/mgmd

   * 
     'ArbitrationRank'

     *This table provides type and value information for the
     ArbitrationRank management node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      0-2
                                          
     *Default*                            1
                                          
     *Range*                              0 - 2
                                          
     *Restart Type*                       N

     This parameter is used to define which nodes can act as
     arbitrators.  Only management nodes and SQL nodes can be
     arbitrators.  'ArbitrationRank' can take one of the following
     values:

        * '0': The node will never be used as an arbitrator.

        * '1': The node has high priority; that is, it will be preferred
          as an arbitrator over low-priority nodes.

        * '2': Indicates a low-priority node which be used as an
          arbitrator only if a node with a higher priority is not
          available for that purpose.

     Normally, the management server should be configured as an
     arbitrator by setting its 'ArbitrationRank' to 1 (the default for
     management nodes) and those for all SQL nodes to 0 (the default for
     SQL nodes).

     You can disable arbitration completely either by setting
     'ArbitrationRank' to 0 on all management and SQL nodes, or by
     setting the 'Arbitration' parameter in the '[ndbd default]' section
     of the 'config.ini' global configuration file.  Setting
     'Arbitration' causes any settings for 'ArbitrationRank' to be
     disregarded.

   * 
     'ArbitrationDelay'

     *This table provides type and value information for the
     ArbitrationDelay management node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      milliseconds
                                          
     *Default*                            0
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     An integer value which causes the management server's responses to
     arbitration requests to be delayed by that number of milliseconds.
     By default, this value is 0; it is normally not necessary to change
     it.

   * 
     'DataDir'

     *This table provides type and value information for the DataDir
     management node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      path
                                          
     *Default*                            .
                                          
     *Range*                              ...
                                          
     *Restart Type*                       N

     This specifies the directory where output files from the management
     server will be placed.  These files include cluster log files,
     process output files, and the daemon's process ID (PID) file.  (For
     log files, this location can be overridden by setting the 'FILE'
     parameter for 'LogDestination' as discussed previously in this
     section.)

     The default value for this parameter is the directory in which
     *note 'ndb_mgmd': mysql-cluster-programs-ndb-mgmd. is located.

   * 
     'PortNumberStats'

     *This table provides type and value information for the
     PortNumberStats management node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      unsigned
                                          
     *Default*                            [none]
                                          
     *Range*                              0 - 64K
                                          
     *Restart Type*                       N

     This parameter specifies the port number used to obtain statistical
     information from an NDB Cluster management server.  It has no
     default value.

   * 
     'Wan'

     *This table provides type and value information for the wan
     management node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      boolean
                                          
     *Default*                            false
                                          
     *Range*                              true, false
                                          
     *Restart Type*                       N

     Use WAN TCP setting as default.

   * 
     'HeartbeatThreadPriority'

     *This table provides type and value information for the
     HeartbeatThreadPriority management node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      string
                                          
     *Default*                            [none]
                                          
     *Range*                              ...
                                          
     *Restart Type*                       S

     Set the scheduling policy and priority of heartbeat threads for
     management and API nodes.

     The syntax for setting this parameter is shown here:

          HeartbeatThreadPriority = POLICY[, PRIORITY]

          POLICY:
            {FIFO | RR}

     When setting this parameter, you must specify a policy.  This is
     one of 'FIFO' (first in, first out) or 'RR' (round robin).  The
     policy value is followed optionally by the priority (an integer).

   * 
     'ExtraSendBufferMemory'

     *This table provides type and value information for the
     ExtraSendBufferMemory management node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.14
                                          
     *Type or units*                      bytes
                                          
     *Default*                            0
                                          
     *Range*                              0 - 32G
                                          
     *Restart Type*                       N

     This parameter specifies the amount of transporter send buffer
     memory to allocate in addition to any that has been set using
     'TotalSendBufferMemory', 'SendBufferMemory', or both.

     This parameter was added in NDB 7.2.14.  (Bug #14555359)

   * 
     'TotalSendBufferMemory'

     *This table provides type and value information for the
     TotalSendBufferMemory management node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      bytes
                                          
     *Default*                            0
                                          
     *Range*                              256K - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     This parameter is used to determine the total amount of memory to
     allocate on this node for shared send buffer memory among all
     configured transporters.

     If this parameter is set, its minimum permitted value is 256KB; 0
     indicates that the parameter has not been set.  For more detailed
     information, see *note mysql-cluster-config-send-buffers::.

   * 
     'HeartbeatIntervalMgmdMgmd'

     *This table provides type and value information for the
     HeartbeatIntervalMgmdMgmd management node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.12
                                          
     *Type or units*                      milliseconds
                                          
     *Default*                            1500
                                          
     *Range*                              100 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     Specify the interval between heartbeat messages used to determine
     whether another management node is on contact with this one.  The
     management node waits after 3 of these intervals to declare the
     connection dead; thus, the default setting of 1500 milliseconds
     causes the management node to wait for approximately 1600 ms before
     timing out.

     This parameter was added in NDB 7.2.12.  (Bug #16426805, Bug
     #17807768)

*Note*:

After making changes in a management node's configuration, it is
necessary to perform a rolling restart of the cluster for the new
configuration to take effect.

To add new management servers to a running NDB Cluster, it is also
necessary to perform a rolling restart of all cluster nodes after
modifying any existing 'config.ini' files.  For more information about
issues arising when using multiple management nodes, see *note
mysql-cluster-limitations-multiple-nodes::.


File: manual.info.tmp,  Node: mysql-cluster-ndbd-definition,  Next: mysql-cluster-api-definition,  Prev: mysql-cluster-mgm-definition,  Up: mysql-cluster-config-file

18.3.3.6 Defining NDB Cluster Data Nodes
........................................

The '[ndbd]' and '[ndbd default]' sections are used to configure the
behavior of the cluster's data nodes.

'[ndbd]' and '[ndbd default]' are always used as the section names
whether you are using *note 'ndbd': mysql-cluster-programs-ndbd. or
*note 'ndbmtd': mysql-cluster-programs-ndbmtd. binaries for the data
node processes.

There are many parameters which control buffer sizes, pool sizes,
timeouts, and so forth.  The only mandatory parameter is either one of
'ExecuteOnComputer' or 'HostName'; this must be defined in the local
'[ndbd]' section.

The parameter 'NoOfReplicas' should be defined in the '[ndbd default]'
section, as it is common to all Cluster data nodes.  It is not strictly
necessary to set 'NoOfReplicas', but it is good practice to set it
explicitly.

Most data node parameters are set in the '[ndbd default]' section.  Only
those parameters explicitly stated as being able to set local values are
permitted to be changed in the '[ndbd]' section.  Where present,
'HostName', 'NodeId' and 'ExecuteOnComputer' _must_ be defined in the
local '[ndbd]' section, and not in any other section of 'config.ini'.
In other words, settings for these parameters are specific to one data
node.

For those parameters affecting memory usage or buffer sizes, it is
possible to use 'K', 'M', or 'G' as a suffix to indicate units of 1024,
1024x1024, or 1024x1024x1024.  (For example, '100K' means 100 x 1024 =
102400.)

Parameter names and values are case-insensitive, unless used in a MySQL
Server 'my.cnf' or 'my.ini' file, in which case they are case-sensitive.

Information about configuration parameters specific to NDB Cluster Disk
Data tables can be found later in this section (see *note
mysql-cluster-ndbd-definition-disk-data-parameters::).

All of these parameters also apply to *note 'ndbmtd':
mysql-cluster-programs-ndbmtd. (the multithreaded version of *note
'ndbd': mysql-cluster-programs-ndbd.).  Three additional data node
configuration parameters--'MaxNoOfExecutionThreads', 'ThreadConfig', and
'NoOfFragmentLogParts'--apply to *note 'ndbmtd':
mysql-cluster-programs-ndbmtd. only; these have no effect when used with
*note 'ndbd': mysql-cluster-programs-ndbd.  For more information, see
*note mysql-cluster-ndbd-definition-ndbmtd-parameters::.  See also *note
mysql-cluster-programs-ndbmtd::.

Restart types

Information about the restart types used by the parameter descriptions
in this section is shown in the following table:

*NDB Cluster restart types*

Symbol  Restart Type           Description
                               
*N*     Node                   The parameter can be updated using a
                               rolling restart (see
                               *note mysql-cluster-rolling-restart::)
                               
*S*     System                 All cluster nodes must be shut down
                               completely, then restarted, to effect a
                               change in this parameter
                               
*I*     Initial                Data nodes must be restarted using the
                               '--initial' option

Identifying data nodes

The 'NodeId' or 'Id' value (that is, the data node identifier) can be
allocated on the command line when the node is started or in the
configuration file.

   * 
     'Id'

     *This table provides type and value information for the Id data
     node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      unsigned
                                          
     *Default*                            [none]
                                          
     *Range*                              1 - 48
                                          
     *Restart Type*                       IS

     A unique node ID is used as the node's address for all cluster
     internal messages.  For data nodes, this is an integer in the range
     1 to 48 inclusive.  Each node in the cluster must have a unique
     identifier.

     'NodeId' is the preferred parameter name to use when identifying
     data nodes.  Although the older 'Id' is still supported for
     backward compatibility, it is now deprecated, and generates a
     warning when used.  'Id' is also subject to removal in a future NDB
     Cluster release.

   * 
     'NodeId'

     *This table provides type and value information for the NodeId data
     node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      unsigned
                                          
     *Default*                            [none]
                                          
     *Range*                              1 - 48
                                          
     *Restart Type*                       IS

     A unique node ID is used as the node's address for all cluster
     internal messages.  For data nodes, this is an integer in the range
     1 to 48 inclusive.  Each node in the cluster must have a unique
     identifier.

     'NodeId' is the preferred parameter name to use when identifying
     data nodes.  Although 'Id' continues to be supported for backward
     compatibility, it is now deprecated, generates a warning when used,
     and is subject to removal in a future version of NDB Cluster.

   * 
     'ExecuteOnComputer'

     *This table provides type and value information for the
     ExecuteOnComputer data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      name
                                          
     *Default*                            [none]
                                          
     *Range*                              ...
                                          
     *Restart Type*                       S

     This refers to the 'Id' set for one of the computers defined in a
     '[computer]' section.

   * 
     'HostName'

     *This table provides type and value information for the HostName
     data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      name or IP address
                                          
     *Default*                            localhost
                                          
     *Range*                              ...
                                          
     *Restart Type*                       N

     Specifying this parameter defines the hostname of the computer on
     which the data node is to reside.  To specify a hostname other than
     'localhost', either this parameter or 'ExecuteOnComputer' is
     required.

   * 
     'ServerPort'

     *This table provides type and value information for the ServerPort
     data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      unsigned
                                          
     *Default*                            [none]
                                          
     *Range*                              1 - 64K
                                          
     *Restart Type*                       S

     Each node in the cluster uses a port to connect to other nodes.  By
     default, this port is allocated dynamically in such a way as to
     ensure that no two nodes on the same host computer receive the same
     port number, so it should normally not be necessary to specify a
     value for this parameter.

     However, if you need to be able to open specific ports in a
     firewall to permit communication between data nodes and API nodes
     (including SQL nodes), you can set this parameter to the number of
     the desired port in an '[ndbd]' section or (if you need to do this
     for multiple data nodes) the '[ndbd default]' section of the
     'config.ini' file, and then open the port having that number for
     incoming connections from SQL nodes, API nodes, or both.

     *Note*:

     Connections from data nodes to management nodes is done using the
     *note 'ndb_mgmd': mysql-cluster-programs-ndb-mgmd. management port
     (the management server's 'PortNumber'; see *note
     mysql-cluster-mgm-definition::) so outgoing connections to that
     port from any data nodes should always be permitted.

   * 'TcpBind_INADDR_ANY'

     Setting this parameter to 'TRUE' or '1' binds 'IP_ADDR_ANY' so that
     connections can be made from anywhere (for autogenerated
     connections).  The default is 'FALSE' ('0').

   * 
     'NodeGroup'

     *This table provides type and value information for the NodeGroup
     data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      

     *Default*                            [none]
                                          
     *Range*                              0 - 65536
                                          
     *Restart Type*                       IS

     This parameter can be used to assign a data node to a specific node
     group.  It is read only when the cluster is started for the first
     time, and cannot be used to reassign a data node to a different
     node group online.  It is generally not desirable to use this
     parameter in the '[ndbd default]' section of the 'config.ini' file,
     and care must be taken not to assign nodes to node groups in such a
     way that an invalid numbers of nodes are assigned to any node
     groups.

     The 'NodeGroup' parameter is chiefly intended for use in adding a
     new node group to a running NDB Cluster without having to perform a
     rolling restart.  For this purpose, you should set it to 65536 (the
     maximum value).  You are not required to set a 'NodeGroup' value
     for all cluster data nodes, only for those nodes which are to be
     started and added to the cluster as a new node group at a later
     time.  For more information, see *note
     mysql-cluster-online-add-node-example::.

   * 
     'NoOfReplicas'

     *This table provides type and value information for the
     NoOfReplicas data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      integer
                                          
     *Default*                            2
                                          
     *Range*                              1 - 2
                                          
     *Restart Type*                       IS

     This global parameter can be set only in the '[ndbd default]'
     section, and defines the number of replicas for each table stored
     in the cluster.  This parameter also specifies the size of node
     groups.  A node group is a set of nodes all storing the same
     information.

     Node groups are formed implicitly.  The first node group is formed
     by the set of data nodes with the lowest node IDs, the next node
     group by the set of the next lowest node identities, and so on.  By
     way of example, assume that we have 4 data nodes and that
     'NoOfReplicas' is set to 2.  The four data nodes have node IDs 2,
     3, 4 and 5.  Then the first node group is formed from nodes 2 and
     3, and the second node group by nodes 4 and 5.  It is important to
     configure the cluster in such a manner that nodes in the same node
     groups are not placed on the same computer because a single
     hardware failure would cause the entire cluster to fail.

     If no node IDs are provided, the order of the data nodes will be
     the determining factor for the node group.  Whether or not explicit
     assignments are made, they can be viewed in the output of the
     management client's 'SHOW' command.

     The default and recommended maximum value for 'NoOfReplicas' is 2.
     _This is the recommended value for most production environments_.

     *Important*:

     While it is theoretically possible for the value of this parameter
     to be 3 or 4, *NDB Cluster 7.2 does not support setting
     'NoOfReplicas' to a value greater than 2 in production*.

     *Warning*:

     Setting 'NoOfReplicas' to 1 means that there is only a single copy
     of all Cluster data; in this case, the loss of a single data node
     causes the cluster to fail because there are no additional copies
     of the data stored by that node.

     The value for this parameter must divide evenly into the number of
     data nodes in the cluster.  For example, if there are two data
     nodes, then 'NoOfReplicas' must be equal to either 1 or 2, since
     2/3 and 2/4 both yield fractional values; if there are four data
     nodes, then 'NoOfReplicas' must be equal to 1, 2, or 4.

   * 
     'DataDir'

     *This table provides type and value information for the DataDir
     data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      path
                                          
     *Default*                            .
                                          
     *Range*                              ...
                                          
     *Restart Type*                       IN

     This parameter specifies the directory where trace files, log
     files, pid files and error logs are placed.

     The default is the data node process working directory.

   * 
     'FileSystemPath'

     *This table provides type and value information for the
     FileSystemPath data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      path
                                          
     *Default*                            DataDir
                                          
     *Range*                              ...
                                          
     *Restart Type*                       IN

     This parameter specifies the directory where all files created for
     metadata, REDO logs, UNDO logs (for Disk Data tables), and data
     files are placed.  The default is the directory specified by
     'DataDir'.

     *Note*:

     This directory must exist before the *note 'ndbd':
     mysql-cluster-programs-ndbd. process is initiated.

     The recommended directory hierarchy for NDB Cluster includes
     '/var/lib/mysql-cluster', under which a directory for the node's
     file system is created.  The name of this subdirectory contains the
     node ID. For example, if the node ID is 2, this subdirectory is
     named 'ndb_2_fs'.

   * 
     'BackupDataDir'

     *This table provides type and value information for the
     BackupDataDir data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      path
                                          
     *Default*                            [see text]
                                          
     *Range*                              ...
                                          
     *Restart Type*                       IN

     This parameter specifies the directory in which backups are placed.

     *Important*:

     The string ''/BACKUP'' is always appended to this value.  For
     example, if you set the value of 'BackupDataDir' to
     '/var/lib/cluster-data', then all backups are stored under
     '/var/lib/cluster-data/BACKUP'.  This also means that the
     _effective_ default backup location is the directory named 'BACKUP'
     under the location specified by the 'FileSystemPath' parameter.

*Data Memory, Index Memory, and String Memory*

'DataMemory' and 'IndexMemory' are '[ndbd]' parameters specifying the
size of memory segments used to store the actual records and their
indexes.  In setting values for these, it is important to understand how
'DataMemory' and 'IndexMemory' are used, as they usually need to be
updated to reflect actual usage by the cluster:

   * 
     'DataMemory'

     *This table provides type and value information for the DataMemory
     data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      bytes
                                          
     *Default*                            80M
                                          
     *Range*                              1M - 1T
                                          
     *Restart Type*                       N

     This parameter defines the amount of space (in bytes) available for
     storing database records.  The entire amount specified by this
     value is allocated in memory, so it is extremely important that the
     machine has sufficient physical memory to accommodate it.

     The memory allocated by 'DataMemory' is used to store both the
     actual records and indexes.  There is a 16-byte overhead on each
     record; an additional amount for each record is incurred because it
     is stored in a 32KB page with 128 byte page overhead (see below).
     There is also a small amount wasted per page due to the fact that
     each record is stored in only one page.

     For variable-size table attributes, the data is stored on separate
     data pages, allocated from 'DataMemory'.  Variable-length records
     use a fixed-size part with an extra overhead of 4 bytes to
     reference the variable-size part.  The variable-size part has 2
     bytes overhead plus 2 bytes per attribute.

     The maximum record size is 14000 bytes.

     The memory space defined by 'DataMemory' is also used to store
     ordered indexes, which use about 10 bytes per record.  Each table
     row is represented in the ordered index.  A common error among
     users is to assume that all indexes are stored in the memory
     allocated by 'IndexMemory', but this is not the case: Only primary
     key and unique hash indexes use this memory; ordered indexes use
     the memory allocated by 'DataMemory'.  However, creating a primary
     key or unique hash index also creates an ordered index on the same
     keys, unless you specify 'USING HASH' in the index creation
     statement.  This can be verified by running *note 'ndb_desc -d
     DB_NAME TABLE_NAME': mysql-cluster-programs-ndb-desc. in the
     management client.

     NDB Cluster can use a maximum of 512 MB for hash indexes per
     partition, which means in some cases it is possible to get 'Table
     is full' errors in MySQL client applications even when *note
     'ndb_mgm -e "ALL REPORT MEMORYUSAGE"':
     mysql-cluster-programs-ndb-mgm. shows significant free
     'DataMemory'.  This can also pose a problem with data node restarts
     on nodes that are heavily loaded with data.

     You can force *note 'NDB': mysql-cluster. to create extra
     partitions for NDB Cluster tables and thus have more memory
     available for hash indexes by using the 'MAX_ROWS' option for *note
     'CREATE TABLE': create-table.  In general, setting 'MAX_ROWS' to
     twice the number of rows that you expect to store in the table
     should be sufficient.

     You can also use the 'MinFreePct' configuration parameter to help
     avoid problems with node restarts.  (NDB 7.2.3 and later; Bug
     #13436216.)

     The memory space allocated by 'DataMemory' consists of 32KB pages,
     which are allocated to table fragments.  Each table is normally
     partitioned into the same number of fragments as there are data
     nodes in the cluster.  Thus, for each node, there are the same
     number of fragments as are set in 'NoOfReplicas'.

     Once a page has been allocated, it is currently not possible to
     return it to the pool of free pages, except by deleting the table.
     (This also means that 'DataMemory' pages, once allocated to a given
     table, cannot be used by other tables.)  Performing a data node
     recovery also compresses the partition because all records are
     inserted into empty partitions from other live nodes.

     The 'DataMemory' memory space also contains UNDO information: For
     each update, a copy of the unaltered record is allocated in the
     'DataMemory'.  There is also a reference to each copy in the
     ordered table indexes.  Unique hash indexes are updated only when
     the unique index columns are updated, in which case a new entry in
     the index table is inserted and the old entry is deleted upon
     commit.  For this reason, it is also necessary to allocate enough
     memory to handle the largest transactions performed by applications
     using the cluster.  In any case, performing a few large
     transactions holds no advantage over using many smaller ones, for
     the following reasons:

        * Large transactions are not any faster than smaller ones

        * Large transactions increase the number of operations that are
          lost and must be repeated in event of transaction failure

        * Large transactions use more memory

     The default value for 'DataMemory' is 80MB; the minimum is 1MB.
     There is no maximum size, but in reality the maximum size has to be
     adapted so that the process does not start swapping when the limit
     is reached.  This limit is determined by the amount of physical RAM
     available on the machine and by the amount of memory that the
     operating system may commit to any one process.  32-bit operating
     systems are generally limited to 2−4GB per process; 64-bit
     operating systems can use more.  For large databases, it may be
     preferable to use a 64-bit operating system for this reason.

   * 
     'IndexMemory'

     *This table provides type and value information for the IndexMemory
     data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      bytes
                                          
     *Default*                            18M
                                          
     *Range*                              1M - 1T
                                          
     *Restart Type*                       N

     This parameter controls the amount of storage used for hash indexes
     in NDB Cluster.  Hash indexes are always used for primary key
     indexes, unique indexes, and unique constraints.  When defining a
     primary key or a unique index, two indexes are created, one of
     which is a hash index used for all tuple accesses as well as lock
     handling.  This index is also used to enforce unique constraints.

     You can estimate the size of a hash index using this formula:

            size  = ( (FRAGMENTS * 32K) + (ROWS * 18) )
                    * REPLICAS

     FRAGMENTS is the number of fragments, REPLICAS is the number of
     replicas (normally 2), and ROWS is the number of rows.  If a table
     has one million rows, 8 fragments, and 2 replicas, the expected
     index memory usage is calculated as shown here:


            ((8 * 32K) + (1000000 * 18)) * 2 = ((8 * 32768) + (1000000 * 18)) * 2
            = (262144 + 18000000) * 2
            = 18262144 * 2 = 36524288 bytes = ~35MB

     In NDB Cluster 7.2 and later, index statistics (when enabled) for
     ordered indexes are stored in the 'mysql.ndb_index_stat_sample'
     table.  Since this table has a hash index, this adds to index
     memory usage.  An upper bound to the number of rows for a given
     ordered index can be calculated as follows:

            sample_size= key_size + ((key_attributes + 1) * 4)

            sample_rows = IndexStatSaveSize
                          * ((0.01 * IndexStatSaveScale * log2(rows * sample_size)) + 1)
                          / sample_size

     In the preceding formula, KEY_SIZE is the size of the ordered index
     key in bytes, KEY_ATTRIBUTES is the number ot attributes in the
     ordered index key, and ROWS is the number of rows in the base
     table.

     Assume that table 't1' has 1 million rows and an ordered index
     named 'ix1' on two four-byte integers.  Assume in addition that
     'IndexStatSaveSize' and 'IndexStatSaveScale' are set to their
     default values (32K and 100, respectively).  Using the previous 2
     formulas, we can calculate as follows:

            sample_size = 8  + ((1 + 2) * 4) = 20 bytes

            sample_rows = 32K
                          * ((0.01 * 100 * log2(1000000*20)) + 1)
                          / 20
                          = 32768 * ( (1 * ~16.811) +1) / 20
                          = 32768 * ~17.811 / 20
                          = ~29182 rows

     The expected index memory usage is thus 2 * 18 * 29182 = ~1050550
     bytes.

     The default value for 'IndexMemory' is 18MB. The minimum is 1MB.

   * 
     'StringMemory'

     *This table provides type and value information for the
     StringMemory data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      % or bytes
                                          
     *Default*                            25
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       S

     This parameter determines how much memory is allocated for strings
     such as table names, and is specified in an '[ndbd]' or '[ndbd
     default]' section of the 'config.ini' file.  A value between '0'
     and '100' inclusive is interpreted as a percent of the maximum
     default value, which is calculated based on a number of factors
     including the number of tables, maximum table name size, maximum
     size of '.FRM' files, 'MaxNoOfTriggers', maximum column name size,
     and maximum default column value.

     A value greater than '100' is interpreted as a number of bytes.

     The default value is 25--that is, 25 percent of the default
     maximum.

     Under most circumstances, the default value should be sufficient,
     but when you have a great many Cluster tables (1000 or more), it is
     possible to get Error 773 'Out of string memory, please modify
     StringMemory config parameter: Permanent error: Schema error', in
     which case you should increase this value.  '25' (25 percent) is
     not excessive, and should prevent this error from recurring in all
     but the most extreme conditions.

The following example illustrates how memory is used for a table.
Consider this table definition:

     CREATE TABLE example (
       a INT NOT NULL,
       b INT NOT NULL,
       c INT NOT NULL,
       PRIMARY KEY(a),
       UNIQUE(b)
     ) ENGINE=NDBCLUSTER;

For each record, there are 12 bytes of data plus 12 bytes overhead.
Having no nullable columns saves 4 bytes of overhead.  In addition, we
have two ordered indexes on columns 'a' and 'b' consuming roughly 10
bytes each per record.  There is a primary key hash index on the base
table using roughly 29 bytes per record.  The unique constraint is
implemented by a separate table with 'b' as primary key and 'a' as a
column.  This other table consumes an additional 29 bytes of index
memory per record in the 'example' table as well 8 bytes of record data
plus 12 bytes of overhead.

Thus, for one million records, we need 58MB for index memory to handle
the hash indexes for the primary key and the unique constraint.  We also
need 64MB for the records of the base table and the unique index table,
plus the two ordered index tables.

You can see that hash indexes takes up a fair amount of memory space;
however, they provide very fast access to the data in return.  They are
also used in NDB Cluster to handle uniqueness constraints.

The only partitioning algorithm is hashing and ordered indexes are local
to each node.  Thus, ordered indexes cannot be used to handle uniqueness
constraints in the general case.

An important point for both 'IndexMemory' and 'DataMemory' is that the
total database size is the sum of all data memory and all index memory
for each node group.  Each node group is used to store replicated
information, so if there are four nodes with two replicas, there will be
two node groups.  Thus, the total data memory available is 2 x
'DataMemory' for each data node.

It is highly recommended that 'DataMemory' and 'IndexMemory' be set to
the same values for all nodes.  Data distribution is even over all nodes
in the cluster, so the maximum amount of space available for any node
can be no greater than that of the smallest node in the cluster.

'DataMemory' and 'IndexMemory' can be changed, but decreasing either of
these can be risky; doing so can easily lead to a node or even an entire
NDB Cluster that is unable to restart due to there being insufficient
memory space.  Increasing these values should be acceptable, but it is
recommended that such upgrades are performed in the same manner as a
software upgrade, beginning with an update of the configuration file,
and then restarting the management server followed by restarting each
data node in turn.

MinFreePct

Beginning with NDB 7.2.3, a proportion (5% by default) of data node
resources including 'DataMemory' and 'IndexMemory' is kept in reserve to
insure that the data node does not exhaust its memory when performing a
restart.  This can be adjusted using the 'MinFreePct' data node
configuration parameter (default 5) introduced in the same version of
NDB Cluster.

*This table provides type and value information for the MinFreePct data
node configuration parameter*

Property                             Value
                                     
*Version (or later)*                 NDB 7.2.3
                                     
*Type or units*                      unsigned
                                     
*Default*                            5
                                     
*Range*                              0 - 100
                                     
*Restart Type*                       N

Updates do not increase the amount of index memory used.  Inserts take
effect immediately; however, rows are not actually deleted until the
transaction is committed.

Transaction parameters

The next few '[ndbd]' parameters that we discuss are important because
they affect the number of parallel transactions and the sizes of
transactions that can be handled by the system.
'MaxNoOfConcurrentTransactions' sets the number of parallel transactions
possible in a node.  'MaxNoOfConcurrentOperations' sets the number of
records that can be in update phase or locked simultaneously.

Both of these parameters (especially 'MaxNoOfConcurrentOperations') are
likely targets for users setting specific values and not using the
default value.  The default value is set for systems using small
transactions, to ensure that these do not use excessive memory.

'MaxDMLOperationsPerTransaction' sets the maximum number of DML
operations that can be performed in a given transaction.

   * 
     'MaxNoOfConcurrentTransactions'

     *This table provides type and value information for the
     MaxNoOfConcurrentTransactions data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      integer
                                          
     *Default*                            4096
                                          
     *Range*                              32 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     Each cluster data node requires a transaction record for each
     active transaction in the cluster.  The task of coordinating
     transactions is distributed among all of the data nodes.  The total
     number of transaction records in the cluster is the number of
     transactions in any given node times the number of nodes in the
     cluster.

     Transaction records are allocated to individual MySQL servers.
     Each connection to a MySQL server requires at least one transaction
     record, plus an additional transaction object per table accessed by
     that connection.  This means that a reasonable minimum for the
     total number of transactions in the cluster can be expressed as

          TotalNoOfConcurrentTransactions =
              (maximum number of tables accessed in any single transaction + 1)
              * number of SQL nodes

     Suppose that there are 10 SQL nodes using the cluster.  A single
     join involving 10 tables requires 11 transaction records; if there
     are 10 such joins in a transaction, then 10 * 11 = 110 transaction
     records are required for this transaction, per MySQL server, or 110
     * 10 = 1100 transaction records total.  Each data node can be
     expected to handle TotalNoOfConcurrentTransactions / number of data
     nodes.  For an NDB Cluster having 4 data nodes, this would mean
     setting 'MaxNoOfConcurrentTransactions' on each data node to 1100 /
     4 = 275.  In addition, you should provide for failure recovery by
     ensuring that a single node group can accommodate all concurrent
     transactions; in other words, that each data node's
     MaxNoOfConcurrentTransactions is sufficient to cover a number of
     transactions equal to TotalNoOfConcurrentTransactions / number of
     node groups.  If this cluster has a single node group, then
     'MaxNoOfConcurrentTransactions' should be set to 1100 (the same as
     the total number of concurrent transactions for the entire
     cluster).

     In addition, each transaction involves at least one operation; for
     this reason, the value set for 'MaxNoOfConcurrentTransactions'
     should always be no more than the value of
     'MaxNoOfConcurrentOperations'.

     This parameter must be set to the same value for all cluster data
     nodes.  This is due to the fact that, when a data node fails, the
     oldest surviving node re-creates the transaction state of all
     transactions that were ongoing in the failed node.

     It is possible to change this value using a rolling restart, but
     the amount of traffic on the cluster must be such that no more
     transactions occur than the lower of the old and new levels while
     this is taking place.

     The default value is 4096.

   * 
     'MaxNoOfConcurrentOperations'

     *This table provides type and value information for the
     MaxNoOfConcurrentOperations data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      integer
                                          
     *Default*                            32K
                                          
     *Range*                              32 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     It is a good idea to adjust the value of this parameter according
     to the size and number of transactions.  When performing
     transactions which involve only a few operations and records, the
     default value for this parameter is usually sufficient.  Performing
     large transactions involving many records usually requires that you
     increase its value.

     Records are kept for each transaction updating cluster data, both
     in the transaction coordinator and in the nodes where the actual
     updates are performed.  These records contain state information
     needed to find UNDO records for rollback, lock queues, and other
     purposes.

     This parameter should be set at a minimum to the number of records
     to be updated simultaneously in transactions, divided by the number
     of cluster data nodes.  For example, in a cluster which has four
     data nodes and which is expected to handle one million concurrent
     updates using transactions, you should set this value to 1000000 /
     4 = 250000.  To help provide resiliency against failures, it is
     suggested that you set this parameter to a value that is high
     enough to permit an individual data node to handle the load for its
     node group.  In other words, you should set the value equal to
     'total number of concurrent operations / number of node groups'.
     (In the case where there is a single node group, this is the same
     as the total number of concurrent operations for the entire
     cluster.)

     Because each transaction always involves at least one operation,
     the value of 'MaxNoOfConcurrentOperations' should always be greater
     than or equal to the value of 'MaxNoOfConcurrentTransactions'.

     Read queries which set locks also cause operation records to be
     created.  Some extra space is allocated within individual nodes to
     accommodate cases where the distribution is not perfect over the
     nodes.

     When queries make use of the unique hash index, there are actually
     two operation records used per record in the transaction.  The
     first record represents the read in the index table and the second
     handles the operation on the base table.

     The default value is 32768.

     This parameter actually handles two values that can be configured
     separately.  The first of these specifies how many operation
     records are to be placed with the transaction coordinator.  The
     second part specifies how many operation records are to be local to
     the database.

     A very large transaction performed on an eight-node cluster
     requires as many operation records in the transaction coordinator
     as there are reads, updates, and deletes involved in the
     transaction.  However, the operation records of the are spread over
     all eight nodes.  Thus, if it is necessary to configure the system
     for one very large transaction, it is a good idea to configure the
     two parts separately.  'MaxNoOfConcurrentOperations' will always be
     used to calculate the number of operation records in the
     transaction coordinator portion of the node.

     It is also important to have an idea of the memory requirements for
     operation records.  These consume about 1KB per record.

   * 
     'MaxNoOfLocalOperations'

     *This table provides type and value information for the
     MaxNoOfLocalOperations data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      integer
                                          
     *Default*                            UNDEFINED
                                          
     *Range*                              32 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     By default, this parameter is calculated as 1.1 x
     'MaxNoOfConcurrentOperations'.  This fits systems with many
     simultaneous transactions, none of them being very large.  If there
     is a need to handle one very large transaction at a time and there
     are many nodes, it is a good idea to override the default value by
     explicitly specifying this parameter.

   * 
     'MaxDMLOperationsPerTransaction'

     *This table provides type and value information for the
     MaxDMLOperationsPerTransaction data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      operations (DML)
                                          
     *Default*                            4294967295
                                          
     *Range*                              32 - 4294967295
                                          
     *Restart Type*                       N

     This parameter limits the size of a transaction.  The transaction
     is aborted if it requires more than this many DML operations.  The
     minimum number of operations per transaction is 32; however, you
     can set 'MaxDMLOperationsPerTransaction' to 0 to disable any
     limitation on the number of DML operations per transaction.  The
     maximum (and default) is 4294967295.

Transaction temporary storage

The next set of '[ndbd]' parameters is used to determine temporary
storage when executing a statement that is part of a Cluster
transaction.  All records are released when the statement is completed
and the cluster is waiting for the commit or rollback.

The default values for these parameters are adequate for most
situations.  However, users with a need to support transactions
involving large numbers of rows or operations may need to increase these
values to enable better parallelism in the system, whereas users whose
applications require relatively small transactions can decrease the
values to save memory.

   * 
     'MaxNoOfConcurrentIndexOperations'

     *This table provides type and value information for the
     MaxNoOfConcurrentIndexOperations data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      integer
                                          
     *Default*                            8K
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     For queries using a unique hash index, another temporary set of
     operation records is used during a query's execution phase.  This
     parameter sets the size of that pool of records.  Thus, this record
     is allocated only while executing a part of a query.  As soon as
     this part has been executed, the record is released.  The state
     needed to handle aborts and commits is handled by the normal
     operation records, where the pool size is set by the parameter
     'MaxNoOfConcurrentOperations'.

     The default value of this parameter is 8192.  Only in rare cases of
     extremely high parallelism using unique hash indexes should it be
     necessary to increase this value.  Using a smaller value is
     possible and can save memory if the DBA is certain that a high
     degree of parallelism is not required for the cluster.

   * 
     'MaxNoOfFiredTriggers'

     *This table provides type and value information for the
     MaxNoOfFiredTriggers data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      integer
                                          
     *Default*                            4000
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     The default value of 'MaxNoOfFiredTriggers' is 4000, which is
     sufficient for most situations.  In some cases it can even be
     decreased if the DBA feels certain the need for parallelism in the
     cluster is not high.

     A record is created when an operation is performed that affects a
     unique hash index.  Inserting or deleting a record in a table with
     unique hash indexes or updating a column that is part of a unique
     hash index fires an insert or a delete in the index table.  The
     resulting record is used to represent this index table operation
     while waiting for the original operation that fired it to complete.
     This operation is short-lived but can still require a large number
     of records in its pool for situations with many parallel write
     operations on a base table containing a set of unique hash indexes.

   * 
     'TransactionBufferMemory'

     *This table provides type and value information for the
     TransactionBufferMemory data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      bytes
                                          
     *Default*                            1M
                                          
     *Range*                              1K - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     The memory affected by this parameter is used for tracking
     operations fired when updating index tables and reading unique
     indexes.  This memory is used to store the key and column
     information for these operations.  It is only very rarely that the
     value for this parameter needs to be altered from the default.

     The default value for 'TransactionBufferMemory' is 1MB.

     Normal read and write operations use a similar buffer, whose usage
     is even more short-lived.  The compile-time parameter
     'ZATTRBUF_FILESIZE' (found in
     'ndb/src/kernel/blocks/Dbtc/Dbtc.hpp') set to 4000 x 128 bytes
     (500KB). A similar buffer for key information, 'ZDATABUF_FILESIZE'
     (also in 'Dbtc.hpp') contains 4000 x 16 = 62.5KB of buffer space.
     'Dbtc' is the module that handles transaction coordination.

Scans and buffering

There are additional '[ndbd]' parameters in the 'Dblqh' module (in
'ndb/src/kernel/blocks/Dblqh/Dblqh.hpp') that affect reads and updates.
These include 'ZATTRINBUF_FILESIZE', set by default to 10000 x 128 bytes
(1250KB) and 'ZDATABUF_FILE_SIZE', set by default to 10000*16 bytes
(roughly 156KB) of buffer space.  To date, there have been neither any
reports from users nor any results from our own extensive tests
suggesting that either of these compile-time limits should be increased.

   * 
     'MaxNoOfConcurrentScans'

     *This table provides type and value information for the
     MaxNoOfConcurrentScans data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      integer
                                          
     *Default*                            256
                                          
     *Range*                              2 - 500
                                          
     *Restart Type*                       N

     This parameter is used to control the number of parallel scans that
     can be performed in the cluster.  Each transaction coordinator can
     handle the number of parallel scans defined for this parameter.
     Each scan query is performed by scanning all partitions in
     parallel.  Each partition scan uses a scan record in the node where
     the partition is located, the number of records being the value of
     this parameter times the number of nodes.  The cluster should be
     able to sustain 'MaxNoOfConcurrentScans' scans concurrently from
     all nodes in the cluster.

     Scans are actually performed in two cases.  The first of these
     cases occurs when no hash or ordered indexes exists to handle the
     query, in which case the query is executed by performing a full
     table scan.  The second case is encountered when there is no hash
     index to support the query but there is an ordered index.  Using
     the ordered index means executing a parallel range scan.  The order
     is kept on the local partitions only, so it is necessary to perform
     the index scan on all partitions.

     The default value of 'MaxNoOfConcurrentScans' is 256.  The maximum
     value is 500.

   * 
     'MaxNoOfLocalScans'

     *This table provides type and value information for the
     MaxNoOfLocalScans data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      integer
                                          
     *Default*                            [see text]
                                          
     *Range*                              32 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     Specifies the number of local scan records if many scans are not
     fully parallelized.  In NDB 7.2.0 and later, when the number of
     local scan records is not provided, it is calculated as shown here:

          4 * MaxNoOfConcurrentScans * [# data nodes] + 2

     The minimum value is 32.

   * 
     'BatchSizePerLocalScan'

     *This table provides type and value information for the
     BatchSizePerLocalScan data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      integer
                                          
     *Default*                            256
                                          
     *Range*                              1 - 992
                                          
     *Restart Type*                       N

     This parameter is used to calculate the number of lock records used
     to handle concurrent scan operations.

     'BatchSizePerLocalScan' has a strong connection to the 'BatchSize'
     defined in the SQL nodes.

   * 
     'LongMessageBuffer'

     *This table provides type and value information for the
     LongMessageBuffer data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.16
                                          
     *Type or units*                      bytes
                                          
     *Default*                            64M
                                          
     *Range*                              512K - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     This is an internal buffer used for passing messages within
     individual nodes and between nodes.  The default is 64MB. (Prior to
     NDB 7.2.16, this was 4MB.)

     This parameter seldom needs to be changed from the default.

   * 
     'MaxParallelScansPerFragment'

     *This table provides type and value information for the
     MaxParallelScansPerFragment data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      bytes
                                          
     *Default*                            256
                                          
     *Range*                              1 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     It is possible to configure the maximum number of parallel scans
     ('TUP' scans and 'TUX' scans) allowed before they begin queuing for
     serial handling.  You can increase this to take advantage of any
     unused CPU when performing large number of scans in parallel and
     improve their performance.

     Beginning with NDB 7.2.0, the default value for this parameter was
     increased from 32 to 256.

*Memory Allocation*

'MaxAllocate'

*This table provides type and value information for the MaxAllocate data
node configuration parameter*

Property                             Value
                                     
*Version (or later)*                 NDB 7.2.1
                                     
*Type or units*                      unsigned
                                     
*Default*                            32M
                                     
*Range*                              1M - 1G
                                     
*Restart Type*                       N

This is the maximum size of the memory unit to use when allocating
memory for tables.  In cases where *note 'NDB': mysql-cluster. gives
'Out of memory' errors, but it is evident by examining the cluster logs
or the output of 'DUMP 1000'
(https://dev.mysql.com/doc/ndb-internals/en/dump-command-1000.html) that
all available memory has not yet been used, you can increase the value
of this parameter (or 'MaxNoOfTables', or both) to cause *note 'NDB':
mysql-cluster. to make sufficient memory available.

*Hash Map Size*

'DefaultHashMapSize'

*This table provides type and value information for the
DefaultHashMapSize data node configuration parameter*

Property                             Value
                                     
*Version (or later)*                 NDB 7.2.11
                                     
*Type or units*                      LDM threads
                                     
*Default*                            3840
                                     
*Range*                              0 - 3840
                                     
*Restart Type*                       N

NDB 7.2.7 and later use a larger default table hash map size (3840) than
in previous releases (240).  Beginning with NDB 7.2.11, the size of the
table hash maps used by *note 'NDB': mysql-cluster. is configurable
using this parameter; previously this value was hard-coded.
'DefaultHashMapSize' can take any of three possible values (0, 240,
3840).  These values and their effects are described in the following
table:

*DefaultHashMapSize parameters*

Value          Description / Effect
               
'0'            Use the lowest value set, if any, for this parameter
               among all data nodes and API nodes in the cluster; if it
               is not set on any data or API node, use the default
               value.
               
'240'          Original hash map size (used by default in all NDB
               Cluster releases prior to NDB 7.2.7)
               
'3840'         Larger hash map size (used by default beginning with NDB
               7.2.7)

The primary intended use for this parameter is to facilitate upgrades
and especially downgrades between NDB 7.2.7 and later NDB Cluster
versions, in which the larger hash map size (3840) is the default, and
earlier releases (in which the default was 240), due to the fact that
this change is not otherwise backward compatible (Bug #14800539).  By
setting this parameter to 240 prior to performing an upgrade from an
older version where this value is in use, you can cause the cluster to
continue using the smaller size for table hash maps, in which case the
tables remain compatible with earlier versions following the upgrade.
'DefaultHashMapSize' can be set for individual data nodes, API nodes, or
both, but setting it once only, in the '[ndbd default]' section of the
'config.ini' file, is the recommended practice.

After increasing this parameter, to have existing tables to take
advantage of the new size, you can run *note 'ALTER TABLE ... REORGANIZE
PARTITION': alter-table-partition-operations. on them, after which they
can use the larger hash map size.  This is in addition to performing a
rolling restart, which makes the larger hash maps available to new
tables, but does not enable existing tables to use them.

Decreasing this parameter online after any tables have been created or
modified with 'DefaultHashMapSize' equal to 3840 is not currently
supported.

Logging and checkpointing

The following '[ndbd]' parameters control log and checkpoint behavior.

   * 
     'NoOfFragmentLogFiles'

     *This table provides type and value information for the
     NoOfFragmentLogFiles data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      integer
                                          
     *Default*                            16
                                          
     *Range*                              3 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       IN

     This parameter sets the number of REDO log files for the node, and
     thus the amount of space allocated to REDO logging.  Because the
     REDO log files are organized in a ring, it is extremely important
     that the first and last log files in the set (sometimes referred to
     as the 'head' and 'tail' log files, respectively) do not meet.
     When these approach one another too closely, the node begins
     aborting all transactions encompassing updates due to a lack of
     room for new log records.

     A 'REDO' log record is not removed until the required number of
     local checkpoints has been completed since that log record was
     inserted.  (In NDB Cluster 7.2, only 2 local checkpoints are
     necessary).  Checkpointing frequency is determined by its own set
     of configuration parameters discussed elsewhere in this chapter.

     The default parameter value is 16, which by default means 16 sets
     of 4 16MB files for a total of 1024MB. The size of the individual
     log files is configurable using the 'FragmentLogFileSize'
     parameter.  In scenarios requiring a great many updates, the value
     for 'NoOfFragmentLogFiles' may need to be set as high as 300 or
     even higher to provide sufficient space for REDO logs.

     If the checkpointing is slow and there are so many writes to the
     database that the log files are full and the log tail cannot be cut
     without jeopardizing recovery, all updating transactions are
     aborted with internal error code 410 ('Out of log file space
     temporarily').  This condition prevails until a checkpoint has
     completed and the log tail can be moved forward.

     *Important*:

     This parameter cannot be changed 'on the fly'; you must restart the
     node using '--initial'.  If you wish to change this value for all
     data nodes in a running cluster, you can do so using a rolling node
     restart (using '--initial' when starting each data node).

   * 
     'FragmentLogFileSize'

     *This table provides type and value information for the
     FragmentLogFileSize data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      bytes
                                          
     *Default*                            16M
                                          
     *Range*                              4M - 1G
                                          
     *Restart Type*                       IN

     Setting this parameter enables you to control directly the size of
     redo log files.  This can be useful in situations when NDB Cluster
     is operating under a high load and it is unable to close fragment
     log files quickly enough before attempting to open new ones (only 2
     fragment log files can be open at one time); increasing the size of
     the fragment log files gives the cluster more time before having to
     open each new fragment log file.  The default value for this
     parameter is 16M.

     For more information about fragment log files, see the description
     for 'NoOfFragmentLogFiles'.

   * 
     'InitFragmentLogFiles'

     *This table provides type and value information for the
     InitFragmentLogFiles data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      [see values]
                                          
     *Default*                            SPARSE
                                          
     *Range*                              SPARSE, FULL
                                          
     *Restart Type*                       IN

     By default, fragment log files are created sparsely when performing
     an initial start of a data node--that is, depending on the
     operating system and file system in use, not all bytes are
     necessarily written to disk.  However, it is possible to override
     this behavior and force all bytes to be written, regardless of the
     platform and file system type being used, by means of this
     parameter.  'InitFragmentLogFiles' takes either of two values:

        * 'SPARSE'.  Fragment log files are created sparsely.  This is
          the default value.

        * 'FULL'.  Force all bytes of the fragment log file to be
          written to disk.

     Depending on your operating system and file system, setting
     'InitFragmentLogFiles=FULL' may help eliminate I/O errors on writes
     to the REDO log.

   * 
     'MaxNoOfOpenFiles'

     *This table provides type and value information for the
     MaxNoOfOpenFiles data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      unsigned
                                          
     *Default*                            0
                                          
     *Range*                              20 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     This parameter sets a ceiling on how many internal threads to
     allocate for open files.  _Any situation requiring a change in this
     parameter should be reported as a bug_.

     The default value is 0.  However, the minimum value to which this
     parameter can be set is 20.

   * 
     'InitialNoOfOpenFiles'

     *This table provides type and value information for the
     InitialNoOfOpenFiles data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      files
                                          
     *Default*                            27
                                          
     *Range*                              20 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     This parameter sets the initial number of internal threads to
     allocate for open files.

     The default value is 27.

   * 
     'MaxNoOfSavedMessages'

     *This table provides type and value information for the
     MaxNoOfSavedMessages data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      integer
                                          
     *Default*                            25
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     This parameter sets the maximum number of errors written in the
     error log as well as the maximum number of trace files that are
     kept before overwriting the existing ones.  Trace files are
     generated when, for whatever reason, the node crashes.

     The default is 25, which sets these maximums to 25 error messages
     and 25 trace files.

   * 
     'MaxLCPStartDelay'

     *This table provides type and value information for the
     MaxLCPStartDelay data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      seconds
                                          
     *Default*                            0
                                          
     *Range*                              0 - 600
                                          
     *Restart Type*                       N

     In parallel data node recovery, only table data is actually copied
     and synchronized in parallel; synchronization of metadata such as
     dictionary and checkpoint information is done in a serial fashion.
     In addition, recovery of dictionary and checkpoint information
     cannot be executed in parallel with performing of local
     checkpoints.  This means that, when starting or restarting many
     data nodes concurrently, data nodes may be forced to wait while a
     local checkpoint is performed, which can result in longer node
     recovery times.

     It is possible to force a delay in the local checkpoint to permit
     more (and possibly all) data nodes to complete metadata
     synchronization; once each data node's metadata synchronization is
     complete, all of the data nodes can recover table data in parallel,
     even while the local checkpoint is being executed.  To force such a
     delay, set 'MaxLCPStartDelay', which determines the number of
     seconds the cluster can wait to begin a local checkpoint while data
     nodes continue to synchronize metadata.  This parameter should be
     set in the '[ndbd default]' section of the 'config.ini' file, so
     that it is the same for all data nodes.  The maximum value is 600;
     the default is 0.

   * 
     'LcpScanProgressTimeout'

     *This table provides type and value information for the
     LcpScanProgressTimeout data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.14
                                          
     *Type or units*                      second
                                          
     *Default*                            60
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     A local checkpoint fragment scan watchdog checks periodically for
     no progress in each fragment scan performed as part of a local
     checkpoint, and shuts down the node if there is no progress after a
     given amount of time has elapsed.  Prior to NDB 7.2.14, this
     interval is always 60 seconds (Bug #16630410).  In NDB 7.2.14 and
     later, this interval can be set using the 'LcpScanProgressTimeout'
     data node configuration parameter, which sets the maximum time for
     which the local checkpoint can be stalled before the LCP fragment
     scan watchdog shuts down the node.

     The default value is 60 seconds (providing compatibility with
     previous releases).  Setting this parameter to 0 disables the LCP
     fragment scan watchdog altogether.

Metadata objects

The next set of '[ndbd]' parameters defines pool sizes for metadata
objects, used to define the maximum number of attributes, tables,
indexes, and trigger objects used by indexes, events, and replication
between clusters.  Note that these act merely as 'suggestions' to the
cluster, and any that are not specified revert to the default values
shown.

   * 
     'MaxNoOfAttributes'

     *This table provides type and value information for the
     MaxNoOfAttributes data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      integer
                                          
     *Default*                            1000
                                          
     *Range*                              32 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     This parameter sets a suggested maximum number of attributes that
     can be defined in the cluster; like 'MaxNoOfTables', it is not
     intended to function as a hard upper limit.

     (In older NDB Cluster releases, this parameter was sometimes
     treated as a hard limit for certain operations.  This caused
     problems with NDB Cluster Replication, when it was possible to
     create more tables than could be replicated, and sometimes led to
     confusion when it was possible [or not possible, depending on the
     circumstances] to create more than 'MaxNoOfAttributes' attributes.)

     The default value is 1000, with the minimum possible value being
     32.  The maximum is 4294967039.  Each attribute consumes around 200
     bytes of storage per node due to the fact that all metadata is
     fully replicated on the servers.

     When setting 'MaxNoOfAttributes', it is important to prepare in
     advance for any *note 'ALTER TABLE': alter-table. statements that
     you might want to perform in the future.  This is due to the fact,
     during the execution of *note 'ALTER TABLE': alter-table. on a
     Cluster table, 3 times the number of attributes as in the original
     table are used, and a good practice is to permit double this
     amount.  For example, if the NDB Cluster table having the greatest
     number of attributes (GREATEST_NUMBER_OF_ATTRIBUTES) has 100
     attributes, a good starting point for the value of
     'MaxNoOfAttributes' would be '6 * GREATEST_NUMBER_OF_ATTRIBUTES =
     600'.

     You should also estimate the average number of attributes per table
     and multiply this by 'MaxNoOfTables'.  If this value is larger than
     the value obtained in the previous paragraph, you should use the
     larger value instead.

     Assuming that you can create all desired tables without any
     problems, you should also verify that this number is sufficient by
     trying an actual *note 'ALTER TABLE': alter-table. after
     configuring the parameter.  If this is not successful, increase
     'MaxNoOfAttributes' by another multiple of 'MaxNoOfTables' and test
     it again.

   * 
     'MaxNoOfTables'

     *This table provides type and value information for the
     MaxNoOfTables data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      integer
                                          
     *Default*                            128
                                          
     *Range*                              8 - 20320
                                          
     *Restart Type*                       N

     A table object is allocated for each table and for each unique hash
     index in the cluster.  This parameter sets a suggested maximum
     number of table objects for the cluster as a whole; like
     'MaxNoOfAttributes', it is not intended to function as a hard upper
     limit.

     (In older NDB Cluster releases, this parameter was sometimes
     treated as a hard limit for certain operations.  This caused
     problems with NDB Cluster Replication, when it was possible to
     create more tables than could be replicated, and sometimes led to
     confusion when it was possible [or not possible, depending on the
     circumstances] to create more than 'MaxNoOfTables' tables.)

     For each attribute that has a *note 'BLOB': blob. data type an
     extra table is used to store most of the *note 'BLOB': blob. data.
     These tables also must be taken into account when defining the
     total number of tables.

     The default value of this parameter is 128.  The minimum is 8 and
     the maximum is 20320.  Each table object consumes approximately
     20KB per node.

     *Note*:

     The sum of 'MaxNoOfTables', 'MaxNoOfOrderedIndexes', and
     'MaxNoOfUniqueHashIndexes' must not exceed '2^32 − 2'
     (4294967294).

   * 
     'MaxNoOfOrderedIndexes'

     *This table provides type and value information for the
     MaxNoOfOrderedIndexes data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      integer
                                          
     *Default*                            128
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     For each ordered index in the cluster, an object is allocated
     describing what is being indexed and its storage segments.  By
     default, each index so defined also defines an ordered index.  Each
     unique index and primary key has both an ordered index and a hash
     index.  'MaxNoOfOrderedIndexes' sets the total number of ordered
     indexes that can be in use in the system at any one time.

     The default value of this parameter is 128.  Each index object
     consumes approximately 10KB of data per node.

     *Note*:

     The sum of 'MaxNoOfTables', 'MaxNoOfOrderedIndexes', and
     'MaxNoOfUniqueHashIndexes' must not exceed '2^32 − 2'
     (4294967294).

   * 
     'MaxNoOfUniqueHashIndexes'

     *This table provides type and value information for the
     MaxNoOfUniqueHashIndexes data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      integer
                                          
     *Default*                            64
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     For each unique index that is not a primary key, a special table is
     allocated that maps the unique key to the primary key of the
     indexed table.  By default, an ordered index is also defined for
     each unique index.  To prevent this, you must specify the 'USING
     HASH' option when defining the unique index.

     The default value is 64.  Each index consumes approximately 15KB
     per node.

     *Note*:

     The sum of 'MaxNoOfTables', 'MaxNoOfOrderedIndexes', and
     'MaxNoOfUniqueHashIndexes' must not exceed '2^32 − 2'
     (4294967294).

   * 
     'MaxNoOfTriggers'

     *This table provides type and value information for the
     MaxNoOfTriggers data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      integer
                                          
     *Default*                            768
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     Internal update, insert, and delete triggers are allocated for each
     unique hash index.  (This means that three triggers are created for
     each unique hash index.)  However, an _ordered_ index requires only
     a single trigger object.  Backups also use three trigger objects
     for each normal table in the cluster.

     Replication between clusters also makes use of internal triggers.

     This parameter sets the maximum number of trigger objects in the
     cluster.

     The default value is 768.

   * 
     'MaxNoOfIndexes'

     This parameter is deprecated and subject to removal in a future
     version of NDB Cluster.  You should use 'MaxNoOfOrderedIndexes' and
     'MaxNoOfUniqueHashIndexes' instead.

     This parameter is used only by unique hash indexes.  There needs to
     be one record in this pool for each unique hash index defined in
     the cluster.

     The default value of this parameter is 128.

   * 
     'MaxNoOfSubscriptions'

     *This table provides type and value information for the
     MaxNoOfSubscriptions data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      unsigned
                                          
     *Default*                            0
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     Each *note 'NDB': mysql-cluster. table in an NDB Cluster requires a
     subscription in the NDB kernel.  For some NDB API applications, it
     may be necessary or desirable to change this parameter.  However,
     for normal usage with MySQL servers acting as SQL nodes, there is
     not any need to do so.

     The default value for 'MaxNoOfSubscriptions' is 0, which is treated
     as equal to 'MaxNoOfTables'.  Each subscription consumes 108 bytes.

   * 
     'MaxNoOfSubscribers'

     *This table provides type and value information for the
     MaxNoOfSubscribers data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      unsigned
                                          
     *Default*                            0
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     This parameter is of interest only when using NDB Cluster
     Replication.  The default value is 0, which is treated as '2 *
     MaxNoOfTables'; that is, there is one subscription per *note 'NDB':
     mysql-cluster. table for each of two MySQL servers (one acting as
     the replication master and the other as the slave).  Each
     subscriber uses 16 bytes of memory.

     When using circular replication, multi-master replication, and
     other replication setups involving more than 2 MySQL servers, you
     should increase this parameter to the number of *note 'mysqld':
     mysqld. processes included in replication (this is often, but not
     always, the same as the number of clusters).  For example, if you
     have a circular replication setup using three NDB Clusters, with
     one *note 'mysqld': mysqld. attached to each cluster, and each of
     these *note 'mysqld': mysqld. processes acts as a master and as a
     slave, you should set 'MaxNoOfSubscribers' equal to '3 *
     MaxNoOfTables'.

     For more information, see *note mysql-cluster-replication::.

   * 
     'MaxNoOfConcurrentSubOperations'

     *This table provides type and value information for the
     MaxNoOfConcurrentSubOperations data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      unsigned
                                          
     *Default*                            256
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     This parameter sets a ceiling on the number of operations that can
     be performed by all API nodes in the cluster at one time.  The
     default value (256) is sufficient for normal operations, and might
     need to be adjusted only in scenarios where there are a great many
     API nodes each performing a high volume of operations concurrently.

Boolean parameters

The behavior of data nodes is also affected by a set of '[ndbd]'
parameters taking on boolean values.  These parameters can each be
specified as 'TRUE' by setting them equal to '1' or 'Y', and as 'FALSE'
by setting them equal to '0' or 'N'.

   * 
     'LateAlloc'

     *This table provides type and value information for the LateAlloc
     data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      numeric
                                          
     *Default*                            1
                                          
     *Range*                              0 - 1
                                          
     *Restart Type*                       N

     Allocate memory for this data node after a connection to the
     management server has been established.  Enabled by default.

   * 
     'LockPagesInMainMemory'

     *This table provides type and value information for the
     LockPagesInMainMemory data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      numeric
                                          
     *Default*                            0
                                          
     *Range*                              0 - 2
                                          
     *Restart Type*                       N

     For a number of operating systems, including Solaris and Linux, it
     is possible to lock a process into memory and so avoid any swapping
     to disk.  This can be used to help guarantee the cluster's
     real-time characteristics.

     This parameter takes one of the integer values '0', '1', or '2',
     which act as shown in the following list:

        * '0': Disables locking.  This is the default value.

        * '1': Performs the lock after allocating memory for the
          process.

        * '2': Performs the lock before memory for the process is
          allocated.

     If the operating system is not configured to permit unprivileged
     users to lock pages, then the data node process making use of this
     parameter may have to be run as system root.
     ('LockPagesInMainMemory' uses the 'mlockall' function.  From Linux
     kernel 2.6.9, unprivileged users can lock memory as limited by 'max
     locked memory'.  For more information, see 'ulimit -l' and
     <http://linux.die.net/man/2/mlock>).

     *Note*:

     In older NDB Cluster releases, this parameter was a Boolean.  '0'
     or 'false' was the default setting, and disabled locking.  '1' or
     'true' enabled locking of the process after its memory was
     allocated.  In NDB Cluster 7.2, using 'true' or 'false' as the
     value of this parameter causes an error.

     *Important*:

     Beginning with 'glibc' 2.10, 'glibc' uses per-thread arenas to
     reduce lock contention on a shared pool, which consumes real
     memory.  In general, a data node process does not need per-thread
     arenas, since it does not perform any memory allocation after
     startup.  (This difference in allocators does not appear to affect
     performance significantly.)

     The 'glibc' behavior is intended to be configurable via the
     'MALLOC_ARENA_MAX' environment variable, but a bug in this
     mechanism prior to 'glibc' 2.16 meant that this variable could not
     be set to less than 8, so that the wasted memory could not be
     reclaimed.  (Bug #15907219; see also
     <http://sourceware.org/bugzilla/show_bug.cgi?id=13137> for more
     information concerning this issue.)

     One possible workaround for this problem is to use the 'LD_PRELOAD'
     environment variable to preload a 'jemalloc' memory allocation
     library to take the place of that supplied with 'glibc'.

   * 
     'StopOnError'

     *This table provides type and value information for the StopOnError
     data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      boolean
                                          
     *Default*                            1
                                          
     *Range*                              0, 1
                                          
     *Restart Type*                       N

     This parameter specifies whether a data node process should exit or
     perform an automatic restart when an error condition is
     encountered.

     This parameter's default value is 1; this means that, by default,
     an error causes the data node process to halt.

     When an error is encountered and 'StopOnError' is 0, the data node
     process is restarted.

     *Important*:

     If the data node process exits in an uncontrolled fashion (due, for
     example, to performing *note 'kill -9': kill. on the data node
     process while performing a query, or to a segmentation fault), and
     'StopOnError' is set to 0, the angel process attempts to restart it
     in exactly the same way as it was started previously--that is,
     using the same startup options that were employed the last time the
     node was started.  Thus, if the data node process was originally
     started using the '--initial' option, it is also restarted with
     '--initial'.  This means that, in such cases, if the failure occurs
     on a sufficient number of data nodes in a very short interval, the
     effect is the same as if you had performed an initial restart of
     the entire cluster, leading to loss of all data.  (Bug #24945638)

     Users of MySQL Cluster Manager should note that, when 'StopOnError'
     equals 1, this prevents the MySQL Cluster Manager agent from
     restarting any data nodes after it has performed its own restart
     and recovery.  See Starting and Stopping the Agent on Linux
     (https://dev.mysql.com/doc/mysql-cluster-manager/1.4/en/mcm-using-start-stop-agent-linux.html),
     for more information.

   * 
     'CrashOnCorruptedTuple'

     *This table provides type and value information for the
     CrashOnCorruptedTuple data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      boolean
                                          
     *Default*                            true
                                          
     *Range*                              true, false
                                          
     *Restart Type*                       S

     When this parameter is enabled, it forces a data node to shut down
     whenever it encounters a corrupted tuple.  In NDB 7.2.1 and later,
     it is enabled by default.  This is a change from NDB Cluster 7.0
     and NDB Cluster 7.1, where it was disabled by default.

   * 
     'Diskless'

     *This table provides type and value information for the Diskless
     data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      true|false (1|0)
                                          
     *Default*                            false
                                          
     *Range*                              true, false
                                          
     *Restart Type*                       IS

     It is possible to specify NDB Cluster tables as _diskless_, meaning
     that tables are not checkpointed to disk and that no logging
     occurs.  Such tables exist only in main memory.  A consequence of
     using diskless tables is that neither the tables nor the records in
     those tables survive a crash.  However, when operating in diskless
     mode, it is possible to run *note 'ndbd':
     mysql-cluster-programs-ndbd. on a diskless computer.

     *Important*:

     This feature causes the _entire_ cluster to operate in diskless
     mode.

     When this feature is enabled, Cluster online backup is disabled.
     In addition, a partial start of the cluster is not possible.

     'Diskless' is disabled by default.

   * 
     'ODirect'

     *This table provides type and value information for the ODirect
     data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      boolean
                                          
     *Default*                            false
                                          
     *Range*                              true, false
                                          
     *Restart Type*                       N

     Enabling this parameter causes *note 'NDB': mysql-cluster. to
     attempt using 'O_DIRECT' writes for LCP, backups, and redo logs,
     often lowering 'kswapd' and CPU usage.  When using NDB Cluster on
     Linux, enable 'ODirect' if you are using a 2.6 or later kernel.

     'ODirect' is disabled by default.

   * 
     'RestartOnErrorInsert'

     *This table provides type and value information for the
     RestartOnErrorInsert data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      error code
                                          
     *Default*                            2
                                          
     *Range*                              0 - 4
                                          
     *Restart Type*                       N

     This feature is accessible only when building the debug version
     where it is possible to insert errors in the execution of
     individual blocks of code as part of testing.

     This feature is disabled by default.

   * 
     'CompressedBackup'

     *This table provides type and value information for the
     CompressedBackup data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      boolean
                                          
     *Default*                            false
                                          
     *Range*                              true, false
                                          
     *Restart Type*                       N

     Enabling this parameter causes backup files to be compressed.  The
     compression used is equivalent to 'gzip --fast', and can save 50%
     or more of the space required on the data node to store
     uncompressed backup files.  Compressed backups can be enabled for
     individual data nodes, or for all data nodes (by setting this
     parameter in the '[ndbd default]' section of the 'config.ini'
     file).

     *Important*:

     You cannot restore a compressed backup to a cluster running a MySQL
     version that does not support this feature.

     The default value is '0' (disabled).

   * 
     'CompressedLCP'

     *This table provides type and value information for the
     CompressedLCP data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      boolean
                                          
     *Default*                            false
                                          
     *Range*                              true, false
                                          
     *Restart Type*                       N

     Setting this parameter to '1' causes local checkpoint files to be
     compressed.  The compression used is equivalent to 'gzip --fast',
     and can save 50% or more of the space required on the data node to
     store uncompressed checkpoint files.  Compressed LCPs can be
     enabled for individual data nodes, or for all data nodes (by
     setting this parameter in the '[ndbd default]' section of the
     'config.ini' file).

     *Important*:

     You cannot restore a compressed local checkpoint to a cluster
     running a MySQL version that does not support this feature.

     The default value is '0' (disabled).

*Controlling Timeouts, Intervals, and Disk Paging*

There are a number of '[ndbd]' parameters specifying timeouts and
intervals between various actions in Cluster data nodes.  Most of the
timeout values are specified in milliseconds.  Any exceptions to this
are mentioned where applicable.

   * 
     'TimeBetweenWatchDogCheck'

     *This table provides type and value information for the
     TimeBetweenWatchDogCheck data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      milliseconds
                                          
     *Default*                            6000
                                          
     *Range*                              70 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     To prevent the main thread from getting stuck in an endless loop at
     some point, a 'watchdog' thread checks the main thread.  This
     parameter specifies the number of milliseconds between checks.  If
     the process remains in the same state after three checks, the
     watchdog thread terminates it.

     This parameter can easily be changed for purposes of
     experimentation or to adapt to local conditions.  It can be
     specified on a per-node basis although there seems to be little
     reason for doing so.

     The default timeout is 6000 milliseconds (6 seconds).

   * 
     'TimeBetweenWatchDogCheckInitial'

     *This table provides type and value information for the
     TimeBetweenWatchDogCheckInitial data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      milliseconds
                                          
     *Default*                            6000
                                          
     *Range*                              70 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     This is similar to the 'TimeBetweenWatchDogCheck' parameter, except
     that 'TimeBetweenWatchDogCheckInitial' controls the amount of time
     that passes between execution checks inside a storage node in the
     early start phases during which memory is allocated.

     The default timeout is 6000 milliseconds (6 seconds).

   * 
     'StartPartialTimeout'

     *This table provides type and value information for the
     StartPartialTimeout data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      milliseconds
                                          
     *Default*                            30000
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     This parameter specifies how long the Cluster waits for all data
     nodes to come up before the cluster initialization routine is
     invoked.  This timeout is used to avoid a partial Cluster startup
     whenever possible.

     This parameter is overridden when performing an initial start or
     initial restart of the cluster.

     The default value is 30000 milliseconds (30 seconds).  0 disables
     the timeout, in which case the cluster may start only if all nodes
     are available.

   * 
     'StartPartitionedTimeout'

     *This table provides type and value information for the
     StartPartitionedTimeout data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      milliseconds
                                          
     *Default*                            60000
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     If the cluster is ready to start after waiting for
     'StartPartialTimeout' milliseconds but is still possibly in a
     partitioned state, the cluster waits until this timeout has also
     passed.  If 'StartPartitionedTimeout' is set to 0, the cluster
     waits indefinitely.

     This parameter is overridden when performing an initial start or
     initial restart of the cluster.

     The default timeout is 60000 milliseconds (60 seconds).

   * 
     'StartFailureTimeout'

     *This table provides type and value information for the
     StartFailureTimeout data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      milliseconds
                                          
     *Default*                            0
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     If a data node has not completed its startup sequence within the
     time specified by this parameter, the node startup fails.  Setting
     this parameter to 0 (the default value) means that no data node
     timeout is applied.

     For nonzero values, this parameter is measured in milliseconds.
     For data nodes containing extremely large amounts of data, this
     parameter should be increased.  For example, in the case of a data
     node containing several gigabytes of data, a period as long as
     10−15 minutes (that is, 600000 to 1000000 milliseconds) might be
     required to perform a node restart.

   * 
     'StartNoNodeGroupTimeout'

     *This table provides type and value information for the
     StartNoNodeGroupTimeout data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      milliseconds
                                          
     *Default*                            15000
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     When a data node is configured with 'Nodegroup = 65536', is
     regarded as not being assigned to any node group.  When that is
     done, the cluster waits 'StartNoNodegroupTimeout' milliseconds,
     then treats such nodes as though they had been added to the list
     passed to the '--nowait-nodes' option, and starts.  The default
     value is '15000' (that is, the management server waits 15 seconds).
     Setting this parameter equal to '0' means that the cluster waits
     indefinitely.

     'StartNoNodegroupTimeout' must be the same for all data nodes in
     the cluster; for this reason, you should always set it in the
     '[ndbd default]' section of the 'config.ini' file, rather than for
     individual data nodes.

     See *note mysql-cluster-online-add-node::, for more information.

   * 
     'HeartbeatIntervalDbDb'

     *This table provides type and value information for the
     HeartbeatIntervalDbDb data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      milliseconds
                                          
     *Default*                            5000
                                          
     *Range*                              10 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     One of the primary methods of discovering failed nodes is by the
     use of heartbeats.  This parameter states how often heartbeat
     signals are sent and how often to expect to receive them.
     Heartbeats cannot be disabled.

     After missing four heartbeat intervals in a row, the node is
     declared dead.  Thus, the maximum time for discovering a failure
     through the heartbeat mechanism is five times the heartbeat
     interval.

     In NDB 7.2.0 and later, the default heartbeat interval is 5000
     milliseconds (5 seconds).  (Previously, the default was 1500
     milliseconds [1.5 seconds]).  This parameter must not be changed
     drastically and should not vary widely between nodes.  If one node
     uses 5000 milliseconds and the node watching it uses 1000
     milliseconds, obviously the node will be declared dead very
     quickly.  This parameter can be changed during an online software
     upgrade, but only in small increments.

     See also *note mysql-cluster-network-latency-issues::, as well as
     the description of the 'ConnectCheckIntervalDelay' configuration
     parameter.

   * 
     'HeartbeatIntervalDbApi'

     *This table provides type and value information for the
     HeartbeatIntervalDbApi data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      milliseconds
                                          
     *Default*                            1500
                                          
     *Range*                              100 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     Each data node sends heartbeat signals to each MySQL server (SQL
     node) to ensure that it remains in contact.  If a MySQL server
     fails to send a heartbeat in time it is declared 'dead,' in which
     case all ongoing transactions are completed and all resources
     released.  The SQL node cannot reconnect until all activities
     initiated by the previous MySQL instance have been completed.  The
     three-heartbeat criteria for this determination are the same as
     described for 'HeartbeatIntervalDbDb'.

     The default interval is 1500 milliseconds (1.5 seconds).  This
     interval can vary between individual data nodes because each data
     node watches the MySQL servers connected to it, independently of
     all other data nodes.

     For more information, see *note
     mysql-cluster-network-latency-issues::.

   * 
     'HeartbeatOrder'

     *This table provides type and value information for the
     HeartbeatOrder data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      numeric
                                          
     *Default*                            0
                                          
     *Range*                              0 - 65535
                                          
     *Restart Type*                       S

     Data nodes send heartbeats to one another in a circular fashion
     whereby each data node monitors the previous one.  If a heartbeat
     is not detected by a given data node, this node declares the
     previous data node in the circle 'dead' (that is, no longer
     accessible by the cluster).  The determination that a data node is
     dead is done globally; in other words; once a data node is declared
     dead, it is regarded as such by all nodes in the cluster.

     It is possible for heartbeats between data nodes residing on
     different hosts to be too slow compared to heartbeats between other
     pairs of nodes (for example, due to a very low heartbeat interval
     or temporary connection problem), such that a data node is declared
     dead, even though the node can still function as part of the
     cluster.  .

     In this type of situation, it may be that the order in which
     heartbeats are transmitted between data nodes makes a difference as
     to whether or not a particular data node is declared dead.  If this
     declaration occurs unnecessarily, this can in turn lead to the
     unnecessary loss of a node group and as thus to a failure of the
     cluster.

     Consider a setup where there are 4 data nodes A, B, C, and D
     running on 2 host computers 'host1' and 'host2', and that these
     data nodes make up 2 node groups, as shown in the following table:

     *Four data nodes A, B, C, D running on two host computers host1,
     host2; each data node belongs to one of two node groups.*

     Node Group               Nodes Running on         Nodes Running on
                              'host1'                  'host2'
                                                       
     _Node Group 0_:          Node A                   Node B
                                                       
     _Node Group 1_:          Node C                   Node D
                              

     Suppose the heartbeats are transmitted in the order A->B->C->D->A.
     In this case, the loss of the heartbeat between the hosts causes
     node B to declare node A dead and node C to declare node B dead.
     This results in loss of Node Group 0, and so the cluster fails.  On
     the other hand, if the order of transmission is A->B->D->C->A (and
     all other conditions remain as previously stated), the loss of the
     heartbeat causes nodes A and D to be declared dead; in this case,
     each node group has one surviving node, and the cluster survives.

     The 'HeartbeatOrder' configuration parameter makes the order of
     heartbeat transmission user-configurable.  The default value for
     'HeartbeatOrder' is zero; allowing the default value to be used on
     all data nodes causes the order of heartbeat transmission to be
     determined by 'NDB'.  If this parameter is used, it must be set to
     a nonzero value (maximum 65535) for every data node in the cluster,
     and this value must be unique for each data node; this causes the
     heartbeat transmission to proceed from data node to data node in
     the order of their 'HeartbeatOrder' values from lowest to highest
     (and then directly from the data node having the highest
     'HeartbeatOrder' to the data node having the lowest value, to
     complete the circle).  The values need not be consecutive.  For
     example, to force the heartbeat transmission order A->B->D->C->A in
     the scenario outlined previously, you could set the
     'HeartbeatOrder' values as shown here:

     *HeartbeatOrder values to force a heartbeat transition order of
     A->B->D->C->A.*

     Node                                 'HeartbeatOrder' Value
                                          
     A                                    10
                                          
     B                                    20
                                          
     C                                    30
                                          
     D                                    25

     To use this parameter to change the heartbeat transmission order in
     a running NDB Cluster, you must first set 'HeartbeatOrder' for each
     data node in the cluster in the global configuration ('config.ini')
     file (or files).  To cause the change to take effect, you must
     perform either of the following:

        * A complete shutdown and restart of the entire cluster.

        * 2 rolling restarts of the cluster in succession.  _All nodes
          must be restarted in the same order in both rolling restarts_.

     You can use 'DUMP 908'
     (https://dev.mysql.com/doc/ndb-internals/en/dump-command-908.html)
     to observe the effect of this parameter in the data node logs.

   * 
     'ConnectCheckIntervalDelay'

     *This table provides type and value information for the
     ConnectCheckIntervalDelay data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      milliseconds
                                          
     *Default*                            0
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     This parameter enables connection checking between data nodes after
     one of them has failed heartbeat checks for 5 intervals of up to
     'HeartbeatIntervalDbDb' milliseconds.

     Such a data node that further fails to respond within an interval
     of 'ConnectCheckIntervalDelay' milliseconds is considered suspect,
     and is considered dead after two such intervals.  This can be
     useful in setups with known latency issues.

     The default value for this parameter is 0 (disabled); this
     represents a change from NDB Cluster 7.1.

   * 
     'TimeBetweenLocalCheckpoints'

     *This table provides type and value information for the
     TimeBetweenLocalCheckpoints data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      number of 4-byte words, as a
                                          base-2 logarithm
                                          
     *Default*                            20
                                          
     *Range*                              0 - 31
                                          
     *Restart Type*                       N

     This parameter is an exception in that it does not specify a time
     to wait before starting a new local checkpoint; rather, it is used
     to ensure that local checkpoints are not performed in a cluster
     where relatively few updates are taking place.  In most clusters
     with high update rates, it is likely that a new local checkpoint is
     started immediately after the previous one has been completed.

     The size of all write operations executed since the start of the
     previous local checkpoints is added.  This parameter is also
     exceptional in that it is specified as the base-2 logarithm of the
     number of 4-byte words, so that the default value 20 means 4MB (4 x
     2^20) of write operations, 21 would mean 8MB, and so on up to a
     maximum value of 31, which equates to 8GB of write operations.

     All the write operations in the cluster are added together.
     Setting 'TimeBetweenLocalCheckpoints' to 6 or less means that local
     checkpoints will be executed continuously without pause,
     independent of the cluster's workload.

   * 
     'TimeBetweenGlobalCheckpoints'

     *This table provides type and value information for the
     TimeBetweenGlobalCheckpoints data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      milliseconds
                                          
     *Default*                            2000
                                          
     *Range*                              20 - 32000
                                          
     *Restart Type*                       N

     When a transaction is committed, it is committed in main memory in
     all nodes on which the data is mirrored.  However, transaction log
     records are not flushed to disk as part of the commit.  The
     reasoning behind this behavior is that having the transaction
     safely committed on at least two autonomous host machines should
     meet reasonable standards for durability.

     It is also important to ensure that even the worst of cases--a
     complete crash of the cluster--is handled properly.  To guarantee
     that this happens, all transactions taking place within a given
     interval are put into a global checkpoint, which can be thought of
     as a set of committed transactions that has been flushed to disk.
     In other words, as part of the commit process, a transaction is
     placed in a global checkpoint group.  Later, this group's log
     records are flushed to disk, and then the entire group of
     transactions is safely committed to disk on all computers in the
     cluster.

     This parameter defines the interval between global checkpoints.
     The default is 2000 milliseconds.

   * 
     'TimeBetweenGlobalCheckpointsTimeout'

     *This table provides type and value information for the
     TimeBetweenGlobalCheckpointsTimeout data node configuration
     parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.20
                                          
     *Type or units*                      milliseconds
                                          
     *Default*                            120000
                                          
     *Range*                              10 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     This parameter defines the minimum timeout between global
     checkpoints.  The default is 120000 milliseconds.

     This parameter was added in NDB 7.2.20.  (Bug #20069617)

   * 
     'TimeBetweenEpochs'

     *This table provides type and value information for the
     TimeBetweenEpochs data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      milliseconds
                                          
     *Default*                            100
                                          
     *Range*                              0 - 32000
                                          
     *Restart Type*                       N

     This parameter defines the interval between synchronization epochs
     for NDB Cluster Replication.  The default value is 100
     milliseconds.

     'TimeBetweenEpochs' is part of the implementation of 'micro-GCPs',
     which can be used to improve the performance of NDB Cluster
     Replication.

   * 
     'TimeBetweenEpochsTimeout'

     *This table provides type and value information for the
     TimeBetweenEpochsTimeout data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      milliseconds
                                          
     *Default*                            0
                                          
     *Range*                              0 - 256000
                                          
     *Restart Type*                       N

     This parameter defines a timeout for synchronization epochs for NDB
     Cluster Replication.  If a node fails to participate in a global
     checkpoint within the time determined by this parameter, the node
     is shut down.  In NDB 7.2.0 and later, the default value is 0; in
     other words, the timeout is disabled.  This represents a change
     from previous versions of NDB Cluster, in which the default value
     was 4000 milliseconds (4 seconds).

     'TimeBetweenEpochsTimeout' is part of the implementation of
     'micro-GCPs', which can be used to improve the performance of NDB
     Cluster Replication.

     The current value of this parameter and a warning are written to
     the cluster log whenever a GCP save takes longer than 1 minute or a
     GCP commit takes longer than 10 seconds.

     Setting this parameter to zero has the effect of disabling GCP
     stops caused by save timeouts, commit timeouts, or both.  The
     maximum possible value for this parameter is 256000 milliseconds.

   * 
     'MaxBufferedEpochs'

     *This table provides type and value information for the
     MaxBufferedEpochs data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      epochs
                                          
     *Default*                            100
                                          
     *Range*                              0 - 100000
                                          
     *Restart Type*                       N

     The number of unprocessed epochs by which a subscribing node can
     lag behind.  Exceeding this number causes a lagging subscriber to
     be disconnected.

     The default value of 100 is sufficient for most normal operations.
     If a subscribing node does lag enough to cause disconnections, it
     is usually due to network or scheduling issues with regard to
     processes or threads.  (In rare circumstances, the problem may be
     due to a bug in the *note 'NDB': mysql-cluster. client.)  It may be
     desirable to set the value lower than the default when epochs are
     longer.

     Disconnection prevents client issues from affecting the data node
     service, running out of memory to buffer data, and eventually
     shutting down.  Instead, only the client is affected as a result of
     the disconnect (by, for example gap events in the binary log),
     forcing the client to reconnect or restart the process.

   * 
     'MaxBufferedEpochBytes'

     *This table provides type and value information for the
     MaxBufferedEpochBytes data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.13
                                          
     *Type or units*                      bytes
                                          
     *Default*                            26214400
                                          
     *Range*                              26214400 (0x01900000) - 4294967039
                                          (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     The total number of bytes allocated for buffering epochs by this
     node.

     This parameter was introduced in NDB 7.2.13.  (Bug #16203623)

   * 
     'TimeBetweenInactiveTransactionAbortCheck'

     *This table provides type and value information for the
     TimeBetweenInactiveTransactionAbortCheck data node configuration
     parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      milliseconds
                                          
     *Default*                            1000
                                          
     *Range*                              1000 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     Timeout handling is performed by checking a timer on each
     transaction once for every interval specified by this parameter.
     Thus, if this parameter is set to 1000 milliseconds, every
     transaction will be checked for timing out once per second.

     The default value is 1000 milliseconds (1 second).

   * 
     'TransactionInactiveTimeout'

     *This table provides type and value information for the
     TransactionInactiveTimeout data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      milliseconds
                                          
     *Default*                            [see text]
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     This parameter states the maximum time that is permitted to lapse
     between operations in the same transaction before the transaction
     is aborted.

     The default for this parameter is '4G' (also the maximum).  For a
     real-time database that needs to ensure that no transaction keeps
     locks for too long, this parameter should be set to a relatively
     small value.  Setting it to 0 means that the application never
     times out.  The unit is milliseconds.

   * 
     'TransactionDeadlockDetectionTimeout'

     *This table provides type and value information for the
     TransactionDeadlockDetectionTimeout data node configuration
     parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      milliseconds
                                          
     *Default*                            1200
                                          
     *Range*                              50 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     When a node executes a query involving a transaction, the node
     waits for the other nodes in the cluster to respond before
     continuing.  This parameter sets the amount of time that the
     transaction can spend executing within a data node, that is, the
     time that the transaction coordinator waits for each data node
     participating in the transaction to execute a request.

     A failure to respond can occur for any of the following reasons:

        * The node is 'dead'

        * The operation has entered a lock queue

        * The node requested to perform the action could be heavily
          overloaded.

     This timeout parameter states how long the transaction coordinator
     waits for query execution by another node before aborting the
     transaction, and is important for both node failure handling and
     deadlock detection.

     The default timeout value is 1200 milliseconds (1.2 seconds).

     The minimum for this parameter is 50 milliseconds.

   * 
     'DiskSyncSize'

     *This table provides type and value information for the
     DiskSyncSize data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      bytes
                                          
     *Default*                            4M
                                          
     *Range*                              32K - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     This is the maximum number of bytes to store before flushing data
     to a local checkpoint file.  This is done to prevent write
     buffering, which can impede performance significantly.  This
     parameter is _not_ intended to take the place of
     'TimeBetweenLocalCheckpoints'.

     *Note*:

     When 'ODirect' is enabled, it is not necessary to set
     'DiskSyncSize'; in fact, in such cases its value is simply ignored.

     The default value is 4M (4 megabytes).

   * 
     'DiskCheckpointSpeed'

     *This table provides type and value information for the
     DiskCheckpointSpeed data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      bytes
                                          
     *Default*                            10M
                                          
     *Range*                              1M - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     The amount of data,in bytes per second, that is sent to disk during
     a local checkpoint.  This allocation is shared by DML operations
     and backups (but not backup logging), which means that backups
     started during times of intensive DML may be impaired by flooding
     of the redo log buffer and may fail altogether if the contention is
     sufficiently severe.

     The default value is 10M (10 megabytes per second).

   * 
     'DiskCheckpointSpeedInRestart'

     *This table provides type and value information for the
     DiskCheckpointSpeedInRestart data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      bytes
                                          
     *Default*                            100M
                                          
     *Range*                              1M - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     The amount of data,in bytes per second, that is sent to disk during
     a local checkpoint as part of a restart operation.

     The default value is 100M (100 megabytes per second).

   * 
     'NoOfDiskPagesToDiskAfterRestartTUP'

     This parameter is deprecated and subject to removal in a future
     version of NDB Cluster.  Use 'DiskCheckpointSpeedInRestart' and
     'DiskSyncSize' instead.

   * 
     'NoOfDiskPagesToDiskAfterRestartACC'

     This parameter is deprecated and subject to removal in a future
     version of NDB Cluster.  Use 'DiskCheckpointSpeedInRestart' and
     'DiskSyncSize' instead.

   * 
     'NoOfDiskPagesToDiskDuringRestartTUP' (DEPRECATED)

     This parameter is deprecated and subject to removal in a future
     version of NDB Cluster.  Use 'DiskCheckpointSpeedInRestart' and
     'DiskSyncSize' instead.

   * 
     'NoOfDiskPagesToDiskDuringRestartACC' (DEPRECATED)

     This parameter is deprecated and subject to removal in a future
     version of NDB Cluster.  Use 'DiskCheckpointSpeedInRestart' and
     'DiskSyncSize' instead.

   * 
     'ArbitrationTimeout'

     *This table provides type and value information for the
     ArbitrationTimeout data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      milliseconds
                                          
     *Default*                            7500
                                          
     *Range*                              10 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     This parameter specifies how long data nodes wait for a response
     from the arbitrator to an arbitration message.  If this is
     exceeded, the network is assumed to have split.

     In NDB 7.2.0 and later, the default value is 7500 milliseconds (7.5
     seconds).  Previously, this was 3000 milliseconds (3 seconds).

   * 
     'Arbitration'

     *This table provides type and value information for the Arbitration
     data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      enumeration
                                          
     *Default*                            Default
                                          
     *Range*                              Default, Disabled, WaitExternal
                                          
     *Restart Type*                       N

     The 'Arbitration' parameter enables a choice of arbitration
     schemes, corresponding to one of 3 possible values for this
     parameter:

        * Default

          This enables arbitration to proceed normally, as determined by
          the 'ArbitrationRank' settings for the management and API
          nodes.  This is the default value.

        * Disabled

          Setting 'Arbitration = Disabled' in the '[ndbd default]'
          section of the 'config.ini' file to accomplishes the same task
          as setting 'ArbitrationRank' to 0 on all management and API
          nodes.  When 'Arbitration' is set in this way, any
          'ArbitrationRank' settings are ignored.

        * WaitExternal

          The 'Arbitration' parameter also makes it possible to
          configure arbitration in such a way that the cluster waits
          until after the time determined by 'ArbitrationTimeout' has
          passed for an external cluster manager application to perform
          arbitration instead of handling arbitration internally.  This
          can be done by setting 'Arbitration = WaitExternal' in the
          '[ndbd default]' section of the 'config.ini' file.  For best
          results with the 'WaitExternal' setting, it is recommended
          that 'ArbitrationTimeout' be 2 times as long as the interval
          required by the external cluster manager to perform
          arbitration.

     *Important*:

     This parameter should be used only in the '[ndbd default]' section
     of the cluster configuration file.  The behavior of the cluster is
     unspecified when 'Arbitration' is set to different values for
     individual data nodes.

   * 
     'RestartSubscriberConnectTimeout'

     *This table provides type and value information for the
     RestartSubscriberConnectTimeout data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.17
                                          
     *Type or units*                      ms
                                          
     *Default*                            12000
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       S

     This parameter determines the time that a data node waits for
     subscribing API nodes to connect.  Once this timeout expires, any
     'missing' API nodes are disconnected from the cluster.  To disable
     this timeout, set 'RestartSubscriberConnectTimeout' to 0.

     While this parameter is specified in milliseconds, the timeout
     itself is resolved to the next-greatest whole second.

     'RestartSubscriberConnectTimeout' was added in NDB 7.2.17.

Buffering and logging

Several '[ndbd]' configuration parameters enable the advanced user to
have more control over the resources used by node processes and to
adjust various buffer sizes at need.

These buffers are used as front ends to the file system when writing log
records to disk.  If the node is running in diskless mode, these
parameters can be set to their minimum values without penalty due to the
fact that disk writes are 'faked' by the *note 'NDB': mysql-cluster.
storage engine's file system abstraction layer.

   * 
     'UndoIndexBuffer'

     *This table provides type and value information for the
     UndoIndexBuffer data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      unsigned
                                          
     *Default*                            2M
                                          
     *Range*                              1M - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     The UNDO index buffer, whose size is set by this parameter, is used
     during local checkpoints.  The *note 'NDB': mysql-cluster. storage
     engine uses a recovery scheme based on checkpoint consistency in
     conjunction with an operational REDO log.  To produce a consistent
     checkpoint without blocking the entire system for writes, UNDO
     logging is done while performing the local checkpoint.  UNDO
     logging is activated on a single table fragment at a time.  This
     optimization is possible because tables are stored entirely in main
     memory.

     The UNDO index buffer is used for the updates on the primary key
     hash index.  Inserts and deletes rearrange the hash index; the NDB
     storage engine writes UNDO log records that map all physical
     changes to an index page so that they can be undone at system
     restart.  It also logs all active insert operations for each
     fragment at the start of a local checkpoint.

     Reads and updates set lock bits and update a header in the hash
     index entry.  These changes are handled by the page-writing
     algorithm to ensure that these operations need no UNDO logging.

     This buffer is 2MB by default.  The minimum value is 1MB, which is
     sufficient for most applications.  For applications doing extremely
     large or numerous inserts and deletes together with large
     transactions and large primary keys, it may be necessary to
     increase the size of this buffer.  If this buffer is too small, the
     NDB storage engine issues internal error code 677 ('Index UNDO
     buffers overloaded').

     *Important*:

     It is not safe to decrease the value of this parameter during a
     rolling restart.

   * 
     'UndoDataBuffer'

     *This table provides type and value information for the
     UndoDataBuffer data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      unsigned
                                          
     *Default*                            16M
                                          
     *Range*                              1M - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     This parameter sets the size of the UNDO data buffer, which
     performs a function similar to that of the UNDO index buffer,
     except the UNDO data buffer is used with regard to data memory
     rather than index memory.  This buffer is used during the local
     checkpoint phase of a fragment for inserts, deletes, and updates.

     Because UNDO log entries tend to grow larger as more operations are
     logged, this buffer is also larger than its index memory
     counterpart, with a default value of 16MB.

     This amount of memory may be unnecessarily large for some
     applications.  In such cases, it is possible to decrease this size
     to a minimum of 1MB.

     It is rarely necessary to increase the size of this buffer.  If
     there is such a need, it is a good idea to check whether the disks
     can actually handle the load caused by database update activity.  A
     lack of sufficient disk space cannot be overcome by increasing the
     size of this buffer.

     If this buffer is too small and gets congested, the NDB storage
     engine issues internal error code 891 ('Data UNDO buffers
     overloaded').

     *Important*:

     It is not safe to decrease the value of this parameter during a
     rolling restart.

   * 
     'RedoBuffer'

     *This table provides type and value information for the RedoBuffer
     data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      bytes
                                          
     *Default*                            32M
                                          
     *Range*                              1M - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     All update activities also need to be logged.  The REDO log makes
     it possible to replay these updates whenever the system is
     restarted.  The NDB recovery algorithm uses a 'fuzzy' checkpoint of
     the data together with the UNDO log, and then applies the REDO log
     to play back all changes up to the restoration point.

     'RedoBuffer' sets the size of the buffer in which the REDO log is
     written.  The default value is 32MB; the minimum value is 1MB.

     If this buffer is too small, the *note 'NDB': mysql-cluster.
     storage engine issues error code 1221 ('REDO log buffers
     overloaded').  For this reason, you should exercise care if you
     attempt to decrease the value of 'RedoBuffer' as part of an online
     change in the cluster's configuration.

     *note 'ndbmtd': mysql-cluster-programs-ndbmtd. allocates a separate
     buffer for each LDM thread (see 'ThreadConfig').  For example, with
     4 LDM threads, an *note 'ndbmtd': mysql-cluster-programs-ndbmtd.
     data node actually has 4 buffers and allocates 'RedoBuffer' bytes
     to each one, for a total of '4 * RedoBuffer' bytes.

   * 
     'EventLogBufferSize'

     *This table provides type and value information for the
     EventLogBufferSize data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      bytes
                                          
     *Default*                            8192
                                          
     *Range*                              0 - 64K
                                          
     *Restart Type*                       S

     Controls the size of the circular buffer used for NDB log events
     within data nodes.

Controlling log messages

In managing the cluster, it is very important to be able to control the
number of log messages sent for various event types to 'stdout'.  For
each event category, there are 16 possible event levels (numbered 0
through 15).  Setting event reporting for a given event category to
level 15 means all event reports in that category are sent to 'stdout';
setting it to 0 means that there will be no event reports made in that
category.

By default, only the startup message is sent to 'stdout', with the
remaining event reporting level defaults being set to 0.  The reason for
this is that these messages are also sent to the management server's
cluster log.

An analogous set of levels can be set for the management client to
determine which event levels to record in the cluster log.

   * 
     'LogLevelStartup'

     *This table provides type and value information for the
     LogLevelStartup data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      integer
                                          
     *Default*                            1
                                          
     *Range*                              0 - 15
                                          
     *Restart Type*                       N

     The reporting level for events generated during startup of the
     process.

     The default level is 1.

   * 
     'LogLevelShutdown'

     *This table provides type and value information for the
     LogLevelShutdown data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      integer
                                          
     *Default*                            0
                                          
     *Range*                              0 - 15
                                          
     *Restart Type*                       N

     The reporting level for events generated as part of graceful
     shutdown of a node.

     The default level is 0.

   * 
     'LogLevelStatistic'

     *This table provides type and value information for the
     LogLevelStatistic data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      integer
                                          
     *Default*                            0
                                          
     *Range*                              0 - 15
                                          
     *Restart Type*                       N

     The reporting level for statistical events such as number of
     primary key reads, number of updates, number of inserts,
     information relating to buffer usage, and so on.

     The default level is 0.

   * 
     'LogLevelCheckpoint'

     *This table provides type and value information for the
     LogLevelCheckpoint data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      log level
                                          
     *Default*                            0
                                          
     *Range*                              0 - 15
                                          
     *Restart Type*                       N

     The reporting level for events generated by local and global
     checkpoints.

     The default level is 0.

   * 
     'LogLevelNodeRestart'

     *This table provides type and value information for the
     LogLevelNodeRestart data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      integer
                                          
     *Default*                            0
                                          
     *Range*                              0 - 15
                                          
     *Restart Type*                       N

     The reporting level for events generated during node restart.

     The default level is 0.

   * 
     'LogLevelConnection'

     *This table provides type and value information for the
     LogLevelConnection data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      integer
                                          
     *Default*                            0
                                          
     *Range*                              0 - 15
                                          
     *Restart Type*                       N

     The reporting level for events generated by connections between
     cluster nodes.

     The default level is 0.

   * 
     'LogLevelError'

     *This table provides type and value information for the
     LogLevelError data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      integer
                                          
     *Default*                            0
                                          
     *Range*                              0 - 15
                                          
     *Restart Type*                       N

     The reporting level for events generated by errors and warnings by
     the cluster as a whole.  These errors do not cause any node failure
     but are still considered worth reporting.

     The default level is 0.

   * 
     'LogLevelCongestion'

     *This table provides type and value information for the
     LogLevelCongestion data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      level
                                          
     *Default*                            0
                                          
     *Range*                              0 - 15
                                          
     *Restart Type*                       N

     The reporting level for events generated by congestion.  These
     errors do not cause node failure but are still considered worth
     reporting.

     The default level is 0.

   * 
     'LogLevelInfo'

     *This table provides type and value information for the
     LogLevelInfo data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      integer
                                          
     *Default*                            0
                                          
     *Range*                              0 - 15
                                          
     *Restart Type*                       N

     The reporting level for events generated for information about the
     general state of the cluster.

     The default level is 0.

   * 
     'MemReportFrequency'

     *This table provides type and value information for the
     MemReportFrequency data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      unsigned
                                          
     *Default*                            0
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     This parameter controls how often data node memory usage reports
     are recorded in the cluster log; it is an integer value
     representing the number of seconds between reports.

     Each data node's data memory and index memory usage is logged as
     both a percentage and a number of 32 KB pages of the 'DataMemory'
     and 'IndexMemory', respectively, set in the 'config.ini' file.  For
     example, if 'DataMemory' is equal to 100 MB, and a given data node
     is using 50 MB for data memory storage, the corresponding line in
     the cluster log might look like this:

          2006-12-24 01:18:16 [MgmSrvr] INFO -- Node 2: Data usage is 50%(1280 32K pages of total 2560)

     'MemReportFrequency' is not a required parameter.  If used, it can
     be set for all cluster data nodes in the '[ndbd default]' section
     of 'config.ini', and can also be set or overridden for individual
     data nodes in the corresponding '[ndbd]' sections of the
     configuration file.  The minimum value--which is also the default
     value--is 0, in which case memory reports are logged only when
     memory usage reaches certain percentages (80%, 90%, and 100%), as
     mentioned in the discussion of statistics events in *note
     mysql-cluster-log-events::.

   * 
     'StartupStatusReportFrequency'

     *This table provides type and value information for the
     StartupStatusReportFrequency data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      seconds
                                          
     *Default*                            0
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     When a data node is started with the '--initial', it initializes
     the redo log file during Start Phase 4 (see *note
     mysql-cluster-start-phases::).  When very large values are set for
     'NoOfFragmentLogFiles', 'FragmentLogFileSize', or both, this
     initialization can take a long time.You can force reports on the
     progress of this process to be logged periodically, by means of the
     'StartupStatusReportFrequency' configuration parameter.  In this
     case, progress is reported in the cluster log, in terms of both the
     number of files and the amount of space that have been initialized,
     as shown here:

          2009-06-20 16:39:23 [MgmSrvr] INFO -- Node 1: Local redo log file initialization status:
          #Total files: 80, Completed: 60
          #Total MBytes: 20480, Completed: 15557
          2009-06-20 16:39:23 [MgmSrvr] INFO -- Node 2: Local redo log file initialization status:
          #Total files: 80, Completed: 60
          #Total MBytes: 20480, Completed: 15570

     These reports are logged each 'StartupStatusReportFrequency'
     seconds during Start Phase 4.  If 'StartupStatusReportFrequency' is
     0 (the default), then reports are written to the cluster log only
     when at the beginning and at the completion of the redo log file
     initialization process.

Debugging Parameters

In NDB Cluster 7.2, it is possible to cause logging of traces for events
generated by creating and dropping tables using 'DictTrace'.  This
parameter is useful only in debugging NDB kernel code.  'DictTrace'
takes an integer value; currently, 0 (default - no logging) and 1
(logging enabled) are the only supported values.

Backup parameters

The '[ndbd]' parameters discussed in this section define memory buffers
set aside for execution of online backups.

   * 
     'BackupDataBufferSize'

     *This table provides type and value information for the
     BackupDataBufferSize data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      bytes
                                          
     *Default*                            16M
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     In creating a backup, there are two buffers used for sending data
     to the disk.  The backup data buffer is used to fill in data
     recorded by scanning a node's tables.  Once this buffer has been
     filled to the level specified as 'BackupWriteSize', the pages are
     sent to disk.  While flushing data to disk, the backup process can
     continue filling this buffer until it runs out of space.  When this
     happens, the backup process pauses the scan and waits until some
     disk writes have completed freeing up memory so that scanning may
     continue.

     The default value for this parameter is 16MB.

   * 
     'BackupLogBufferSize'

     *This table provides type and value information for the
     BackupLogBufferSize data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      bytes
                                          
     *Default*                            16M
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     The backup log buffer fulfills a role similar to that played by the
     backup data buffer, except that it is used for generating a log of
     all table writes made during execution of the backup.  The same
     principles apply for writing these pages as with the backup data
     buffer, except that when there is no more space in the backup log
     buffer, the backup fails.  For that reason, the size of the backup
     log buffer must be large enough to handle the load caused by write
     activities while the backup is being made.  See *note
     mysql-cluster-backup-configuration::.

     The default value for this parameter should be sufficient for most
     applications.  In fact, it is more likely for a backup failure to
     be caused by insufficient disk write speed than it is for the
     backup log buffer to become full.  If the disk subsystem is not
     configured for the write load caused by applications, the cluster
     is unlikely to be able to perform the desired operations.

     It is preferable to configure cluster nodes in such a manner that
     the processor becomes the bottleneck rather than the disks or the
     network connections.

     The default value for this parameter is 16MB.

   * 
     'BackupMemory'

     *This table provides type and value information for the
     BackupMemory data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      bytes
                                          
     *Default*                            32M
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     This parameter is simply the sum of 'BackupDataBufferSize' and
     'BackupLogBufferSize'.

     The default value of this parameter in NDB Cluster 7.2 is 16MB +
     16MB = 32MB.

     *Important*:

     If 'BackupDataBufferSize' and 'BackupLogBufferSize' taken together
     exceed the default value for 'BackupMemory', then this parameter
     must be set explicitly in the 'config.ini' file to their sum.

   * 
     'BackupReportFrequency'

     *This table provides type and value information for the
     BackupReportFrequency data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      seconds
                                          
     *Default*                            0
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     This parameter controls how often backup status reports are issued
     in the management client during a backup, as well as how often such
     reports are written to the cluster log (provided cluster event
     logging is configured to permit it--see *note
     mysql-cluster-logging-and-checkpointing::).
     'BackupReportFrequency' represents the time in seconds between
     backup status reports.

     The default value is 0.

   * 
     'BackupWriteSize'

     *This table provides type and value information for the
     BackupWriteSize data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      bytes
                                          
     *Default*                            256K
                                          
     *Range*                              2K - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     This parameter specifies the default size of messages written to
     disk by the backup log and backup data buffers.

     The default value for this parameter is 256KB.

   * 
     'BackupMaxWriteSize'

     *This table provides type and value information for the
     BackupMaxWriteSize data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      bytes
                                          
     *Default*                            1M
                                          
     *Range*                              2K - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     This parameter specifies the maximum size of messages written to
     disk by the backup log and backup data buffers.

     The default value for this parameter is 1MB.

*Note*:

The location of the backup files is determined by the 'BackupDataDir'
data node configuration parameter.

Additional requirements

When specifying these parameters, the following relationships must hold
true.  Otherwise, the data node will be unable to start.

   * 'BackupDataBufferSize >= BackupWriteSize + 188KB'

   * 'BackupLogBufferSize >= BackupWriteSize + 16KB'

   * 'BackupMaxWriteSize >= BackupWriteSize'

*NDB Cluster Realtime Performance Parameters*

The '[ndbd]' parameters discussed in this section are used in scheduling
and locking of threads to specific CPUs on multiprocessor data node
hosts.

*Note*:

To make use of these parameters, the data node process must be run as
system root.

   * 
     'LockExecuteThreadToCPU'

     *This table provides type and value information for the
     LockExecuteThreadToCPU data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      set of CPU IDs
                                          
     *Default*                            0
                                          
     *Range*                              ...
                                          
     *Restart Type*                       N

     When used with *note 'ndbd': mysql-cluster-programs-ndbd, this
     parameter (now a string) specifies the ID of the CPU assigned to
     handle the *note 'NDBCLUSTER': mysql-cluster. execution thread.
     When used with *note 'ndbmtd': mysql-cluster-programs-ndbmtd, the
     value of this parameter is a comma-separated list of CPU IDs
     assigned to handle execution threads.  Each CPU ID in the list
     should be an integer in the range 0 to 65535 (inclusive).

     The number of IDs specified should match the number of execution
     threads determined by 'MaxNoOfExecutionThreads'.  However, there is
     no guarantee that threads are assigned to CPUs in any given order
     when using this parameter; beginning with in NDB 7.2.5, you can
     obtain more finely-grained control of this type using
     'ThreadConfig'.

     'LockExecuteThreadToCPU' has no default value.

   * 
     'LockMaintThreadsToCPU'

     *This table provides type and value information for the
     LockMaintThreadsToCPU data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      CPU ID
                                          
     *Default*                            0
                                          
     *Range*                              0 - 64K
                                          
     *Restart Type*                       N

     This parameter specifies the ID of the CPU assigned to handle *note
     'NDBCLUSTER': mysql-cluster. maintenance threads.

     The value of this parameter is an integer in the range 0 to 65535
     (inclusive).  In NDB Cluster 7.2, there is no default value.

   * 
     'RealtimeScheduler'

     *This table provides type and value information for the
     RealtimeScheduler data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      boolean
                                          
     *Default*                            false
                                          
     *Range*                              true, false
                                          
     *Restart Type*                       N

     Setting this parameter to 1 enables real-time scheduling of data
     node threads.

     Prior to NDB 7.2.14, this parameter did not work correctly with
     data nodes running *note 'ndbmtd': mysql-cluster-programs-ndbmtd.
     (Bug #16961971)

     The default is 0 (scheduling disabled).

   * 
     'SchedulerExecutionTimer'

     *This table provides type and value information for the
     SchedulerExecutionTimer data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      µs
                                          
     *Default*                            50
                                          
     *Range*                              0 - 11000
                                          
     *Restart Type*                       N

     This parameter specifies the time in microseconds for threads to be
     executed in the scheduler before being sent.  Setting it to 0
     minimizes the response time; to achieve higher throughput, you can
     increase the value at the expense of longer response times.

     The default is 50 μsec, which our testing shows to increase
     throughput slightly in high-load cases without materially delaying
     requests.

   * 
     'SchedulerSpinTimer'

     *This table provides type and value information for the
     SchedulerSpinTimer data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      µs
                                          
     *Default*                            0
                                          
     *Range*                              0 - 500
                                          
     *Restart Type*                       N

     This parameter specifies the time in microseconds for threads to be
     executed in the scheduler before sleeping.

     The default value is 0.

   * 
     'BuildIndexThreads'

     *This table provides type and value information for the
     BuildIndexThreads data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      numeric
                                          
     *Default*                            0
                                          
     *Range*                              0 - 128
                                          
     *Restart Type*                       S

     This parameter determines the number of threads to create when
     rebuilding ordered indexes during a system or node start, as well
     as when running *note 'ndb_restore':
     mysql-cluster-programs-ndb-restore. '--rebuild-indexes'.  It is
     supported only when there is more than one fragment for the table
     per data node (for example, when the 'MAX_ROWS' option has been
     used with *note 'CREATE TABLE': create-table.).

     Setting this parameter to 0 (the default) disables multithreaded
     building of ordered indexes.

     This parameter is supported when using *note 'ndbd':
     mysql-cluster-programs-ndbd. or *note 'ndbmtd':
     mysql-cluster-programs-ndbmtd.

     You can enable multithreaded builds during data node initial
     restarts by setting the 'TwoPassInitialNodeRestartCopy' data node
     configuration parameter to 'TRUE'.

   * 
     'TwoPassInitialNodeRestartCopy'

     *This table provides type and value information for the
     TwoPassInitialNodeRestartCopy data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      boolean
                                          
     *Default*                            false
                                          
     *Range*                              true, false
                                          
     *Restart Type*                       N

     Multithreaded building of ordered indexes can be enabled for
     initial restarts of data nodes by setting this configuration
     parameter to 'TRUE', which enables two-pass copying of data during
     initial node restarts.

     You must also set 'BuildIndexThreads' to a nonzero value.

   * 
     'Numa'

     *This table provides type and value information for the Numa data
     node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      numeric
                                          
     *Default*                            1
                                          
     *Range*                              ...
                                          
     *Restart Type*                       N

     This parameter determines whether Non-Uniform Memory Access (NUMA)
     is controlled by the operating system or by the data node process,
     whether the data node uses *note 'ndbd':
     mysql-cluster-programs-ndbd. or *note 'ndbmtd':
     mysql-cluster-programs-ndbmtd.  By default, 'NDB' attempts to use
     an interleaved NUMA memory allocation policy on any data node where
     the host operating system provides NUMA support.

     Setting 'Numa = 0' means that the datanode process does not itself
     attempt to set a policy for memory allocation, and permits this
     behavior to be determined by the operating system, which may be
     further guided by the separate 'numactl' tool.  That is, 'Numa = 0'
     yields the system default behavior, which can be customised by
     'numactl'.  For many Linux systems, the system default behavior is
     to allocate socket-local memory to any given process at allocation
     time.  This can be problematic when using *note 'ndbmtd':
     mysql-cluster-programs-ndbmtd.; this is because 'nbdmtd' allocates
     all memory at startup, leading to an imbalance, giving different
     access speeds for different sockets, especially when locking pages
     in main memory.

     Setting 'Numa = 1' means that the data node process uses 'libnuma'
     to request interleaved memory allocation.  (This can also be
     accomplished manually, on the operating system level, using
     'numactl'.)  Using interleaved allocation in effect tells the data
     node process to ignore non-uniform memory access but does not
     attempt to take any advantage of fast local memory; instead, the
     data node process tries to avoid imbalances due to slow remote
     memory.  If interleaved allocation is not desired, set 'Numa' to 0
     so that the desired behavior can be determined on the operating
     system level.

     The 'Numa' configuration parameter is supported only on Linux
     systems where 'libnuma.so' is available.

Multi-Threading Configuration Parameters (ndbmtd)

*note 'ndbmtd': mysql-cluster-programs-ndbmtd. runs by default as a
single-threaded process and must be configured to use multiple threads,
using either of two methods, both of which require setting configuration
parameters in the 'config.ini' file.  The first method is simply to set
an appropriate value for the 'MaxNoOfExecutionThreads' configuration
parameter.  In NDB 7.2.3 and later, a second method is also supported,
whereby it is possible to set up more complex rules for *note 'ndbmtd':
mysql-cluster-programs-ndbmtd. multithreading using 'ThreadConfig'.  The
next few paragraphs provide information about these parameters and their
use with multithreaded data nodes.

   * 
     'MaxNoOfExecutionThreads'

     *This table provides type and value information for the
     MaxNoOfExecutionThreads multi-threaded data node configuration
     parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.5
                                          
     *Type or units*                      integer
                                          
     *Default*                            2
                                          
     *Range*                              2 - 36
                                          
     *Restart Type*                       IS

     Starting with NDB 7.2.5, this parameter directly controls the
     number of execution threads used by *note 'ndbmtd':
     mysql-cluster-programs-ndbmtd, up to a maximum of 36.  Although
     this parameter is set in '[ndbd]' or '[ndbd default]' sections of
     the 'config.ini' file, it is exclusive to *note 'ndbmtd':
     mysql-cluster-programs-ndbmtd. and does not apply to *note 'ndbd':
     mysql-cluster-programs-ndbd.

     Setting 'MaxNoOfExecutionThreads' sets the number of threads by
     type as determined in the following table:

     *MaxNoOfExecutionThreads values and the corresponding number of
     threads by thread type (LQH, TC, Send, Receive), NDB 7.4.2 and
     earlier*

     'MaxNoOfExecutionThreads' Value       LDM       TC        Send      Receive
                                           Threads   Threads   Threads   Threads
                                                                         
     0 ..  3                               1         1         0         1
                                                                         
     4 ..  6                               2         1         0         1
                                                                         
     7 ..  8                               4         1         0         1
                                                                         
     9                                     4         2         0         1
                                                                         
     10                                    4         2         1         1
                                                                         
     11                                    4         3         1         1
                                                                         
     12                                    4         3         1         2
                                                                         
     13                                    4         3         2         2
                                                                         
     14                                    4         4         2         2
                                                                         
     15                                    4         5         2         2
                                                                         
     16                                    8         3         1         2
                                                                         
     17                                    8         4         1         2
                                                                         
     18                                    8         4         2         2
                                                                         
     19                                    8         5         2         2
                                                                         
     20                                    8         5         2         3
                                                                         
     21                                    8         5         3         3
                                                                         
     22                                    8         6         3         3
                                                                         
     23                                    8         7         3         3
                                                                         
     24                                    12        5         2         3
                                                                         
     25                                    12        6         2         3
                                                                         
     26                                    12        6         3         3
                                                                         
     27                                    12        7         3         3
                                                                         
     28                                    12        7         3         4
                                                                         
     29                                    12        8         3         4
                                                                         
     30                                    12        8         4         4
                                                                         
     31                                    12        9         4         4
                                                                         
     32                                    16        8         3         3
                                                                         
     33                                    16        8         3         4
                                                                         
     34                                    16        8         4         4
                                                                         
     35                                    16        9         4         4
                                                                         
     36                                    16        10        4         4
                                                               

     'NoOfFragmentLogParts' should be set equal to the number of LDM
     threads used by *note 'ndbmtd': mysql-cluster-programs-ndbmtd. as
     determined by the setting for 'MaxNoOfExecutionThreads'; see the
     description of this parameter for more information.

     There is always one SUMA (replication) thread.  There was no
     separate send thread in NDB 7.2.4 and earlier, as well as no means
     of changing the number of TC threads.

     The number of LDM threads also determines the number of partitions
     used by an 'NDB' table that is not explicitly partitioned; this is
     the number of LDM threads times the number of data nodes in the
     cluster.  (If *note 'ndbd': mysql-cluster-programs-ndbd. is used on
     the data nodes rather than *note 'ndbmtd':
     mysql-cluster-programs-ndbmtd, then there is always a single LDM
     thread; in this case, the number of partitions created
     automatically is simply equal to the number of data nodes.  See
     *note mysql-cluster-nodes-groups::, for more information.

     Adding large tablespaces for Disk Data tables when using more than
     the default number of LDM threads may cause issues with resource
     and CPU usage if the disk page buffer is insufficiently large; see
     the description of the 'DiskPageBufferMemory' configuration
     parameter, for more information.

     The thread types are described later in this section (see
     'ThreadConfig').

     Setting this parameter outside the permitted range of values causes
     the management server to abort on startup with the error 'Error
     line NUMBER: Illegal value VALUE for parameter
     MaxNoOfExecutionThreads'.

     For 'MaxNoOfExecutionThreads', a value of 0 or 1 is rounded up
     internally by *note 'NDB': mysql-cluster. to 2, so that 2 is
     considered this parameter's default and minimum value.

     'MaxNoOfExecutionThreads' is generally intended to be set equal to
     the number of CPU threads available, and to allocate a number of
     threads of each type suitable to typical workloads.  It does not
     assign particular threads to specified CPUs.  For cases where it is
     desirable to vary from the settings provided, or to bind threads to
     CPUs, you should use 'ThreadConfig' instead, which allows you to
     allocate each thread directly to a desired type, CPU, or both.

     In NDB 7.2.5 and later, the multithreaded data node process always
     spawns, at a minimum, the threads listed here:

        * 1 local query handler (LDM) thread

        * 1 receive thread

        * 1 subscription manager (SUMA or replication) thread

     For a 'MaxNoOfExecutionThreads' value of 8 or less, no TC threads
     are created, and TC handling is instead performed by the main
     thread.

     Changing the number of LDM threads always requires a system
     restart, whether it is changed using this parameter or
     'ThreadConfig'.  If the cluster's 'IndexMemory' usage is greater
     than 50%, changing this requires an initial restart of the cluster.
     (A maximum of 30-35% 'IndexMemory' usage is recommended in such
     cases.)  Otherwise, resource usage and LDM thread allocation cannot
     be balanced between nodes, which can result in underutilized and
     overutilized LDM threads, and ultimately data node failures.

     In NDB 7.2.4 and earlier, there were only 4 thread types, with LDM
     threads being responsible for their own sends.  In addition, it was
     not possible to cause *note 'ndbmtd':
     mysql-cluster-programs-ndbmtd. to use more than 1 TC thread.

   * 
     'NoOfFragmentLogParts'

     *This table provides type and value information for the
     NoOfFragmentLogParts multi-threaded data node configuration
     parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.5
                                          
     *Type or units*                      numeric
                                          
     *Default*                            4
                                          
     *Range*                              4, 8, 12, 16
                                          
     *Restart Type*                       IN

     Set the number of log file groups for redo logs belonging to this
     *note 'ndbmtd': mysql-cluster-programs-ndbmtd.  The value must be
     an even multiple of 4 between 4 and 16, inclusive.

     'NoOfFragmentLogParts' should be set equal to the number of LDM
     threads used by *note 'ndbmtd': mysql-cluster-programs-ndbmtd. as
     determined by the setting for 'MaxNoOfExecutionThreads'; see the
     description of this parameter for more information.

   * 
     'ThreadConfig'

     *This table provides type and value information for the
     ThreadConfig multi-threaded data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.3
                                          
     *Type or units*                      string
                                          
     *Default*                            "
                                          
     *Range*                              ...
                                          
     *Restart Type*                       IS

     This parameter is used with *note 'ndbmtd':
     mysql-cluster-programs-ndbmtd. to assign threads of different types
     to different CPUs.  Its value is a string whose format has the
     following syntax:

          ThreadConfig := ENTRY[,ENTRY[,...]]

          ENTRY := TYPE={PARAM[,PARAM[,...]]}

          TYPE := ldm | main | recv | send | rep | io | tc | watchdog

          PARAM := count=NUMBER
            | cpubind=CPU_LIST
            | cpuset=CPU_LIST
            | spintime=NUMBER
            | realtime={0|1}

     The curly braces ('{'...'}') surrounding the list of parameters are
     required, even if there is only one parameter in the list.

     A PARAM (parameter) specifies any or all of the following
     information:

        * The number of threads of the given type ('count').

        * The set of CPUs to which the threads of the given type are to
          be nonexclusively bound.  This is determined by either one of
          'cpubind' or 'cpuset').  'cpubind' causes each thread to be
          bound (nonexclusively) to a CPU in the set; 'cpuset' means
          that each thread is bound (nonexclusively) to the set of CPUs
          specified.

          Only one of 'cpubind' or 'cpuset' can be provided in a single
          configuration.

        * 'spintime' determines the wait time in microseconds the thread
          spins before going to sleep.

          The default value for 'spintime' is the value of the
          'SchedulerSpinTimer' data node configuration parameter.

          'spintime' does not apply to I/O threads or watchdog threads
          and so cannot be set for these thread types.

        * 'realtime' can be set to 0 or 1.  If it is set to 1, the
          threads run with real-time priority.  This also means that
          'thread_prio' cannot be set.

          The 'realtime' parameter is set by default to the value of the
          'RealtimeScheduler' data node configuration parameter.

     The TYPE attribute represents an NDB thread type.  The thread types
     supported in NDB Cluster 7.2 and the range of permitted 'count'
     values for each are provided in the following list:

        * 'ldm': Local query handler ('DBLQH' kernel block) that handles
          data.  The more LDM threads that are used, the more highly
          partitioned the data becomes.  Each LDM thread maintains its
          own sets of data and index partitions, as well as its own redo
          log.  The value set for 'ldm' must be one of one of the values
          1, 2, 4, 6, 8, 12, or 16.

          Changing the number of LDM threads requires a system restart
          to be effective and safe for cluster operations.  (This is
          also true when this is done using 'MaxNoOfExecutionThreads'.)
          If 'IndexMemory' usage is in excess of 50%, an initial restart
          of the cluster is required; a maximum of 30-35% 'IndexMemory'
          usage is recommended in such cases.  Otherwise, 'IndexMemory'
          and 'DataMemory' usage as well as the allocation of LDM
          threads cannot be balanced between nodes, which can ultimately
          lead to data node failures.

          Adding large tablespaces (hundreds of gigabytes or more) for
          Disk Data tables when using more than the default number of
          LDMs may cause issues with resource and CPU usage if
          'DiskPageBufferMemory' is not sufficiently large.

        * 'tc': Transaction coordinator thread ('DBTC' kernel block)
          containing the state of an ongoing transaction.  In NDB 7.2.5
          and later, the number of TC threads is configurable, with a
          total of 16 possible.

          Optimally, every new transaction can be assigned to a new TC
          thread.  In most cases 1 TC thread per 2 LDM threads is
          sufficient to guarantee that this can happen.  In cases where
          the number of writes is relatively small when compared to the
          number of reads, it is possible that only 1 TC thread per 4
          LQH threads is required to maintain transaction states.
          Conversely, in applications that perform a great many updates,
          it may be necessary for the ratio of TC threads to LDM threads
          to approach 1 (for example, 3 TC threads to 4 LDM threads).

          Range: (_NDB 7.2.5 and later_:) 0 - 16; (_prior to NDB
          7.2.5_:) 1 (not settable).

        * 'main': Data dictionary and transaction coordinator ('DBDIH'
          and 'DBTC' kernel blocks), providing schema management.  This
          is always handled by a single dedicated thread.

          Range: 1 only.

        * 'recv': Receive thread ('CMVMI' kernel block).  Each receive
          thread handles one or more sockets for communicating with
          other nodes in an NDB Cluster, with one socket per node.
          Previously, this was limited to a single thread, but NDB
          Cluster 7.2 implements multiple receive threads (up to 8).

          Range: 1 - 8.

        * 'send': Send thread ('CMVMI' kernel block).  Added in NDB
          7.2.5.  To increase throughput, it is possible in NDB 7.2.5
          and later to perform sends from one or more separate,
          dedicated threads (maximum 8).

          Previously, all threads handled their own sending directly;
          this can still be made to happen by setting the number of send
          threads to 0 (this also happens when 'MaxNoOfExecutionThreads'
          is set less than 10).  While doing so can have an adeverse
          impact on throughput, it can also in some cases provide
          decreased latency.

          Range: 0 - 8.

        * 'rep': Replication thread ('SUMA' kernel block).  Asynchronous
          replication operations are always handled by a single,
          dedicated thread.

          Range: 1 only.

        * 'io': File system and other miscellaneous operations.  These
          are not demanding tasks, and are always handled as a group by
          a single, dedicated I/O thread.

          Range: 1 only.

        * 'watchdog': Settings to this parameter are actually applied to
          several threads of this type having specific uses.  These
          threads include the 'SocketServer' thread which receives
          connection setups from other nodes, the 'SocketClient' thread
          which attempts to set up connections to other nodes, and the
          thread watchdog thread that checks that threads are
          progressing.

          Range: 1 only.

Simple examples:

     # Example 1.

     ThreadConfig=ldm={count=2,cpubind=1,2},main={cpubind=12},rep={cpubind=11}

     # Example 2.

     Threadconfig=main={cpubind=0},ldm={count=4,cpubind=1,2,5,6},io={cpubind=3}

It is usually desirable when configuring thread usage for a data node
host to reserve one or more number of CPUs for operating system and
other tasks.  Thus, for a host machine with 24 CPUs, you might want to
use 20 CPU threads (leaving 4 for other uses), with 8 LDM threads, 4 TC
threads (half the number of LDM threads), 3 send threads, 3 receive
threads, and 1 thread each for schema management, asynchronous
replication, and I/O operations.  (This is almost the same distribution
of threads used when 'MaxNoOfExecutionThreads' is set equal to 20.)  The
following 'ThreadConfig' setting performs these assignments,
additionally binding all of these threads to specific CPUs:

     ThreadConfig=ldm{count=8,cpubind=1,2,3,4,5,6,7,8},main={cpubind=9},io={cpubind=9}, \
     rep={cpubind=10},tc{count=4,cpubind=11,12,13,14},recv={count=3,cpubind=15,16,17}, \
     send{count=3,cpubind=18,19,20}

It should be possible in most cases to bind the main (schema management)
thread and the I/O thread to the same CPU, as we have done in the
example just shown.

In order to take advantage of the enhanced stability that the use of
'ThreadConfig' offers, it is necessary to insure that CPUs are isolated,
and that they not subject to interrupts, or to being scheduled for other
tasks by the operating system.  On many Linux systems, you can do this
by setting 'IRQBALANCE_BANNED_CPUS' in '/etc/sysconfig/irqbalance' to
'0xFFFFF0', and by using the 'isolcpus' boot option in 'grub.conf'.  For
specific information, see your operating system or platform
documentation.

Disk Data Configuration Parameters

Configuration parameters affecting Disk Data behavior include the
following:

   * 
     'DiskPageBufferEntries'

     *This table provides type and value information for the
     DiskPageBufferEntries data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.19
                                          
     *Type or units*                      32K pages
                                          
     *Default*                            10
                                          
     *Range*                              1 - 1000
                                          
     *Restart Type*                       N

     This is the number of page entries (page references) to allocate.
     It is specified as a number of 32K pages in 'DiskPageBufferMemory'.
     The default is sufficient for most cases but you may need to
     increase the value of this parameter if you encounter problems with
     very large transactions on Disk Data tables.  Each page entry
     requires approximately 100 bytes.

   * 
     'DiskPageBufferMemory'

     *This table provides type and value information for the
     DiskPageBufferMemory data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      bytes
                                          
     *Default*                            64M
                                          
     *Range*                              4M - 1T
                                          
     *Restart Type*                       N

     This determines the amount of space used for caching pages on disk,
     and is set in the '[ndbd]' or '[ndbd default]' section of the
     'config.ini' file.  It is measured in bytes.  Each page takes up 32
     KB. This means that NDB Cluster Disk Data storage always uses N *
     32 KB memory where N is some nonnegative integer.

     The default value for this parameter is '64M' (2000 pages of 32 KB
     each).

     If the value for 'DiskPageBufferMemory' is set too low in
     conjunction with using more than the default number of LDM threads
     in 'ThreadConfig' (for example '{ldm=6...}'), problems can arise
     when trying to add a large (for example 500G) data file to a
     disk-based 'NDB' table, wherein the process takes indefinitely long
     while occupying one of the CPU cores.

     This is due to the fact that, as part of adding a data file to a
     tablespace, extent pages are locked into memory in an extra PGMAN
     worker thread, for quick metadata access.  When adding a large
     file, this worker has insufficient memory for all of the data file
     metadata.  In such cases, you should either increase
     'DiskPageBufferMemory', or add smaller tablespace files.  You may
     also need to adjust 'DiskPageBufferEntries'.

     You can query the *note 'ndbinfo.diskpagebuffer':
     mysql-cluster-ndbinfo-diskpagebuffer. table to help determine
     whether the value for this parameter should be increased to
     minimize unnecessary disk seeks.  See *note
     mysql-cluster-ndbinfo-diskpagebuffer::, for more information.

   * 
     'SharedGlobalMemory'

     *This table provides type and value information for the
     SharedGlobalMemory data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      bytes
                                          
     *Default*                            128M
                                          
     *Range*                              0 - 64T
                                          
     *Restart Type*                       N

     This parameter determines the amount of memory that is used for log
     buffers, disk operations (such as page requests and wait queues),
     and metadata for tablespaces, log file groups, 'UNDO' files, and
     data files.  The shared global memory pool also provides memory
     used for satisfying the memory requirements of the
     'UNDO_BUFFER_SIZE' option used with *note 'CREATE LOGFILE GROUP':
     create-logfile-group. and *note 'ALTER LOGFILE GROUP':
     alter-logfile-group. statements, including any default value
     implied for this options by the setting of the
     'InitialLogFileGroup' data node configuration parameter.
     'SharedGlobalMemory' can be set in the '[ndbd]' or '[ndbd default]'
     section of the 'config.ini' configuration file, and is measured in
     bytes.

     As of NDB 7.2.0, the default value is '128M'.  (Previously, this
     was '20M'.)

   * 
     'DiskIOThreadPool'

     *This table provides type and value information for the
     DiskIOThreadPool data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      threads
                                          
     *Default*                            2
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     This parameter determines the number of unbound threads used for
     Disk Data file access.  Before 'DiskIOThreadPool' was introduced,
     exactly one thread was spawned for each Disk Data file, which could
     lead to performance issues, particularly when using very large data
     files.  With 'DiskIOThreadPool', you can--for example--access a
     single large data file using several threads working in parallel.

     This parameter applies to Disk Data I/O threads only.

     The optimum value for this parameter depends on your hardware and
     configuration, and includes these factors:

        * Physical distribution of Disk Data files

          You can obtain better performance by placing data files, undo
          log files, and the data node file system on separate physical
          disks.  If you do this with some or all of these sets of
          files, then you can set 'DiskIOThreadPool' higher to enable
          separate threads to handle the files on each disk.

        * Disk performance and types

          The number of threads that can be accommodated for Disk Data
          file handling is also dependent on the speed and throughput of
          the disks.  Faster disks and higher throughput allow for more
          disk I/O threads.  Our test results indicate that solid-state
          disk drives can handle many more disk I/O threads than
          conventional disks, and thus higher values for
          'DiskIOThreadPool'.

     In NDB Cluster 7.2, the default value for this parameter is 2.

   * Disk Data file system parameters

     The parameters in the following list make it possible to place NDB
     Cluster Disk Data files in specific directories without the need
     for using symbolic links.

        * 
          'FileSystemPathDD'

          *This table provides type and value information for the
          FileSystemPathDD data node configuration parameter*

          Property                             Value
                                               
          *Version (or later)*                 NDB 7.2.1
                                               
          *Type or units*                      filename
                                               
          *Default*                            [see text]
                                               
          *Range*                              ...
                                               
          *Restart Type*                       IN

          If this parameter is specified, then NDB Cluster Disk Data
          data files and undo log files are placed in the indicated
          directory.  This can be overridden for data files, undo log
          files, or both, by specifying values for
          'FileSystemPathDataFiles', 'FileSystemPathUndoFiles', or both,
          as explained for these parameters.  It can also be overridden
          for data files by specifying a path in the 'ADD DATAFILE'
          clause of a *note 'CREATE TABLESPACE': create-tablespace. or
          *note 'ALTER TABLESPACE': alter-tablespace. statement, and for
          undo log files by specifying a path in the 'ADD UNDOFILE'
          clause of a *note 'CREATE LOGFILE GROUP':
          create-logfile-group. or *note 'ALTER LOGFILE GROUP':
          alter-logfile-group. statement.  If 'FileSystemPathDD' is not
          specified, then 'FileSystemPath' is used.

          If a 'FileSystemPathDD' directory is specified for a given
          data node (including the case where the parameter is specified
          in the '[ndbd default]' section of the 'config.ini' file),
          then starting that data node with '--initial' causes all files
          in the directory to be deleted.

        * 
          'FileSystemPathDataFiles'

          *This table provides type and value information for the
          FileSystemPathDataFiles data node configuration parameter*

          Property                             Value
                                               
          *Version (or later)*                 NDB 7.2.1
                                               
          *Type or units*                      filename
                                               
          *Default*                            [see text]
                                               
          *Range*                              ...
                                               
          *Restart Type*                       IN

          If this parameter is specified, then NDB Cluster Disk Data
          data files are placed in the indicated directory.  This
          overrides any value set for 'FileSystemPathDD'.  This
          parameter can be overridden for a given data file by
          specifying a path in the 'ADD DATAFILE' clause of a *note
          'CREATE TABLESPACE': create-tablespace. or *note 'ALTER
          TABLESPACE': alter-tablespace. statement used to create that
          data file.  If 'FileSystemPathDataFiles' is not specified,
          then 'FileSystemPathDD' is used (or 'FileSystemPath', if
          'FileSystemPathDD' has also not been set).

          If a 'FileSystemPathDataFiles' directory is specified for a
          given data node (including the case where the parameter is
          specified in the '[ndbd default]' section of the 'config.ini'
          file), then starting that data node with '--initial' causes
          all files in the directory to be deleted.

        * 
          'FileSystemPathUndoFiles'

          *This table provides type and value information for the
          FileSystemPathUndoFiles data node configuration parameter*

          Property                             Value
                                               
          *Version (or later)*                 NDB 7.2.1
                                               
          *Type or units*                      filename
                                               
          *Default*                            [see text]
                                               
          *Range*                              ...
                                               
          *Restart Type*                       IN

          If this parameter is specified, then NDB Cluster Disk Data
          undo log files are placed in the indicated directory.  This
          overrides any value set for 'FileSystemPathDD'.  This
          parameter can be overridden for a given data file by
          specifying a path in the 'ADD UNDO' clause of a *note 'CREATE
          LOGFILE GROUP': create-logfile-group. or *note 'ALTER LOGFILE
          GROUP': alter-logfile-group. statement used to create that
          data file.  If 'FileSystemPathUndoFiles' is not specified,
          then 'FileSystemPathDD' is used (or 'FileSystemPath', if
          'FileSystemPathDD' has also not been set).

          If a 'FileSystemPathUndoFiles' directory is specified for a
          given data node (including the case where the parameter is
          specified in the '[ndbd default]' section of the 'config.ini'
          file), then starting that data node with '--initial' causes
          all files in the directory to be deleted.

     For more information, see *note mysql-cluster-disk-data-objects::.

   * Disk Data object creation parameters

     The next two parameters enable you--when starting the cluster for
     the first time--to cause a Disk Data log file group, tablespace, or
     both, to be created without the use of SQL statements.

        * 
          'InitialLogFileGroup'

          *This table provides type and value information for the
          InitialLogFileGroup data node configuration parameter*

          Property                             Value
                                               
          *Version (or later)*                 NDB 7.2.1
                                               
          *Type or units*                      string
                                               
          *Default*                            [see text]
                                               
          *Range*                              ...
                                               
          *Restart Type*                       S

          This parameter can be used to specify a log file group that is
          created when performing an initial start of the cluster.
          'InitialLogFileGroup' is specified as shown here:

               InitialLogFileGroup = [name=NAME;] [undo_buffer_size=SIZE;] FILE-SPECIFICATION-LIST

               FILE-SPECIFICATION-LIST:
                   FILE-SPECIFICATION[; FILE-SPECIFICATION[; ...]]

               FILE-SPECIFICATION:
                   FILENAME:SIZE

          The 'name' of the log file group is optional and defaults to
          'DEFAULT-LG'.  The 'undo_buffer_size' is also optional; if
          omitted, it defaults to '64M'.  Each FILE-SPECIFICATION
          corresponds to an undo log file, and at least one must be
          specified in the FILE-SPECIFICATION-LIST.  Undo log files are
          placed according to any values that have been set for
          'FileSystemPath', 'FileSystemPathDD', and
          'FileSystemPathUndoFiles', just as if they had been created as
          the result of a *note 'CREATE LOGFILE GROUP':
          create-logfile-group. or *note 'ALTER LOGFILE GROUP':
          alter-logfile-group. statement.

          Consider the following:

               InitialLogFileGroup = name=LG1; undo_buffer_size=128M; undo1.log:250M; undo2.log:150M

          This is equivalent to the following SQL statements:

               CREATE LOGFILE GROUP LG1
                   ADD UNDOFILE 'undo1.log'
                   INITIAL_SIZE 250M
                   UNDO_BUFFER_SIZE 128M
                   ENGINE NDBCLUSTER;

               ALTER LOGFILE GROUP LG1
                   ADD UNDOFILE 'undo2.log'
                   INITIAL_SIZE 150M
                   ENGINE NDBCLUSTER;

          This logfile group is created when the data nodes are started
          with '--initial'.

          Resources for the initial log file group are taken from the
          global memory pool whose size is determined by the value of
          the 'SharedGlobalMemory' data node configuration parameter; if
          this parameter is set too low and the values set in
          'InitialLogFileGroup' for the logfile group's initial size or
          undo buffer size are too high, the cluster may fail to create
          the default log file group when starting, or fail to start
          altogether.

          This parameter, if used, should always be set in the '[ndbd
          default]' section of the 'config.ini' file.  The behavior of
          an NDB Cluster when different values are set on different data
          nodes is not defined.

        * 
          'InitialTablespace'

          *This table provides type and value information for the
          InitialTablespace data node configuration parameter*

          Property                             Value
                                               
          *Version (or later)*                 NDB 7.2.1
                                               
          *Type or units*                      string
                                               
          *Default*                            [see text]
                                               
          *Range*                              ...
                                               
          *Restart Type*                       S

          This parameter can be used to specify an NDB Cluster Disk Data
          tablespace that is created when performing an initial start of
          the cluster.  'InitialTablespace' is specified as shown here:

               InitialTablespace = [name=NAME;] [extent_size=SIZE;] FILE-SPECIFICATION-LIST

          The 'name' of the tablespace is optional and defaults to
          'DEFAULT-TS'.  The 'extent_size' is also optional; it defaults
          to '1M'.  The FILE-SPECIFICATION-LIST uses the same syntax as
          shown with the 'InitialLogfileGroup' parameter, the only
          difference being that each FILE-SPECIFICATION used with
          'InitialTablespace' corresponds to a data file.  At least one
          must be specified in the FILE-SPECIFICATION-LIST.  Data files
          are placed according to any values that have been set for
          'FileSystemPath', 'FileSystemPathDD', and
          'FileSystemPathDataFiles', just as if they had been created as
          the result of a *note 'CREATE TABLESPACE': create-tablespace.
          or *note 'ALTER TABLESPACE': alter-tablespace. statement.

          For example, consider the following line specifying
          'InitialTablespace' in the '[ndbd default]' section of the
          'config.ini' file (as with 'InitialLogfileGroup', this
          parameter should always be set in the '[ndbd default]'
          section, as the behavior of an NDB Cluster when different
          values are set on different data nodes is not defined):

               InitialTablespace = name=TS1; extent_size=8M; data1.dat:2G; data2.dat:4G

          This is equivalent to the following SQL statements:

               CREATE TABLESPACE TS1
                   ADD DATAFILE 'data1.dat'
                   EXTENT_SIZE 8M
                   INITIAL_SIZE 2G
                   ENGINE NDBCLUSTER;

               ALTER TABLESPACE TS1
                   ADD DATAFILE 'data2.dat'
                   INITIAL_SIZE 4G
                   ENGINE NDBCLUSTER;

          This tablespace is created when the data nodes are started
          with '--initial', and can be used whenever creating NDB
          Cluster Disk Data tables thereafter.

Disk Data and GCP Stop errors

Errors encountered when using Disk Data tables such as 'Node NODEID
killed this node because GCP stop was detected' (error 2303) are often
referred to as 'GCP stop errors'.  Such errors occur when the redo log
is not flushed to disk quickly enough; this is usually due to slow disks
and insufficient disk throughput.

You can help prevent these errors from occurring by using faster disks,
and by placing Disk Data files on a separate disk from the data node
file system.  Reducing the value of 'TimeBetweenGlobalCheckpoints' tends
to decrease the amount of data to be written for each global checkpoint,
and so may provide some protection against redo log buffer overflows
when trying to write a global checkpoint; however, reducing this value
also permits less time in which to write the GCP, so this must be done
with caution.

In addition to the considerations given for 'DiskPageBufferMemory' as
explained previously, it is also very important that the
'DiskIOThreadPool' configuration parameter be set correctly; having
'DiskIOThreadPool' set too high is very likely to cause GCP stop errors
(Bug #37227).

GCP stops can be caused by save or commit timeouts; the
'TimeBetweenEpochsTimeout' data node configuration parameter determines
the timeout for commits.  However, it is possible to disable both types
of timeouts by setting this parameter to 0.

Parameters for configuring send buffer memory allocation

Send buffer memory is allocated dynamically from a memory pool shared
between all transporters, which means that the size of the send buffer
can be adjusted as necessary.  (Previously, the NDB kernel used a
fixed-size send buffer for every node in the cluster, which was
allocated when the node started and could not be changed while the node
was running.)  The 'TotalSendBufferMemory' and 'OverLoadLimit' data node
configuration parameters permit the setting of limits on this memory
allocation.  For more information about the use of these parameters (as
well as 'SendBufferMemory'), see *note
mysql-cluster-config-send-buffers::.

   * 
     'ExtraSendBufferMemory'

     This parameter specifies the amount of transporter send buffer
     memory to allocate in addition to any set using
     'TotalSendBufferMemory', 'SendBufferMemory', or both.

     This parameter was added in NDB 7.2.5.  (Bug #11760629, Bug #53053)

   * 
     'TotalSendBufferMemory'

     This parameter is used to determine the total amount of memory to
     allocate on this node for shared send buffer memory among all
     configured transporters.

     Prior to NDB 7.2.5, this parameter did not work correctly with
     *note 'ndbmtd': mysql-cluster-programs-ndbmtd.  (Bug #13633845)

     If this parameter is set, its minimum permitted value is 256KB; 0
     indicates that the parameter has not been set.  For more detailed
     information, see *note mysql-cluster-config-send-buffers::.

   * 
     'ReservedSendBufferMemory'

     This parameter is present in *note 'NDBCLUSTER': mysql-cluster.
     source code beginning with NDB 6.4.0.  However, it is not currently
     enabled.

     As of NDB 7.2.5, this parameter is deprecated, and is subject to
     removal in a future release of NDB Cluster (Bug #11760629, Bug
     #53053).

For more detailed information about the behavior and use of
'TotalSendBufferMemory' and 'ReservedSendBufferMemory', and about
configuring send buffer memory parameters in NDB Cluster, see *note
mysql-cluster-config-send-buffers::.

See also *note mysql-cluster-online-add-node::.

Redo log over-commit handling

It is possible to control a data node's handling of operations when too
much time is taken flushing redo logs to disk.  This occurs when a given
redo log flush takes longer than 'RedoOverCommitLimit' seconds, more
than 'RedoOverCommitCounter' times, causing any pending transactions to
be aborted.  When this happens, the API node that sent the transaction
can handle the operations that should have been committed either by
queuing the operations and re-trying them, or by aborting them, as
determined by 'DefaultOperationRedoProblemAction'.  The data node
configuration parameters for setting the timeout and number of times it
may be exceeded before the API node takes this action are described in
the following list:

   * 
     'RedoOverCommitCounter'

     *This table provides type and value information for the
     RedoOverCommitCounter data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      numeric
                                          
     *Default*                            3
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     When 'RedoOverCommitLimit' is exceeded when trying to write a given
     redo log to disk this many times or more, any transactions that
     were not committed as a result are aborted, and an API node where
     any of these transactions originated handles the operations making
     up those transactions according to its value for
     'DefaultOperationRedoProblemAction' (by either queuing the
     operations to be re-tried, or aborting them).

     'RedoOverCommitCounter' defaults to 3.  Set it to 0 to disable the
     limit.

   * 
     'RedoOverCommitLimit'

     *This table provides type and value information for the
     RedoOverCommitLimit data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      seconds
                                          
     *Default*                            20
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     This parameter sets an upper limit in seconds for trying to write a
     given redo log to disk before timing out.  The number of times the
     data node tries to flush this redo log, but takes longer than
     'RedoOverCommitLimit', is kept and compared with
     'RedoOverCommitCounter', and when flushing takes too long more
     times than the value of that parameter, any transactions that were
     not committed as a result of the flush timeout are aborted.  When
     this occurs, the API node where any of these transactions
     originated handles the operations making up those transactions
     according to its 'DefaultOperationRedoProblemAction' setting (it
     either queues the operations to be re-tried, or aborts them).

     By default, 'RedoOverCommitLimit' is 20 seconds.  Set to 0 to
     disable checking for redo log flush timeouts.  This parameter was
     added in NDB 7.1.10.

Controlling restart attempts

It is possible to exercise finely-grained control over restart attempts
by data nodes when they fail to start using the 'MaxStartFailRetries'
and 'StartFailRetryDelay' data node configuration parameters.

'MaxStartFailRetries' limits the total number of retries made before
giving up on starting the data node, 'StartFailRetryDelay' sets the
number of seconds between retry attempts.  These parameters are listed
here:

   * 
     'StartFailRetryDelay'

     *This table provides type and value information for the
     StartFailRetryDelay data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      unsigned
                                          
     *Default*                            0
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     Use this parameter to set the number of seconds between restart
     attempts by the data node in the event on failure on startup.  The
     default is 0 (no delay).

     Both this parameter and 'MaxStartFailRetries' are ignored unless
     'StopOnError' is equal to 0.

   * 
     'MaxStartFailRetries'

     *This table provides type and value information for the
     MaxStartFailRetries data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      unsigned
                                          
     *Default*                            3
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     Use this parameter to limit the number restart attempts made by the
     data node in the event that it fails on startup.  The default is 3
     attempts.

     Both this parameter and 'StartFailRetryDelay' are ignored unless
     'StopOnError' is equal to 0.

NDB index statistics parameters

The parameters in the following list relate to NDB index statistics
generation, which was introduced in NDB 7.2.1.

   * 
     'IndexStatAutoCreate'

     *This table provides type and value information for the
     IndexStatAutoCreate data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      integer
                                          
     *Default*                            0
                                          
     *Range*                              0, 1
                                          
     *Restart Type*                       S

     Enable (set equal to 1) or disable (set equal to 0) automatic
     statistics collection when indexes are created.  Disabled by
     default.

     This parameter was added in NDB 7.2.1.

   * 
     'IndexStatAutoUpdate'

     *This table provides type and value information for the
     IndexStatAutoUpdate data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      integer
                                          
     *Default*                            0
                                          
     *Range*                              0, 1
                                          
     *Restart Type*                       S

     Enable (set equal to 1) or disable (set equal to 0) monitoring of
     indexes for changes and trigger automatic statistics updates these
     are detected.  The amount and degree of change needed to trigger
     the updates are determined by the settings for the
     'IndexStatTriggerPct' and 'IndexStatTriggerScale' options.

     This parameter was added in NDB 7.2.1.

   * 
     'IndexStatSaveSize'

     *This table provides type and value information for the
     IndexStatSaveSize data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      bytes
                                          
     *Default*                            32768
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       IN

     Maximum space in bytes allowed for the saved statistics of any
     given index in the *note 'NDB': mysql-cluster. system tables and in
     the *note 'mysqld': mysqld. memory cache.  This consumes
     'IndexMemory'.

     At least one sample is always produced, regardless of any size
     limit.  Note that this size is scaled by 'IndexStatSaveScale'.

     This parameter was added in NDB 7.2.1.

     The size specified by 'IndexStatSaveSize' is scaled by the value of
     'IndexStatTriggerPct' for a large index, times 0.01.  Note that
     this is further multiplied by the logarithm to the base 2 of the
     index size.  Setting 'IndexStatTriggerPct' equal to 0 disables the
     scaling effect.

   * 
     'IndexStatSaveScale'

     *This table provides type and value information for the
     IndexStatSaveScale data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      percentage
                                          
     *Default*                            100
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       IN

     The size specified by 'IndexStatSaveSize' is scaled by the value of
     'IndexStatTriggerPct' for a large index, times 0.01.  Note that
     this is further multiplied by the logarithm to the base 2 of the
     index size.  Setting 'IndexStatTriggerPct' equal to 0 disables the
     scaling effect.

     This parameter was added in NDB 7.2.1.

   * 
     'IndexStatTriggerPct'

     *This table provides type and value information for the
     IndexStatTriggerPct data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      percentage
                                          
     *Default*                            100
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       IN

     Percentage change in updates that triggers an index statistics
     update.  The value is scaled by 'IndexStatTriggerScale'.  You can
     disable this trigger altogether by setting 'IndexStatTriggerPct' to
     0.

     This parameter was added in NDB 7.2.1.

   * 
     'IndexStatTriggerScale'

     *This table provides type and value information for the
     IndexStatTriggerScale data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      percentage
                                          
     *Default*                            100
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       IN

     Scale 'IndexStatTriggerPct' by this amount times 0.01 for a large
     index.  A value of 0 disables scaling.

     This parameter was added in NDB 7.2.1.

   * 
     'IndexStatUpdateDelay'

     *This table provides type and value information for the
     IndexStatUpdateDelay data node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      seconds
                                          
     *Default*                            60
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       IN

     Minimum delay in seconds between automatic index statistics updates
     for a given index.  Setting this variable to 0 disables any delay.
     The default is 60 seconds.

     This parameter was added in NDB 7.2.1.


File: manual.info.tmp,  Node: mysql-cluster-api-definition,  Next: mysql-cluster-options-variables,  Prev: mysql-cluster-ndbd-definition,  Up: mysql-cluster-config-file

18.3.3.7 Defining SQL and Other API Nodes in an NDB Cluster
...........................................................

The '[mysqld]' and '[api]' sections in the 'config.ini' file define the
behavior of the MySQL servers (SQL nodes) and other applications (API
nodes) used to access cluster data.  None of the parameters shown is
required.  If no computer or host name is provided, any host can use
this SQL or API node.

Generally speaking, a '[mysqld]' section is used to indicate a MySQL
server providing an SQL interface to the cluster, and an '[api]' section
is used for applications other than *note 'mysqld': mysqld. processes
accessing cluster data, but the two designations are actually
synonymous; you can, for instance, list parameters for a MySQL server
acting as an SQL node in an '[api]' section.

*Note*:

For a discussion of MySQL server options for NDB Cluster, see *note
mysql-cluster-program-options-mysqld::.  For information about MySQL
server system variables relating to NDB Cluster, see *note
mysql-cluster-system-variables::.

Restart types

Information about the restart types used by the parameter descriptions
in this section is shown in the following table:

*NDB Cluster restart types*

Symbol  Restart Type           Description
                               
*N*     Node                   The parameter can be updated using a
                               rolling restart (see
                               *note mysql-cluster-rolling-restart::)
                               
*S*     System                 All cluster nodes must be shut down
                               completely, then restarted, to effect a
                               change in this parameter
                               
*I*     Initial                Data nodes must be restarted using the
                               '--initial' option

   * 
     'Id'

     *This table provides type and value information for the Id API node
     configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      unsigned
                                          
     *Default*                            [none]
                                          
     *Range*                              1 - 255
                                          
     *Restart Type*                       IS

     The 'Id' is an integer value used to identify the node in all
     cluster internal messages.  The permitted range of values is 1 to
     255 inclusive.  This value must be unique for each node in the
     cluster, regardless of the type of node.

     *Note*:

     Data node IDs must be less than 49, regardless of the NDB Cluster
     version used.  If you plan to deploy a large number of data nodes,
     it is a good idea to limit the node IDs for API nodes (and
     management nodes) to values greater than 48.

     'NodeId' is the preferred parameter name to use when identifying
     API nodes.  ('Id' continues to be supported for backward
     compatibility, but is now deprecated and generates a warning when
     used.  It is also subject to future removal.)

   * 
     'ConnectionMap'

     *This table provides type and value information for the
     ConnectionMap API node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      string
                                          
     *Default*                            [none]
                                          
     *Range*                              ...
                                          
     *Restart Type*                       N

     Specifies which data nodes to connect.

   * 
     'NodeId'

     *This table provides type and value information for the NodeId API
     node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      unsigned
                                          
     *Default*                            [none]
                                          
     *Range*                              1 - 255
                                          
     *Restart Type*                       IS

     The 'NodeId' is an integer value used to identify the node in all
     cluster internal messages.  The permitted range of values is 1 to
     255 inclusive.  This value must be unique for each node in the
     cluster, regardless of the type of node.

     *Note*:

     Data node IDs must be less than 49, regardless of the NDB Cluster
     version used.  If you plan to deploy a large number of data nodes,
     it is a good idea to limit the node IDs for API nodes (and
     management nodes) to values greater than 48.

     'NodeId' is the preferred parameter name to use when identifying
     management nodes in NDB Cluster 7.2 and later.  Previously, 'Id'
     was used for this purpose and this continues to be supported for
     backward compatibility.  'Id' is now deprecated and generates a
     warning when used; it is subject to removal in a future release of
     NDB Cluster.

   * 
     'ExecuteOnComputer'

     *This table provides type and value information for the
     ExecuteOnComputer API node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      name
                                          
     *Default*                            [none]
                                          
     *Range*                              ...
                                          
     *Restart Type*                       S

     This refers to the 'Id' set for one of the computers (hosts)
     defined in a '[computer]' section of the configuration file.

   * 
     'HostName'

     *This table provides type and value information for the HostName
     API node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      name or IP address
                                          
     *Default*                            [none]
                                          
     *Range*                              ...
                                          
     *Restart Type*                       N

     Specifying this parameter defines the hostname of the computer on
     which the SQL node (API node) is to reside.  To specify a hostname,
     either this parameter or 'ExecuteOnComputer' is required.

     If no 'HostName' or 'ExecuteOnComputer' is specified in a given
     '[mysql]' or '[api]' section of the 'config.ini' file, then an SQL
     or API node may connect using the corresponding 'slot' from any
     host which can establish a network connection to the management
     server host machine.  _This differs from the default behavior for
     data nodes, where 'localhost' is assumed for 'HostName' unless
     otherwise specified_.

   * 
     'ArbitrationRank'

     *This table provides type and value information for the
     ArbitrationRank API node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      0-2
                                          
     *Default*                            0
                                          
     *Range*                              0 - 2
                                          
     *Restart Type*                       N

     This parameter defines which nodes can act as arbitrators.  Both
     management nodes and SQL nodes can be arbitrators.  A value of 0
     means that the given node is never used as an arbitrator, a value
     of 1 gives the node high priority as an arbitrator, and a value of
     2 gives it low priority.  A normal configuration uses the
     management server as arbitrator, setting its 'ArbitrationRank' to 1
     (the default for management nodes) and those for all SQL nodes to 0
     (the default for SQL nodes).

     By setting 'ArbitrationRank' to 0 on all management and SQL nodes,
     you can disable arbitration completely.  You can also control
     arbitration by overriding this parameter; to do so, set the
     'Arbitration' parameter in the '[ndbd default]' section of the
     'config.ini' global configuration file.

   * 
     'ArbitrationDelay'

     *This table provides type and value information for the
     ArbitrationDelay API node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      milliseconds
                                          
     *Default*                            0
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     Setting this parameter to any other value than 0 (the default)
     means that responses by the arbitrator to arbitration requests will
     be delayed by the stated number of milliseconds.  It is usually not
     necessary to change this value.

   * 
     'BatchByteSize'

     *This table provides type and value information for the
     BatchByteSize API node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      bytes
                                          
     *Default*                            16K
                                          
     *Range*                              1K - 1M
                                          
     *Restart Type*                       N

     For queries that are translated into full table scans or range
     scans on indexes, it is important for best performance to fetch
     records in properly sized batches.  It is possible to set the
     proper size both in terms of number of records ('BatchSize') and in
     terms of bytes ('BatchByteSize').  The actual batch size is limited
     by both parameters.

     The speed at which queries are performed can vary by more than 40%
     depending upon how this parameter is set.

     This parameter is measured in bytes.  The default value prior to
     NDB 7.2.1 was 32K; in NDB 7.2.1 and later, the default is 16K.

   * 
     'BatchSize'

     *This table provides type and value information for the BatchSize
     API node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      records
                                          
     *Default*                            256
                                          
     *Range*                              1 - 992
                                          
     *Restart Type*                       N

     This parameter is measured in number of records and is by default
     set to 256 (NDB 7.2.1 and later; previously, the default was 64).
     The maximum size is 992.

   * 
     'ExtraSendBufferMemory'

     *This table provides type and value information for the
     ExtraSendBufferMemory API node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.14
                                          
     *Type or units*                      bytes
                                          
     *Default*                            0
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     This parameter specifies the amount of transporter send buffer
     memory to allocate in addition to any that has been set using
     'TotalSendBufferMemory', 'SendBufferMemory', or both.

     This parameter was added in NDB 7.2.14.  (Bug #14555359)

   * 
     'HeartbeatThreadPriority'

     *This table provides type and value information for the
     HeartbeatThreadPriority API node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      string
                                          
     *Default*                            [none]
                                          
     *Range*                              ...
                                          
     *Restart Type*                       S

     Use this parameter to set the scheduling policy and priority of
     heartbeat threads for management and API nodes.  The syntax for
     setting this parameter is shown here:

          HeartbeatThreadPriority = POLICY[, PRIORITY]

          POLICY:
            {FIFO | RR}

     When setting this parameter, you must specify a policy.  This is
     one of 'FIFO' (first in, first in) or 'RR' (round robin).  This
     followed optionally by the priority (an integer).

   * 
     'MaxScanBatchSize'

     *This table provides type and value information for the
     MaxScanBatchSize API node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      bytes
                                          
     *Default*                            256K
                                          
     *Range*                              32K - 16M
                                          
     *Restart Type*                       N

     The batch size is the size of each batch sent from each data node.
     Most scans are performed in parallel to protect the MySQL Server
     from receiving too much data from many nodes in parallel; this
     parameter sets a limit to the total batch size over all nodes.

     The default value of this parameter is set to 256KB. Its maximum
     size is 16MB.

   * 
     'TotalSendBufferMemory'

     *This table provides type and value information for the
     TotalSendBufferMemory API node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      bytes
                                          
     *Default*                            0
                                          
     *Range*                              256K - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     This parameter is available beginning with NDB 6.4.0.  It is used
     to determine the total amount of memory to allocate on this node
     for shared send buffer memory among all configured transporters.

     If this parameter is set, its minimum permitted value is 256KB; 0
     indicates that the parameter has not been set.  For more detailed
     information, see *note mysql-cluster-config-send-buffers::.

   * 
     'AutoReconnect'

     *This table provides type and value information for the
     AutoReconnect API node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      boolean
                                          
     *Default*                            false
                                          
     *Range*                              true, false
                                          
     *Restart Type*                       N

     This parameter is 'false' by default.  This forces disconnected API
     nodes (including MySQL Servers acting as SQL nodes) to use a new
     connection to the cluster rather than attempting to re-use an
     existing one, as re-use of connections can cause problems when
     using dynamically-allocated node IDs.  (Bug #45921)

     *Note*:

     This parameter can be overridden using the NDB API. For more
     information, see Ndb_cluster_connection::set_auto_reconnect()
     (https://dev.mysql.com/doc/ndbapi/en/ndb-ndb-cluster-connection-set-auto-reconnect.html),
     and Ndb_cluster_connection::get_auto_reconnect()
     (https://dev.mysql.com/doc/ndbapi/en/ndb-ndb-cluster-connection-get-auto-reconnect.html).

   * 
     'DefaultOperationRedoProblemAction'

     *This table provides type and value information for the
     DefaultOperationRedoProblemAction API node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.10
                                          
     *Type or units*                      enumeration
                                          
     *Default*                            QUEUE
                                          
     *Range*                              ABORT, QUEUE
                                          
     *Restart Type*                       S

     This parameter (along with 'RedoOverCommitLimit' and
     'RedoOverCommitCounter') controls the data node's handling of
     operations when too much time is taken flushing redo logs to disk.
     This occurs when a given redo log flush takes longer than
     'RedoOverCommitLimit' seconds, more than 'RedoOverCommitCounter'
     times, causing any pending transactions to be aborted.

     When this happens, the node can respond in either of two ways,
     according to the value of 'DefaultOperationRedoProblemAction',
     listed here:

        * 'ABORT': Any pending operations from aborted transactions are
          also aborted.

        * 'QUEUE': Pending operations from transactions that were
          aborted are queued up to be re-tried.  This the default in NDB
          Cluster 7.2 and later.  In NDB 7.2.21 and later, pending
          operations are still aborted when the redo log runs out of
          space--that is, when P_TAIL_PROBLEM errors occur.  (Bug
          #20782580)

     Prior to NDB 7.2.10, setting this parameter did not have any
     effect.  (Bug #15855588)

   * 
     'DefaultHashMapSize'

     *This table provides type and value information for the
     DefaultHashMapSize API node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.11
                                          
     *Type or units*                      buckets
                                          
     *Default*                            3840
                                          
     *Range*                              0 - 3840
                                          
     *Restart Type*                       N

     NDB 7.2.7 and later use a larger default table hash map size (3840)
     than in previous releases (240).  Beginning with NDB 7.2.11, the
     size of the table hash maps used by *note 'NDB': mysql-cluster. is
     configurable using this parameter; previously this value was
     hard-coded.  'DefaultHashMapSize' can take any of three possible
     values (0, 240, 3840).  These values and their effects are
     described in the following table.

     *DefaultHashMapSize parameter values*

     Value          Description / Effect
                    
     '0'            Use the lowest value set, if any, for this parameter
                    among all data nodes and API nodes in the cluster; if it
                    is not set on any data or API node, use the default
                    value.
                    
     '240'          Original hash map size (used by default prior to NDB
                    7.2.7.
                    
     '3840'         Larger hash map size as (used by default in NDB 7.2.7
                    and later

     The primary intended use for this parameter is to facilitate
     upgrades and especially downgrades between NDB 7.2.7 and later NDB
     Cluster versions, in which the larger hash map size (3840) is the
     default, and earlier releases (in which the default was 240), due
     to the fact that this change is not otherwise backward compatible
     (Bug #14800539).  By setting this parameter to 240 prior to
     performing an upgrade from an older version where this value is in
     use, you can cause the cluster to continue using the smaller size
     for table hash maps, in which case the tables remain compatible
     with earlier versions following the upgrade.  'DefaultHashMapSize'
     can be set for individual data nodes, API nodes, or both, but
     setting it once only, in the '[ndbd default]' section of the
     'config.ini' file, is the recommended practice.

     After increasing this parameter, to have existing tables to take
     advantage of the new size, you can run *note 'ALTER TABLE ...
     REORGANIZE PARTITION': alter-table-partition-operations. on them,
     after which they can use the larger hash map size.  This is in
     addition to performing a rolling restart, which makes the larger
     hash maps available to new tables, but does not enable existing
     tables to use them.

     Decreasing this parameter online after any tables have been created
     or modified with 'DefaultHashMapSize' equal to 3840 is not
     currently supported.

   * 
     'Wan'

     *This table provides type and value information for the wan API
     node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      boolean
                                          
     *Default*                            false
                                          
     *Range*                              true, false
                                          
     *Restart Type*                       N

     Use WAN TCP setting as default.

   * 
     'ConnectBackoffMaxTime'

     *This table provides type and value information for the
     ConnectBackoffMaxTime API node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.18
                                          
     *Type or units*                      integer
                                          
     *Default*                            0
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     Starting with NDB 7.2.18, in an NDB Cluster with many unstarted
     data nodes, the value of this parameter can be raised to circumvent
     connection attempts to data nodes which have not yet begun to
     function in the cluster, as well as moderate high traffic to
     management nodes.  As long as the API node is not connected to any
     new data nodes, the value of the 'StartConnectBackoffMaxTime'
     parameter is applied; otherwise, 'ConnectBackoffMaxTime' is used to
     determine the length of time in milliseconds to wait between
     connection attempts.

     Time elapsed _during_ node connection attempts is not taken into
     account when calculating elapsed time for this parameter.  The
     timeout is applied with approximately 100 ms resolution, starting
     with a 100 ms delay; for each subsequent attempt, the length of
     this period is doubled until it reaches 'ConnectBackoffMaxTime'
     milliseconds, up to a maximum of 100000 ms (100s).

     Once the API node is connected to a data node and that node reports
     (in a heartbeat message) that it has connected to other data nodes,
     connection attempts to those data nodes are no longer affected by
     this parameter, and are made every 100 ms thereafter until
     connected.  Note that, once a data node has started, it can take up
     'HeartbeatIntervalDbApi' for the API node to be notified that this
     has occurred.

   * 
     'StartConnectBackoffMaxTime'

     *This table provides type and value information for the
     StartConnectBackoffMaxTime API node configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.18
                                          
     *Type or units*                      integer
                                          
     *Default*                            1500
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     Starting with NDB 7.2.18, in an NDB Cluster with many unstarted
     data nodes, the value of this parameter can be raised to circumvent
     connection attempts to data nodes which have not yet begun to
     function in the cluster, as well as moderate high traffic to
     management nodes.  As long as the API node is not connected to any
     new data nodes, the value of the 'StartConnectBackoffMaxTime'
     parameter is applied; otherwise, 'ConnectBackoffMaxTime' is used to
     determine the length of time in milliseconds to wait between
     connection attempts.

     Time elapsed _during_ node connection attempts is not taken into
     account when calculating elapsed time for this parameter.  The
     timeout is applied with approximately 100 ms resolution, starting
     with a 100 ms delay; for each subsequent attempt, the length of
     this period is doubled until it reaches
     'StartConnectBackoffMaxTime' milliseconds, up to a maximum of
     100000 ms (100s).

     Once the API node is connected to a data node and that node reports
     (in a heartbeat message) that it has connected to other data nodes,
     connection attempts to those data nodes are no longer affected by
     this parameter, and are made every 100 ms thereafter until
     connected.  Note that, once a data node has started, it can take up
     'HeartbeatIntervalDbApi' for the API node to be notified that this
     has occurred.

You can also obtain information from a MySQL server running as an NDB
Cluster SQL node using *note 'SHOW STATUS': show-status. in the *note
'mysql': mysql. client, as shown here:

     mysql> SHOW STATUS LIKE 'ndb%';
     +-----------------------------+----------------+
     | Variable_name               | Value          |
     +-----------------------------+----------------+
     | Ndb_cluster_node_id         | 5              |
     | Ndb_config_from_host        | 198.51.100.112 |
     | Ndb_config_from_port        | 1186           |
     | Ndb_number_of_storage_nodes | 4              |
     +-----------------------------+----------------+
     4 rows in set (0.02 sec)

For information about the status variables appearing in the output from
this statement, see *note mysql-cluster-status-variables::.

*Note*:

To add new SQL or API nodes to the configuration of a running NDB
Cluster, it is necessary to perform a rolling restart of all cluster
nodes after adding new '[mysqld]' or '[api]' sections to the
'config.ini' file (or files, if you are using more than one management
server).  This must be done before the new SQL or API nodes can connect
to the cluster.

It is _not_ necessary to perform any restart of the cluster if new SQL
or API nodes can employ previously unused API slots in the cluster
configuration to connect to the cluster.


File: manual.info.tmp,  Node: mysql-cluster-options-variables,  Next: mysql-cluster-tcp-definition,  Prev: mysql-cluster-api-definition,  Up: mysql-cluster-config-file

18.3.3.8 MySQL Server Options and Variables for NDB Cluster
...........................................................

* Menu:

* mysql-cluster-program-options-mysqld::  MySQL Server Options for NDB Cluster
* mysql-cluster-system-variables::  NDB Cluster System Variables
* mysql-cluster-status-variables::  NDB Cluster Status Variables

This section provides information about MySQL server options, server and
status variables that are specific to NDB Cluster.  For general
information on using these, and for other options and variables not
specific to NDB Cluster, see *note mysqld-server::.

For NDB Cluster configuration parameters used in the cluster
configuration file (usually named 'config.ini'), see *note
mysql-cluster-configuration::.


File: manual.info.tmp,  Node: mysql-cluster-program-options-mysqld,  Next: mysql-cluster-system-variables,  Prev: mysql-cluster-options-variables,  Up: mysql-cluster-options-variables

18.3.3.9 MySQL Server Options for NDB Cluster
.............................................

This section provides descriptions of *note 'mysqld': mysqld. server
options relating to NDB Cluster.  For information about *note 'mysqld':
mysqld. options not specific to NDB Cluster, and for general information
about the use of options with *note 'mysqld': mysqld, see *note
server-options::.

For information about command-line options used with other NDB Cluster
processes (*note 'ndbd': mysql-cluster-programs-ndbd, *note 'ndb_mgmd':
mysql-cluster-programs-ndb-mgmd, and *note 'ndb_mgm':
mysql-cluster-programs-ndb-mgm.), see *note
mysql-cluster-program-options-common::.  For information about
command-line options used with *note 'NDB': mysql-cluster. utility
programs (such as *note 'ndb_desc': mysql-cluster-programs-ndb-desc,
*note 'ndb_size.pl': mysql-cluster-programs-ndb-size-pl, and *note
'ndb_show_tables': mysql-cluster-programs-ndb-show-tables.), see *note
mysql-cluster-programs::.

   * 
     '--ndbcluster'

     *Type and value information for ndbcluster*

     Property                  Value
                               
     Name                      'ndbcluster'
                               
     Command Line              Yes
                               
     System Variable           No
                               
     Option File               Yes
                               
     Scope                     

     Dynamic                   No
                               
     Type: Default, Range      boolean: OFF (Version: 5.5)
                               
     Notes                     
                               DESCRIPTION: Enable NDB Cluster (if this
                               version of MySQL supports it)
                               
                               Disabled by '--skip-ndbcluster'.

     The *note 'NDBCLUSTER': mysql-cluster. storage engine is necessary
     for using NDB Cluster.  If a *note 'mysqld': mysqld. binary
     includes support for the *note 'NDBCLUSTER': mysql-cluster. storage
     engine, the engine is disabled by default.  Use the '--ndbcluster'
     option to enable it.  Use '--skip-ndbcluster' to explicitly disable
     the engine.

   * 
     '--ndb-batch-size=#'

     *Type and value information for ndb-batch-size*

     Property                  Value
                               
     Name                      'ndb-batch-size'
                               
     Command Line              Yes
                               
     System Variable           Yes
                               
     Option File               Yes
                               
     Scope                     Global
                               
     Dynamic                   No
                               
     Type: Default, Range      integer: 32768 / 0 - 31536000 (Version: 5.5)
                               
     Notes                     
                               DESCRIPTION: Size (in bytes) to use for NDB
                               transaction batches

     This sets the size in bytes that is used for NDB transaction
     batches.

   * 
     '--ndb-cluster-connection-pool=#'

     * Type and value information for ndb-cluster-connection-pool*

     Property                  Value
                               
     Name                      'ndb-cluster-connection-pool'
                               
     Command Line              Yes
                               
     System Variable           Yes
                               
     Status Variable           Yes
                               
     Option File               Yes
                               
     Scope                     Global
                               
     Dynamic                   No
                               
     Type                      

     Default, Range            1 / 1 - 63 (Version: NDB 7.2)
                               
     Notes                     
                               DESCRIPTION: Number of connections to the
                               cluster used by MySQL

     By setting this option to a value greater than 1 (the default), a
     *note 'mysqld': mysqld. process can use multiple connections to the
     cluster, effectively mimicking several SQL nodes.  Each connection
     requires its own '[api]' or '[mysqld]' section in the cluster
     configuration ('config.ini') file, and counts against the maximum
     number of API connections supported by the cluster.

     Suppose that you have 2 cluster host computers, each running an SQL
     node whose *note 'mysqld': mysqld. process was started with
     '--ndb-cluster-connection-pool=4'; this means that the cluster must
     have 8 API slots available for these connections (instead of 2).
     All of these connections are set up when the SQL node connects to
     the cluster, and are allocated to threads in a round-robin fashion.

     This option is useful only when running *note 'mysqld': mysqld. on
     host machines having multiple CPUs, multiple cores, or both.  For
     best results, the value should be smaller than the total number of
     cores available on the host machine.  Setting it to a value greater
     than this is likely to degrade performance severely.

     *Important*:

     Because each SQL node using connection pooling occupies multiple
     API node slots--each slot having its own node ID in the
     cluster--you must _not_ use a node ID as part of the cluster
     connection string when starting any *note 'mysqld': mysqld. process
     that employs connection pooling.

     Setting a node ID in the connection string when using the
     '--ndb-cluster-connection-pool' option causes node ID allocation
     errors when the SQL node attempts to connect to the cluster.

     *Note*:

     In some older releases of NDB Cluster prior to NDB Cluster 7.2,
     there was also a separate status variable corresponding to this
     option; however, the status variable was removed as redundant as of
     these versions.  (Bug #60119)

   * 
     '--ndb-blob-read-batch-bytes=BYTES'

     *Type and value information for ndb-blob-read-batch-bytes*

     Property                  Value
                               
     Name                      'ndb-blob-read-batch-bytes'
                               
     Command Line              Yes
                               
     System Variable           Yes
                               
     Option File               Yes
                               
     Scope                     Both
                               
     Dynamic                   Yes
                               
     Type: Default, Range      integer: 65536 / 0 - 4294967295 (Version:
                               5.5)
                               
     Notes                     
                               DESCRIPTION: Specifies size in bytes that
                               large BLOB reads should be batched into.  0 =
                               no limit.

     This option can be used to set the size (in bytes) for batching of
     *note 'BLOB': blob. data reads in NDB Cluster applications.  When
     this batch size is exceeded by the amount of *note 'BLOB': blob.
     data to be read within the current transaction, any pending *note
     'BLOB': blob. read operations are immediately executed.

     The maximum value for this option is 4294967295; the default is
     65536.  Setting it to 0 has the effect of disabling *note 'BLOB':
     blob. read batching.

     *Note*:

     In NDB API applications, you can control *note 'BLOB': blob. write
     batching with the 'setMaxPendingBlobReadBytes()'
     (https://dev.mysql.com/doc/ndbapi/en/ndb-ndbtransaction-setmaxpendingblobreadbytes.html)
     and 'getMaxPendingBlobReadBytes()'
     (https://dev.mysql.com/doc/ndbapi/en/ndb-ndbtransaction-getmaxpendingblobreadbytes.html)
     methods.

   * 
     '--ndb-blob-write-batch-bytes=BYTES'

     *Type and value information for ndb-blob-write-batch-bytes*

     Property                  Value
                               
     Name                      'ndb-blob-write-batch-bytes'
                               
     Command Line              Yes
                               
     System Variable           Yes
                               
     Option File               Yes
                               
     Scope                     Both
                               
     Dynamic                   Yes
                               
     Type: Default, Range      integer: 65536 / 0 - 4294967295 (Version:
                               5.5)
                               
     Notes                     
                               DESCRIPTION: Specifies size in bytes that
                               large BLOB writes should be batched into.  0
                               = no limit.

     This option can be used to set the size (in bytes) for batching of
     *note 'BLOB': blob. data writes in NDB Cluster applications.  When
     this batch size is exceeded by the amount of *note 'BLOB': blob.
     data to be written within the current transaction, any pending
     *note 'BLOB': blob. write operations are immediately executed.

     The maximum value for this option is 4294967295; the default is
     65536.  Setting it to 0 has the effect of disabling *note 'BLOB':
     blob. write batching.

     *Note*:

     In NDB API applications, you can control *note 'BLOB': blob. write
     batching with the 'setMaxPendingBlobWriteBytes()'
     (https://dev.mysql.com/doc/ndbapi/en/ndb-ndbtransaction-setmaxpendingblobwritebytes.html)
     and 'getMaxPendingBlobWriteBytes()'
     (https://dev.mysql.com/doc/ndbapi/en/ndb-ndbtransaction-getmaxpendingblobwritebytes.html)
     methods.

   * 
     '--ndb-connectstring=CONNECTION_STRING'

     *Type and value information for ndb-connectstring*

     Property                  Value
                               
     Name                      'ndb-connectstring'
                               
     Command Line              Yes
                               
     System Variable           No
                               
     Option File               Yes
                               
     Scope                     

     Dynamic                   No
                               
     Type: Default, Range      string: (Version: 5.5)
                               
     Notes                     
                               DESCRIPTION: Point to the management server
                               that distributes the cluster configuration

     When using the *note 'NDBCLUSTER': mysql-cluster. storage engine,
     this option specifies the management server that distributes
     cluster configuration data.  See *note
     mysql-cluster-connection-strings::, for syntax.

   * 
     '--ndb-deferred-constraints=[0|1]'

     *Type and value information for ndb-deferred-constraints*

     Property                  Value
                               
     Name                      'ndb-deferred-constraints'
                               
     Command Line              Yes
                               
     System Variable           Yes
                               
     Option File               Yes
                               
     Scope                     Both
                               
     Dynamic                   Yes
                               
     Type: Default, Range      integer: 0 / 0 - 1 (Version: 5.5)
                               
     Notes                     
                               DESCRIPTION: Specifies that constraint checks
                               on unique indexes (where these are supported)
                               should be deferred until commit time.  Not
                               normally needed or used; for testing purposes
                               only.

     Controls whether or not constraint checks on unique indexes are
     deferred until commit time, where such checks are supported.  '0'
     is the default.

     This option is not normally needed for operation of NDB Cluster or
     NDB Cluster Replication, and is intended primarily for use in
     testing.

   * 
     '--ndb-distribution=[KEYHASH|LINHASH]'

     *Type and value information for ndb-distribution*

     Property                  Value
                               
     Name                      'ndb-distribution'
                               
     Command Line              Yes
                               
     System Variable           Yes
                               
     Option File               Yes
                               
     Scope                     Global
                               
     Dynamic                   Yes
                               
     Type: Default, Range      enumeration: KEYHASH / LINHASH, KEYHASH
                               (Version: 5.5)
                               
     Notes                     
                               DESCRIPTION: Default distribution for new
                               tables in NDBCLUSTER (KEYHASH or LINHASH,
                               default is KEYHASH)

     Controls the default distribution method for *note 'NDB':
     mysql-cluster. tables.  Can be set to either of 'KEYHASH' (key
     hashing) or 'LINHASH' (linear hashing).  'KEYHASH' is the default.

   * 
     '--ndb-log-apply-status'

     *Type and value information for ndb-log-apply-status*

     Property                  Value
                               
     Name                      'ndb-log-apply-status'
                               
     Command Line              Yes
                               
     System Variable           Yes
                               
     Option File               Yes
                               
     Scope                     Global
                               
     Dynamic                   No
                               
     Type: Default, Range      boolean: OFF (Version: 5.5)
                               
     Notes                     
                               DESCRIPTION: Cause a MySQL server acting as a
                               slave to log mysql.ndb_apply_status updates
                               received from its immediate master in its own
                               binary log, using its own server ID.
                               Effective only if the server is started with
                               the -ndbcluster option.

     Causes a slave *note 'mysqld': mysqld. to log any updates received
     from its immediate master to the 'mysql.ndb_apply_status' table in
     its own binary log using its own server ID rather than the server
     ID of the master.  In a circular or chain replication setting, this
     allows such updates to propagate to the 'mysql.ndb_apply_status'
     tables of any MySQL servers configured as slaves of the current
     *note 'mysqld': mysqld.

     In a chain replication setup, using this option allows downstream
     (slave) clusters to be aware of their positions relative to all of
     their upstream contributors (masters).

     In a circular replication setup, this option causes changes to
     'ndb_apply_status' tables to complete the entire circuit,
     eventually propagating back to the originating NDB Cluster.  This
     also allows a cluster acting as a master to see when its changes
     (epochs) have been applied to the other clusters in the circle.

     This option has no effect unless the MySQL server is started with
     the '--ndbcluster' option.

   * 
     '--ndb-log-empty-epochs=[ON|OFF]'

     *Type and value information for ndb-log-empty-epochs*

     Property                  Value
                               
     Name                      'ndb-log-empty-epochs'
                               
     Command Line              Yes
                               
     System Variable           Yes
                               
     Option File               Yes
                               
     Scope                     Global
                               
     Dynamic                   Yes
                               
     Type: Default, Range      boolean: OFF (Version: 5.5)
                               
     Notes                     
                               DESCRIPTION: When enabled, causes epochs in
                               which there were no changes to be written to
                               the ndb_apply_status and ndb_binlog_index
                               tables, even when -log-slave-updates is
                               enabled.

     Causes epochs during which there were no changes to be written to
     the 'ndb_apply_status' and 'ndb_binlog_index' tables, even when
     'log_slave_updates' is enabled.

     By default this option is disabled.  Disabling
     '--ndb-log-empty-epochs' causes epoch transactions with no changes
     not to be written to the binary log, although a row is still
     written even for an empty epoch in 'ndb_binlog_index'.

     Because '--ndb-log-empty-epochs=1' causes the size of the
     'ndb_binlog_index' table to increase independently of the size of
     the binary log, users should be prepared to manage the growth of
     this table, even if they expect the cluster to be idle a large part
     of the time.

   * 
     '--ndb-log-empty-update=[ON|OFF]'

     *Type and value information for ndb-log-empty-update*

     Property                  Value
                               
     Name                      'ndb-log-empty-update'
                               
     Command Line              Yes
                               
     System Variable           Yes
                               
     Option File               Yes
                               
     Scope                     Global
                               
     Dynamic                   Yes
                               
     Type: Default, Range      boolean: OFF (Version: 5.5)
                               
     Notes                     
                               DESCRIPTION: When enabled, causes updates
                               that produced no changes to be written to the
                               ndb_apply_status and ndb_binlog_index tables,
                               even when -log-slave-updates is enabled.

     Causes updates that produced no changes to be written to the
     'ndb_apply_status' and 'ndb_binlog_index' tables, when when
     'log_slave_updates' is enabled.

     By default this option is disabled ('OFF').  Disabling
     '--ndb-log-empty-update' causes updates with no changes not to be
     written to the binary log.

   * 
     '--ndb-log-orig'

     *Type and value information for ndb-log-orig*

     Property                  Value
                               
     Name                      'ndb-log-orig'
                               
     Command Line              Yes
                               
     System Variable           Yes
                               
     Option File               Yes
                               
     Scope                     Global
                               
     Dynamic                   No
                               
     Type: Default, Range      boolean: OFF (Version: 5.5)
                               
     Notes                     
                               DESCRIPTION: Log originating server id and
                               epoch in mysql.ndb_binlog_index table

     Log the originating server ID and epoch in the 'ndb_binlog_index'
     table.

     Note that this makes it possible for a given epoch to have multiple
     rows in 'ndb_binlog_index', one for each originating epoch.

     For more information, see *note mysql-cluster-replication-schema::.

   * 
     '--ndb-log-transaction-id'

     *Type and value information for ndb-log-transaction-id*

     Property                  Value
                               
     Name                      'ndb-log-transaction-id'
                               
     Command Line              Yes
                               
     System Variable           Yes
                               
     Option File               Yes
                               
     Scope                     Global
                               
     Dynamic                   No
                               
     Type: Default, Range      boolean: OFF (Version: 5.5)
                               
     Notes                     
                               DESCRIPTION: Write NDB transaction IDs in the
                               binary log.  Requires -log-bin-v1-events=OFF.

     Causes a slave *note 'mysqld': mysqld. to write the NDB transaction
     ID in each row of the binary log.  Such logging requires the use of
     the Version 2 event format for the binary log; thus, the
     'log_bin_use_v1_row_events' system variable must be disabled to use
     this option.

     This option is available beginning with NDB 7.2.1 (and is not
     supported in mainline MySQL Server 5.5).  It is required to enable
     NDB Cluster Replication conflict detection and resolution using the
     'NDB$EPOCH_TRANS()' function introduced in the same NDB Cluster
     release.

     The default value is 'FALSE'.

     For more information, see *note
     mysql-cluster-replication-conflict-resolution::.

   * 
     '--ndb-mgmd-host=HOST[:PORT]'

     *Type and value information for ndb-mgmd-host*

     Property                  Value
                               
     Name                      'ndb-mgmd-host'
                               
     Command Line              Yes
                               
     System Variable           No
                               
     Option File               Yes
                               
     Scope                     

     Dynamic                   No
                               
     Type: Default, Range      string: localhost:1186 (Version: 5.5)
                               
     Notes                     
                               DESCRIPTION: Set the host (and port, if
                               desired) for connecting to management server

     Can be used to set the host and port number of a single management
     server for the program to connect to.  If the program requires node
     IDs or references to multiple management servers (or both) in its
     connection information, use the '--ndb-connectstring' option
     instead.

   * 
     '--ndb-nodeid=#'

     * Type and value information for ndb-nodeid*

     Property                  Value
                               
     Name                      'ndb-nodeid'
                               
     Command Line              Yes
                               
     System Variable           No
                               
     Status Variable           Yes
                               
     Option File               Yes
                               
     Scope                     Global
                               
     Dynamic                   No
                               
     Type                      

     Default, Range            / 1 - 63 (Version: 5.0.45)
                               
     Default, Range            / 1 - 255 (Version: 5.1.5)
                               
     Notes                     
                               DESCRIPTION: NDB Cluster node ID for this
                               MySQL server

     Set this MySQL server's node ID in an NDB Cluster.

     The '--ndb-nodeid' option overrides any node ID set with
     '--ndb-connectstring', regardless of the order in which the two
     options are used.

     In addition, if '--ndb-nodeid' is used, then either a matching node
     ID must be found in a '[mysqld]' or '[api]' section of
     'config.ini', or there must be an 'open' '[mysqld]' or '[api]'
     section in the file (that is, a section without a 'NodeId' or 'Id'
     parameter specified).  This is also true if the node ID is
     specified as part of the connection string.

     Regardless of how the node ID is determined, its is shown as the
     value of the global status variable 'Ndb_cluster_node_id' in the
     output of *note 'SHOW STATUS': show-status, and as
     'cluster_node_id' in the 'connection' row of the output of *note
     'SHOW ENGINE NDBCLUSTER STATUS': show-engine.

     For more information about node IDs for NDB Cluster SQL nodes, see
     *note mysql-cluster-api-definition::.

   * '--ndb-optimization-delay=MILLISECONDS'

     *Type and value information for ndb-optimization-delay*

     Property                  Value
                               
     Name                      'ndb-optimization-delay'
                               
     Command Line              Yes
                               
     System Variable           Yes
                               
     Option File               Yes
                               
     Scope                     Global
                               
     Dynamic                   Yes
                               
     Type: Default, Range      integer: 10 / 0 - 100000 (Version: NDB 7.2)
                               
     Notes                     
                               DESCRIPTION: Sets the number of milliseconds
                               to wait between processing sets of rows by
                               OPTIMIZE TABLE on NDB tables

     Set the number of milliseconds to wait between sets of rows by
     *note 'OPTIMIZE TABLE': optimize-table. statements on *note 'NDB':
     mysql-cluster. tables.  The default is 10.

   * 'ndb-transid-mysql-connection-map=STATE'

     *Type and value information for ndb-transid-mysql-connection-map*

     Property                  Value
                               
     Name                      'ndb-transid-mysql-connection-map'
                               
     Command Line              Yes
                               
     System Variable           No
                               
     Option File               No
                               
     Scope                     

     Dynamic                   No
                               
     Type: Default, Range      enumeration: ON / ON, OFF, FORCE (Version:
                               5.5)
                               
     Notes                     
                               DESCRIPTION: Enable or disable the
                               ndb_transid_mysql_connection_map plugin; that
                               is, enable or disable the INFORMATION_SCHEMA
                               table having that name

     Enables or disables the plugin that handles the *note
     'ndb_transid_mysql_connection_map':
     ndb-transid-mysql-connection-map-table. table in the
     'INFORMATION_SCHEMA' database.  Takes one of the values 'ON',
     'OFF', or 'FORCE'.  'ON' (the default) enables the plugin.  'OFF'
     disables the plugin, which makes 'ndb_transid_mysql_connection_map'
     inaccessible.  'FORCE' keeps the MySQL Server from starting if the
     plugin fails to load and start.

     You can see whether the *note 'ndb_transid_mysql_connection_map':
     ndb-transid-mysql-connection-map-table. table plugin is running by
     checking the output of *note 'SHOW PLUGINS': show-plugins.

     This option was added in NDB 7.2.2.

   * '--ndb-wait-connected=SECONDS'

     *Type and value information for ndb-wait-connected*

     Property                  Value
                               
     Name                      'ndb-wait-connected'
                               
     Command Line              Yes
                               
     System Variable           Yes
                               
     Option File               Yes
                               
     Scope                     Global
                               
     Dynamic                   No
                               
     Type: Default, Range      integer: 0 / 0 - 31536000 (Version: 5.5)
                               
     Type: Default, Range      integer: 30 / 0 - 31536000 (Version: NDB 7.2)
                               
     Type: Default, Range      integer: 0 / 0 - 31536000 (Version: 5.5)
                               
     Type: Default, Range      integer: 30 / 0 - 31536000 (Version: NDB 7.2)
                               
     Notes                     
                               DESCRIPTION: Time (in seconds) for the MySQL
                               server to wait for connection to cluster
                               management and data nodes before accepting
                               MySQL client connections

     This option sets the period of time that the MySQL server waits for
     connections to NDB Cluster management and data nodes to be
     established before accepting MySQL client connections.  The time is
     specified in seconds.  The default value is '30'.

   * '--ndb-wait-setup=SECONDS'

     *Type and value information for ndb-wait-setup*

     Property                  Value
                               
     Name                      'ndb-wait-setup'
                               
     Command Line              Yes
                               
     System Variable           Yes
                               
     Option File               Yes
                               
     Scope                     Global
                               
     Dynamic                   No
                               
     Type: Default, Range      integer: 15 / 0 - 31536000 (Version: NDB 7.2)
                               
     Type: Default, Range      integer: 15 / 0 - 31536000 (Version: NDB 7.2)
                               
     Type: Default, Range      integer: 15 / 0 - 31536000 (Version: NDB 7.2)
                               
     Type: Default, Range      integer: 30 / 0 - 31536000 (Version: NDB 7.2)
                               
     Type: Default, Range      integer: 15 / 0 - 31536000 (Version: NDB 7.2)
                               
     Type: Default, Range      integer: 30 / 0 - 31536000 (Version: NDB 7.2)
                               
     Notes                     
                               DESCRIPTION: Time (in seconds) for the MySQL
                               server to wait for NDB engine setup to
                               complete

     This variable shows the period of time that the MySQL server waits
     for the *note 'NDB': mysql-cluster. storage engine to complete
     setup before timing out and treating *note 'NDB': mysql-cluster. as
     unavailable.  The time is specified in seconds.  The default value
     is '30'.

   * 
     '--server-id-bits=#'

     * Type and value information for server-id-bits*

     Property                  Value
                               
     Name                      'server-id-bits'
                               
     Command Line              Yes
                               
     System Variable           Yes
                               
     Status Variable           No
                               
     Option File               Yes
                               
     Scope                     Global
                               
     Dynamic                   No
                               
     Type                      

     Default, Range            32 / 7 - 32 (Version: NDB 7.2)
                               
     Notes                     
                               DESCRIPTION: Sets the number of least
                               significant bits in the server_id actually
                               used for identifying the server, permitting
                               NDB API applications to store application
                               data in the most significant bits.  server_id
                               must be less than 2 to the power of this
                               value.

     This option indicates the number of least significant bits within
     the 32-bit 'server_id' which actually identify the server.
     Indicating that the server is actually identified by fewer than 32
     bits makes it possible for some of the remaining bits to be used
     for other purposes, such as storing user data generated by
     applications using the NDB API's Event API within the 'AnyValue' of
     an 'OperationOptions'
     (https://dev.mysql.com/doc/ndbapi/en/ndb-operationoptions.html)
     structure (NDB Cluster uses the 'AnyValue' to store the server ID).

     When extracting the effective server ID from 'server_id' for
     purposes such as detection of replication loops, the server ignores
     the remaining bits.  The '--server-id-bits' option is used to mask
     out any irrelevant bits of 'server_id' in the IO and SQL threads
     when deciding whether an event should be ignored based on the
     server ID.

     This data can be read from the binary log by *note 'mysqlbinlog':
     mysqlbinlog, provided that it is run with its own
     '--server-id-bits' option set to 32 (the default).

     The value of 'server_id' must be less than 2 ^ 'server_id_bits';
     otherwise, *note 'mysqld': mysqld. refuses to start.

     This system variable is supported only by NDB Cluster.  It is not
     supported in the standard MySQL 5.5 Server.

   * 
     '--skip-ndbcluster'

     *Type and value information for skip-ndbcluster*

     Property                  Value
                               
     Name                      'skip-ndbcluster'
                               
     Command Line              Yes
                               
     System Variable           No
                               
     Option File               Yes
                               
     Scope                     

     Dynamic                   No
                               
     Notes                     
                               DESCRIPTION: Disable the NDB Cluster storage
                               engine

     Disable the *note 'NDBCLUSTER': mysql-cluster. storage engine.
     This is the default for binaries that were built with *note
     'NDBCLUSTER': mysql-cluster. storage engine support; the server
     allocates memory and other resources for this storage engine only
     if the '--ndbcluster' option is given explicitly.  See *note
     mysql-cluster-quick::, for an example.


File: manual.info.tmp,  Node: mysql-cluster-system-variables,  Next: mysql-cluster-status-variables,  Prev: mysql-cluster-program-options-mysqld,  Up: mysql-cluster-options-variables

18.3.3.10 NDB Cluster System Variables
......................................

This section provides detailed information about MySQL server system
variables that are specific to NDB Cluster and the *note 'NDB':
mysql-cluster. storage engine.  For system variables not specific to NDB
Cluster, see *note server-system-variables::.  For general information
on using system variables, see *note using-system-variables::.

   * 'have_ndbcluster'

     *Type and value information for have_ndbcluster*

     Property                  Value
                               
     Name                      'have_ndbcluster'
                               
     Command Line              No
                               
     System Variable           Yes
                               
     Option File               No
                               
     Scope                     Global
                               
     Dynamic                   No
                               
     Type: Default, Range      boolean: (Version: 5.5)
                               
     Notes                     
                               DESCRIPTION: Whether mysqld supports NDB
                               Cluster tables (set by -ndbcluster option)

     'YES' if *note 'mysqld': mysqld. supports *note 'NDBCLUSTER':
     mysql-cluster. tables.  'DISABLED' if '--skip-ndbcluster' is used.

     This variable is deprecated and is removed in MySQL 5.6.  Use *note
     'SHOW ENGINES': show-engines. instead.

   * 'ndb_autoincrement_prefetch_sz'

     Property               Value
                            
     *Command-Line          '--ndb-autoincrement-prefetch-sz=#'
     Format*                

     *System Variable*      'ndb_autoincrement_prefetch_sz'
                            
     *Scope*                Global, Session
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '1'
                            
     *Minimum Value*        '1'
                            
     *Maximum Value*        '65536'

     Determines the probability of gaps in an autoincremented column.
     Set it to '1' to minimize this.  Setting it to a high value for
     optimization makes inserts faster, but decreases the likelihood
     that consecutive autoincrement numbers will be used in a batch of
     inserts.

     This variable affects only the number of 'AUTO_INCREMENT' IDs that
     are fetched between statements; within a given statement, at least
     32 IDs are obtained at a time.

     *Important*:

     This variable does not affect inserts performed using *note 'INSERT
     ... SELECT': insert-select.

   * 'ndb_cache_check_time'

     Property               Value
                            
     *Command-Line          '--ndb-cache-check-time=#'
     Format*                

     *System Variable*      'ndb_cache_check_time'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '0'

     The number of milliseconds that elapse between checks of NDB
     Cluster SQL nodes by the MySQL query cache.  Setting this to 0 (the
     default and minimum value) means that the query cache checks for
     validation on every query.

     The recommended maximum value for this variable is 1000, which
     means that the check is performed once per second.  A larger value
     means that the check is performed and possibly invalidated due to
     updates on different SQL nodes less often.  It is generally not
     desirable to set this to a value greater than 2000.

   * 'ndb_deferred_constraints'

     Property               Value
                            
     *Command-Line          '--ndb-deferred-constraints=#'
     Format*                

     *System Variable*      'ndb_deferred_constraints'
                            
     *Scope*                Global, Session
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '0'
                            
     *Minimum Value*        '0'
                            
     *Maximum Value*        '1'

     Controls whether or not constraint checks are deferred, where these
     are supported.  '0' is the default.

     This variable is not normally needed for operation of NDB Cluster
     or NDB Cluster Replication, and is intended primarily for use in
     testing.

   * 'ndb_distribution'

     Property               Value
                            
     *Command-Line          '--ndb-distribution={KEYHASH|LINHASH}'
     Format*                

     *System Variable*      'ndb_distribution'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Enumeration
                            
     *Default Value*        'KEYHASH'
                            
     *Valid Values*         'LINHASH' 'KEYHASH'

     Controls the default distribution method for *note 'NDB':
     mysql-cluster. tables.  Can be set to either of 'KEYHASH' (key
     hashing) or 'LINHASH' (linear hashing).  'KEYHASH' is the default.

   * 'ndb_eventbuffer_max_alloc'

     Property               Value
                            
     *Command-Line          '--ndb-eventbuffer-max-alloc=#'
     Format*                

     *Introduced*           5.5.34-ndb-7.2.14
                            
     *System Variable*      'ndb_eventbuffer_max_alloc'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '0'
                            
     *Minimum Value*        '0'
                            
     *Maximum Value*        '4294967295'

     Sets the maximum amount memory (in bytes) that can be allocated for
     buffering events by the NDB API. 0 means that no limit is imposed,
     and is the default.

     This variable was added in NDB 7.2.14.

   * 'ndb_extra_logging'

     Property               Value
                            
     *Command-Line          'ndb_extra_logging=#'
     Format*                

     *System Variable*      'ndb_extra_logging'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '1'

     This variable enables recording in the MySQL error log of
     information specific to the *note 'NDB': mysql-cluster. storage
     engine.

     When this variable is set to 0, the only information specific to
     'NDB' that is written to the MySQL error log relates to transaction
     handling.  If it set to a value greater than 0 but less than 10,
     'NDB' table schema and connection events are also logged, as well
     as whether or not conflict resolution is in use, and other 'NDB'
     errors and information.  If the value is set to 10 or more,
     information about 'NDB' internals, such as the progress of data
     distribution among cluster nodes, is also written to the MySQL
     error log.  The default is 1.

   * 'ndb_force_send'

     Property               Value
                            
     *Command-Line          '--ndb-force-send[={OFF|ON}]'
     Format*                

     *System Variable*      'ndb_force_send'
                            
     *Scope*                Global, Session
                            
     *Dynamic*              Yes
                            
     *Type*                 Boolean
                            
     *Default Value*        'ON'

     Forces sending of buffers to *note 'NDB': mysql-cluster.
     immediately, without waiting for other threads.  Defaults to 'ON'.

   * 'ndb_index_stat_cache_entries'

     Property               Value
                            
     *Command-Line          '--ndb-index-stat-cache-entries=#'
     Format*                

     *Deprecated*           Yes (removed in 5.5.36-ndb-7.2.16)
                            
     *System Variable*      'ndb_index_stat_cache_entries'
                            
     *Scope*                Global, Session
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '32'
                            
     *Minimum Value*        '0'
                            
     *Maximum Value*        '4294967295'

     Sets the granularity of the statistics by determining the number of
     starting and ending keys to store in the statistics memory cache.
     Zero means no caching takes place; in this case, the data nodes are
     always queried directly.  Default value: '32'.

     *Note*:

     If 'ndb_index_stat_enable' is 'OFF', then setting this variable has
     no effect.

     This variable was deprecated in MySQL 5.1, and is removed from NDB
     7.2.16 and later.

   * 'ndb_index_stat_enable'

     Property               Value
                            
     *Command-Line          '--ndb-index-stat-enable[={OFF|ON}]'
     Format*                

     *System Variable*      'ndb_index_stat_enable'
                            
     *Scope*                Global, Session
                            
     *Dynamic*              Yes
                            
     *Type*                 Boolean
                            
     *Default Value*        'ON'

     Use *note 'NDB': mysql-cluster. index statistics in query
     optimization.  'ON' is the default in NDB Cluster 7.2 and later.

   * 'ndb_index_stat_option'

     Property               Value
                            
     *Command-Line          '--ndb-index-stat-option=value'
     Format*                

     *Introduced*           5.5.15-ndb-7.2.1
                            
     *System Variable*      'ndb_index_stat_option'
                            
     *Scope*                Global, Session
                            
     *Dynamic*              Yes
                            
     *Type*                 String
                            
     *Default Value*        'loop_checkon=1000ms,loop_idle=1000ms,loop_busy=100ms,
                            update_batch=1,read_batch=4,idle_batch=32,check_batch=32,
                            check_delay=1m,delete_batch=8,clean_delay=0,error_batch=4,
                            error_delay=1m,evict_batch=8,evict_delay=1m,cache_limit=32M,
                            cache_lowpct=90'

     This variable is used for providing tuning options for NDB index
     statistics generation.  The list consist of comma-separated
     name-value pairs of option names and values.  Note that this list
     must not contain any space characters.

     Options not used when setting 'ndb_index_stat_option' are not
     changed from their default values.  For example, you can set
     'ndb_index_stat_option = 'loop_idle=1000ms,cache_limit=32M''.

     Time values can be optionally suffixed with 'h' (hours), 'm'
     (minutes), or 's' (seconds).  Millisecond values can optionally be
     specified using 'ms'; millisecond values cannot be specified using
     'h', 'm', or 's'.)  Integer values can be suffixed with 'K', 'M',
     or 'G'.

     The names of the options that can be set using this variable are
     shown in the table that follows.  The table also provides brief
     descriptions of the options, their default values, and (where
     applicable) their minimum and maximum values.

     *ndb_index_stat_option options and values*

     Name               Description        Default/Units      Minimum/Maximum
                                                              
     'loop_enable'                         1000 ms            0/4G
                                                              
     'loop_idle'        Time to sleep      1000 ms            0/4G
                        when idle                             
                        
     'loop_busy'        Time to sleep      100 ms             0/4G
                        when more work                        
                        is waiting
                        
     'update_batch'                        1                  0/4G
                                                              
     'read_batch'                          4                  1/4G
                                                              
     'idle_batch'                          32                 1/4G
                                                              
     'check_batch'                         8                  1/4G
                                                              
     'check_delay'      How often to       10 m               1/4G
                        check for new                         
                        statistics
                        
     'delete_batch'                        8                  0/4G
                                                              
     'clean_delay'                         1 m                0/4G
                                                              
     'error_batch'                         4                  1/4G
                                                              
     'error_delay'                         1 m                1/4G
                                                              
     'evict_batch'                         8                  1/4G
                                                              
     'evict_delay'      Clean LRU cache,   1 m                0/4G
                        from read time                        
                        
     'cache_limit'      Maximum amount     32 M               0/4G
                        of memory in                          
                        bytes used for
                        cached index
                        statistics by
                        this
                        *note 'mysqld': mysqld.;
                        clean up the
                        cache when this
                        is exceeded.
                        
     'cache_lowpct'                        90                 0/100
                                                              
     'zero_total'       Setting this to    0                  0/1
                        1 resets all       
                        accumulating
                        counters in
                        'ndb_index_stat_status'
                        to 0.  This
                        option value is
                        also reset to 0
                        when this is
                        done.
                        

   * 'ndb_index_stat_update_freq'

     Property               Value
                            
     *Command-Line          '--ndb-index-stat-update-freq=#'
     Format*                

     *Deprecated*           Yes (removed in 5.5.36-ndb-7.2.16)
                            
     *System Variable*      'ndb_index_stat_update_freq'
                            
     *Scope*                Global, Session
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '20'
                            
     *Minimum Value*        '0'
                            
     *Maximum Value*        '4294967295'

     How often to query data nodes instead of the statistics cache.  For
     example, a value of '20' (the default) means to direct every 20^th
     query to the data nodes.

     *Note*:

     If 'ndb_index_stat_cache_entries' is '0', then setting this
     variable has no effect; in this case, every query is sent directly
     to the data nodes.

     This variable was deprecated in MySQL 5.1, and is removed from NDB
     7.2.16 and later.

   * 
     'ndb_join_pushdown'

     Property               Value
                            
     *System Variable*      'ndb_join_pushdown'
                            
     *Scope*                Global, Session
                            
     *Dynamic*              Yes
                            
     *Type*                 Boolean
                            
     *Default Value*        'ON'

     Added in NDB 7.2.0, this variable controls whether joins on *note
     'NDB': mysql-cluster. tables are pushed down to the NDB kernel
     (data nodes).  Previously, a join was handled using multiple
     accesses of *note 'NDB': mysql-cluster. by the SQL node; however,
     when 'ndb_join_pushdown' is enabled, a pushable join is sent in its
     entirety to the data nodes, where it can be distributed among the
     data nodes and executed in parallel on multiple copies of the data,
     with a single, merged result being returned to *note 'mysqld':
     mysqld.  This can reduce greatly the number of round trips between
     an SQL node and the data nodes required to handle such a join.

     By default, 'ndb_join_pushdown' is enabled.

     Conditions for NDB pushdown joins

     In order for a join to be pushable, it must meet the following
     conditions:

       1. Only columns can be compared, and all columns to be joined
          must use _exactly_ the same data type.

          This means that expressions such as 't1.a = t2.a + CONSTANT'
          cannot be pushed down, and that (for example) a join on an
          *note 'INT': integer-types. column and a *note 'BIGINT':
          integer-types. column also cannot be pushed down.

       2. Queries referencing *note 'BLOB': blob. or *note 'TEXT': blob.
          columns are not supported.

       3. Explicit locking is not supported; however, the *note 'NDB':
          mysql-cluster. storage engine's characteristic implicit
          row-based locking is enforced.

          This means that a join using 'FOR UPDATE' cannot be pushed
          down.

       4. In order for a join to be pushed down, child tables in the
          join must be accessed using one of the 'ref', 'eq_ref', or 
          'const' access methods, or some combination of these methods.

          Outer joined child tables can only be pushed using 'eq_ref'.

          If the root of the pushed join is an 'eq_ref' or 'const', only
          child tables joined by 'eq_ref' can be appended.  (A table
          joined by 'ref' is likely to become the root of another pushed
          join.)

          If the query optimizer decides on 'Using join cache' for a
          candidate child table, that table cannot be pushed as a child.
          However, it may be the root of another set of pushed tables.

       5. Joins referencing tables explicitly partitioned by '[LINEAR]
          HASH', 'LIST', or 'RANGE' currently cannot be pushed down.

     You can see whether a given join can be pushed down by checking it
     with *note 'EXPLAIN': explain.; when the join can be pushed down,
     you can see references to the 'pushed join' in the 'Extra' column
     of the output, as shown in this example:

          mysql> EXPLAIN
              ->     SELECT e.first_name, e.last_name, t.title, d.dept_name
              ->         FROM employees e
              ->         JOIN dept_emp de ON e.emp_no=de.emp_no
              ->         JOIN departments d ON d.dept_no=de.dept_no
              ->         JOIN titles t ON e.emp_no=t.emp_no\G
          *************************** 1. row ***************************
                     id: 1
            select_type: SIMPLE
                  table: d
                   type: ALL
          possible_keys: PRIMARY
                    key: NULL
                key_len: NULL
                    ref: NULL
                   rows: 9
                  Extra: Parent of 4 pushed join@1
          *************************** 2. row ***************************
                     id: 1
            select_type: SIMPLE
                  table: de
                   type: ref
          possible_keys: PRIMARY,emp_no,dept_no
                    key: dept_no
                key_len: 4
                    ref: employees.d.dept_no
                   rows: 5305
                  Extra: Child of 'd' in pushed join@1
          *************************** 3. row ***************************
                     id: 1
            select_type: SIMPLE
                  table: e
                   type: eq_ref
          possible_keys: PRIMARY
                    key: PRIMARY
                key_len: 4
                    ref: employees.de.emp_no
                   rows: 1
                  Extra: Child of 'de' in pushed join@1
          *************************** 4. row ***************************
                     id: 1
            select_type: SIMPLE
                  table: t
                   type: ref
          possible_keys: PRIMARY,emp_no
                    key: emp_no
                key_len: 4
                    ref: employees.de.emp_no
                   rows: 19
                  Extra: Child of 'e' in pushed join@1
          4 rows in set (0.00 sec)

     *Note*:

     If inner joined child tables are joined by 'ref', _and_ the result
     is ordered or grouped by a sorted index, this index cannot provide
     sorted rows, which forces writing to a sorted tempfile.

     Two additional sources of information about pushed join performance
     are available:

       1. The status variables 'Ndb_pushed_queries_defined',
          'Ndb_pushed_queries_dropped', 'Ndb_pushed_queries_executed',
          and 'Ndb_pushed_reads' (all introduced in NDB 7.2.0).

       2. The counters in the *note 'ndbinfo.counters':
          mysql-cluster-ndbinfo-counters. table that belong to the
          'DBSPJ' kernel block.  (These counters and the 'DBSPJ' block
          were also introduced in NDB 7.2.0).  See *note
          mysql-cluster-ndbinfo-counters::, for information about these
          counters.  See also The DBSPJ Block
          (https://dev.mysql.com/doc/ndb-internals/en/ndb-internals-kernel-blocks-dbspj.html),
          in the 'NDB Cluster API Developer Guide'.

   * 'ndb_log_apply_status'

     Property               Value
                            
     *Command-Line          '--ndb-log-apply-status[={OFF|ON}]'
     Format*                

     *System Variable*      'ndb_log_apply_status'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Boolean
                            
     *Default Value*        'OFF'

     A read-only variable which shows whether the server was started
     with the '--ndb-log-apply-status' option.

   * 'ndb_log_bin'

     Property               Value
                            
     *Command-Line          '--ndb-log-bin[={OFF|ON}]'
     Format*                

     *System Variable*      'ndb_log_bin'
                            
     *Scope*                Global, Session
                            
     *Dynamic*              Yes
                            
     *Type*                 Boolean
                            
     *Default Value*        'ON'

     Causes updates to 'NDB' tables to be written to the binary log.
     Setting this variable has no effect if binary logging is not
     already enabled for the server using 'log_bin'.  'ndb_log_bin'
     defaults to 1 (ON); normally, there is never any need to change
     this value in a production environment.

   * 'ndb_log_binlog_index'

     Property               Value
                            
     *Command-Line          '--ndb-log-binlog-index[={OFF|ON}]'
     Format*                

     *System Variable*      'ndb_log_binlog_index'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Boolean
                            
     *Default Value*        'ON'

     Causes a mapping of epochs to positions in the binary log to be
     inserted into the 'ndb_binlog_index' table.  Setting this variable
     has no effect if binary logging is not already enabled for the
     server using 'log_bin'.  (In addition, 'ndb_log_bin' must not be
     disabled.)  'ndb_log_binlog_index' defaults to '1' ('ON');
     normally, there is never any need to change this value in a
     production environment.

   * 
     'ndb_log_empty_epochs'

     Property               Value
                            
     *Command-Line          '--ndb-log-empty-epochs[={OFF|ON}]'
     Format*                

     *System Variable*      'ndb_log_empty_epochs'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Boolean
                            
     *Default Value*        'OFF'

     When this variable is set to 0, epoch transactions with no changes
     are not written to the binary log, although a row is still written
     even for an empty epoch in 'ndb_binlog_index'.

   * 
     'ndb_log_empty_update'

     Property               Value
                            
     *Command-Line          '--ndb-log-empty-update[={OFF|ON}]'
     Format*                

     *System Variable*      'ndb_log_empty_update'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Boolean
                            
     *Default Value*        'OFF'

     When this variable is set to 'ON' ('1'), update transactions with
     no changes are written to the binary log, even when
     'log_slave_updates' is enabled.

   * 
     'ndb_log_orig'

     Property               Value
                            
     *Command-Line          '--ndb-log-orig[={OFF|ON}]'
     Format*                

     *System Variable*      'ndb_log_orig'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Boolean
                            
     *Default Value*        'OFF'

     Shows whether the originating server ID and epoch are logged in the
     'ndb_binlog_index' table.  Set using the '--ndb-log-orig' server
     option.

   * 
     'ndb_log_transaction_id'

     Property               Value
                            
     *Introduced*           5.5.15-ndb-7.2.1
                            
     *System Variable*      'ndb_log_transaction_id'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Boolean
                            
     *Default Value*        'OFF'

     This read-only, Boolean system variable shows whether a slave *note
     'mysqld': mysqld. writes NDB transaction IDs in the binary log
     (required to use 'active-active' NDB Cluster Replication with
     'NDB$EPOCH_TRANS()' conflict detection).  To change the setting,
     use the '--ndb-log-transaction-id' option.

     'ndb_log_transaction_id' is available in NDB 7.2.1 and later.  It
     is not supported in mainline MySQL Server 5.5.

     For more information, see *note
     mysql-cluster-replication-conflict-resolution::.

   * 'ndb_optimized_node_selection'

     Property               Value
                            
     *Command-Line          '--ndb-optimized-node-selection=#'
     Format*                

     *System Variable*      'ndb_optimized_node_selection'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Integer
                            
     *Default Value*        '3'
                            
     *Minimum Value*        '0'
                            
     *Maximum Value*        '3'

     There are two forms of optimized node selection, described here:

       1. The SQL node uses _promixity_ to determine the transaction
          coordinator; that is, the 'closest' data node to the SQL node
          is chosen as the transaction coordinator.  For this purpose, a
          data node having a shared memory connection with the SQL node
          is considered to be 'closest' to the SQL node; the next
          closest (in order of decreasing proximity) are: TCP connection
          to 'localhost'; SCI connection; TCP connection from a host
          other than 'localhost'.

       2. The SQL thread uses _distribution awareness_ to select the
          data node.  That is, the data node housing the cluster
          partition accessed by the first statement of a given
          transaction is used as the transaction coordinator for the
          entire transaction.  (This is effective only if the first
          statement of the transaction accesses no more than one cluster
          partition.)

     This option takes one of the integer values '0', '1', '2', or '3'.
     '3' is the default.  These values affect node selection as follows:

        * '0': Node selection is not optimized.  Each data node is
          employed as the transaction coordinator 8 times before the SQL
          thread proceeds to the next data node.

        * '1': Proximity to the SQL node is used to determine the
          transaction coordinator.

        * '2': Distribution awareness is used to select the transaction
          coordinator.  However, if the first statement of the
          transaction accesses more than one cluster partition, the SQL
          node reverts to the round-robin behavior seen when this option
          is set to '0'.

        * '3': If distribution awareness can be employed to determine
          the transaction coordinator, then it is used; otherwise
          proximity is used to select the transaction coordinator.
          (This is the default behavior.)

     Proximity is determined as follows:

       1. Start with the value set for the 'Group' parameter (default
          55).

       2. For an API node sharing the same host with other API nodes,
          decrement the value by 1.  Assuming the default value for
          'Group', the effective value for data nodes on same host as
          the API node is 54, and for remote data nodes 55.

   * 'ndb_report_thresh_binlog_epoch_slip'

     Property               Value
                            
     *Command-Line          '--ndb-report-thresh-binlog-epoch-slip=#'
     Format*                

     *System Variable*      'ndb_report_thresh_binlog_epoch_slip'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '3'
                            
     *Minimum Value*        '0'
                            
     *Maximum Value*        '256'

     This is a threshold on the number of epochs to be behind before
     reporting binary log status.  For example, a value of '3' (the
     default) means that if the difference between which epoch has been
     received from the storage nodes and which epoch has been applied to
     the binary log is 3 or more, a status message is sent to the
     cluster log.

   * 'ndb_report_thresh_binlog_mem_usage'

     Property               Value
                            
     *Command-Line          '--ndb-report-thresh-binlog-mem-usage=#'
     Format*                

     *System Variable*      'ndb_report_thresh_binlog_mem_usage'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '10'
                            
     *Minimum Value*        '0'
                            
     *Maximum Value*        '10'

     This is a threshold on the percentage of free memory remaining
     before reporting binary log status.  For example, a value of '10'
     (the default) means that if the amount of available memory for
     receiving binary log data from the data nodes falls below 10%, a
     status message is sent to the cluster log.

   * 'ndb_table_no_logging'

     Property               Value
                            
     *System Variable*      'ndb_table_no_logging'
                            
     *Scope*                Session
                            
     *Dynamic*              Yes
                            
     *Type*                 Boolean
                            
     *Default Value*        'OFF'

     When this variable is set to 'ON' or '1', it causes *note 'NDB':
     mysql-cluster. tables not to be checkpointed to disk.  More
     specifically, this setting applies to tables which are created or
     altered using 'ENGINE NDB' when 'ndb_table_no_logging' is enabled,
     and continues to apply for the lifetime of the table, even if
     'ndb_table_no_logging' is later changed.  Suppose that 'A', 'B',
     'C', and 'D' are tables that we create (and perhaps also alter),
     and that we also change the setting for 'ndb_table_no_logging' as
     shown here:

          SET @@ndb_table_no_logging = 1;

          CREATE TABLE A ... ENGINE NDB;

          CREATE TABLE B ... ENGINE MYISAM;
          CREATE TABLE C ... ENGINE MYISAM;

          ALTER TABLE B ENGINE NDB;

          SET @@ndb_table_no_logging = 0;

          CREATE TABLE D ... ENGINE NDB;
          ALTER TABLE C ENGINE NDB;

          SET @@ndb_table_no_logging = 1;

     After the previous sequence of events, tables 'A' and 'B' are not
     checkpointed; 'A' was created with 'ENGINE NDB' and B was altered
     to use 'NDB', both while 'ndb_table_no_logging' was enabled.
     However, tables 'C' and 'D' are logged; 'C' was altered to use
     *note 'NDB': mysql-cluster. and 'D' was created using 'ENGINE NDB',
     both while 'ndb_table_no_logging' was disabled.  Setting
     'ndb_table_no_logging' back to '1' or 'ON' does _not_ cause table
     'C' or 'D' to be checkpointed.

     *Note*:

     'ndb_table_no_logging' has no effect on the creation of *note
     'NDB': mysql-cluster. table schema files; to suppress these, use
     'ndb_table_temporary' instead.

   * 'ndb_table_temporary'

     Property               Value
                            
     *System Variable*      'ndb_table_temporary'
                            
     *Scope*                Session
                            
     *Dynamic*              Yes
                            
     *Type*                 Boolean
                            
     *Default Value*        'OFF'

     When set to 'ON' or '1', this variable causes *note 'NDB':
     mysql-cluster. tables not to be written to disk: This means that no
     table schema files are created, and that the tables are not logged.

     *Note*:

     Setting this variable currently has no effect in NDB Cluster 7.0
     and later.  This is a known issue; see Bug #34036.

   * 'ndb_use_copying_alter_table'

     Property               Value
                            
     *System Variable*      'ndb_use_copying_alter_table'
                            
     *Scope*                Global, Session
                            
     *Dynamic*              No

     Forces *note 'NDB': mysql-cluster. to use copying of tables in the
     event of problems with online *note 'ALTER TABLE': alter-table.
     operations.  The default value is 'OFF'.

   * 'ndb_use_exact_count'

     Property               Value
                            
     *System Variable*      'ndb_use_exact_count'
                            
     *Scope*                Global, Session
                            
     *Dynamic*              Yes
                            
     *Type*                 Boolean
                            
     *Default Value*        'OFF'

     Forces *note 'NDB': mysql-cluster. to use a count of records during
     'SELECT COUNT(*)' query planning to speed up this type of query.
     The default value is 'OFF', which allows for faster queries
     overall.

   * 'ndb_use_transactions'

     Property               Value
                            
     *Command-Line          '--ndb-use-transactions[={OFF|ON}]'
     Format*                

     *System Variable*      'ndb_use_transactions'
                            
     *Scope*                Global, Session
                            
     *Dynamic*              Yes
                            
     *Type*                 Boolean
                            
     *Default Value*        'ON'

     You can disable *note 'NDB': mysql-cluster. transaction support by
     setting this variable's values to 'OFF' (not recommended).  The
     default is 'ON'.

   * 'ndb_version'

     Property               Value
                            
     *System Variable*      'ndb_version'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 String
                            
     *Default Value*        ''

     'NDB' engine version, as a composite integer.

   * 'ndb_version_string'

     Property               Value
                            
     *System Variable*      'ndb_version_string'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 String
                            
     *Default Value*        ''

     'NDB' engine version in 'ndb-X.Y.Z' format.

   * 
     'server_id_bits'

     *Type and value information for server_id_bits*

     Property                  Value
                               
     Name                      'server_id_bits'
                               
     Command Line              Yes
                               
     System Variable           Yes
                               
     Option File               Yes
                               
     Scope                     Global
                               
     Dynamic                   No
                               
     Type: Default, Range      integer: 32 / 7 - 32 (Version: 5.5)
                               
     Notes                     
                               DESCRIPTION: Sets the number of least
                               significant bits in the server_id actually
                               used for identifying the server, permitting
                               NDB API applications to store application
                               data in the most significant bits.  server_id
                               must be less than 2 to the power of this
                               value.

     The effective value of 'server_id' if the server was started with
     the '--server-id-bits' option set to a nondefault value.

     If the value of 'server_id' greater than or equal to 2 to the power
     of 'server_id_bits', *note 'mysqld': mysqld. refuses to start.

     This system variable is supported only by NDB Cluster.
     'server_id_bits' is not supported by the standard MySQL Server.

   * 'slave_allow_batching'

     *Type and value information for slave_allow_batching*

     Property                  Value
                               
     Name                      'slave_allow_batching'
                               
     Command Line              Yes
                               
     System Variable           Yes
                               
     Option File               Yes
                               
     Scope                     Global
                               
     Dynamic                   Yes
                               
     Type: Default, Range      boolean: OFF (Version: 5.5)
                               
     Notes                     
                               DESCRIPTION: Turns update batching on and off
                               for a replication slave

     Whether or not batched updates are enabled on NDB Cluster
     replication slaves.

     This variable is available for *note 'mysqld': mysqld. only as
     supplied with NDB Cluster or built from the NDB Cluster sources.
     For more information, see *note
     mysql-cluster-replication-starting::.

     Setting this variable had no effect in NDB Cluster 7.2 prior to NDB
     7.2.10.  (Bug #15953730)

   * 
     'transaction_allow_batching'

     *Type and value information for transaction_allow_batching*

     Property                  Value
                               
     Name                      'transaction_allow_batching'
                               
     Command Line              No
                               
     System Variable           Yes
                               
     Option File               No
                               
     Scope                     Session
                               
     Dynamic                   Yes
                               
     Type: Default, Range      boolean: OFF (Version: 5.5)
                               
     Notes                     
                               DESCRIPTION: Allows batching of statements
                               within a transaction.  Disable AUTOCOMMIT to
                               use.

     When set to '1' or 'ON', this variable enables batching of
     statements within the same transaction.  To use this variable,
     'autocommit' must first be disabled by setting it to '0' or 'OFF';
     otherwise, setting 'transaction_allow_batching' has no effect.

     It is safe to use this variable with transactions that performs
     writes only, as having it enabled can lead to reads from the
     'before' image.  You should ensure that any pending transactions
     are committed (using an explicit *note 'COMMIT': commit. if
     desired) before issuing a *note 'SELECT': select.

     *Important*:

     'transaction_allow_batching' should not be used whenever there is
     the possibility that the effects of a given statement depend on the
     outcome of a previous statement within the same transaction.

     This variable is currently supported for NDB Cluster only.

     *Important*:

     Due an issue in the NDB Cluster 7.2 codebase (Bug #64697) prior to
     General Availability, this variable is not available prior to NDB
     7.2.6.

The system variables in the following list all relate to the *note
'ndbinfo': mysql-cluster-ndbinfo. information database.

   * 'ndbinfo_database'

     *Type and value information for ndbinfo_database*

     Property                  Value
                               
     Name                      'ndbinfo_database'
                               
     Command Line              No
                               
     System Variable           Yes
                               
     Option File               No
                               
     Scope                     Global
                               
     Dynamic                   No
                               
     Type: Default, Range      string: ndbinfo (Version: 5.5)
                               
     Notes                     
                               DESCRIPTION: The name used for the NDB
                               information database; read only

     Shows the name used for the 'NDB' information database; the default
     is 'ndbinfo'.  This is a read-only variable whose value is
     determined at compile time; you can set it by starting the server
     using '--ndbinfo-database=NAME', which sets the value shown for
     this variable but does not actually change the name used for the
     NDB information database.

   * 'ndbinfo_max_bytes'

     *Type and value information for ndbinfo_max_bytes*

     Property                  Value
                               
     Name                      'ndbinfo_max_bytes'
                               
     Command Line              Yes
                               
     System Variable           Yes
                               
     Option File               No
                               
     Scope                     Both
                               
     Dynamic                   Yes
                               
     Type: Default, Range      integer: 0 / - (Version: 5.5)
                               
     Notes                     
                               DESCRIPTION: Used for debugging only

     Used in testing and debugging only.

   * 'ndbinfo_max_rows'

     *Type and value information for ndbinfo_max_rows*

     Property                  Value
                               
     Name                      'ndbinfo_max_rows'
                               
     Command Line              Yes
                               
     System Variable           Yes
                               
     Option File               No
                               
     Scope                     Both
                               
     Dynamic                   Yes
                               
     Type: Default, Range      integer: 10 / - (Version: 5.5)
                               
     Notes                     
                               DESCRIPTION: Used for debugging only

     Used in testing and debugging only.

   * 'ndbinfo_offline'

     *Type and value information for ndbinfo_offline*

     Property                  Value
                               
     Name                      'ndbinfo_offline'
                               
     Command Line              No
                               
     System Variable           Yes
                               
     Option File               No
                               
     Scope                     Global
                               
     Dynamic                   Yes
                               
     Type: Default, Range      boolean: OFF (Version: 5.5)
                               
     Notes                     
                               DESCRIPTION: Put the ndbinfo database into
                               offline mode, in which no rows are returned
                               from tables or views

     Place the *note 'ndbinfo': mysql-cluster-ndbinfo. database into
     offline mode, in which tables and views can be opened even when
     they do not actually exist, or when they exist but have different
     definitions in *note 'NDB': mysql-cluster.  No rows are returned
     from such tables (or views).

   * 'ndbinfo_show_hidden'

     *Type and value information for ndbinfo_show_hidden*

     Property                  Value
                               
     Name                      'ndbinfo_show_hidden'
                               
     Command Line              Yes
                               
     System Variable           Yes
                               
     Option File               No
                               
     Scope                     Both
                               
     Dynamic                   Yes
                               
     Type: Default, Range      boolean: OFF (Version: 5.5)
                               
     Notes                     
                               DESCRIPTION: Whether to show ndbinfo internal
                               base tables in the mysql client.  The default
                               is OFF.

     Whether or not the *note 'ndbinfo': mysql-cluster-ndbinfo.
     database's underlying internal tables are shown in the 'mysql'
     client.  The default is 'OFF'.

   * 'ndbinfo_table_prefix'

     *Type and value information for ndbinfo_table_prefix*

     Property                  Value
                               
     Name                      'ndbinfo_table_prefix'
                               
     Command Line              Yes
                               
     System Variable           Yes
                               
     Option File               No
                               
     Scope                     Both
                               
     Dynamic                   Yes
                               
     Type: Default, Range      string: ndb$ (Version: 5.5)
                               
     Notes                     
                               DESCRIPTION: The prefix to use for naming
                               ndbinfo internal base tables

     The prefix used in naming the ndbinfo database's base tables
     (normally hidden, unless exposed by setting 'ndbinfo_show_hidden').
     This is a read-only variable whose default value is 'ndb$'.  You
     can start the server with the '--ndbinfo-table-prefix' option, but
     this merely sets the variable and does not change the actual prefix
     used to name the hidden base tables; the prefix itself is
     determined at compile time.

   * 'ndbinfo_version'

     *Type and value information for ndbinfo_version*

     Property                  Value
                               
     Name                      'ndbinfo_version'
                               
     Command Line              No
                               
     System Variable           Yes
                               
     Option File               No
                               
     Scope                     Global
                               
     Dynamic                   No
                               
     Type: Default, Range      string: (Version: 5.5)
                               
     Notes                     
                               DESCRIPTION: The version of the ndbinfo
                               engine; read only

     Shows the version of the *note 'ndbinfo': mysql-cluster-ndbinfo.
     engine in use; read-only.


File: manual.info.tmp,  Node: mysql-cluster-status-variables,  Prev: mysql-cluster-system-variables,  Up: mysql-cluster-options-variables

18.3.3.11 NDB Cluster Status Variables
......................................

This section provides detailed information about MySQL server status
variables that relate to NDB Cluster and the *note 'NDB': mysql-cluster.
storage engine.  For status variables not specific to NDB Cluster, and
for general information on using status variables, see *note
server-status-variables::.

   * 'Handler_discover'

     The MySQL server can ask the *note 'NDBCLUSTER': mysql-cluster.
     storage engine if it knows about a table with a given name.  This
     is called discovery.  'Handler_discover' indicates the number of
     times that tables have been discovered using this mechanism.

   * 'Ndb_api_bytes_sent_count_session'

     Amount of data (in bytes) sent to the data nodes in this client
     session.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it relates to the current session only, and is not affected by any
     other clients of this *note 'mysqld': mysqld.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_bytes_sent_count_slave'

     Amount of data (in bytes) sent to the data nodes by this slave.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.  If this MySQL server does not
     act as a replication slave, or does not use NDB tables, this value
     is always 0.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_bytes_sent_count'

     Amount of data (in bytes) sent to the data nodes by this MySQL
     Server (SQL node).

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_bytes_received_count_session'

     Amount of data (in bytes) received from the data nodes in this
     client session.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it relates to the current session only, and is not affected by any
     other clients of this *note 'mysqld': mysqld.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_bytes_received_count_slave'

     Amount of data (in bytes) received from the data nodes by this
     slave.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.  If this MySQL server does not
     act as a replication slave, or does not use NDB tables, this value
     is always 0.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_bytes_received_count'

     Amount of data (in bytes) received from the data nodes by this
     MySQL Server (SQL node).

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_event_data_count_injector'

     The number of row change events received by the NDB binlog injector
     thread.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_event_data_count'

     The number of row change events received by this MySQL Server (SQL
     node).

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_event_nondata_count_injector'

     The number of events received, other than row change events, by the
     NDB binary log injector thread.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_event_nondata_count'

     The number of events received, other than row change events, by
     this MySQL Server (SQL node).

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_event_bytes_count_injector'

     The number of bytes of events received by the NDB binlog injector
     thread.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_event_bytes_count'

     The number of bytes of events received by this MySQL Server (SQL
     node).

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_pk_op_count_session'

     The number of operations in this client session based on or using
     primary keys.  This includes operations on blob tables, implicit
     unlock operations, and auto-increment operations, as well as
     user-visible primary key operations.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it relates to the current session only, and is not affected by any
     other clients of this *note 'mysqld': mysqld.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_pk_op_count_slave'

     The number of operations by this slave based on or using primary
     keys.  This includes operations on blob tables, implicit unlock
     operations, and auto-increment operations, as well as user-visible
     primary key operations.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.  If this MySQL server does not
     act as a replication slave, or does not use NDB tables, this value
     is always 0.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_pk_op_count'

     The number of operations by this MySQL Server (SQL node) based on
     or using primary keys.  This includes operations on blob tables,
     implicit unlock operations, and auto-increment operations, as well
     as user-visible primary key operations.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_pruned_scan_count_session'

     The number of scans in this client session that have been pruned to
     a single partition.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it relates to the current session only, and is not affected by any
     other clients of this *note 'mysqld': mysqld.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_pruned_scan_count_slave'

     The number of scans by this slave that have been pruned to a single
     partition.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.  If this MySQL server does not
     act as a replication slave, or does not use NDB tables, this value
     is always 0.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_pruned_scan_count'

     The number of scans by this MySQL Server (SQL node) that have been
     pruned to a single partition.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_range_scan_count_session'

     The number of range scans that have been started in this client
     session.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it relates to the current session only, and is not affected by any
     other clients of this *note 'mysqld': mysqld.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_range_scan_count_slave'

     The number of range scans that have been started by this slave.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.  If this MySQL server does not
     act as a replication slave, or does not use NDB tables, this value
     is always 0.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_range_scan_count'

     The number of range scans that have been started by this MySQL
     Server (SQL node).

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_read_row_count_session'

     The total number of rows that have been read in this client
     session.  This includes all rows read by any primary key, unique
     key, or scan operation made in this client session.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it relates to the current session only, and is not affected by any
     other clients of this *note 'mysqld': mysqld.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_read_row_count_slave'

     The total number of rows that have been read by this slave.  This
     includes all rows read by any primary key, unique key, or scan
     operation made by this slave.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.  If this MySQL server does not
     act as a replication slave, or does not use NDB tables, this value
     is always 0.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_read_row_count'

     The total number of rows that have been read by this MySQL Server
     (SQL node).  This includes all rows read by any primary key, unique
     key, or scan operation made by this MySQL Server (SQL node).

     You should be aware that this value may not be completely accurate
     with regard to rows read by *note 'SELECT': select. 'COUNT(*)'
     queries, due to the fact that, in this case, the MySQL server
     actually reads pseudo-rows in the form '[TABLE FRAGMENT ID]:[NUMBER
     OF ROWS IN FRAGMENT]' and sums the rows per fragment for all
     fragments in the table to derive an estimated count for all rows.
     'Ndb_api_read_row_count' uses this estimate and not the actual
     number of rows in the table.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_scan_batch_count_session'

     The number of batches of rows received in this client session.  1
     batch is defined as 1 set of scan results from a single fragment.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it relates to the current session only, and is not affected by any
     other clients of this *note 'mysqld': mysqld.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_scan_batch_count_slave'

     The number of batches of rows received by this slave.  1 batch is
     defined as 1 set of scan results from a single fragment.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.  If this MySQL server does not
     act as a replication slave, or does not use NDB tables, this value
     is always 0.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_scan_batch_count'

     The number of batches of rows received by this MySQL Server (SQL
     node).  1 batch is defined as 1 set of scan results from a single
     fragment.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_table_scan_count_session'

     The number of table scans that have been started in this client
     session, including scans of internal tables,.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it relates to the current session only, and is not affected by any
     other clients of this *note 'mysqld': mysqld.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_table_scan_count_slave'

     The number of table scans that have been started by this slave,
     including scans of internal tables,.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.  If this MySQL server does not
     act as a replication slave, or does not use NDB tables, this value
     is always 0.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_table_scan_count'

     The number of table scans that have been started by this MySQL
     Server (SQL node), including scans of internal tables,.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_trans_abort_count_session'

     The number of transactions aborted in this client session.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it relates to the current session only, and is not affected by any
     other clients of this *note 'mysqld': mysqld.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_trans_abort_count_slave'

     The number of transactions aborted by this slave.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.  If this MySQL server does not
     act as a replication slave, or does not use NDB tables, this value
     is always 0.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_trans_abort_count'

     The number of transactions aborted by this MySQL Server (SQL node).

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_trans_close_count_session'

     The number of transactions closed in this client session.  This
     value may be greater than the sum of
     'Ndb_api_trans_commit_count_session' and
     'Ndb_api_trans_abort_count_session', since some transactions may
     have been rolled back.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it relates to the current session only, and is not affected by any
     other clients of this *note 'mysqld': mysqld.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_trans_close_count_slave'

     The number of transactions closed by this slave.  This value may be
     greater than the sum of 'Ndb_api_trans_commit_count_slave' and
     'Ndb_api_trans_abort_count_slave', since some transactions may have
     been rolled back.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.  If this MySQL server does not
     act as a replication slave, or does not use NDB tables, this value
     is always 0.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_trans_close_count'

     The number of transactions closed by this MySQL Server (SQL node).
     This value may be greater than the sum of
     'Ndb_api_trans_commit_count' and 'Ndb_api_trans_abort_count', since
     some transactions may have been rolled back.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_trans_commit_count_session'

     The number of transactions committed in this client session.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it relates to the current session only, and is not affected by any
     other clients of this *note 'mysqld': mysqld.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_trans_commit_count_slave'

     The number of transactions committed by this slave.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.  If this MySQL server does not
     act as a replication slave, or does not use NDB tables, this value
     is always 0.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_trans_commit_count'

     The number of transactions committed by this MySQL Server (SQL
     node).

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_trans_local_read_row_count_session'

     The total number of rows that have been read in this client
     session.  This includes all rows read by any primary key, unique
     key, or scan operation made in this client session.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it relates to the current session only, and is not affected by any
     other clients of this *note 'mysqld': mysqld.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_trans_local_read_row_count_slave'

     The total number of rows that have been read by this slave.  This
     includes all rows read by any primary key, unique key, or scan
     operation made by this slave.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.  If this MySQL server does not
     act as a replication slave, or does not use NDB tables, this value
     is always 0.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_trans_local_read_row_count'

     The total number of rows that have been read by this MySQL Server
     (SQL node).  This includes all rows read by any primary key, unique
     key, or scan operation made by this MySQL Server (SQL node).

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_trans_start_count_session'

     The number of transactions started in this client session.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it relates to the current session only, and is not affected by any
     other clients of this *note 'mysqld': mysqld.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_trans_start_count_slave'

     The number of transactions started by this slave.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.  If this MySQL server does not
     act as a replication slave, or does not use NDB tables, this value
     is always 0.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_trans_start_count'

     The number of transactions started by this MySQL Server (SQL node).

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_uk_op_count_session'

     The number of operations in this client session based on or using
     unique keys.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it relates to the current session only, and is not affected by any
     other clients of this *note 'mysqld': mysqld.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_uk_op_count_slave'

     The number of operations by this slave based on or using unique
     keys.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.  If this MySQL server does not
     act as a replication slave, or does not use NDB tables, this value
     is always 0.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_uk_op_count'

     The number of operations by this MySQL Server (SQL node) based on
     or using unique keys.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_wait_exec_complete_count_session'

     The number of times a thread has been blocked in this client
     session while waiting for execution of an operation to complete.
     This includes all 'execute()'
     (https://dev.mysql.com/doc/ndbapi/en/ndb-ndbtransaction-execute.html)
     calls as well as implicit executes for blob and auto-increment
     operations not visible to clients.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it relates to the current session only, and is not affected by any
     other clients of this *note 'mysqld': mysqld.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_wait_exec_complete_count_slave'

     The number of times a thread has been blocked by this slave while
     waiting for execution of an operation to complete.  This includes
     all 'execute()'
     (https://dev.mysql.com/doc/ndbapi/en/ndb-ndbtransaction-execute.html)
     calls as well as implicit executes for blob and auto-increment
     operations not visible to clients.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.  If this MySQL server does not
     act as a replication slave, or does not use NDB tables, this value
     is always 0.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_wait_exec_complete_count'

     The number of times a thread has been blocked by this MySQL Server
     (SQL node) while waiting for execution of an operation to complete.
     This includes all 'execute()'
     (https://dev.mysql.com/doc/ndbapi/en/ndb-ndbtransaction-execute.html)
     calls as well as implicit executes for blob and auto-increment
     operations not visible to clients.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_wait_meta_request_count_session'

     The number of times a thread has been blocked in this client
     session waiting for a metadata-based signal, such as is expected
     for DDL requests, new epochs, and seizure of transaction records.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it relates to the current session only, and is not affected by any
     other clients of this *note 'mysqld': mysqld.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_wait_meta_request_count_slave'

     The number of times a thread has been blocked by this slave waiting
     for a metadata-based signal, such as is expected for DDL requests,
     new epochs, and seizure of transaction records.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.  If this MySQL server does not
     act as a replication slave, or does not use NDB tables, this value
     is always 0.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_wait_meta_request_count'

     The number of times a thread has been blocked by this MySQL Server
     (SQL node) waiting for a metadata-based signal, such as is expected
     for DDL requests, new epochs, and seizure of transaction records.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_wait_nanos_count_session'

     Total time (in nanoseconds) spent in this client session waiting
     for any type of signal from the data nodes.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it relates to the current session only, and is not affected by any
     other clients of this *note 'mysqld': mysqld.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_wait_nanos_count_slave'

     Total time (in nanoseconds) spent by this slave waiting for any
     type of signal from the data nodes.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.  If this MySQL server does not
     act as a replication slave, or does not use NDB tables, this value
     is always 0.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_wait_nanos_count'

     Total time (in nanoseconds) spent by this MySQL Server (SQL node)
     waiting for any type of signal from the data nodes.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_wait_scan_result_count_session'

     The number of times a thread has been blocked in this client
     session while waiting for a scan-based signal, such as when waiting
     for more results from a scan, or when waiting for a scan to close.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it relates to the current session only, and is not affected by any
     other clients of this *note 'mysqld': mysqld.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_wait_scan_result_count_slave'

     The number of times a thread has been blocked by this slave while
     waiting for a scan-based signal, such as when waiting for more
     results from a scan, or when waiting for a scan to close.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.  If this MySQL server does not
     act as a replication slave, or does not use NDB tables, this value
     is always 0.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_api_wait_scan_result_count'

     The number of times a thread has been blocked by this MySQL Server
     (SQL node) while waiting for a scan-based signal, such as when
     waiting for more results from a scan, or when waiting for a scan to
     close.

     Although this variable can be read using either *note 'SHOW GLOBAL
     STATUS': show-status. or *note 'SHOW SESSION STATUS': show-status,
     it is effectively global in scope.

     For more information, see *note mysql-cluster-ndb-api-statistics::.

   * 'Ndb_cluster_node_id'

     If the server is acting as an NDB Cluster node, then the value of
     this variable its node ID in the cluster.

     If the server is not part of an NDB Cluster, then the value of this
     variable is 0.

   * 'Ndb_config_from_host'

     If the server is part of an NDB Cluster, the value of this variable
     is the host name or IP address of the Cluster management server
     from which it gets its configuration data.

     If the server is not part of an NDB Cluster, then the value of this
     variable is an empty string.

   * 'Ndb_config_from_port'

     If the server is part of an NDB Cluster, the value of this variable
     is the number of the port through which it is connected to the
     Cluster management server from which it gets its configuration
     data.

     If the server is not part of an NDB Cluster, then the value of this
     variable is 0.

   * 'Ndb_conflict_fn_max'

     Used in NDB Cluster Replication conflict resolution, this variable
     shows the number of times that a row was not applied on the current
     SQL node due to 'greatest timestamp wins' conflict resolution since
     the last time that this *note 'mysqld': mysqld. was started.

     For more information, see *note
     mysql-cluster-replication-conflict-resolution::.

   * 'Ndb_conflict_fn_old'

     Used in NDB Cluster Replication conflict resolution, this variable
     shows the number of times that a row was not applied as the result
     of 'same timestamp wins' conflict resolution on a given *note
     'mysqld': mysqld. since the last time it was restarted.

     For more information, see *note
     mysql-cluster-replication-conflict-resolution::.

   * 'Ndb_conflict_fn_epoch'

     Used in NDB Cluster Replication conflict resolution, this variable
     shows the number of rows found to be in conflict using
     'NDB$EPOCH()' conflict resolution on a given *note 'mysqld':
     mysqld. since the last time it was restarted.

     For more information, see *note
     mysql-cluster-replication-conflict-resolution::.

   * 'Ndb_conflict_fn_epoch_trans'

     Used in NDB Cluster Replication conflict resolution, this variable
     shows the number of rows found to be in conflict using
     'NDB$EPOCH_TRANS()' conflict resolution on a given *note 'mysqld':
     mysqld. since the last time it was restarted.

     For more information, see *note
     mysql-cluster-replication-conflict-resolution::.

   * 'Ndb_conflict_trans_row_conflict_count'

     Used in NDB Cluster Replication conflict resolution, this status
     variable shows the number of rows found to be directly in-conflict
     by a transactional conflict function on a given *note 'mysqld':
     mysqld. since the last time it was restarted.

     Currently, the only transactional conflict detection function
     supported by NDB Cluster is NDB$EPOCH_TRANS(), so this status
     variable is effectively the same as 'Ndb_conflict_fn_epoch_trans'.

     For more information, see *note
     mysql-cluster-replication-conflict-resolution::.

   * 'Ndb_conflict_trans_row_reject_count'

     Used in NDB Cluster Replication conflict resolution, this status
     variable shows the total number of rows realigned due to being
     determined as conflicting by a transactional conflict detection
     function.  This includes not only
     'Ndb_conflict_trans_row_conflict_count', but any rows in or
     dependent on conflicting transactions.

     For more information, see *note
     mysql-cluster-replication-conflict-resolution::.

   * 'Ndb_conflict_trans_reject_count'

     Used in NDB Cluster Replication conflict resolution, this status
     variable shows the number of transactions found to be in conflict
     by a transactional conflict detection function.

     For more information, see *note
     mysql-cluster-replication-conflict-resolution::.

   * 'Ndb_conflict_trans_detect_iter_count'

     Used in NDB Cluster Replication conflict resolution, this shows the
     number of internal iterations required to commit an epoch
     transaction.  Should be (slightly) greater than or equal to
     'Ndb_conflict_trans_conflict_commit_count'.

     For more information, see *note
     mysql-cluster-replication-conflict-resolution::.

   * 'Ndb_conflict_trans_conflict_commit_count'

     Used in NDB Cluster Replication conflict resolution, this shows the
     number of epoch transactions committed after they required
     transactional conflict handling.

     For more information, see *note
     mysql-cluster-replication-conflict-resolution::.

   * 'Ndb_execute_count'

     Provides the number of round trips to the *note 'NDB':
     mysql-cluster. kernel made by operations.

   * 'Ndb_number_of_data_nodes'

     If the server is part of an NDB Cluster, the value of this variable
     is the number of data nodes in the cluster.

     If the server is not part of an NDB Cluster, then the value of this
     variable is 0.

   * 'Ndb_pushed_queries_defined'

     The total number of joins pushed down to the NDB kernel for
     distributed handling on the data nodes.  Note that joins tested
     using *note 'EXPLAIN': explain. that can be pushed down contribute
     to this number.  Added in NDB 7.2.0.

   * 'Ndb_pushed_queries_dropped'

     The number of joins that were pushed down to the NDB kernel but
     that could not be handled there.  Added in NDB 7.2.0.

   * 'Ndb_pushed_queries_executed'

     The number of joins successfully pushed down to *note 'NDB':
     mysql-cluster. and executed there.  Added in NDB 7.2.0.

   * 'Ndb_pushed_reads'

     The number of rows returned to *note 'mysqld': mysqld. from the NDB
     kernel by joins that were pushed down.  Note that executing *note
     'EXPLAIN': explain. on joins that can be pushed down to *note
     'NDB': mysql-cluster. does not add to this number.  Added in NDB
     7.2.0.

   * 'Ndb_pruned_scan_count'

     This variable holds a count of the number of scans executed by
     *note 'NDBCLUSTER': mysql-cluster. since the NDB Cluster was last
     started where *note 'NDBCLUSTER': mysql-cluster. was able to use
     partition pruning.

     Using this variable together with 'Ndb_scan_count' can be helpful
     in schema design to maximize the ability of the server to prune
     scans to a single table partition, thereby involving only a single
     data node.

   * 'Ndb_scan_count'

     This variable holds a count of the total number of scans executed
     by *note 'NDBCLUSTER': mysql-cluster. since the NDB Cluster was
     last started.


File: manual.info.tmp,  Node: mysql-cluster-tcp-definition,  Next: mysql-cluster-tcp-definition-direct,  Prev: mysql-cluster-options-variables,  Up: mysql-cluster-config-file

18.3.3.12 NDB Cluster TCP/IP Connections
........................................

TCP/IP is the default transport mechanism for all connections between
nodes in an NDB Cluster.  Normally it is not necessary to define TCP/IP
connections; NDB Cluster automatically sets up such connections for all
data nodes, management nodes, and SQL or API nodes.

*Note*:

For an exception to this rule, see *note
mysql-cluster-tcp-definition-direct::.

To override the default connection parameters, it is necessary to define
a connection using one or more '[tcp]' sections in the 'config.ini'
file.  Each '[tcp]' section explicitly defines a TCP/IP connection
between two NDB Cluster nodes, and must contain at a minimum the
parameters 'NodeId1' and 'NodeId2', as well as any connection parameters
to override.

It is also possible to change the default values for these parameters by
setting them in the '[tcp default]' section.

*Important*:

Any '[tcp]' sections in the 'config.ini' file should be listed _last_,
following all other sections in the file.  However, this is not required
for a '[tcp default]' section.  This requirement is a known issue with
the way in which the 'config.ini' file is read by the NDB Cluster
management server.

Connection parameters which can be set in '[tcp]' and '[tcp default]'
sections of the 'config.ini' file are listed here:

Restart types

Information about the restart types used by the parameter descriptions
in this section is shown in the following table:

*NDB Cluster restart types*

Symbol  Restart Type           Description
                               
*N*     Node                   The parameter can be updated using a
                               rolling restart (see
                               *note mysql-cluster-rolling-restart::)
                               
*S*     System                 All cluster nodes must be shut down
                               completely, then restarted, to effect a
                               change in this parameter
                               
*I*     Initial                Data nodes must be restarted using the
                               '--initial' option

   * 
     'NodeId1'

     *This table provides type and value information for the NodeId1 TCP
     configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      numeric
                                          
     *Default*                            [none]
                                          
     *Range*                              1 - 255
                                          
     *Restart Type*                       N

     To identify a connection between two nodes it is necessary to
     provide their node IDs in the '[tcp]' section of the configuration
     file as the values of 'NodeId1' and 'NodeId2'.  These are the same
     unique 'Id' values for each of these nodes as described in *note
     mysql-cluster-api-definition::.

   * 
     'NodeId2'

     *This table provides type and value information for the NodeId2 TCP
     configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      numeric
                                          
     *Default*                            [none]
                                          
     *Range*                              1 - 255
                                          
     *Restart Type*                       N

     To identify a connection between two nodes it is necessary to
     provide their node IDs in the '[tcp]' section of the configuration
     file as the values of 'NodeId1' and 'NodeId2'.  These are the same
     unique 'Id' values for each of these nodes as described in *note
     mysql-cluster-api-definition::.

   * 
     'HostName1'

     *This table provides type and value information for the HostName1
     TCP configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      name or IP address
                                          
     *Default*                            [none]
                                          
     *Range*                              ...
                                          
     *Restart Type*                       N

     The 'HostName1' and 'HostName2' parameters can be used to specify
     specific network interfaces to be used for a given TCP connection
     between two nodes.  The values used for these parameters can be
     host names or IP addresses.

   * 
     'HostName2'

     *This table provides type and value information for the HostName1
     TCP configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      name or IP address
                                          
     *Default*                            [none]
                                          
     *Range*                              ...
                                          
     *Restart Type*                       N

     The 'HostName1' and 'HostName2' parameters can be used to specify
     specific network interfaces to be used for a given TCP connection
     between two nodes.  The values used for these parameters can be
     host names or IP addresses.

   * 
     'OverloadLimit'

     *This table provides type and value information for the
     OverloadLimit TCP configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      bytes
                                          
     *Default*                            0
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     When more than this many unsent bytes are in the send buffer, the
     connection is considered overloaded.

     This parameter can be used to determine the amount of unsent data
     that must be present in the send buffer before the connection is
     considered overloaded.  See *note
     mysql-cluster-config-send-buffers::, and *note
     mysql-cluster-ndbinfo-transporters::, for more information.

     In some older releases, the effective value of this parameter was
     limited by the size of 'SendBufferMemory'; in NDB Cluster 7.2, the
     actual value for 'OverloadLimit' (up to the stated maximum of 4G)
     is used instead.

   * 
     'SendBufferMemory'

     *This table provides type and value information for the
     SendBufferMemory TCP configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      unsigned
                                          
     *Default*                            2M
                                          
     *Range*                              256K - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     TCP transporters use a buffer to store all messages before
     performing the send call to the operating system.  When this buffer
     reaches 64KB its contents are sent; these are also sent when a
     round of messages have been executed.  To handle temporary overload
     situations it is also possible to define a bigger send buffer.

     If this parameter is set explicitly, then the memory is not
     dedicated to each transporter; instead, the value used denotes the
     hard limit for how much memory (out of the total available
     memory--that is, 'TotalSendBufferMemory') that may be used by a
     single transporter.  For more information about configuring dynamic
     transporter send buffer memory allocation in NDB Cluster, see *note
     mysql-cluster-config-send-buffers::.

     The default size of the send buffer in NDB Cluster 7.2 is 2MB,
     which is the size recommended in most situations.  The minimum size
     is 64 KB; the theoretical maximum is 4 GB.

   * 
     'SendSignalId'

     *This table provides type and value information for the
     SendSignalId TCP configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      boolean
                                          
     *Default*                            [see text]
                                          
     *Range*                              true, false
                                          
     *Restart Type*                       N

     To be able to retrace a distributed message datagram, it is
     necessary to identify each message.  When this parameter is set to
     'Y', message IDs are transported over the network.  This feature is
     disabled by default in production builds, and enabled in '-debug'
     builds.

   * 
     'Checksum'

     *This table provides type and value information for the Checksum
     TCP configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      boolean
                                          
     *Default*                            false
                                          
     *Range*                              true, false
                                          
     *Restart Type*                       N

     This parameter is a boolean parameter (enabled by setting it to 'Y'
     or '1', disabled by setting it to 'N' or '0').  It is disabled by
     default.  When it is enabled, checksums for all messages are
     calculated before they placed in the send buffer.  This feature
     ensures that messages are not corrupted while waiting in the send
     buffer, or by the transport mechanism.

   * 
     'PortNumber' (_OBSOLETE_)

     This formerly specified the port number to be used for listening
     for connections from other nodes.  This parameter is deprecated and
     should no longer be used; use the 'ServerPort' data node
     configuration parameter for this purpose instead.

   * 
     'ReceiveBufferMemory'

     *This table provides type and value information for the
     ReceiveBufferMemory TCP configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      bytes
                                          
     *Default*                            2M
                                          
     *Range*                              16K - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     Specifies the size of the buffer used when receiving data from the
     TCP/IP socket.

     The default value of this parameter is 2MB. The minimum possible
     value is 16KB; the theoretical maximum is 4GB.

   * 
     'TCP_RCV_BUF_SIZE'

     *This table provides type and value information for the
     TCP_RCV_BUF_SIZE TCP configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      unsigned
                                          
     *Default*                            70080
                                          
     *Range*                              1 - 2G
                                          
     *Restart Type*                       N

     Determines the size of the receive buffer set during TCP
     transporter initialization.  The default is recommended for most
     common usage cases.

   * 
     'TCP_SND_BUF_SIZE'

     *This table provides type and value information for the
     TCP_SND_BUF_SIZE TCP configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      unsigned
                                          
     *Default*                            71540
                                          
     *Range*                              1 - 2G
                                          
     *Restart Type*                       N

     Determines the size of the send buffer set during TCP transporter
     initialization.  The default is recommended for most common usage
     cases.

   * 
     'TCP_MAXSEG_SIZE'

     *This table provides type and value information for the
     TCP_MAXSEG_SIZE TCP configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      unsigned
                                          
     *Default*                            0
                                          
     *Range*                              0 - 2G
                                          
     *Restart Type*                       N

     Determines the size of the memory set during TCP transporter
     initialization.  The default is recommended for most common usage
     cases.

   * 'TcpBind_INADDR_ANY'

     Setting this parameter to 'TRUE' or '1' binds 'IP_ADDR_ANY' so that
     connections can be made from anywhere (for autogenerated
     connections).  The default is 'FALSE' ('0').

   * 'Group'

     When 'ndb_optimized_node_selection' is enabled, node proximity is
     used in some cases to select which node to connect to.  This
     parameter can be used to influence proximity by setting it to a
     lower value, which is interpreted as 'closer'.  See the description
     of the system variable for more information.


File: manual.info.tmp,  Node: mysql-cluster-tcp-definition-direct,  Next: mysql-cluster-shm-definition,  Prev: mysql-cluster-tcp-definition,  Up: mysql-cluster-config-file

18.3.3.13 NDB Cluster TCP/IP Connections Using Direct Connections
.................................................................

Setting up a cluster using direct connections between data nodes
requires specifying explicitly the crossover IP addresses of the data
nodes so connected in the '[tcp]' section of the cluster 'config.ini'
file.

In the following example, we envision a cluster with at least four
hosts, one each for a management server, an SQL node, and two data
nodes.  The cluster as a whole resides on the '172.23.72.*' subnet of a
LAN. In addition to the usual network connections, the two data nodes
are connected directly using a standard crossover cable, and communicate
with one another directly using IP addresses in the '1.1.0.*' address
range as shown:

     # Management Server
     [ndb_mgmd]
     Id=1
     HostName=172.23.72.20

     # SQL Node
     [mysqld]
     Id=2
     HostName=172.23.72.21

     # Data Nodes
     [ndbd]
     Id=3
     HostName=172.23.72.22

     [ndbd]
     Id=4
     HostName=172.23.72.23

     # TCP/IP Connections
     [tcp]
     NodeId1=3
     NodeId2=4
     HostName1=1.1.0.1
     HostName2=1.1.0.2

The 'HostName1' and 'HostName2' parameters are used only when specifying
direct connections.

The use of direct TCP connections between data nodes can improve the
cluster's overall efficiency by enabling the data nodes to bypass an
Ethernet device such as a switch, hub, or router, thus cutting down on
the cluster's latency.  It is important to note that to take the best
advantage of direct connections in this fashion with more than two data
nodes, you must have a direct connection between each data node and
every other data node in the same node group.


File: manual.info.tmp,  Node: mysql-cluster-shm-definition,  Next: mysql-cluster-sci-definition,  Prev: mysql-cluster-tcp-definition-direct,  Up: mysql-cluster-config-file

18.3.3.14 NDB Cluster Shared-Memory Connections
...............................................

NDB Cluster attempts to use the shared memory transporter and configure
it automatically where possible.  '[shm]' sections in the 'config.ini'
file explicitly define shared-memory connections between nodes in the
cluster.  When explicitly defining shared memory as the connection
method, it is necessary to define at least 'NodeId1', 'NodeId2', and
'ShmKey'.  All other parameters have default values that should work
well in most cases.

*Important*:

_SHM functionality is considered experimental only_.  It is not
officially supported in any current NDB Cluster release, and testing
results indicate that SHM performance is not appreciably greater than
when using TCP/IP for the transporter.

For these reasons, you must determine for yourself or by using our free
resources (forums, mailing lists) whether SHM can be made to work
correctly in your specific case.

Restart types

Information about the restart types used by the parameter descriptions
in this section is shown in the following table:

*NDB Cluster restart types*

Symbol  Restart Type           Description
                               
*N*     Node                   The parameter can be updated using a
                               rolling restart (see
                               *note mysql-cluster-rolling-restart::)
                               
*S*     System                 All cluster nodes must be shut down
                               completely, then restarted, to effect a
                               change in this parameter
                               
*I*     Initial                Data nodes must be restarted using the
                               '--initial' option

   * 
     'Checksum'

     *This table provides type and value information for the Checksum
     shared memory configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      boolean
                                          
     *Default*                            true
                                          
     *Range*                              true, false
                                          
     *Restart Type*                       N

     This parameter is a boolean ('Y'/'N') parameter which is disabled
     by default.  When it is enabled, checksums for all messages are
     calculated before being placed in the send buffer.

     This feature prevents messages from being corrupted while waiting
     in the send buffer.  It also serves as a check against data being
     corrupted during transport.

   * 
     'HostName1'

     *This table provides type and value information for the HostName1
     shared memory configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      name or IP address
                                          
     *Default*                            [none]
                                          
     *Range*                              ...
                                          
     *Restart Type*                       N

     The 'HostName1' and 'HostName2' parameters can be used to specify
     specific network interfaces to be used for a given SHM connection
     between two nodes.  The values used for these parameters can be
     host names or IP addresses.

   * 
     'HostName2'

     *This table provides type and value information for the HostName1
     shared memory configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      name or IP address
                                          
     *Default*                            [none]
                                          
     *Range*                              ...
                                          
     *Restart Type*                       N

     The 'HostName1' and 'HostName2' parameters can be used to specify
     specific network interfaces to be used for a given SHM connection
     between two nodes.  The values used for these parameters can be
     host names or IP addresses.

   * 
     'NodeId1'

     *This table provides type and value information for the NodeId1
     shared memory configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      numeric
                                          
     *Default*                            [none]
                                          
     *Range*                              1 - 255
                                          
     *Restart Type*                       N

     To identify a connection between two nodes it is necessary to
     provide node identifiers for each of them, as 'NodeId1' and
     'NodeId2'.

   * 
     'NodeId2'

     *This table provides type and value information for the NodeId2
     shared memory configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      numeric
                                          
     *Default*                            [none]
                                          
     *Range*                              1 - 255
                                          
     *Restart Type*                       N

     To identify a connection between two nodes it is necessary to
     provide node identifiers for each of them, as 'NodeId1' and
     'NodeId2'.

   * 
     'NodeIdServer'

     *This table provides type and value information for the
     NodeIdServer shared memory configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      numeric
                                          
     *Default*                            [none]
                                          
     *Range*                              1 - 63
                                          
     *Restart Type*                       N

     Identify the server end of a shared memory connection.

   * 
     'OverloadLimit'

     *This table provides type and value information for the
     OverloadLimit shared memory configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      bytes
                                          
     *Default*                            0
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     When more than this many unsent bytes are in the send buffer, the
     connection is considered overloaded.

     This parameter can be used to determine the amount of unsent data
     that must be present in the send buffer before the connection is
     considered overloaded.  See *note
     mysql-cluster-config-send-buffers::, for more information.

     In some older releases, the effective value of this parameter was
     limited by the size of 'SendBufferMemory'; in NDB Cluster 7.2, the
     actual value for 'OverloadLimit' (up to the stated maximum of 4G)
     is used instead.

   * 
     'SendSignalId'

     *This table provides type and value information for the
     SendSignalId shared memory configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      boolean
                                          
     *Default*                            false
                                          
     *Range*                              true, false
                                          
     *Restart Type*                       N

     To retrace the path of a distributed message, it is necessary to
     provide each message with a unique identifier.  Setting this
     parameter to 'Y' causes these message IDs to be transported over
     the network as well.  This feature is disabled by default in
     production builds, and enabled in '-debug' builds.

   * 
     'ShmKey'

     *This table provides type and value information for the ShmKey
     shared memory configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      unsigned
                                          
     *Default*                            0
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     When setting up shared memory segments, a node ID, expressed as an
     integer, is used to identify uniquely the shared memory segment to
     use for the communication.  There is no default value.

   * 
     'ShmSize'

     *This table provides type and value information for the ShmSize
     shared memory configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      bytes
                                          
     *Default*                            1M
                                          
     *Range*                              64K - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     Each SHM connection has a shared memory segment where messages
     between nodes are placed by the sender and read by the reader.  The
     size of this segment is defined by 'ShmSize'.  The default value is
     1MB.

   * 
     'SigNum'

     *This table provides type and value information for the Signum
     shared memory configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      unsigned
                                          
     *Default*                            [none]
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     When using the shared memory transporter, a process sends an
     operating system signal to the other process when there is new data
     available in the shared memory.  Should that signal conflict with
     an existing signal, this parameter can be used to change it.  This
     is a possibility when using SHM due to the fact that different
     operating systems use different signal numbers.

     The default value of 'SigNum' is 0; therefore, it must be set to
     avoid errors in the cluster log when using the shared memory
     transporter.  Typically, this parameter is set to 10 in the '[shm
     default]' section of the 'config.ini' file.


File: manual.info.tmp,  Node: mysql-cluster-sci-definition,  Next: mysql-cluster-config-send-buffers,  Prev: mysql-cluster-shm-definition,  Up: mysql-cluster-config-file

18.3.3.15 SCI Transport Connections in NDB Cluster
..................................................

'[sci]' sections in the 'config.ini' file explicitly define SCI
(Scalable Coherent Interface) connections between cluster nodes.  Using
SCI transporters in NDB Cluster requires specialized hardware as well as
specially-built MySQL binaries; compiling such binaries is not supported
using an NDB 7.2 or later distribution.

Restart types

Information about the restart types used by the parameter descriptions
in this section is shown in the following table:

*NDB Cluster restart types*

Symbol  Restart Type           Description
                               
*N*     Node                   The parameter can be updated using a
                               rolling restart (see
                               *note mysql-cluster-rolling-restart::)
                               
*S*     System                 All cluster nodes must be shut down
                               completely, then restarted, to effect a
                               change in this parameter
                               
*I*     Initial                Data nodes must be restarted using the
                               '--initial' option

The following parameters are present in 'NDB' source code as well as the
output of *note 'ndb_config': mysql-cluster-programs-ndb-config. and
other 'NDB' programs, but are nonfunctional in NDB 7.2 and later.

   * 
     'NodeId1'

     *This table provides type and value information for the NodeId1 SCI
     configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      numeric
                                          
     *Default*                            [none]
                                          
     *Range*                              1 - 255
                                          
     *Restart Type*                       N

     To identify a connection between two nodes it is necessary to
     provide node identifiers for each of them, as 'NodeId1' and
     'NodeId2'.

   * 
     'NodeId2'

     *This table provides type and value information for the NodeId2 SCI
     configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      numeric
                                          
     *Default*                            [none]
                                          
     *Range*                              1 - 255
                                          
     *Restart Type*                       N

     To identify a connection between two nodes it is necessary to
     provide node identifiers for each of them, as 'NodeId1' and
     'NodeId2'.

   * 
     'Group'

     *This table provides type and value information for the Group
     shared memory configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      unsigned
                                          
     *Default*                            35
                                          
     *Range*                              0 - 200
                                          
     *Restart Type*                       N

     Determines the group proximity; a smaller value is interpreted as
     being closer.  The default value is sufficient for most conditions.

   * 
     'Host1SciId0'

     *This table provides type and value information for the Host1SciId0
     SCI configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      unsigned
                                          
     *Default*                            [none]
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     This identifies the SCI node ID on the first Cluster node
     (identified by 'NodeId1').

   * 'Host1SciId1'

     *This table provides type and value information for the Host1SciId1
     SCI configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      unsigned
                                          
     *Default*                            0
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     It is possible to set up SCI Transporters for failover between two
     SCI cards which then should use separate networks between the
     nodes.  This identifies the node ID and the second SCI card to be
     used on the first node.

   * 'Host2SciId0'

     *This table provides type and value information for the Host2SciId0
     SCI configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      unsigned
                                          
     *Default*                            [none]
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     This identifies the SCI node ID on the second Cluster node
     (identified by 'NodeId2').

   * 'Host2SciId1'

     *This table provides type and value information for the Host2SciId1
     SCI configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      unsigned
                                          
     *Default*                            0
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     When using two SCI cards to provide failover, this parameter
     identifies the second SCI card to be used on the second node.

   * 
     'HostName1'

     *This table provides type and value information for the HostName1
     SCI configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      name or IP address
                                          
     *Default*                            [none]
                                          
     *Range*                              ...
                                          
     *Restart Type*                       N

     The 'HostName1' and 'HostName2' parameters can be used to specify
     specific network interfaces to be used for a given SCI connection
     between two nodes.  The values used for these parameters can be
     host names or IP addresses.

   * 
     'HostName2'

     *This table provides type and value information for the HostName1
     SCI configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      name or IP address
                                          
     *Default*                            [none]
                                          
     *Range*                              ...
                                          
     *Restart Type*                       N

     The 'HostName1' and 'HostName2' parameters can be used to specify
     specific network interfaces to be used for a given SCI connection
     between two nodes.  The values used for these parameters can be
     host names or IP addresses.

   * 
     'SharedBufferSize'

     *This table provides type and value information for the
     SharedBufferSize SCI configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      unsigned
                                          
     *Default*                            10M
                                          
     *Range*                              64K - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     Each SCI transporter has a shared memory segment used for
     communication between the two nodes.  Setting the size of this
     segment to the default value of 1MB should be sufficient for most
     applications.  Using a smaller value can lead to problems when
     performing many parallel inserts; if the shared buffer is too
     small, this can also result in a crash of the *note 'ndbd':
     mysql-cluster-programs-ndbd. process.

   * 'SendLimit'

     *This table provides type and value information for the SendLimit
     SCI configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      unsigned
                                          
     *Default*                            8K
                                          
     *Range*                              128 - 32K
                                          
     *Restart Type*                       N

     A small buffer in front of the SCI media stores messages before
     transmitting them over the SCI network.  By default, this is set to
     8KB. Our benchmarks show that performance is best at 64KB but 16KB
     reaches within a few percent of this, and there was little if any
     advantage to increasing it beyond 8KB.

   * 
     'SendSignalId'

     *This table provides type and value information for the
     SendSignalId SCI configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      boolean
                                          
     *Default*                            true
                                          
     *Range*                              true, false
                                          
     *Restart Type*                       N

     To trace a distributed message it is necessary to identify each
     message uniquely.  When this parameter is set to 'Y', message IDs
     are transported over the network.  This feature is disabled by
     default in production builds, and enabled in '-debug' builds.

   * 
     'Checksum'

     *This table provides type and value information for the Checksum
     SCI configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      boolean
                                          
     *Default*                            false
                                          
     *Range*                              true, false
                                          
     *Restart Type*                       N

     This parameter is a boolean value, and is disabled by default.
     When 'Checksum' is enabled, checksums are calculated for all
     messages before they are placed in the send buffer.  This feature
     prevents messages from being corrupted while waiting in the send
     buffer.  It also serves as a check against data being corrupted
     during transport.

   * 
     'OverloadLimit'

     *This table provides type and value information for the
     OverloadLimit SCI configuration parameter*

     Property                             Value
                                          
     *Version (or later)*                 NDB 7.2.1
                                          
     *Type or units*                      bytes
                                          
     *Default*                            0
                                          
     *Range*                              0 - 4294967039 (0xFFFFFEFF)
                                          
     *Restart Type*                       N

     When more than this many unsent bytes are in the send buffer, the
     connection is considered overloaded.  See *note
     mysql-cluster-config-send-buffers::, for more information.


File: manual.info.tmp,  Node: mysql-cluster-config-send-buffers,  Prev: mysql-cluster-sci-definition,  Up: mysql-cluster-config-file

18.3.3.16 Configuring NDB Cluster Send Buffer Parameters
........................................................

Formerly, the NDB kernel employed a send buffer whose size was fixed at
2MB for each node in the cluster, this buffer being allocated when the
node started.  Because the size of this buffer could not be changed
after the cluster was started, it was necessary to make it large enough
in advance to accommodate the maximum possible load on any transporter
socket.  However, this was an inefficient use of memory, since much of
it often went unused, and could result in large amounts of resources
being wasted when scaling up to many API nodes.

This problem was eventually solved (in NDB Cluster 7.0) by employing a
unified send buffer whose memory is allocated dynamically from a pool
shared by all transporters.  This means that the size of the send buffer
can be adjusted as necessary.  Configuration of the unified send buffer
can accomplished by setting the following parameters:

   * TotalSendBufferMemory

     This parameter can be set for all types of NDB Cluster nodes--that
     is, it can be set in the '[ndbd]', '[mgm]', and '[api]' (or
     '[mysql]') sections of the 'config.ini' file.  It represents the
     total amount of memory (in bytes) to be allocated by each node for
     which it is set for use among all configured transporters.  If set,
     its minimum is 256KB; the maximum is 4294967039.

     To be backward-compatible with existing configurations, this
     parameter takes as its default value the sum of the maximum send
     buffer sizes of all configured transporters, plus an additional
     32KB (one page) per transporter.  The maximum depends on the type
     of transporter, as shown in the following table:

     *Transporter types with maximum send buffer sizes*

     Transporter            Maximum Send Buffer Size (bytes)
                            
     TCP                    'SendBufferMemory' (default = 2M)
                            
     SCI                    'SendLimit' (default = 8K) plus 16K
                            
     SHM                    20K

     This enables existing configurations to function in close to the
     same way as they did with NDB Cluster 6.3 and earlier, with the
     same amount of memory and send buffer space available to each
     transporter.  However, memory that is unused by one transporter is
     not available to other transporters.

   * OverloadLimit

     This parameter is used in the 'config.ini' file '[tcp]' section,
     and denotes the amount of unsent data (in bytes) that must be
     present in the send buffer before the connection is considered
     overloaded.  When such an overload condition occurs, transactions
     that affect the overloaded connection fail with NDB API Error 1218
     ('Send Buffers overloaded in NDB kernel') until the overload status
     passes.  The default value is 0, in which case the effective
     overload limit is calculated as 'SendBufferMemory * 0.8' for a
     given connection.  The maximum value for this parameter is 4G.

   * SendBufferMemory

     This value denotes a hard limit for the amount of memory that may
     be used by a single transporter out of the entire pool specified by
     'TotalSendBufferMemory'.  However, the sum of 'SendBufferMemory'
     for all configured transporters may be greater than the
     'TotalSendBufferMemory' that is set for a given node.  This is a
     way to save memory when many nodes are in use, as long as the
     maximum amount of memory is never required by all transporters at
     the same time.

   * ReservedSendBufferMemory

     This optional data node parameter, if set, gives an amount of
     memory (in bytes) that is reserved for connections between data
     nodes; this memory is not allocated to send buffers used for
     communications with management servers or API nodes.  This provides
     a way to protect the cluster against misbehaving API nodes that use
     excess send memory and thus cause failures in communications
     internally in the NDB kernel.  If set, its the minimum permitted
     value for this parameters is 256KB; the maximum is 4294967039.

You can use the *note 'ndbinfo.transporters':
mysql-cluster-ndbinfo-transporters. table to monitor send buffer memory
usage, and to detect slowdown and overload conditions that can adversely
affect performance.


File: manual.info.tmp,  Node: mysql-cluster-interconnects,  Prev: mysql-cluster-config-file,  Up: mysql-cluster-configuration

18.3.4 Using High-Speed Interconnects with NDB Cluster
------------------------------------------------------

Even before design of *note 'NDBCLUSTER': mysql-cluster. began in 1996,
it was evident that one of the major problems to be encountered in
building parallel databases would be communication between the nodes in
the network.  For this reason, *note 'NDBCLUSTER': mysql-cluster. was
designed from the very beginning to permit the use of a number of
different data transport mechanisms.  In this Manual, we use the term
_transporter_ for these.

The NDB Cluster codebase provides for four different transporters:

   * _TCP/IP using 100 Mbps or gigabit Ethernet_, as discussed in *note
     mysql-cluster-tcp-definition::.

   * _Direct (machine-to-machine) TCP/IP_; although this transporter
     uses the same TCP/IP protocol as mentioned in the previous item, it
     requires setting up the hardware differently and is configured
     differently as well.  For this reason, it is considered a separate
     transport mechanism for NDB Cluster.  See *note
     mysql-cluster-tcp-definition-direct::, for details.

   * _Shared memory (SHM)_. For more information about SHM, see *note
     mysql-cluster-shm-definition::.

     *Note*:

     SHM is considered experimental only, and is not officially
     supported.

   * _Scalable Coherent Interface (SCI)_. For more information about
     SHM, see *note mysql-cluster-sci-definition::.

     *Note*:

     Using SCI transporters in NDB Cluster requires specialized
     hardware, software, and MySQL binaries not available using an NDB
     7.2 or later distribution.

Most users today employ TCP/IP over Ethernet because it is ubiquitous.
TCP/IP is also by far the best-tested transporter for use with NDB
Cluster.

We are working to make sure that communication with the *note 'ndbd':
mysql-cluster-programs-ndbd. process is made in 'chunks' that are as
large as possible because this benefits all types of data transmission.


File: manual.info.tmp,  Node: mysql-cluster-programs,  Next: mysql-cluster-management,  Prev: mysql-cluster-configuration,  Up: mysql-cluster

18.4 NDB Cluster Programs
=========================

* Menu:

* mysql-cluster-programs-ndbd::  'ndbd' -- The NDB Cluster Data Node Daemon
* mysql-cluster-programs-ndbinfo-select-all::  'ndbinfo_select_all' -- Select From ndbinfo Tables
* mysql-cluster-programs-ndbmtd::  'ndbmtd' -- The NDB Cluster Data Node Daemon (Multi-Threaded)
* mysql-cluster-programs-ndb-mgmd::  'ndb_mgmd' -- The NDB Cluster Management Server Daemon
* mysql-cluster-programs-ndb-mgm::  'ndb_mgm' -- The NDB Cluster Management Client
* mysql-cluster-programs-ndb-blob-tool::  'ndb_blob_tool' -- Check and Repair BLOB and TEXT columns of NDB Cluster Tables
* mysql-cluster-programs-ndb-config::  'ndb_config' -- Extract NDB Cluster Configuration Information
* mysql-cluster-programs-ndb-cpcd::  'ndb_cpcd' -- Automate Testing for NDB Development
* mysql-cluster-programs-ndb-delete-all::  'ndb_delete_all' -- Delete All Rows from an NDB Table
* mysql-cluster-programs-ndb-desc::  'ndb_desc' -- Describe NDB Tables
* mysql-cluster-programs-ndb-drop-index::  'ndb_drop_index' -- Drop Index from an NDB Table
* mysql-cluster-programs-ndb-drop-table::  'ndb_drop_table' -- Drop an NDB Table
* mysql-cluster-programs-ndb-error-reporter::  'ndb_error_reporter' -- NDB Error-Reporting Utility
* mysql-cluster-programs-ndb-index-stat::  'ndb_index_stat' -- NDB Index Statistics Utility
* mysql-cluster-programs-ndb-move-data::  'ndb_move_data' -- NDB Data Copy Utility
* mysql-cluster-programs-ndb-print-backup-file::  'ndb_print_backup_file' -- Print NDB Backup File Contents
* mysql-cluster-programs-ndb-print-file::  'ndb_print_file' -- Print NDB Disk Data File Contents
* mysql-cluster-programs-ndb-print-schema-file::  'ndb_print_schema_file' -- Print NDB Schema File Contents
* mysql-cluster-programs-ndb-print-sys-file::  'ndb_print_sys_file' -- Print NDB System File Contents
* mysql-cluster-programs-ndb-redo-log-reader::  'ndb_redo_log_reader' -- Check and Print Content of Cluster Redo Log
* mysql-cluster-programs-ndb-restore::  'ndb_restore' -- Restore an NDB Cluster Backup
* mysql-cluster-programs-ndb-select-all::  'ndb_select_all' -- Print Rows from an NDB Table
* mysql-cluster-programs-ndb-select-count::  'ndb_select_count' -- Print Row Counts for NDB Tables
* mysql-cluster-programs-ndb-show-tables::  'ndb_show_tables' -- Display List of NDB Tables
* mysql-cluster-programs-ndb-size-pl::  'ndb_size.pl' -- NDBCLUSTER Size Requirement Estimator
* mysql-cluster-programs-ndb-waiter::  'ndb_waiter' -- Wait for NDB Cluster to Reach a Given Status
* mysql-cluster-program-options-common::  Options Common to NDB Cluster Programs -- Options Common to NDB Cluster Programs

Using and managing an NDB Cluster requires several specialized programs,
which we describe in this chapter.  We discuss the purposes of these
programs in an NDB Cluster, how to use the programs, and what startup
options are available for each of them.

These programs include the NDB Cluster data, management, and SQL node
processes (*note 'ndbd': mysql-cluster-programs-ndbd, *note 'ndbmtd':
mysql-cluster-programs-ndbmtd, *note 'ndb_mgmd':
mysql-cluster-programs-ndb-mgmd, and *note 'mysqld': mysqld.) and the
management client (*note 'ndb_mgm': mysql-cluster-programs-ndb-mgm.).

For information about using *note 'mysqld': mysqld. as an NDB Cluster
process, see *note mysql-cluster-mysqld::.

Other *note 'NDB': mysql-cluster. utility, diagnostic, and example
programs are included with the NDB Cluster distribution.  These include
*note 'ndb_restore': mysql-cluster-programs-ndb-restore, *note
'ndb_show_tables': mysql-cluster-programs-ndb-show-tables, and *note
'ndb_config': mysql-cluster-programs-ndb-config.  These programs are
also covered in this section.

The final portion of this section contains tables of options that are
common to all the various NDB Cluster programs.


File: manual.info.tmp,  Node: mysql-cluster-programs-ndbd,  Next: mysql-cluster-programs-ndbinfo-select-all,  Prev: mysql-cluster-programs,  Up: mysql-cluster-programs

18.4.1 'ndbd' -- The NDB Cluster Data Node Daemon
-------------------------------------------------

*note 'ndbd': mysql-cluster-programs-ndbd. is the process that is used
to handle all the data in tables using the NDB Cluster storage engine.
This is the process that empowers a data node to accomplish distributed
transaction handling, node recovery, checkpointing to disk, online
backup, and related tasks.

In an NDB Cluster, a set of *note 'ndbd': mysql-cluster-programs-ndbd.
processes cooperate in handling data.  These processes can execute on
the same computer (host) or on different computers.  The correspondences
between data nodes and Cluster hosts is completely configurable.

The following table includes command options specific to the NDB Cluster
data node program *note 'ndbd': mysql-cluster-programs-ndbd.  Additional
descriptions follow the table.  For options common to most NDB Cluster
programs (including *note 'ndbd': mysql-cluster-programs-ndbd.), see
*note mysql-cluster-program-options-common::.

*Command-line options for the ndbd program*

Format                   Description              Added, Deprecated, or
                                                  Removed
                                                  
                         Local bind address       
' --bind-address=name                             All MySQL 5.5 based
'                                                 releases
                                                  
                         Time to wait between     
' --connect-delay=# '    attempts to contact a    ADDED: NDB 7.2.9
                         management server, in    
                         seconds; 0 means do
                         not wait between
                         attempts
                         
                         Set the number of        
' --connect-retries=#    times to retry a         ADDED: NDB 7.2.9
'                        connection before        
                         giving up; 0 means 1
                         attempt only (and no
                         retries)
                         
                         Start ndbd as daemon     
'--daemon',              (default); override      All MySQL 5.5 based
                         with -nodaemon           releases
' -d '                                            

' --foreground '         Run ndbd in              All MySQL 5.5 based
                         foreground, provided     releases
                         for debugging purposes   
                         (implies -nodaemon)
                         
                         Perform initial start    
' --initial '            of ndbd, including       All MySQL 5.5 based
                         cleaning the file        releases
                         system.  Consult the     
                         documentation before
                         using this option
                         
                         Perform partial          
' --initial-start '      initial start            All MySQL 5.5 based
                         (requires                releases
                         -nowait-nodes)           
                         
                         Used to install the      
' --install[=name] '     data node process as a   All MySQL 5.5 based
                         Windows service.  Does   releases
                         not apply on             
                         non-Windows platforms.
                         
                         Don't start ndbd         
'--nostart',             immediately; ndbd        All MySQL 5.5 based
                         waits for command to     releases
' -n '                   start from ndb_mgmd      
                         
                         Do not start ndbd as     
' --nodaemon '           daemon; provided for     All MySQL 5.5 based
                         testing purposes         releases
                                                  
                         Do not wait for these    
' --nowait-nodes=list    data nodes to start      All MySQL 5.5 based
'                        (takes comma-separated   releases
                         list of node IDs).       
                         Also requires
                         -ndb-nodeid to be
                         used.
                         
                         Used to remove a data    
' --remove[=name] '      node process that was    All MySQL 5.5 based
                         previously installed     releases
                         as a Windows service.    
                         Does not apply on
                         non-Windows platforms.
                         
                         Causes the data log to   
'--verbose',             write extra debugging    All MySQL 5.5 based
                         information to the       releases
' -v '                   node log.
                         

*Note*:

All of these options also apply to the multithreaded version of this
program (*note 'ndbmtd': mysql-cluster-programs-ndbmtd.) and you may
substitute '*note 'ndbmtd': mysql-cluster-programs-ndbmtd.' for '*note
'ndbd': mysql-cluster-programs-ndbd.' wherever the latter occurs in this
section.

   * '--bind-address'

     Property               Value
                            
     *Command-Line          '--bind-address=name'
     Format*                

     *Type*                 String
                            
     *Default Value*        ''

     Causes *note 'ndbd': mysql-cluster-programs-ndbd. to bind to a
     specific network interface (host name or IP address).  This option
     has no default value.

   * '--daemon', '-d'

     Property               Value
                            
     *Command-Line          '--daemon'
     Format*                

     *Type*                 Boolean
                            
     *Default Value*        'TRUE'

     Instructs *note 'ndbd': mysql-cluster-programs-ndbd. or *note
     'ndbmtd': mysql-cluster-programs-ndbmtd. to execute as a daemon
     process.  This is the default behavior.  '--nodaemon' can be used
     to prevent the process from running as a daemon.

     This option has no effect when running *note 'ndbd':
     mysql-cluster-programs-ndbd. or *note 'ndbmtd':
     mysql-cluster-programs-ndbmtd. on Windows platforms.

   * 
     '--connect-delay=#'

     Property               Value
                            
     *Command-Line          '--connect-delay=#'
     Format*                

     *Introduced*           5.5.28-ndb-7.2.9
                            
     *Type*                 Numeric
                            
     *Default Value*        '5'
                            
     *Minimum Value*        '0'
                            
     *Maximum Value*        '3600'

     Determines the time to wait between attempts to contact a
     management server when starting (the time between attempts is
     controlled by the '--connect-retries' option).  The default is 5
     attempts.

     This option was added in NDB 7.2.9.

   * 
     '--connect-retries=#'

     Property               Value
                            
     *Command-Line          '--connect-retries=#'
     Format*                

     *Introduced*           5.5.28-ndb-7.2.9
                            
     *Type*                 Numeric
                            
     *Default Value*        '12'
                            
     *Minimum Value*        '0'
                            
     *Maximum Value*        '65535'

     Determines the number of times that the data node attempts to
     contact a management server when starting.  Setting this option to
     -1 causes the data node to keep trying to make contact
     indefinitely.  The default is 12 attempts.  The time to wait
     between attempts is controlled by the '--connect-delay' option.

     This option was added in NDB 7.2.9.

   * '--foreground'

     Property               Value
                            
     *Command-Line          '--foreground'
     Format*                

     *Type*                 Boolean
                            
     *Default Value*        'FALSE'

     Causes *note 'ndbd': mysql-cluster-programs-ndbd. or *note
     'ndbmtd': mysql-cluster-programs-ndbmtd. to execute as a foreground
     process, primarily for debugging purposes.  This option implies the
     '--nodaemon' option.

     This option has no effect when running *note 'ndbd':
     mysql-cluster-programs-ndbd. or *note 'ndbmtd':
     mysql-cluster-programs-ndbmtd. on Windows platforms.

   * 
     '--initial'

     Property               Value
                            
     *Command-Line          '--initial'
     Format*                

     *Type*                 Boolean
                            
     *Default Value*        'FALSE'

     Instructs *note 'ndbd': mysql-cluster-programs-ndbd. to perform an
     initial start.  An initial start erases any files created for
     recovery purposes by earlier instances of *note 'ndbd':
     mysql-cluster-programs-ndbd.  It also re-creates recovery log
     files.  On some operating systems, this process can take a
     substantial amount of time.

     An '--initial' start is to be used _only_ when starting the *note
     'ndbd': mysql-cluster-programs-ndbd. process under very special
     circumstances; this is because this option causes all files to be
     removed from the NDB Cluster file system and all redo log files to
     be re-created.  These circumstances are listed here:

        * When performing a software upgrade which has changed the
          contents of any files.

        * When restarting the node with a new version of *note 'ndbd':
          mysql-cluster-programs-ndbd.

        * As a measure of last resort when for some reason the node
          restart or system restart repeatedly fails.  In this case, be
          aware that this node can no longer be used to restore data due
          to the destruction of the data files.

     *Warning*:

     To avoid the possibility of eventual data loss, it is recommended
     that you _not_ use the '--initial' option together with
     'StopOnError = 0'.  Instead, set 'StopOnError' to 0 in 'config.ini'
     only after the cluster has been started, then restart the data
     nodes normally--that is, without the '--initial' option.  See the
     description of the 'StopOnError' parameter for a detailed
     explanation of this issue.  (Bug #24945638)

     Use of this option prevents the 'StartPartialTimeout' and
     'StartPartitionedTimeout' configuration parameters from having any
     effect.

     *Important*:

     This option does _not_ affect either of the following types of
     files:

        * Backup files that have already been created by the affected
          node

        * NDB Cluster Disk Data files (see *note
          mysql-cluster-disk-data::).

     This option also has no effect on recovery of data by a data node
     that is just starting (or restarting) from data nodes that are
     already running.  This recovery of data occurs automatically, and
     requires no user intervention in an NDB Cluster that is running
     normally.

     It is permissible to use this option when starting the cluster for
     the very first time (that is, before any data node files have been
     created); however, it is _not_ necessary to do so.

   * 
     '--initial-start'

     Property               Value
                            
     *Command-Line          '--initial-start'
     Format*                

     *Type*                 Boolean
                            
     *Default Value*        'FALSE'

     This option is used when performing a partial initial start of the
     cluster.  Each node should be started with this option, as well as
     '--nowait-nodes'.

     Suppose that you have a 4-node cluster whose data nodes have the
     IDs 2, 3, 4, and 5, and you wish to perform a partial initial start
     using only nodes 2, 4, and 5--that is, omitting node 3:

          shell> ndbd --ndb-nodeid=2 --nowait-nodes=3 --initial-start
          shell> ndbd --ndb-nodeid=4 --nowait-nodes=3 --initial-start
          shell> ndbd --ndb-nodeid=5 --nowait-nodes=3 --initial-start

     When using this option, you must also specify the node ID for the
     data node being started with the '--ndb-nodeid' option.

     *Important*:

     Do not confuse this option with the '--nowait-nodes' option for
     *note 'ndb_mgmd': mysql-cluster-programs-ndb-mgmd, which can be
     used to enable a cluster configured with multiple management
     servers to be started without all management servers being online.

   * 
     '--install[=NAME]'

     Property               Value
                            
     *Command-Line          '--install[=name]'
     Format*                

     *Platform Specific*    Windows
                            
     *Type*                 String
                            
     *Default Value*        'ndbd'

     Causes *note 'ndbd': mysql-cluster-programs-ndbd. to be installed
     as a Windows service.  Optionally, you can specify a name for the
     service; if not set, the service name defaults to 'ndbd'.  Although
     it is preferable to specify other *note 'ndbd':
     mysql-cluster-programs-ndbd. program options in a 'my.ini' or
     'my.cnf' configuration file, it is possible to use together with
     '--install'.  However, in such cases, the '--install' option must
     be specified first, before any other options are given, for the
     Windows service installation to succeed.

     It is generally not advisable to use this option together with the
     '--initial' option, since this causes the data node file system to
     be wiped and rebuilt every time the service is stopped and started.
     Extreme care should also be taken if you intend to use any of the
     other *note 'ndbd': mysql-cluster-programs-ndbd. options that
     affect the starting of data nodes--including '--initial-start',
     '--nostart', and '--nowait-nodes'--together with '--install', and
     you should make absolutely certain you fully understand and allow
     for any possible consequences of doing so.

     The '--install' option has no effect on non-Windows platforms.

   * '--nodaemon'

     Property               Value
                            
     *Command-Line          '--nodaemon'
     Format*                

     *Type*                 Boolean
                            
     *Default Value*        'FALSE'

     Prevents *note 'ndbd': mysql-cluster-programs-ndbd. or *note
     'ndbmtd': mysql-cluster-programs-ndbmtd. from executing as a daemon
     process.  This option overrides the '--daemon' option.  This is
     useful for redirecting output to the screen when debugging the
     binary.

     The default behavior for *note 'ndbd': mysql-cluster-programs-ndbd.
     and *note 'ndbmtd': mysql-cluster-programs-ndbmtd. on Windows is to
     run in the foreground, making this option unnecessary on Windows
     platforms, where it has no effect.

   * 
     '--nostart', '-n'

     Property               Value
                            
     *Command-Line          '--nostart'
     Format*                

     *Type*                 Boolean
                            
     *Default Value*        'FALSE'

     Instructs *note 'ndbd': mysql-cluster-programs-ndbd. not to start
     automatically.  When this option is used, *note 'ndbd':
     mysql-cluster-programs-ndbd. connects to the management server,
     obtains configuration data from it, and initializes communication
     objects.  However, it does not actually start the execution engine
     until specifically requested to do so by the management server.
     This can be accomplished by issuing the proper 'START' command in
     the management client (see *note
     mysql-cluster-mgm-client-commands::).

   * 
     '--nowait-nodes=NODE_ID_1[, NODE_ID_2[, ...]]'

     Property               Value
                            
     *Command-Line          '--nowait-nodes=list'
     Format*                

     *Type*                 String
                            
     *Default Value*        ''

     This option takes a list of data nodes which for which the cluster
     will not wait for before starting.

     This can be used to start the cluster in a partitioned state.  For
     example, to start the cluster with only half of the data nodes
     (nodes 2, 3, 4, and 5) running in a 4-node cluster, you can start
     each *note 'ndbd': mysql-cluster-programs-ndbd. process with
     '--nowait-nodes=3,5'.  In this case, the cluster starts as soon as
     nodes 2 and 4 connect, and does _not_ wait
     'StartPartitionedTimeout' milliseconds for nodes 3 and 5 to connect
     as it would otherwise.

     If you wanted to start up the same cluster as in the previous
     example without one *note 'ndbd': mysql-cluster-programs-ndbd.
     (say, for example, that the host machine for node 3 has suffered a
     hardware failure) then start nodes 2, 4, and 5 with
     '--nowait-nodes=3'.  Then the cluster will start as soon as nodes
     2, 4, and 5 connect and will not wait for node 3 to start.

   * 
     '--remove[=NAME]'

     Property               Value
                            
     *Command-Line          '--remove[=name]'
     Format*                

     *Platform Specific*    Windows
                            
     *Type*                 String
                            
     *Default Value*        'ndbd'

     Causes an *note 'ndbd': mysql-cluster-programs-ndbd. process that
     was previously installed as a Windows service to be removed.
     Optionally, you can specify a name for the service to be
     uninstalled; if not set, the service name defaults to 'ndbd'.

     The '--remove' option has no effect on non-Windows platforms.

   * 
     '--verbose', '-v'

     Causes extra debug output to be written to the node log.

     In NDB 7.6.4 and later, you can also use 'NODELOG DEBUG ON'
     (https://dev.mysql.com/doc/refman/5.7/en/mysql-cluster-mgm-client-commands.html#ndbclient-nodelog-debug)
     and 'NODELOG DEBUG OFF'
     (https://dev.mysql.com/doc/refman/5.7/en/mysql-cluster-mgm-client-commands.html#ndbclient-nodelog-debug)
     to enable and disable this extra logging while the data node is
     running.

*note 'ndbd': mysql-cluster-programs-ndbd. generates a set of log files
which are placed in the directory specified by 'DataDir' in the
'config.ini' configuration file.

These log files are listed below.  NODE_ID is the node's unique
identifier.  Note that NODE_ID represents the node's unique identifier.
For example, 'ndb_2_error.log' is the error log generated by the data
node whose node ID is '2'.

   * 
     'ndb_NODE_ID_error.log' is a file containing records of all crashes
     which the referenced *note 'ndbd': mysql-cluster-programs-ndbd.
     process has encountered.  Each record in this file contains a brief
     error string and a reference to a trace file for this crash.  A
     typical entry in this file might appear as shown here:

          Date/Time: Saturday 30 July 2004 - 00:20:01
          Type of error: error
          Message: Internal program error (failed ndbrequire)
          Fault ID: 2341
          Problem data: DbtupFixAlloc.cpp
          Object of reference: DBTUP (Line: 173)
          ProgramName: NDB Kernel
          ProcessID: 14909
          TraceFile: ndb_2_trace.log.2
          ***EOM***

     Listings of possible *note 'ndbd': mysql-cluster-programs-ndbd.
     exit codes and messages generated when a data node process shuts
     down prematurely can be found in Data Node Error Messages
     (https://dev.mysql.com/doc/ndb-internals/en/ndb-node-error-messages.html).

     *Important*:

     _The last entry in the error log file is not necessarily the newest
     one_ (nor is it likely to be).  Entries in the error log are _not_
     listed in chronological order; rather, they correspond to the order
     of the trace files as determined in the
     'ndb_NODE_ID_trace.log.next' file (see below).  Error log entries
     are thus overwritten in a cyclical and not sequential fashion.

   * 
     'ndb_NODE_ID_trace.log.TRACE_ID' is a trace file describing exactly
     what happened just before the error occurred.  This information is
     useful for analysis by the NDB Cluster development team.

     It is possible to configure the number of these trace files that
     will be created before old files are overwritten.  TRACE_ID is a
     number which is incremented for each successive trace file.

   * 'ndb_NODE_ID_trace.log.next' is the file that keeps track of the
     next trace file number to be assigned.

   * 'ndb_NODE_ID_out.log' is a file containing any data output by the
     *note 'ndbd': mysql-cluster-programs-ndbd. process.  This file is
     created only if *note 'ndbd': mysql-cluster-programs-ndbd. is
     started as a daemon, which is the default behavior.

   * 'ndb_NODE_ID.pid' is a file containing the process ID of the *note
     'ndbd': mysql-cluster-programs-ndbd. process when started as a
     daemon.  It also functions as a lock file to avoid the starting of
     nodes with the same identifier.

   * 'ndb_NODE_ID_signal.log' is a file used only in debug versions of
     *note 'ndbd': mysql-cluster-programs-ndbd, where it is possible to
     trace all incoming, outgoing, and internal messages with their data
     in the *note 'ndbd': mysql-cluster-programs-ndbd. process.

It is recommended not to use a directory mounted through NFS because in
some environments this can cause problems whereby the lock on the '.pid'
file remains in effect even after the process has terminated.

To start *note 'ndbd': mysql-cluster-programs-ndbd, it may also be
necessary to specify the host name of the management server and the port
on which it is listening.  Optionally, one may also specify the node ID
that the process is to use.

     shell> ndbd --connect-string="nodeid=2;host=ndb_mgmd.mysql.com:1186"

See *note mysql-cluster-connection-strings::, for additional information
about this issue.  *note mysql-cluster-program-options-common::,
describes other command-line options which can be used with *note
'ndbd': mysql-cluster-programs-ndbd.  For information about data node
configuration parameters, see *note mysql-cluster-ndbd-definition::.

When *note 'ndbd': mysql-cluster-programs-ndbd. starts, it actually
initiates two processes.  The first of these is called the 'angel
process'; its only job is to discover when the execution process has
been completed, and then to restart the *note 'ndbd':
mysql-cluster-programs-ndbd. process if it is configured to do so.
Thus, if you attempt to kill *note 'ndbd': mysql-cluster-programs-ndbd.
using the Unix *note 'kill': kill. command, it is necessary to kill both
processes, beginning with the angel process.  The preferred method of
terminating an *note 'ndbd': mysql-cluster-programs-ndbd. process is to
use the management client and stop the process from there.

The execution process uses one thread for reading, writing, and scanning
data, as well as all other activities.  This thread is implemented
asynchronously so that it can easily handle thousands of concurrent
actions.  In addition, a watch-dog thread supervises the execution
thread to make sure that it does not hang in an endless loop.  A pool of
threads handles file I/O, with each thread able to handle one open file.
Threads can also be used for transporter connections by the transporters
in the *note 'ndbd': mysql-cluster-programs-ndbd. process.  In a
multi-processor system performing a large number of operations
(including updates), the *note 'ndbd': mysql-cluster-programs-ndbd.
process can consume up to 2 CPUs if permitted to do so.

For a machine with many CPUs it is possible to use several *note 'ndbd':
mysql-cluster-programs-ndbd. processes which belong to different node
groups; however, such a configuration is still considered experimental
and is not supported for MySQL 5.5 in a production setting.  See *note
mysql-cluster-limitations::.


File: manual.info.tmp,  Node: mysql-cluster-programs-ndbinfo-select-all,  Next: mysql-cluster-programs-ndbmtd,  Prev: mysql-cluster-programs-ndbd,  Up: mysql-cluster-programs

18.4.2 'ndbinfo_select_all' -- Select From ndbinfo Tables
---------------------------------------------------------

*note 'ndbinfo_select_all': mysql-cluster-programs-ndbinfo-select-all.
is a client program that selects all rows and columns from one or more
tables in the *note 'ndbinfo': mysql-cluster-ndbinfo. database.  It is
included with the NDB Cluster distribution beginning with NDB 7.2.2.

Not all 'ndbinfo' tables available in the *note 'mysql': mysql. client
can be read by this program.  In addition, *note 'ndbinfo_select_all':
mysql-cluster-programs-ndbinfo-select-all. can show information about
some tables internal to 'ndbinfo' which cannot be accessed using SQL,
including the 'tables' and 'columns' metadata tables.

To select from one or more 'ndbinfo' tables using *note
'ndbinfo_select_all': mysql-cluster-programs-ndbinfo-select-all, it is
necessary to supply the names of the tables when invoking the program as
shown here:

     shell> ndbinfo_select_all TABLE_NAME1  [TABLE_NAME2] [...]

For example:

     shell> ndbinfo_select_all logbuffers logspaces
     == logbuffers ==
     node_id log_type        log_id  log_part        total   used    high
     5       0       0       0       33554432        262144  0
     6       0       0       0       33554432        262144  0
     7       0       0       0       33554432        262144  0
     8       0       0       0       33554432        262144  0
     == logspaces ==
     node_id log_type        log_id  log_part        total   used    high
     5       0       0       0       268435456       0       0
     5       0       0       1       268435456       0       0
     5       0       0       2       268435456       0       0
     5       0       0       3       268435456       0       0
     6       0       0       0       268435456       0       0
     6       0       0       1       268435456       0       0
     6       0       0       2       268435456       0       0
     6       0       0       3       268435456       0       0
     7       0       0       0       268435456       0       0
     7       0       0       1       268435456       0       0
     7       0       0       2       268435456       0       0
     7       0       0       3       268435456       0       0
     8       0       0       0       268435456       0       0
     8       0       0       1       268435456       0       0
     8       0       0       2       268435456       0       0
     8       0       0       3       268435456       0       0
     shell>

The following table includes options that are specific to *note
'ndbinfo_select_all': mysql-cluster-programs-ndbinfo-select-all.
Additional descriptions follow the table.  For options common to most
NDB Cluster programs (including *note 'nd