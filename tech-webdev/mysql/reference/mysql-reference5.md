ny of those lower levels.

Details of the privilege-checking procedure are presented in *note
request-access::.

If you are using table, column, or routine privileges for even one user,
the server examines table, column, and routine privileges for all users
and this slows down MySQL a bit.  Similarly, if you limit the number of
queries, updates, or connections for any users, the server must monitor
these values.

MySQL enables you to grant privileges on databases or tables that do not
exist.  For tables, the privileges to be granted must include the
'CREATE' privilege.  _This behavior is by design_, and is intended to
enable the database administrator to prepare user accounts and
privileges for databases or tables that are to be created at a later
time.

*Important*:

_MySQL does not automatically revoke any privileges when you drop a
database or table_.  However, if you drop a routine, any routine-level
privileges granted for that routine are revoked.

*Global Privileges*

Global privileges are administrative or apply to all databases on a
given server.  To assign global privileges, use 'ON *.*' syntax:

     GRANT ALL ON *.* TO 'someuser'@'somehost';
     GRANT SELECT, INSERT ON *.* TO 'someuser'@'somehost';

The 'CREATE TABLESPACE', 'CREATE USER', 'FILE', 'PROCESS', 'RELOAD',
'REPLICATION CLIENT', 'REPLICATION SLAVE', 'SHOW DATABASES', 'SHUTDOWN',
and 'SUPER' privileges are administrative and can only be granted
globally.

Other privileges can be granted globally or at more specific levels.

'GRANT OPTION' granted at the global level for any global privilege
applies to all global privileges.

MySQL stores global privileges in the 'mysql.user' system table.

*Database Privileges*

Database privileges apply to all objects in a given database.  To assign
database-level privileges, use 'ON DB_NAME.*' syntax:

     GRANT ALL ON mydb.* TO 'someuser'@'somehost';
     GRANT SELECT, INSERT ON mydb.* TO 'someuser'@'somehost';

If you use 'ON *' syntax (rather than 'ON *.*'), privileges are assigned
at the database level for the default database.  An error occurs if
there is no default database.

The 'CREATE', 'DROP', 'EVENT', 'GRANT OPTION', 'LOCK TABLES', and
'REFERENCES' privileges can be specified at the database level.  Table
or routine privileges also can be specified at the database level, in
which case they apply to all tables or routines in the database.

MySQL stores database privileges in the 'mysql.db' system table.

*Table Privileges*

Table privileges apply to all columns in a given table.  To assign
table-level privileges, use 'ON DB_NAME.TBL_NAME' syntax:

     GRANT ALL ON mydb.mytbl TO 'someuser'@'somehost';
     GRANT SELECT, INSERT ON mydb.mytbl TO 'someuser'@'somehost';

If you specify TBL_NAME rather than DB_NAME.TBL_NAME, the statement
applies to TBL_NAME in the default database.  An error occurs if there
is no default database.

The permissible PRIV_TYPE values at the table level are 'ALTER', 'CREATE
VIEW', 'CREATE', 'DELETE', 'DROP', 'GRANT OPTION', 'INDEX', 'INSERT',
'REFERENCES', 'SELECT', 'SHOW VIEW', 'TRIGGER', and 'UPDATE'.

MySQL stores table privileges in the 'mysql.tables_priv' system table.

*Column Privileges*

Column privileges apply to single columns in a given table.  Each
privilege to be granted at the column level must be followed by the
column or columns, enclosed within parentheses.

     GRANT SELECT (col1), INSERT (col1, col2) ON mydb.mytbl TO 'someuser'@'somehost';

The permissible PRIV_TYPE values for a column (that is, when you use a
COLUMN_LIST clause) are 'INSERT', 'REFERENCES', 'SELECT', and 'UPDATE'.

MySQL stores column privileges in the 'mysql.columns_priv' system table.

*Stored Routine Privileges*

The 'ALTER ROUTINE', 'CREATE ROUTINE', 'EXECUTE', and 'GRANT OPTION'
privileges apply to stored routines (procedures and functions).  They
can be granted at the global and database levels.  Except for 'CREATE
ROUTINE', these privileges can be granted at the routine level for
individual routines.

     GRANT CREATE ROUTINE ON mydb.* TO 'someuser'@'somehost';
     GRANT EXECUTE ON PROCEDURE mydb.myproc TO 'someuser'@'somehost';

The permissible PRIV_TYPE values at the routine level are 'ALTER
ROUTINE', 'EXECUTE', and 'GRANT OPTION'.  'CREATE ROUTINE' is not a
routine-level privilege because you must have the privilege at the
global or database level to create a routine in the first place.

MySQL stores routine-level privileges in the 'mysql.procs_priv' system
table.

*Proxy User Privileges*

The 'PROXY' privilege enables one user to be a proxy for another.  The
proxy user impersonates or takes the identity of the proxied user; that
is, it assumes the privileges of the proxied user.

     GRANT PROXY ON 'localuser'@'localhost' TO 'externaluser'@'somehost';

When 'PROXY' is granted, it must be the only privilege named in the
*note 'GRANT': grant. statement, the 'REQUIRE' clause cannot be given,
and the only permitted 'WITH' option is 'WITH GRANT OPTION'.

Proxying requires that the proxy user authenticate through a plugin that
returns the name of the proxied user to the server when the proxy user
connects, and that the proxy user have the 'PROXY' privilege for the
proxied user.  For details and examples, see *note proxy-users::.

MySQL stores proxy privileges in the 'mysql.proxies_priv' system table.

*Implicit Account Creation*

If an account named in a *note 'GRANT': grant. statement does not exist,
the action taken depends on the 'NO_AUTO_CREATE_USER' SQL mode:

   * If 'NO_AUTO_CREATE_USER' is not enabled, *note 'GRANT': grant.
     creates the account.  _This is very insecure_ unless you specify a
     nonempty password using 'IDENTIFIED BY'.

   * If 'NO_AUTO_CREATE_USER' is enabled, *note 'GRANT': grant. fails
     and does not create the account, unless you specify a nonempty
     password using 'IDENTIFIED BY' or name an authentication plugin
     using 'IDENTIFIED WITH'.

*Other Account Characteristics*

MySQL can check X.509 certificate attributes in addition to the usual
authentication that is based on the user name and credentials.  For
background information on the use of SSL with MySQL, see *note
encrypted-connections::.

The optional 'REQUIRE' clause specifies SSL-related options for a MySQL
account, using one or more TLS_OPTION values.

Order of 'REQUIRE' options does not matter, but no option can be
specified twice.  The 'AND' keyword is optional between 'REQUIRE'
options.

*note 'GRANT': grant. permits these TLS_OPTION values:

   * 'NONE'

     Indicates that the account has no SSL or X.509 requirements.

          GRANT ALL PRIVILEGES ON test.* TO 'root'@'localhost'
            REQUIRE NONE;

     Unencrypted connections are permitted if the user name and password
     are valid.  Encrypted connections can be used, at the client's
     option, if the client has the proper certificate and key files.
     That is, the client need not specify any SSL command options, in
     which case the connection will be unencrypted.  To use an encrypted
     connection, the client must specify either the '--ssl-ca' option,
     or all three of the '--ssl-ca', '--ssl-key', and '--ssl-cert'
     options.

     'NONE' is the default if no SSL-related 'REQUIRE' options are
     specified.

   * 'SSL'

     Tells the server to permit only encrypted connections for the
     account.

          GRANT ALL PRIVILEGES ON test.* TO 'root'@'localhost'
            REQUIRE SSL;

     To connect, the client must specify the '--ssl-ca' option to
     authenticate the server certificate, and may additionally specify
     the '--ssl-key' and '--ssl-cert' options.  If neither the
     '--ssl-ca' option nor '--ssl-capath' option is specified, the
     client does not authenticate the server certificate.

   * 'X509'

     Requires that clients present a valid certificate, but the exact
     certificate, issuer, and subject do not matter.  The only
     requirement is that it should be possible to verify its signature
     with one of the CA certificates.  Use of X.509 certificates always
     implies encryption, so the 'SSL' option is unnecessary in this
     case.

          GRANT ALL PRIVILEGES ON test.* TO 'root'@'localhost'
            REQUIRE X509;

     For accounts with 'REQUIRE X509', clients must specify the
     '--ssl-key' and '--ssl-cert' options to connect.  (It is
     recommended but not required that '--ssl-ca' also be specified so
     that the public certificate provided by the server can be
     verified.)  This is true for 'ISSUER' and 'SUBJECT' as well because
     those 'REQUIRE' options imply the requirements of 'X509'.

   * 'ISSUER 'ISSUER''

     Requires that clients present a valid X.509 certificate issued by
     CA ''ISSUER''.  If a client presents a certificate that is valid
     but has a different issuer, the server rejects the connection.  Use
     of X.509 certificates always implies encryption, so the 'SSL'
     option is unnecessary in this case.

          GRANT ALL PRIVILEGES ON test.* TO 'root'@'localhost'
            REQUIRE ISSUER '/C=SE/ST=Stockholm/L=Stockholm/
              O=MySQL/CN=CA/emailAddress=ca@example.com';

     Because 'ISSUER' implies the requirements of 'X509', clients must
     specify the '--ssl-key' and '--ssl-cert' options to connect.  (It
     is recommended but not required that '--ssl-ca' also be specified
     so that the public certificate provided by the server can be
     verified.)

     *Note*:

     If MySQL is linked against a version of OpenSSL older than 0.9.6h,
     use 'Email' rather than 'emailAddress' in the ''ISSUER'' value.

   * 'SUBJECT 'SUBJECT''

     Requires that clients present a valid X.509 certificate containing
     the subject SUBJECT.  If a client presents a certificate that is
     valid but has a different subject, the server rejects the
     connection.  Use of X.509 certificates always implies encryption,
     so the 'SSL' option is unnecessary in this case.

          GRANT ALL PRIVILEGES ON test.* TO 'root'@'localhost'
            REQUIRE SUBJECT '/C=SE/ST=Stockholm/L=Stockholm/
              O=MySQL demo client certificate/
              CN=client/emailAddress=client@example.com';

     MySQL does a simple string comparison of the ''SUBJECT'' value to
     the value in the certificate, so lettercase and component ordering
     must be given exactly as present in the certificate.

     *Note*:

     Regarding 'emailAddress', see the note in the description of
     'REQUIRE ISSUER'.

     Because 'SUBJECT' implies the requirements of 'X509', clients must
     specify the '--ssl-key' and '--ssl-cert' options to connect.  (It
     is recommended but not required that '--ssl-ca' also be specified
     so that the public certificate provided by the server can be
     verified.)

   * 'CIPHER 'CIPHER''

     Requires a specific cipher method for encrypting connections.  This
     option is needed to ensure that ciphers and key lengths of
     sufficient strength are used.  SSL itself can be weak if old
     algorithms using short encryption keys are used.

          GRANT ALL PRIVILEGES ON test.* TO 'root'@'localhost'
            REQUIRE CIPHER 'EDH-RSA-DES-CBC3-SHA';

The 'SUBJECT', 'ISSUER', and 'CIPHER' options can be combined in the
'REQUIRE' clause like this:

     GRANT ALL PRIVILEGES ON test.* TO 'root'@'localhost'
       REQUIRE SUBJECT '/C=SE/ST=Stockholm/L=Stockholm/
         O=MySQL demo client certificate/
         CN=client/emailAddress=client@example.com'
       AND ISSUER '/C=SE/ST=Stockholm/L=Stockholm/
         O=MySQL/CN=CA/emailAddress=ca@example.com'
       AND CIPHER 'EDH-RSA-DES-CBC3-SHA';

The optional 'WITH' clause is used for these purposes:

   * To enable a user to grant privileges to other users

   * To specify resource limits for a user

The 'WITH GRANT OPTION' clause gives the user the ability to give to
other users any privileges the user has at the specified privilege
level.

To grant the 'GRANT OPTION' privilege to an account without otherwise
changing its privileges, do this:

     GRANT USAGE ON *.* TO 'someuser'@'somehost' WITH GRANT OPTION;

Be careful to whom you give the 'GRANT OPTION' privilege because two
users with different privileges may be able to combine privileges!

You cannot grant another user a privilege which you yourself do not
have; the 'GRANT OPTION' privilege enables you to assign only those
privileges which you yourself possess.

Be aware that when you grant a user the 'GRANT OPTION' privilege at a
particular privilege level, any privileges the user possesses (or may be
given in the future) at that level can also be granted by that user to
other users.  Suppose that you grant a user the 'INSERT' privilege on a
database.  If you then grant the 'SELECT' privilege on the database and
specify 'WITH GRANT OPTION', that user can give to other users not only
the 'SELECT' privilege, but also 'INSERT'.  If you then grant the
'UPDATE' privilege to the user on the database, the user can grant
'INSERT', 'SELECT', and 'UPDATE'.

For a nonadministrative user, you should not grant the 'ALTER' privilege
globally or for the 'mysql' system database.  If you do that, the user
can try to subvert the privilege system by renaming tables!

For additional information about security risks associated with
particular privileges, see *note privileges-provided::.

It is possible to place limits on use of server resources by an account,
as discussed in *note user-resources::.  To do so, use a 'WITH' clause
that specifies one or more RESOURCE_OPTION values.  Limits not specified
retain their current values.

Order of 'WITH' options does not matter, except that if a given resource
limit is specified multiple times, the last instance takes precedence.

*note 'GRANT': grant. permits these RESOURCE_OPTION values:

   * 'MAX_QUERIES_PER_HOUR COUNT', 'MAX_UPDATES_PER_HOUR COUNT',
     'MAX_CONNECTIONS_PER_HOUR COUNT'

     These options restrict how many queries, updates, and connections
     to the server are permitted to this account during any given
     one-hour period.  (Queries for which results are served from the
     query cache do not count against the 'MAX_QUERIES_PER_HOUR' limit.)
     If COUNT is '0' (the default), this means that there is no
     limitation for the account.

   * 'MAX_USER_CONNECTIONS COUNT'

     Restricts the maximum number of simultaneous connections to the
     server by the account.  A nonzero COUNT specifies the limit for the
     account explicitly.  If COUNT is '0' (the default), the server
     determines the number of simultaneous connections for the account
     from the global value of the 'max_user_connections' system
     variable.  If 'max_user_connections' is also zero, there is no
     limit for the account.

To specify resource limits for an existing user without affecting
existing privileges, use *note 'GRANT USAGE': grant. at the global level
('ON *.*') and name the limits to be changed.  For example:

     GRANT USAGE ON *.* TO ...
       WITH MAX_QUERIES_PER_HOUR 500 MAX_UPDATES_PER_HOUR 100;

*MySQL and Standard SQL Versions of GRANT*

The biggest differences between the MySQL and standard SQL versions of
*note 'GRANT': grant. are:

   * MySQL associates privileges with the combination of a host name and
     user name and not with only a user name.

   * Standard SQL does not have global or database-level privileges, nor
     does it support all the privilege types that MySQL supports.

   * MySQL does not support the standard SQL 'UNDER' privilege.

   * Standard SQL privileges are structured in a hierarchical manner.
     If you remove a user, all privileges the user has been granted are
     revoked.  This is also true in MySQL if you use *note 'DROP USER':
     drop-user.  See *note drop-user::.

   * In standard SQL, when you drop a table, all privileges for the
     table are revoked.  In standard SQL, when you revoke a privilege,
     all privileges that were granted based on that privilege are also
     revoked.  In MySQL, privileges can be dropped only with explicit
     *note 'DROP USER': drop-user. or *note 'REVOKE': revoke. statements
     or by manipulating the MySQL grant tables directly.

   * In MySQL, it is possible to have the 'INSERT' privilege for only
     some of the columns in a table.  In this case, you can still
     execute *note 'INSERT': insert. statements on the table, provided
     that you insert values only for those columns for which you have
     the 'INSERT' privilege.  The omitted columns are set to their
     implicit default values if strict SQL mode is not enabled.  In
     strict mode, the statement is rejected if any of the omitted
     columns have no default value.  (Standard SQL requires you to have
     the 'INSERT' privilege on all columns.)  For information about
     strict SQL mode and implicit default values, see *note sql-mode::,
     and *note data-type-defaults::.


File: manual.info.tmp,  Node: rename-user,  Next: revoke,  Prev: grant,  Up: account-management-statements

13.7.1.4 RENAME USER Statement
..............................

     RENAME USER OLD_USER TO NEW_USER
         [, OLD_USER TO NEW_USER] ...

The *note 'RENAME USER': rename-user. statement renames existing MySQL
accounts.  An error occurs for old accounts that do not exist or new
accounts that already exist.

To use *note 'RENAME USER': rename-user, you must have the global
'CREATE USER' privilege, or the 'UPDATE' privilege for the 'mysql'
system database.  When the 'read_only' system variable is enabled, *note
'RENAME USER': rename-user. additionally requires the 'SUPER' privilege.

Each account name uses the format described in *note account-names::.
For example:

     RENAME USER 'jeffrey'@'localhost' TO 'jeff'@'127.0.0.1';

The host name part of the account name, if omitted, defaults to ''%''.

*note 'RENAME USER': rename-user. causes the privileges held by the old
user to be those held by the new user.  However, *note 'RENAME USER':
rename-user. does not automatically drop or invalidate databases or
objects within them that the old user created.  This includes stored
programs or views for which the 'DEFINER' attribute names the old user.
Attempts to access such objects may produce an error if they execute in
definer security context.  (For information about security context, see
*note stored-objects-security::.)

The privilege changes take effect as indicated in *note
privilege-changes::.


File: manual.info.tmp,  Node: revoke,  Next: set-password,  Prev: rename-user,  Up: account-management-statements

13.7.1.5 REVOKE Statement
.........................

     REVOKE
         PRIV_TYPE [(COLUMN_LIST)]
           [, PRIV_TYPE [(COLUMN_LIST)]] ...
         ON [OBJECT_TYPE] PRIV_LEVEL
         FROM USER [, USER] ...

     REVOKE ALL [PRIVILEGES], GRANT OPTION
         FROM USER [, USER] ...

     REVOKE PROXY ON USER
         FROM USER [, USER] ...

The *note 'REVOKE': revoke. statement enables system administrators to
revoke privileges from MySQL accounts.

For details on the levels at which privileges exist, the permissible
PRIV_TYPE, PRIV_LEVEL, and OBJECT_TYPE values, and the syntax for
specifying users and passwords, see *note grant::.

When the 'read_only' system variable is enabled, *note 'REVOKE': revoke.
requires the 'SUPER' privilege in addition to any other required
privileges described in the following discussion.

Each account name uses the format described in *note account-names::.
For example:

     REVOKE INSERT ON *.* FROM 'jeffrey'@'localhost';

The host name part of the account name, if omitted, defaults to ''%''.

To use the first *note 'REVOKE': revoke. syntax, you must have the
'GRANT OPTION' privilege, and you must have the privileges that you are
revoking.

To revoke all privileges, use the second syntax, which drops all global,
database, table, column, and routine privileges for the named user or
users:

     REVOKE ALL PRIVILEGES, GRANT OPTION FROM USER [, USER] ...

To use this *note 'REVOKE': revoke. syntax, you must have the global
'CREATE USER' privilege, or the 'UPDATE' privilege for the 'mysql'
system database.

User accounts from which privileges are to be revoked must exist, but
the privileges to be revoked need not be currently granted to them.

*note 'REVOKE': revoke. removes privileges, but does not remove rows
from the 'mysql.user' system table.  To remove a user account entirely,
use *note 'DROP USER': drop-user.  See *note drop-user::.

If the grant tables hold privilege rows that contain mixed-case database
or table names and the 'lower_case_table_names' system variable is set
to a nonzero value, *note 'REVOKE': revoke. cannot be used to revoke
these privileges.  It will be necessary to manipulate the grant tables
directly.  (*note 'GRANT': grant. will not create such rows when
'lower_case_table_names' is set, but such rows might have been created
prior to setting the variable.)

When successfully executed from the *note 'mysql': mysql. program, *note
'REVOKE': revoke. responds with 'Query OK, 0 rows affected'.  To
determine what privileges remain after the operation, use *note 'SHOW
GRANTS': show-grants.  See *note show-grants::.


File: manual.info.tmp,  Node: set-password,  Prev: revoke,  Up: account-management-statements

13.7.1.6 SET PASSWORD Statement
...............................

     SET PASSWORD [FOR USER] = PASSWORD_OPTION

     PASSWORD_OPTION: {
         PASSWORD('AUTH_STRING')
       | OLD_PASSWORD('AUTH_STRING')
       | 'HASH_STRING'
     }

The *note 'SET PASSWORD': set-password. statement assigns a password to
a MySQL user account, specified as either a cleartext (unencrypted) or
encrypted value:

   * ''AUTH_STRING'' represents a cleartext password.

   * ''HASH_STRING'' represents an encrypted password.

*Important*:

*note 'SET PASSWORD': set-password. may be recorded in server logs or on
the client side in a history file such as '~/.mysql_history', which
means that cleartext passwords may be read by anyone having read access
to that information.  For information about password logging in the
server logs, see *note password-logging::.  For similar information
about client-side logging, see *note mysql-logging::.

*note 'SET PASSWORD': set-password. can be used with or without a 'FOR'
clause that explicitly names a user account:

   * With a 'FOR USER' clause, the statement sets the password for the
     named account, which must exist:

          SET PASSWORD FOR 'jeffrey'@'localhost' = PASSWORD_OPTION;

   * With no 'FOR USER' clause, the statement sets the password for the
     current user:

          SET PASSWORD = PASSWORD_OPTION;

     Any client who connects to the server using a nonanonymous account
     can change the password for that account.  (In particular, you can
     change your own password.)  To see which account the server
     authenticated you as, invoke the 'CURRENT_USER()' function:

          SELECT CURRENT_USER();

If a 'FOR USER' clause is given, the account name uses the format
described in *note account-names::.  For example:

     SET PASSWORD FOR 'bob'@'%.example.org' = PASSWORD('AUTH_STRING');

The host name part of the account name, if omitted, defaults to ''%''.

Setting the password for a named account (with a 'FOR' clause) requires
the *note 'UPDATE': update. privilege for the 'mysql' system database.
Setting the password for yourself (for a nonanonymous account with no
'FOR' clause) requires no special privileges.  When the 'read_only'
system variable is enabled, *note 'SET PASSWORD': set-password. requires
the 'SUPER' privilege in addition to any other required privileges.

The password can be specified in these ways:

   * Use the 'PASSWORD()' function

     The 'PASSWORD()' argument is the cleartext (unencrypted) password.
     'PASSWORD()' hashes the password and returns the encrypted password
     string for storage in the account row in the 'mysql.user' system
     table.

     The 'PASSWORD()' function hashes the password using the hashing
     method determined by the value of the 'old_passwords' system
     variable value.  It should be set to a value compatible with the
     hash format required by the account authentication plugin.  For
     example, if the account uses the 'mysql_native_password'
     authentication plugin, 'old_passwords' should be 0 for 'PASSWORD()'
     to produce a hash value in the correct format.  For
     'mysql_old_password', 'old_passwords' should be 1.

     Permitted 'old_passwords' values are described later in this
     section.

   * Use the 'OLD_PASSWORD()' function:

     The ''AUTH_STRING'' function argument is the cleartext
     (unencrypted) password.  'OLD_PASSWORD()' hashes the password using
     pre-4.1 hashing and returns the encrypted password string for
     storage in the account row in the 'mysql.user' system table.  This
     hashing method is appropriate only for accounts that use the
     'mysql_old_password' authentication plugin.

   * Use an already encrypted password string

     The password is specified as a string literal.  It must represent
     the already encrypted password value, in the hash format required
     by the authentication method used for the account.

The following table shows, for each password hashing method, the
permitted value of 'old_passwords' and which authentication plugins use
the hashing method.

Password Hashing Method       old_passwords  Associated Authentication
                              Value          Plugin
                                             
MySQL 4.1 native hashing      '0' or 'OFF'   'mysql_native_password'
                                             
Pre-4.1 ('old') hashing       '1' or 'ON'    'mysql_old_password'
                              

*Caution*:

If you are connecting to a MySQL 4.1 or later server using a pre-4.1
client program, do not change your password without first reading *note
password-hashing::.  The default password hashing format changed in
MySQL 4.1, and if you change your password, it might be stored using a
hashing format that pre-4.1 clients cannot generate, thus preventing you
from connecting to the server afterward.

For additional information about setting passwords and authentication
plugins, see *note assigning-passwords::, and *note
pluggable-authentication::.


File: manual.info.tmp,  Node: table-maintenance-statements,  Next: component-statements,  Prev: account-management-statements,  Up: sql-server-administration-statements

13.7.2 Table Maintenance Statements
-----------------------------------

* Menu:

* analyze-table::                ANALYZE TABLE Statement
* check-table::                  CHECK TABLE Statement
* checksum-table::               CHECKSUM TABLE Statement
* optimize-table::               OPTIMIZE TABLE Statement
* repair-table::                 REPAIR TABLE Statement


File: manual.info.tmp,  Node: analyze-table,  Next: check-table,  Prev: table-maintenance-statements,  Up: table-maintenance-statements

13.7.2.1 ANALYZE TABLE Statement
................................

     ANALYZE [NO_WRITE_TO_BINLOG | LOCAL]
         TABLE TBL_NAME [, TBL_NAME] ...

*note 'ANALYZE TABLE': analyze-table. performs a key distribution
analysis and stores the distribution for the named table or tables.  For
'MyISAM' tables, this statement is equivalent to using *note 'myisamchk
--analyze': myisamchk.

This statement requires 'SELECT' and 'INSERT' privileges for the table.

*note 'ANALYZE TABLE': analyze-table. works with 'InnoDB', 'NDB', and
'MyISAM' tables.  It does not work with views.

*note 'ANALYZE TABLE': analyze-table. is supported for partitioned
tables, and you can use 'ALTER TABLE ... ANALYZE PARTITION' to analyze
one or more partitions; for more information, see *note alter-table::,
and *note partitioning-maintenance::.

During the analysis, the table is locked with a read lock for 'InnoDB'
and 'MyISAM'.

*note 'ANALYZE TABLE': analyze-table. removes the table from the table
definition cache, which requires a flush lock.  If there are long
running statements or transactions still using the table, subsequent
statements and transactions must wait for those operations to finish
before the flush lock is released.  Because *note 'ANALYZE TABLE':
analyze-table. itself typically finishes quickly, it may not be apparent
that delayed transactions or statements involving the same table are due
to the remaining flush lock.

By default, the server writes *note 'ANALYZE TABLE': analyze-table.
statements to the binary log so that they replicate to replication
slaves.  To suppress logging, specify the optional 'NO_WRITE_TO_BINLOG'
keyword or its alias 'LOCAL'.

   * *note analyze-table-output::

   * *note analyze-table-key-distribution-analysis::

*ANALYZE TABLE Output*

*note 'ANALYZE TABLE': analyze-table. returns a result set with the
columns shown in the following table.

Column      Value
            
'Table'     The table name
            
'Op'        Always 'analyze'
            
'Msg_type'  'status', 'error', 'info', 'note', or
            'warning'
            
'Msg_text'  An informational message

*Key Distribution Analysis*

If the table has not changed since the last key distribution analysis,
the table is not analyzed again.

MySQL uses the stored key distribution to decide the table join order
for joins on something other than a constant.  In addition, key
distributions can be used when deciding which indexes to use for a
specific table within a query.

To check the stored key distribution cardinality, use the *note 'SHOW
INDEX': show-index. statement or the 'INFORMATION_SCHEMA' *note
'STATISTICS': statistics-table. table.  See *note show-index::, and
*note statistics-table::.

For 'InnoDB' tables, *note 'ANALYZE TABLE': analyze-table. determines
index cardinality by performing random dives on each of the index trees
and updating index cardinality estimates accordingly.  Because these are
only estimates, repeated runs of *note 'ANALYZE TABLE': analyze-table.
could produce different numbers.  This makes *note 'ANALYZE TABLE':
analyze-table. fast on 'InnoDB' tables but not 100% accurate because it
does not take all rows into account.  You can adjust the number of
random dives by modifying the 'innodb_stats_sample_pages' system
variable.

For more information about key distribution in 'InnoDB', see *note
innodb-statistics-estimation::, and *note
innodb-analyze-table-complexity::.

MySQL uses index cardinality estimates in join optimization.  If a join
is not optimized in the right way, try running *note 'ANALYZE TABLE':
analyze-table.  In the few cases that *note 'ANALYZE TABLE':
analyze-table. does not produce values good enough for your particular
tables, you can use 'FORCE INDEX' with your queries to force the use of
a particular index, or set the 'max_seeks_for_key' system variable to
ensure that MySQL prefers index lookups over table scans.  See *note
optimizer-issues::.


File: manual.info.tmp,  Node: check-table,  Next: checksum-table,  Prev: analyze-table,  Up: table-maintenance-statements

13.7.2.2 CHECK TABLE Statement
..............................

     CHECK TABLE TBL_NAME [, TBL_NAME] ... [OPTION] ...

     OPTION: {
         FOR UPGRADE
       | QUICK
       | FAST
       | MEDIUM
       | EXTENDED
       | CHANGED
     }

*note 'CHECK TABLE': check-table. checks a table or tables for errors.
For 'MyISAM' tables, the key statistics are updated as well.  *note
'CHECK TABLE': check-table. can also check views for problems, such as
tables that are referenced in the view definition that no longer exist.

To check a table, you must have some privilege for it.

*note 'CHECK TABLE': check-table. works for *note 'InnoDB':
innodb-storage-engine, *note 'MyISAM': myisam-storage-engine, *note
'ARCHIVE': archive-storage-engine, and *note 'CSV': csv-storage-engine.
tables.

Before running *note 'CHECK TABLE': check-table. on 'InnoDB' tables, see
*note check-table-innodb::.

*note 'CHECK TABLE': check-table. is supported for partitioned tables,
and you can use 'ALTER TABLE ... CHECK PARTITION' to check one or more
partitions; for more information, see *note alter-table::, and *note
partitioning-maintenance::.

   * *note check-table-output::

   * *note check-table-version-compatibility::

   * *note check-table-data-consistency::

   * *note check-table-innodb::

*CHECK TABLE Output*

*note 'CHECK TABLE': check-table. returns a result set with the columns
shown in the following table.

Column      Value
            
'Table'     The table name
            
'Op'        Always 'check'
            
'Msg_type'  'status', 'error', 'info', 'note', or
            'warning'
            
'Msg_text'  An informational message

The statement might produce many rows of information for each checked
table.  The last row has a 'Msg_type' value of 'status' and the
'Msg_text' normally should be 'OK'.  For a 'MyISAM' table, if you don't
get 'OK' or 'Table is already up to date', you should normally run a
repair of the table.  See *note myisam-table-maintenance::.  'Table is
already up to date' means that the storage engine for the table
indicated that there was no need to check the table.

*Checking Version Compatibility*

The 'FOR UPGRADE' option checks whether the named tables are compatible
with the current version of MySQL. With 'FOR UPGRADE', the server checks
each table to determine whether there have been any incompatible changes
in any of the table's data types or indexes since the table was created.
If not, the check succeeds.  Otherwise, if there is a possible
incompatibility, the server runs a full check on the table (which might
take some time).  If the full check succeeds, the server marks the
table's '.frm' file with the current MySQL version number.  Marking the
'.frm' file ensures that further checks for the table with the same
version of the server will be fast.

Incompatibilities might occur because the storage format for a data type
has changed or because its sort order has changed.  Our aim is to avoid
these changes, but occasionally they are necessary to correct problems
that would be worse than an incompatibility between releases.

'FOR UPGRADE' discovers these incompatibilities:

   * The indexing order for end-space in *note 'TEXT': blob. columns for
     'InnoDB' and 'MyISAM' tables changed between MySQL 4.1 and 5.0.

   * The storage method of the new *note 'DECIMAL': fixed-point-types.
     data type changed between MySQL 5.0.3 and 5.0.5.

   * If your table was created by a different version of the MySQL
     server than the one you are currently running, 'FOR UPGRADE'
     indicates that the table has an '.frm' file with an incompatible
     version.  In this case, the result set returned by *note 'CHECK
     TABLE': check-table. contains a line with a 'Msg_type' value of
     'error' and a 'Msg_text' value of 'Table upgrade required. Please
     do "REPAIR TABLE `TBL_NAME`" to fix it!'

   * Changes are sometimes made to character sets or collations that
     require table indexes to be rebuilt.  For details about such
     changes, see *note upgrading-from-previous-series::.  For
     information about rebuilding tables, see *note rebuilding-tables::.

*Checking Data Consistency*

The following table shows the other check options that can be given.
These options are passed to the storage engine, which may use or ignore
them.

Type        Meaning
            
'QUICK'     Do not scan the rows to check for incorrect links.  Applies
            to 'InnoDB' and 'MyISAM' tables and views.
            
'FAST'      Check only tables that have not been closed properly.
            Applies only to 'MyISAM' tables and views; ignored for
            'InnoDB'.
            
'CHANGED'   Check only tables that have been changed since the last
            check or that have not been closed properly.  Applies only
            to 'MyISAM' tables and views; ignored for 'InnoDB'.
            
'MEDIUM'    Scan rows to verify that deleted links are valid.  This
            also calculates a key checksum for the rows and verifies
            this with a calculated checksum for the keys.  Applies only
            to 'MyISAM' tables and views; ignored for 'InnoDB'.
            
'EXTENDED'  Do a full key lookup for all keys for each row.  This
            ensures that the table is 100% consistent, but takes a long
            time.  Applies only to 'MyISAM' tables and views; ignored
            for 'InnoDB'.

If none of the options 'QUICK', 'MEDIUM', or 'EXTENDED' are specified,
the default check type for dynamic-format 'MyISAM' tables is 'MEDIUM'.
This has the same result as running *note 'myisamchk --medium-check
TBL_NAME': myisamchk. on the table.  The default check type also is
'MEDIUM' for static-format 'MyISAM' tables, unless 'CHANGED' or 'FAST'
is specified.  In that case, the default is 'QUICK'.  The row scan is
skipped for 'CHANGED' and 'FAST' because the rows are very seldom
corrupted.

You can combine check options, as in the following example that does a
quick check on the table to determine whether it was closed properly:

     CHECK TABLE test_table FAST QUICK;

*Note*:

If *note 'CHECK TABLE': check-table. finds no problems with a table that
is marked as 'corrupted' or 'not closed properly', *note 'CHECK TABLE':
check-table. may remove the mark.

If a table is corrupted, the problem is most likely in the indexes and
not in the data part.  All of the preceding check types check the
indexes thoroughly and should thus find most errors.

To check a table that you assume is okay, use no check options or the
'QUICK' option.  The latter should be used when you are in a hurry and
can take the very small risk that 'QUICK' does not find an error in the
data file.  (In most cases, under normal usage, MySQL should find any
error in the data file.  If this happens, the table is marked as
'corrupted' and cannot be used until it is repaired.)

'FAST' and 'CHANGED' are mostly intended to be used from a script (for
example, to be executed from 'cron') to check tables periodically.  In
most cases, 'FAST' is to be preferred over 'CHANGED'.  (The only case
when it is not preferred is when you suspect that you have found a bug
in the 'MyISAM' code.)

'EXTENDED' is to be used only after you have run a normal check but
still get errors from a table when MySQL tries to update a row or find a
row by key.  This is very unlikely if a normal check has succeeded.

Use of *note 'CHECK TABLE ... EXTENDED': check-table. might influence
execution plans generated by the query optimizer.

Some problems reported by *note 'CHECK TABLE': check-table. cannot be
corrected automatically:

   * 'Found row where the auto_increment column has the value 0'.

     This means that you have a row in the table where the
     'AUTO_INCREMENT' index column contains the value 0.  (It is
     possible to create a row where the 'AUTO_INCREMENT' column is 0 by
     explicitly setting the column to 0 with an *note 'UPDATE': update.
     statement.)

     This is not an error in itself, but could cause trouble if you
     decide to dump the table and restore it or do an *note 'ALTER
     TABLE': alter-table. on the table.  In this case, the
     'AUTO_INCREMENT' column changes value according to the rules of
     'AUTO_INCREMENT' columns, which could cause problems such as a
     duplicate-key error.

     To get rid of the warning, execute an *note 'UPDATE': update.
     statement to set the column to some value other than 0.

*CHECK TABLE Usage Notes for InnoDB Tables*

The following notes apply to *note 'InnoDB': innodb-storage-engine.
tables:

   * If *note 'CHECK TABLE': check-table. encounters a corrupt page, the
     server exits to prevent error propagation (Bug #10132).  If the
     corruption occurs in a secondary index but table data is readable,
     running *note 'CHECK TABLE': check-table. can still cause a server
     exit.

   * If *note 'CHECK TABLE': check-table. encounters a corrupted
     'DB_TRX_ID' or 'DB_ROLL_PTR' field in a clustered index, *note
     'CHECK TABLE': check-table. can cause 'InnoDB' to access an invalid
     undo log record, resulting in an MVCC-related server exit.

   * If *note 'CHECK TABLE': check-table. encounters errors in 'InnoDB'
     tables or indexes, it reports an error, and usually marks the index
     and sometimes marks the table as corrupted, preventing further use
     of the index or table.  Such errors include an incorrect number of
     entries in a secondary index or incorrect links.

   * If *note 'CHECK TABLE': check-table. finds an incorrect number of
     entries in a secondary index, it reports an error but does not
     cause a server exit or prevent access to the file.

   * *note 'CHECK TABLE': check-table. surveys the index page structure,
     then surveys each key entry.  It does not validate the key pointer
     to a clustered record or follow the path for *note 'BLOB': blob.
     pointers.

   * When an 'InnoDB' table is stored in its own '.ibd' file, the first
     3 pages of the '.ibd' file contain header information rather than
     table or index data.  The *note 'CHECK TABLE': check-table.
     statement does not detect inconsistencies that affect only the
     header data.  To verify the entire contents of an 'InnoDB' '.ibd'
     file, use the *note 'innochecksum': innochecksum. command.

   * When running *note 'CHECK TABLE': check-table. on large 'InnoDB'
     tables, other threads may be blocked during *note 'CHECK TABLE':
     check-table. execution.  To avoid timeouts, the semaphore wait
     threshold (600 seconds) is extended by 2 hours (7200 seconds) for
     *note 'CHECK TABLE': check-table. operations.  If 'InnoDB' detects
     semaphore waits of 240 seconds or more, it starts printing 'InnoDB'
     monitor output to the error log.  If a lock request extends beyond
     the semaphore wait threshold, 'InnoDB' aborts the process.  To
     avoid the possibility of a semaphore wait timeout entirely, run
     *note 'CHECK TABLE QUICK': check-table. instead of *note 'CHECK
     TABLE': check-table.


File: manual.info.tmp,  Node: checksum-table,  Next: optimize-table,  Prev: check-table,  Up: table-maintenance-statements

13.7.2.3 CHECKSUM TABLE Statement
.................................

     CHECKSUM TABLE TBL_NAME [, TBL_NAME] ... [QUICK | EXTENDED]

*note 'CHECKSUM TABLE': checksum-table. reports a table checksum.

This statement requires the 'SELECT' privilege for the table.

This statement is not supported for views.  If you run *note 'CHECKSUM
TABLE': checksum-table. against a view, the 'Checksum' value is always
'NULL', and a warning is returned.

For a nonexistent table, *note 'CHECKSUM TABLE': checksum-table. returns
'NULL' and generates a warning.

During the checksum operation, the table is locked with a read lock for
'InnoDB' and 'MyISAM'.

With 'QUICK', the live table checksum is reported if it is available, or
'NULL' otherwise.  This is very fast.  A live checksum is enabled by
specifying the 'CHECKSUM=1' table option when you create the table;
currently, this is supported only for 'MyISAM' tables.  The 'QUICK'
method is not supported with 'InnoDB' tables.  See *note create-table::.

With 'EXTENDED', the entire table is read row by row and the checksum is
calculated.  This can be very slow for large tables.

If neither 'QUICK' nor 'EXTENDED' is specified, MySQL returns a live
checksum if the table storage engine supports it and scans the table
otherwise.

In MySQL 5.5, *note 'CHECKSUM TABLE': checksum-table. returns 0 for
partitioned tables unless you include the 'EXTENDED' option.  This issue
is resolved in MySQL 5.6.  (Bug #11933226, Bug #60681)

The checksum value depends on the table row format.  If the row format
changes, the checksum also changes.  For example, the storage format for
temporal types such as *note 'TIME': time, *note 'DATETIME': datetime,
and *note 'TIMESTAMP': datetime. changes in MySQL 5.6 prior to MySQL
5.6.5, so if a 5.5 table is upgraded to MySQL 5.6, the checksum value
may change.

*Important*:

If the checksums for two tables are different, then it is almost certain
that the tables are different in some way.  However, because the hashing
function used by *note 'CHECKSUM TABLE': checksum-table. is not
guaranteed to be collision-free, there is a slight chance that two
tables which are not identical can produce the same checksum.


File: manual.info.tmp,  Node: optimize-table,  Next: repair-table,  Prev: checksum-table,  Up: table-maintenance-statements

13.7.2.4 OPTIMIZE TABLE Statement
.................................

     OPTIMIZE [NO_WRITE_TO_BINLOG | LOCAL]
         TABLE TBL_NAME [, TBL_NAME] ...

*note 'OPTIMIZE TABLE': optimize-table. reorganizes the physical storage
of table data and associated index data, to reduce storage space and
improve I/O efficiency when accessing the table.  The exact changes made
to each table depend on the storage engine used by that table.

Use *note 'OPTIMIZE TABLE': optimize-table. in these cases, depending on
the type of table:

   * After doing substantial insert, update, or delete operations on an
     'InnoDB' table that has its own .ibd file because it was created
     with the 'innodb_file_per_table' option enabled.  The table and
     indexes are reorganized, and disk space can be reclaimed for use by
     the operating system.

   * After deleting a large part of a 'MyISAM' or 'ARCHIVE' table, or
     making many changes to a 'MyISAM' or 'ARCHIVE 'table with
     variable-length rows (tables that have *note 'VARCHAR': char, *note
     'VARBINARY': binary-varbinary, *note 'BLOB': blob, or *note 'TEXT':
     blob. columns).  Deleted rows are maintained in a linked list and
     subsequent *note 'INSERT': insert. operations reuse old row
     positions.  You can use *note 'OPTIMIZE TABLE': optimize-table. to
     reclaim the unused space and to defragment the data file.  After
     extensive changes to a table, this statement may also improve
     performance of statements that use the table, sometimes
     significantly.

This statement requires 'SELECT' and 'INSERT' privileges for the table.

*note 'OPTIMIZE TABLE': optimize-table. works for *note 'InnoDB':
innodb-storage-engine, *note 'MyISAM': myisam-storage-engine, and *note
'ARCHIVE': archive-storage-engine. tables.  *note 'OPTIMIZE TABLE':
optimize-table. is also supported for dynamic columns of in-memory *note
'NDB': mysql-cluster. tables.  It does not work for fixed-width columns
of in-memory tables, nor does it work for Disk Data tables.  The
performance of 'OPTIMIZE' on NDB Cluster tables can be tuned using
'--ndb-optimization-delay', which controls the length of time to wait
between processing batches of rows by *note 'OPTIMIZE TABLE':
optimize-table.  For more information, see *note
mysql-cluster-limitations-resolved::.

For NDB Cluster tables, *note 'OPTIMIZE TABLE': optimize-table. can be
interrupted by (for example) killing the SQL thread performing the
'OPTIMIZE' operation.

By default, *note 'OPTIMIZE TABLE': optimize-table. does _not_ work for
tables created using any other storage engine and returns a result
indicating this lack of support.  You can make *note 'OPTIMIZE TABLE':
optimize-table. work for other storage engines by starting *note
'mysqld': mysqld. with the '--skip-new' option.  In this case, *note
'OPTIMIZE TABLE': optimize-table. is just mapped to *note 'ALTER TABLE':
alter-table.

This statement does not work with views.

*note 'OPTIMIZE TABLE': optimize-table. is supported for partitioned
tables.  For information about using this statement with partitioned
tables and table partitions, see *note partitioning-maintenance::.

By default, the server writes *note 'OPTIMIZE TABLE': optimize-table.
statements to the binary log so that they replicate to replication
slaves.  To suppress logging, specify the optional 'NO_WRITE_TO_BINLOG'
keyword or its alias 'LOCAL'.

   * *note optimize-table-output::

   * *note optimize-table-innodb-details::

   * *note optimize-table-myisam-details::

   * *note optimize-table-other-considerations::

*OPTIMIZE TABLE Output*

*note 'OPTIMIZE TABLE': optimize-table. returns a result set with the
columns shown in the following table.

Column      Value
            
'Table'     The table name
            
'Op'        Always 'optimize'
            
'Msg_type'  'status', 'error', 'info', 'note', or
            'warning'
            
'Msg_text'  An informational message

*note 'OPTIMIZE TABLE': optimize-table. table catches and throws any
errors that occur while copying table statistics from the old file to
the newly created file.  For example.  if the user ID of the owner of
the '.frm', '.MYD', or '.MYI' file is different from the user ID of the
*note 'mysqld': mysqld. process, *note 'OPTIMIZE TABLE': optimize-table.
generates a "cannot change ownership of the file" error unless *note
'mysqld': mysqld. is started by the 'root' user.

*InnoDB Details*

For 'InnoDB' tables, *note 'OPTIMIZE TABLE': optimize-table. is mapped
to *note 'ALTER TABLE': alter-table, which rebuilds the table to update
index statistics and free unused space in the clustered index.  This is
displayed in the output of *note 'OPTIMIZE TABLE': optimize-table. when
you run it on an 'InnoDB' table, as shown here:

     mysql> OPTIMIZE TABLE foo;
     +----------+----------+----------+-------------------------------------------------------------------+
     | Table    | Op       | Msg_type | Msg_text                                                          |
     +----------+----------+----------+-------------------------------------------------------------------+
     | test.foo | optimize | note     | Table does not support optimize, doing recreate + analyze instead |
     | test.foo | optimize | status   | OK                                                                |
     +----------+----------+----------+-------------------------------------------------------------------+

This operation does not use fast index creation.  Secondary indexes are
not created as efficiently because keys are inserted in the order they
appeared in the primary key.  See *note
innodb-create-index-limitations::.

'InnoDB' stores data using a page-allocation method and does not suffer
from fragmentation in the same way that legacy storage engines (such as
'MyISAM') will.  When considering whether or not to run optimize,
consider the workload of transactions that your server will process:

   * Some level of fragmentation is expected.  'InnoDB' only fills pages
     93% full, to leave room for updates without having to split pages.

   * Delete operations might leave gaps that leave pages less filled
     than desired, which could make it worthwhile to optimize the table.

   * Updates to rows usually rewrite the data within the same page,
     depending on the data type and row format, when sufficient space is
     available.  See *note innodb-compression-internals:: and *note
     innodb-row-format::.

   * High-concurrency workloads might leave gaps in indexes over time,
     as 'InnoDB' retains multiple versions of the same data due through
     its MVCC mechanism.  See *note innodb-multi-versioning::.

*MyISAM Details*

For 'MyISAM' tables, *note 'OPTIMIZE TABLE': optimize-table. works as
follows:

  1. If the table has deleted or split rows, repair the table.

  2. If the index pages are not sorted, sort them.

  3. If the table's statistics are not up to date (and the repair could
     not be accomplished by sorting the index), update them.

*Other Considerations*

Note that MySQL locks the table during the time *note 'OPTIMIZE TABLE':
optimize-table. is running.

*note 'OPTIMIZE TABLE': optimize-table. does not sort R-tree indexes,
such as spatial indexes on 'POINT' columns.  (Bug #23578)


File: manual.info.tmp,  Node: repair-table,  Prev: optimize-table,  Up: table-maintenance-statements

13.7.2.5 REPAIR TABLE Statement
...............................

     REPAIR [NO_WRITE_TO_BINLOG | LOCAL]
         TABLE TBL_NAME [, TBL_NAME] ...
         [QUICK] [EXTENDED] [USE_FRM]

*note 'REPAIR TABLE': repair-table. repairs a possibly corrupted table,
for certain storage engines only.

This statement requires 'SELECT' and 'INSERT' privileges for the table.

Although normally you should never have to run *note 'REPAIR TABLE':
repair-table, if disaster strikes, this statement is very likely to get
back all your data from a 'MyISAM' table.  If your tables become
corrupted often, try to find the reason for it, to eliminate the need to
use *note 'REPAIR TABLE': repair-table.  See *note crashing::, and *note
myisam-table-problems::.

*note 'REPAIR TABLE': repair-table. checks the table to see whether an
upgrade is required.  If so, it performs the upgrade, following the same
rules as *note 'CHECK TABLE ... FOR UPGRADE': check-table.  See *note
check-table::, for more information.

*Important*:

   * Make a backup of a table before performing a table repair
     operation; under some circumstances the operation might cause data
     loss.  Possible causes include but are not limited to file system
     errors.  See *note backup-and-recovery::.

   * If the server crashes during a *note 'REPAIR TABLE': repair-table.
     operation, it is essential after restarting it that you immediately
     execute another *note 'REPAIR TABLE': repair-table. statement for
     the table before performing any other operations on it.  In the
     worst case, you might have a new clean index file without
     information about the data file, and then the next operation you
     perform could overwrite the data file.  This is an unlikely but
     possible scenario that underscores the value of making a backup
     first.

   * In the event that a table on the master becomes corrupted and you
     run *note 'REPAIR TABLE': repair-table. on it, any resulting
     changes to the original table are _not_ propagated to slaves.

   * *note repair-table-support::

   * *note repair-table-options::

   * *note repair-table-output::

   * *note repair-table-table-repair-considerations::

*REPAIR TABLE Storage Engine and Partitioning Support*

*note 'REPAIR TABLE': repair-table. works for *note 'MyISAM':
myisam-storage-engine, *note 'ARCHIVE': archive-storage-engine, and
*note 'CSV': csv-storage-engine. tables.  For *note 'MyISAM':
myisam-storage-engine. tables, it has the same effect as *note
'myisamchk --recover TBL_NAME': myisamchk. by default.  This statement
does not work with views.

*note 'REPAIR TABLE': repair-table. is supported for partitioned tables.
However, the 'USE_FRM' option cannot be used with this statement on a
partitioned table.

You can use 'ALTER TABLE ... REPAIR PARTITION' to repair one or more
partitions; for more information, see *note alter-table::, and *note
partitioning-maintenance::.

*REPAIR TABLE Options*

   * 'NO_WRITE_TO_BINLOG' or 'LOCAL'

     By default, the server writes *note 'REPAIR TABLE': repair-table.
     statements to the binary log so that they replicate to replication
     slaves.  To suppress logging, specify the optional
     'NO_WRITE_TO_BINLOG' keyword or its alias 'LOCAL'.

   * 'QUICK'

     If you use the 'QUICK' option, *note 'REPAIR TABLE': repair-table.
     tries to repair only the index file, and not the data file.  This
     type of repair is like that done by *note 'myisamchk --recover
     --quick': myisamchk.

   * 'EXTENDED'

     If you use the 'EXTENDED' option, MySQL creates the index row by
     row instead of creating one index at a time with sorting.  This
     type of repair is like that done by *note 'myisamchk
     --safe-recover': myisamchk.

   * 'USE_FRM'

     The 'USE_FRM' option is available for use if the '.MYI' index file
     is missing or if its header is corrupted.  This option tells MySQL
     not to trust the information in the '.MYI' file header and to
     re-create it using information from the '.frm' file.  This kind of
     repair cannot be done with *note 'myisamchk': myisamchk.

     *Caution*:

     Use the 'USE_FRM' option _only_ if you cannot use regular 'REPAIR'
     modes.  Telling the server to ignore the '.MYI' file makes
     important table metadata stored in the '.MYI' unavailable to the
     repair process, which can have deleterious consequences:

        * The current 'AUTO_INCREMENT' value is lost.

        * The link to deleted records in the table is lost, which means
          that free space for deleted records will remain unoccupied
          thereafter.

        * The '.MYI' header indicates whether the table is compressed.
          If the server ignores this information, it cannot tell that a
          table is compressed and repair can cause change or loss of
          table contents.  This means that 'USE_FRM' should not be used
          with compressed tables.  That should not be necessary, anyway:
          Compressed tables are read only, so they should not become
          corrupt.

     If you use 'USE_FRM' for a table that was created by a different
     version of the MySQL server than the one you are currently running,
     *note 'REPAIR TABLE': repair-table. does not attempt to repair the
     table.  In this case, the result set returned by *note 'REPAIR
     TABLE': repair-table. contains a line with a 'Msg_type' value of
     'error' and a 'Msg_text' value of 'Failed repairing incompatible
     .FRM file'.

     If 'USE_FRM' is used, *note 'REPAIR TABLE': repair-table. does not
     check the table to see whether an upgrade is required.

*REPAIR TABLE Output*

*note 'REPAIR TABLE': repair-table. returns a result set with the
columns shown in the following table.

Column      Value
            
'Table'     The table name
            
'Op'        Always 'repair'
            
'Msg_type'  'status', 'error', 'info', 'note', or
            'warning'
            
'Msg_text'  An informational message

The *note 'REPAIR TABLE': repair-table. statement might produce many
rows of information for each repaired table.  The last row has a
'Msg_type' value of 'status' and 'Msg_test' normally should be 'OK'.
For a 'MyISAM' table, if you do not get 'OK', you should try repairing
it with *note 'myisamchk --safe-recover': myisamchk.  (*note 'REPAIR
TABLE': repair-table. does not implement all the options of *note
'myisamchk': myisamchk.  With *note 'myisamchk --safe-recover':
myisamchk, you can also use options that *note 'REPAIR TABLE':
repair-table. does not support, such as '--max-record-length'.)

*note 'REPAIR TABLE': repair-table. table catches and throws any errors
that occur while copying table statistics from the old corrupted file to
the newly created file.  For example.  if the user ID of the owner of
the '.frm', '.MYD', or '.MYI' file is different from the user ID of the
*note 'mysqld': mysqld. process, *note 'REPAIR TABLE': repair-table.
generates a "cannot change ownership of the file" error unless *note
'mysqld': mysqld. is started by the 'root' user.

*Table Repair Considerations*

You may be able to increase *note 'REPAIR TABLE': repair-table.
performance by setting certain system variables.  See *note
repair-table-optimization::.


File: manual.info.tmp,  Node: component-statements,  Next: set-statement,  Prev: table-maintenance-statements,  Up: sql-server-administration-statements

13.7.3 Plugin and User-Defined Function Statements
--------------------------------------------------

* Menu:

* create-function-udf::          CREATE FUNCTION Syntax for User-Defined Functions
* drop-function-udf::            DROP FUNCTION Statement
* install-plugin::               INSTALL PLUGIN Statement
* uninstall-plugin::             UNINSTALL PLUGIN Statement


File: manual.info.tmp,  Node: create-function-udf,  Next: drop-function-udf,  Prev: component-statements,  Up: component-statements

13.7.3.1 CREATE FUNCTION Syntax for User-Defined Functions
..........................................................

     CREATE [AGGREGATE] FUNCTION FUNCTION_NAME
         RETURNS {STRING|INTEGER|REAL|DECIMAL}
         SONAME SHARED_LIBRARY_NAME

A user-defined function (UDF) is a way to extend MySQL with a new
function that works like a native (built-in) MySQL function such as
'ABS()' or 'CONCAT()'.

FUNCTION_NAME is the name that should be used in SQL statements to
invoke the function.  The 'RETURNS' clause indicates the type of the
function's return value.  *note 'DECIMAL': fixed-point-types. is a legal
value after 'RETURNS', but currently *note 'DECIMAL': fixed-point-types.
functions return string values and should be written like 'STRING'
functions.

SHARED_LIBRARY_NAME is the base name of the shared library file that
contains the code that implements the function.  The file must be
located in the plugin directory.  This directory is given by the value
of the 'plugin_dir' system variable.  For more information, see *note
udf-loading::.

To create a function, you must have the 'INSERT' privilege for the
'mysql' system database.  This is necessary because *note 'CREATE
FUNCTION': create-function. adds a row to the 'mysql.func' system table
that records the function's name, type, and shared library name.

An active function is one that has been loaded with *note 'CREATE
FUNCTION': create-function. and not removed with *note 'DROP FUNCTION':
drop-function.  All active functions are reloaded each time the server
starts, unless you start *note 'mysqld': mysqld. with the
'--skip-grant-tables' option.  In this case, UDF initialization is
skipped and UDFs are unavailable.

For instructions on writing user-defined functions, see *note
adding-udf::.  For the UDF mechanism to work, functions must be written
in C or C++ (or another language that can use C calling conventions),
your operating system must support dynamic loading and you must have
compiled *note 'mysqld': mysqld. dynamically (not statically).

An 'AGGREGATE' function works exactly like a native MySQL aggregate
(summary) function such as 'SUM' or 'COUNT()'.

*Note*:

To upgrade the shared library associated with a UDF, issue a *note 'DROP
FUNCTION': drop-function. statement, upgrade the shared library, and
then issue a *note 'CREATE FUNCTION': create-function. statement.  If
you upgrade the shared library first and then use *note 'DROP FUNCTION':
drop-function, the server may crash.


File: manual.info.tmp,  Node: drop-function-udf,  Next: install-plugin,  Prev: create-function-udf,  Up: component-statements

13.7.3.2 DROP FUNCTION Statement
................................

     DROP FUNCTION FUNCTION_NAME

This statement drops the user-defined function (UDF) named
FUNCTION_NAME.

To drop a function, you must have the 'DELETE' privilege for the 'mysql'
system database.  This is because *note 'DROP FUNCTION': drop-function.
removes a row from the 'mysql.func' system table that records the
function's name, type, and shared library name.

*Note*:

To upgrade the shared library associated with a UDF, issue a *note 'DROP
FUNCTION': drop-function. statement, upgrade the shared library, and
then issue a *note 'CREATE FUNCTION': create-function. statement.  If
you upgrade the shared library first and then use *note 'DROP FUNCTION':
drop-function, the server may crash.

*note 'DROP FUNCTION': drop-function. is also used to drop stored
functions (see *note drop-procedure::).


File: manual.info.tmp,  Node: install-plugin,  Next: uninstall-plugin,  Prev: drop-function-udf,  Up: component-statements

13.7.3.3 INSTALL PLUGIN Statement
.................................

     INSTALL PLUGIN PLUGIN_NAME SONAME 'SHARED_LIBRARY_NAME'

This statement installs a server plugin.  It requires the 'INSERT'
privilege for the 'mysql.plugin' system table.

PLUGIN_NAME is the name of the plugin as defined in the plugin
descriptor structure contained in the library file (see *note
plugin-data-structures::).  Plugin names are not case-sensitive.  For
maximal compatibility, plugin names should be limited to ASCII letters,
digits, and underscore because they are used in C source files, shell
command lines, M4 and Bourne shell scripts, and SQL environments.

SHARED_LIBRARY_NAME is the name of the shared library that contains the
plugin code.  The name includes the file name extension (for example,
'libmyplugin.so', 'libmyplugin.dll', or 'libmyplugin.dylib').

The shared library must be located in the plugin directory (the
directory named by the 'plugin_dir' system variable).  The library must
be in the plugin directory itself, not in a subdirectory.  By default,
'plugin_dir' is the 'plugin' directory under the directory named by the
'pkglibdir' configuration variable, but it can be changed by setting the
value of 'plugin_dir' at server startup.  For example, set its value in
a 'my.cnf' file:

     [mysqld]
     plugin_dir=/PATH/TO/PLUGIN/DIRECTORY

If the value of 'plugin_dir' is a relative path name, it is taken to be
relative to the MySQL base directory (the value of the 'basedir' system
variable).

*note 'INSTALL PLUGIN': install-plugin. loads and initializes the plugin
code to make the plugin available for use.  A plugin is initialized by
executing its initialization function, which handles any setup that the
plugin must perform before it can be used.  When the server shuts down,
it executes the deinitialization function for each plugin that is loaded
so that the plugin has a chance to perform any final cleanup.

*note 'INSTALL PLUGIN': install-plugin. also registers the plugin by
adding a line that indicates the plugin name and library file name to
the 'mysql.plugin' system table.  At server startup, the server loads
and initializes any plugin that is listed in 'mysql.plugin'.  This means
that a plugin is installed with *note 'INSTALL PLUGIN': install-plugin.
only once, not every time the server starts.  Plugin loading at startup
does not occur if the server is started with the '--skip-grant-tables'
option.

A plugin library can contain multiple plugins.  For each of them to be
installed, use a separate *note 'INSTALL PLUGIN': install-plugin.
statement.  Each statement names a different plugin, but all of them
specify the same library name.

*note 'INSTALL PLUGIN': install-plugin. causes the server to read option
('my.cnf') files just as during server startup.  This enables the plugin
to pick up any relevant options from those files.  It is possible to add
plugin options to an option file even before loading a plugin (if the
'loose' prefix is used).  It is also possible to uninstall a plugin,
edit 'my.cnf', and install the plugin again.  Restarting the plugin this
way enables it to the new option values without a server restart.

For options that control individual plugin loading at server startup,
see *note plugin-loading::.  If you need to load plugins for a single
server startup when the '--skip-grant-tables' option is given (which
tells the server not to read system tables), use the '--plugin-load'
option.  See *note server-options::.

To remove a plugin, use the *note 'UNINSTALL PLUGIN': uninstall-plugin.
statement.

For additional information about plugin loading, see *note
plugin-loading::.

To see what plugins are installed, use the *note 'SHOW PLUGINS':
show-plugins. statement or query the 'INFORMATION_SCHEMA' the *note
'PLUGINS': plugins-table. table.

If you recompile a plugin library and need to reinstall it, you can use
either of the following methods:

   * Use *note 'UNINSTALL PLUGIN': uninstall-plugin. to uninstall all
     plugins in the library, install the new plugin library file in the
     plugin directory, and then use *note 'INSTALL PLUGIN':
     install-plugin. to install all plugins in the library.  This
     procedure has the advantage that it can be used without stopping
     the server.  However, if the plugin library contains many plugins,
     you must issue many *note 'INSTALL PLUGIN': install-plugin. and
     *note 'UNINSTALL PLUGIN': uninstall-plugin. statements.

   * Stop the server, install the new plugin library file in the plugin
     directory, and restart the server.


File: manual.info.tmp,  Node: uninstall-plugin,  Prev: install-plugin,  Up: component-statements

13.7.3.4 UNINSTALL PLUGIN Statement
...................................

     UNINSTALL PLUGIN PLUGIN_NAME

This statement removes an installed server plugin.  It requires the
'DELETE' privilege for the 'mysql.plugin' system table.  *note
'UNINSTALL PLUGIN': uninstall-plugin. is the complement of *note
'INSTALL PLUGIN': install-plugin.

PLUGIN_NAME must be the name of some plugin that is listed in the
'mysql.plugin' table.  The server executes the plugin's deinitialization
function and removes the row for the plugin from the 'mysql.plugin'
system table, so that subsequent server restarts will not load and
initialize the plugin.  *note 'UNINSTALL PLUGIN': uninstall-plugin. does
not remove the plugin's shared library file.

You cannot uninstall a plugin if any table that uses it is open.

Plugin removal has implications for the use of associated tables.  For
example, if a full-text parser plugin is associated with a 'FULLTEXT'
index on the table, uninstalling the plugin makes the table unusable.
Any attempt to access the table results in an error.  The table cannot
even be opened, so you cannot drop an index for which the plugin is
used.  This means that uninstalling a plugin is something to do with
care unless you do not care about the table contents.  If you are
uninstalling a plugin with no intention of reinstalling it later and you
care about the table contents, you should dump the table with *note
'mysqldump': mysqldump. and remove the 'WITH PARSER' clause from the
dumped *note 'CREATE TABLE': create-table. statement so that you can
reload the table later.  If you do not care about the table, *note 'DROP
TABLE': drop-table. can be used even if any plugins associated with the
table are missing.

For additional information about plugin loading, see *note
plugin-loading::.


File: manual.info.tmp,  Node: set-statement,  Next: show,  Prev: component-statements,  Up: sql-server-administration-statements

13.7.4 SET Statements
---------------------

* Menu:

* set-variable::                 SET Syntax for Variable Assignment
* set-character-set::            SET CHARACTER SET Statement
* set-names::                    SET NAMES Statement

The *note 'SET': set-statement. statement has several forms.
Descriptions for those forms that are not associated with a specific
server capability appear in subsections of this section:

   * *note 'SET VAR_NAME = VALUE': set-variable. enables you to assign
     values to variables that affect the operation of the server or
     clients.  See *note set-variable::.

   * *note 'SET CHARACTER SET': set-character-set. and *note 'SET
     NAMES': set-names. assign values to character set and collation
     variables associated with the current connection to the server.
     See *note set-character-set::, and *note set-names::.

Descriptions for the other forms appear elsewhere, grouped with other
statements related to the capability they help implement:

   * *note 'SET PASSWORD': set-password. assigns account passwords.  See
     *note set-password::.

   * *note 'SET TRANSACTION ISOLATION LEVEL': set-transaction. sets the
     isolation level for transaction processing.  See *note
     set-transaction::.


File: manual.info.tmp,  Node: set-variable,  Next: set-character-set,  Prev: set-statement,  Up: set-statement

13.7.4.1 SET Syntax for Variable Assignment
...........................................

     SET VARIABLE = EXPR [, VARIABLE = EXPR] ...

     VARIABLE: {
         USER_VAR_NAME
       | PARAM_NAME
       | LOCAL_VAR_NAME
       | {GLOBAL | @@GLOBAL.} SYSTEM_VAR_NAME
       | [SESSION | @@SESSION. | @@] SYSTEM_VAR_NAME
     }

     SET ONE_SHOT SYSTEM_VAR_NAME = EXPR

*note 'SET': set-variable. syntax for variable assignment enables you to
assign values to different types of variables that affect the operation
of the server or clients:

   * User-defined variables.  See *note user-variables::.

   * Stored procedure and function parameters, and stored program local
     variables.  See *note stored-program-variables::.

   * System variables.  See *note server-system-variables::.  System
     variables also can be set at server startup, as described in *note
     using-system-variables::.

Older versions of MySQL employed 'SET OPTION', but this syntax is
deprecated in favor of *note 'SET': set-variable. without 'OPTION'.

A *note 'SET': set-variable. statement that assigns variable values is
not written to the binary log, so in replication scenarios it affects
only the host on which you execute it.  To affect all replication hosts,
execute the statement on each host.

The following sections describe *note 'SET': set-variable. syntax for
setting variables.  They use the '=' assignment operator, but the ':='
assignment operator is also permitted for this purpose.

   * *note set-variable-user-variables::

   * *note set-variable-parameters-local-variables::

   * *note set-variable-system-variables::

   * *note set-variable-error-handling::

   * *note set-variable-multiple-assignments::

   * *note variable-references-in-expressions::

   * *note set-variable-one-shot::

*User-Defined Variable Assignment*

User-defined variables are created locally within a session and exist
only within the context of that session; see *note user-variables::.

A user-defined variable is written as '@VAR_NAME' and is assigned an
expression value as follows:

     SET @VAR_NAME = EXPR;

Examples:

     SET @name = 43;
     SET @total_tax = (SELECT SUM(tax) FROM taxable_transactions);

As demonstrated by those statements, EXPR can range from simple (a
literal value) to more complex (the value returned by a scalar
subquery).

*Parameter and Local Variable Assignment*

*note 'SET': set-variable. applies to parameters and local variables in
the context of the stored object within which they are defined.  The
following procedure uses the 'increment' procedure parameter and
'counter' local variable:

     CREATE PROCEDURE p(increment INT)
     BEGIN
       DECLARE counter INT DEFAULT 0;
       WHILE counter < 10 DO
         -- ... do work ...
         SET counter = counter + increment;
       END WHILE;
     END;

*System Variable Assignment*

The MySQL server maintains system variables that configure its
operation.  A system variable can have a global value that affects
server operation as a whole, a session value that affects the current
session, or both.  Many system variables are dynamic and can be changed
at runtime using the *note 'SET': set-variable. statement to affect
operation of the current server instance.  (To make a global system
variable setting permanent so that it applies across server restarts,
you should also set it in an option file.)

If you change a session system variable, the value remains in effect
within your session until you change the variable to a different value
or the session ends.  The change has no effect on other sessions.

If you change a global system variable, the value is remembered and used
to initialize the session value for new sessions until you change the
variable to a different value or the server exits.  The change is
visible to any client that accesses the global value.  However, the
change affects the corresponding session value only for clients that
connect after the change.  The global variable change does not affect
the session value for any current client sessions (not even the session
within which the global value change occurs).

*Note*:

Setting a global system variable value always requires special
privileges.  Setting a session system variable value normally requires
no special privileges and can be done by any user, although there are
exceptions.  For more information, see *note
system-variable-privileges::.

The following discussion describes the syntax options for setting system
variables:

   * To assign a value to a global system variable, precede the variable
     name by the 'GLOBAL' keyword or the '@@GLOBAL.' qualifier:

          SET GLOBAL max_connections = 1000;
          SET @@GLOBAL.max_connections = 1000;

   * To assign a value to a session system variable, precede the
     variable name by the 'SESSION' or 'LOCAL' keyword, by the
     '@@SESSION.', '@@LOCAL.', or '@@' qualifier, or by no keyword or no
     modifier at all:

          SET SESSION sql_mode = 'TRADITIONAL';
          SET LOCAL sql_mode = 'TRADITIONAL';
          SET @@SESSION.sql_mode = 'TRADITIONAL';
          SET @@LOCAL.sql_mode = 'TRADITIONAL';
          SET @@sql_mode = 'TRADITIONAL';
          SET sql_mode = 'TRADITIONAL';

     A client can change its own session variables, but not those of any
     other client.

To set a global system variable value to the compiled-in MySQL default
value or a session system variable to the current corresponding global
value, set the variable to the value 'DEFAULT'.  For example, the
following two statements are identical in setting the session value of
'max_join_size' to the current global value:

     SET @@SESSION.max_join_size = DEFAULT;
     SET @@SESSION.max_join_size = @@GLOBAL.max_join_size;

To display system variable names and values:

   * Use the *note 'SHOW VARIABLES': show-variables. statement; see
     *note show-variables::.

*SET Error Handling*

If any variable assignment in a *note 'SET': set-variable. statement
fails, the entire statement fails and no variables are changed.

*note 'SET': set-variable. produces an error under the circumstances
described here.  Most of the examples show *note 'SET': set-variable.
statements that use keyword syntax (for example, 'GLOBAL' or 'SESSION'),
but the principles are also true for statements that use the
corresponding modifiers (for example, '@@GLOBAL.' or '@@SESSION.').

   * Use of *note 'SET': set-variable. (any variant) to set a read-only
     variable:

          mysql> SET GLOBAL version = 'abc';
          ERROR 1238 (HY000): Variable 'version' is a read only variable

   * Use of 'GLOBAL' to set a variable that has only a session value:

          mysql> SET GLOBAL sql_log_bin = ON;
          ERROR 1231 (42000): Variable 'sql_log_bin' can't be
          set to the value of 'ON'

   * Use of 'SESSION' to set a variable that has only a global value:

          mysql> SET SESSION max_connections = 1000;
          ERROR 1229 (HY000): Variable 'max_connections' is a
          GLOBAL variable and should be set with SET GLOBAL

   * Omission of 'GLOBAL' to set a variable that has only a global
     value:

          mysql> SET max_connections = 1000;
          ERROR 1229 (HY000): Variable 'max_connections' is a
          GLOBAL variable and should be set with SET GLOBAL

   * The '@@GLOBAL.', '@@SESSION.', and '@@' modifiers apply only to
     system variables.  An error occurs for attempts to apply them to
     user-defined variables, stored procedure or function parameters, or
     stored program local variables.

   * Not all system variables can be set to 'DEFAULT'.  In such cases,
     assigning 'DEFAULT' results in an error.

   * It is not permitted to assign 'DEFAULT' to user-defined variables,
     and not supported for stored procedure or function parameters or
     stored program local variables.  This results in an error for
     user-defined variables, and the results are undefined for
     parameters or local variables.

*Multiple Variable Assignment*

A *note 'SET': set-variable. statement can contain multiple variable
assignments, separated by commas.  This statement assigns a value to a
user-defined variable and a system variable:

     SET @x = 1, SESSION sql_mode = '';

If you set multiple system variables in a single statement, the most
recent 'GLOBAL' or 'SESSION' keyword in the statement is used for
following assignments that have no keyword specified.

Examples of multiple-variable assignment:

     SET GLOBAL sort_buffer_size = 1000000, SESSION sort_buffer_size = 1000000;
     SET @@GLOBAL.sort_buffer_size = 1000000, @@LOCAL.sort_buffer_size = 1000000;
     SET GLOBAL max_connections = 1000, sort_buffer_size = 1000000;

The '@@GLOBAL.', '@@SESSION.', and '@@' modifiers apply only to the
immediately following system variable, not any remaining system
variables.  This statement sets the 'sort_buffer_size' global value to
50000 and the session value to 1000000:

     SET @@GLOBAL.sort_buffer_size = 50000, sort_buffer_size = 1000000;

*System Variable References in Expressions*

To refer to the value of a system variable in expressions, use one of
the '@@'-modifiers.  For example, you can retrieve system variable
values in a *note 'SELECT': select. statement like this:

     SELECT @@GLOBAL.sql_mode, @@SESSION.sql_mode, @@sql_mode;

*Note*:

A reference to a system variable in an expression as '@@VAR_NAME' (with
'@@' rather than '@@GLOBAL.' or '@@SESSION.') returns the session value
if it exists and the global value otherwise.  This differs from 'SET
@@VAR_NAME = EXPR', which always refers to the session value.

*ONE_SHOT Assignment*

The 'SET ONE_SHOT' syntax is _only_ for internal use for replication:
*note 'mysqlbinlog': mysqlbinlog. uses 'SET ONE_SHOT' to modify
temporarily the values of character set, collation, and time zone
variables to reflect at rollforward what they were originally.
'ONE_SHOT' is for internal use only and is deprecated.

'ONE_SHOT' is intended for use only with the permitted set of variables.
It changes the variables as requested, but only for the next non-*note
'SET': set-variable. statement.  After that, the server resets all
character set, collation, and time zone-related system variables to
their previous values.  Example:

     mysql> SET ONE_SHOT character_set_connection = latin5;

     mysql> SET ONE_SHOT collation_connection = latin5_turkish_ci;

     mysql> SHOW VARIABLES LIKE '%_connection';
     +--------------------------+-------------------+
     | Variable_name            | Value             |
     +--------------------------+-------------------+
     | character_set_connection | latin5            |
     | collation_connection     | latin5_turkish_ci |
     +--------------------------+-------------------+

     mysql> SHOW VARIABLES LIKE '%_connection';
     +--------------------------+-------------------+
     | Variable_name            | Value             |
     +--------------------------+-------------------+
     | character_set_connection | latin1            |
     | collation_connection     | latin1_swedish_ci |
     +--------------------------+-------------------+


File: manual.info.tmp,  Node: set-character-set,  Next: set-names,  Prev: set-variable,  Up: set-statement

13.7.4.2 SET CHARACTER SET Statement
....................................

     SET {CHARACTER SET | CHARSET}
         {'CHARSET_NAME' | DEFAULT}

This statement maps all strings sent between the server and the current
client with the given mapping.  'SET CHARACTER SET' sets three session
system variables: 'character_set_client' and 'character_set_results' are
set to the given character set, and 'character_set_connection' to the
value of 'character_set_database'.  See *note charset-connection::.

CHARSET_NAME may be quoted or unquoted.

The default character set mapping can be restored by using the value
'DEFAULT'.  The default depends on the server configuration.

Some character sets cannot be used as the client character set.
Attempting to use them with *note 'SET CHARACTER SET':
set-character-set. produces an error.  See *note
charset-connection-impermissible-client-charset::.


File: manual.info.tmp,  Node: set-names,  Prev: set-character-set,  Up: set-statement

13.7.4.3 SET NAMES Statement
............................

     SET NAMES {'CHARSET_NAME'
         [COLLATE 'COLLATION_NAME'] | DEFAULT}

This statement sets the three session system variables
'character_set_client', 'character_set_connection', and
'character_set_results' to the given character set.  Setting
'character_set_connection' to 'charset_name' also sets
'collation_connection' to the default collation for 'charset_name'.  See
*note charset-connection::.

The optional 'COLLATE' clause may be used to specify a collation
explicitly.  If given, the collation must one of the permitted
collations for CHARSET_NAME.

CHARSET_NAME and COLLATION_NAME may be quoted or unquoted.

The default mapping can be restored by using a value of 'DEFAULT'.  The
default depends on the server configuration.

Some character sets cannot be used as the client character set.
Attempting to use them with *note 'SET NAMES': set-names. produces an
error.  See *note charset-connection-impermissible-client-charset::.


File: manual.info.tmp,  Node: show,  Next: other-administrative-statements,  Prev: set-statement,  Up: sql-server-administration-statements

13.7.5 SHOW Statements
----------------------

* Menu:

* show-authors::                 SHOW AUTHORS Statement
* show-binary-logs::             SHOW BINARY LOGS Statement
* show-binlog-events::           SHOW BINLOG EVENTS Statement
* show-character-set::           SHOW CHARACTER SET Statement
* show-collation::               SHOW COLLATION Statement
* show-columns::                 SHOW COLUMNS Statement
* show-contributors::            SHOW CONTRIBUTORS Statement
* show-create-database::         SHOW CREATE DATABASE Statement
* show-create-event::            SHOW CREATE EVENT Statement
* show-create-function::         SHOW CREATE FUNCTION Statement
* show-create-procedure::        SHOW CREATE PROCEDURE Statement
* show-create-table::            SHOW CREATE TABLE Statement
* show-create-trigger::          SHOW CREATE TRIGGER Statement
* show-create-view::             SHOW CREATE VIEW Statement
* show-databases::               SHOW DATABASES Statement
* show-engine::                  SHOW ENGINE Statement
* show-engines::                 SHOW ENGINES Statement
* show-errors::                  SHOW ERRORS Statement
* show-events::                  SHOW EVENTS Statement
* show-function-code::           SHOW FUNCTION CODE Statement
* show-function-status::         SHOW FUNCTION STATUS Statement
* show-grants::                  SHOW GRANTS Statement
* show-index::                   SHOW INDEX Statement
* show-master-status::           SHOW MASTER STATUS Statement
* show-open-tables::             SHOW OPEN TABLES Statement
* show-plugins::                 SHOW PLUGINS Statement
* show-privileges::              SHOW PRIVILEGES Statement
* show-procedure-code::          SHOW PROCEDURE CODE Statement
* show-procedure-status::        SHOW PROCEDURE STATUS Statement
* show-processlist::             SHOW PROCESSLIST Statement
* show-profile::                 SHOW PROFILE Statement
* show-profiles::                SHOW PROFILES Statement
* show-relaylog-events::         SHOW RELAYLOG EVENTS Statement
* show-slave-hosts::             SHOW SLAVE HOSTS Statement
* show-slave-status::            SHOW SLAVE STATUS Statement
* show-status::                  SHOW STATUS Statement
* show-table-status::            SHOW TABLE STATUS Statement
* show-tables::                  SHOW TABLES Statement
* show-triggers::                SHOW TRIGGERS Statement
* show-variables::               SHOW VARIABLES Statement
* show-warnings::                SHOW WARNINGS Statement

*note 'SHOW': show. has many forms that provide information about
databases, tables, columns, or status information about the server.
This section describes those following:

     SHOW AUTHORS
     SHOW {BINARY | MASTER} LOGS
     SHOW BINLOG EVENTS [IN 'LOG_NAME'] [FROM POS] [LIMIT [OFFSET,] ROW_COUNT]
     SHOW CHARACTER SET [LIKE_OR_WHERE]
     SHOW COLLATION [LIKE_OR_WHERE]
     SHOW [FULL] COLUMNS FROM TBL_NAME [FROM DB_NAME] [LIKE_OR_WHERE]
     SHOW CONTRIBUTORS
     SHOW CREATE DATABASE DB_NAME
     SHOW CREATE EVENT EVENT_NAME
     SHOW CREATE FUNCTION FUNC_NAME
     SHOW CREATE PROCEDURE PROC_NAME
     SHOW CREATE TABLE TBL_NAME
     SHOW CREATE TRIGGER TRIGGER_NAME
     SHOW CREATE VIEW VIEW_NAME
     SHOW DATABASES [LIKE_OR_WHERE]
     SHOW ENGINE ENGINE_NAME {STATUS | MUTEX}
     SHOW [STORAGE] ENGINES
     SHOW ERRORS [LIMIT [OFFSET,] ROW_COUNT]
     SHOW EVENTS
     SHOW FUNCTION CODE FUNC_NAME
     SHOW FUNCTION STATUS [LIKE_OR_WHERE]
     SHOW GRANTS FOR USER
     SHOW INDEX FROM TBL_NAME [FROM DB_NAME]
     SHOW MASTER STATUS
     SHOW OPEN TABLES [FROM DB_NAME] [LIKE_OR_WHERE]
     SHOW PLUGINS
     SHOW PROCEDURE CODE PROC_NAME
     SHOW PROCEDURE STATUS [LIKE_OR_WHERE]
     SHOW PRIVILEGES
     SHOW [FULL] PROCESSLIST
     SHOW PROFILE [TYPES] [FOR QUERY N] [OFFSET N] [LIMIT N]
     SHOW PROFILES
     SHOW RELAYLOG EVENTS [IN 'LOG_NAME'] [FROM POS] [LIMIT [OFFSET,] ROW_COUNT]
     SHOW SLAVE HOSTS
     SHOW SLAVE STATUS
     SHOW [GLOBAL | SESSION] STATUS [LIKE_OR_WHERE]
     SHOW TABLE STATUS [FROM DB_NAME] [LIKE_OR_WHERE]
     SHOW [FULL] TABLES [FROM DB_NAME] [LIKE_OR_WHERE]
     SHOW TRIGGERS [FROM DB_NAME] [LIKE_OR_WHERE]
     SHOW [GLOBAL | SESSION] VARIABLES [LIKE_OR_WHERE]
     SHOW WARNINGS [LIMIT [OFFSET,] ROW_COUNT]

     LIKE_OR_WHERE:
         LIKE 'PATTERN'
       | WHERE EXPR

If the syntax for a given *note 'SHOW': show. statement includes a 'LIKE
'PATTERN'' part, ''PATTERN'' is a string that can contain the SQL '%'
and '_' wildcard characters.  The pattern is useful for restricting
statement output to matching values.

Several *note 'SHOW': show. statements also accept a 'WHERE' clause that
provides more flexibility in specifying which rows to display.  See
*note extended-show::.

Many MySQL APIs (such as PHP) enable you to treat the result returned
from a *note 'SHOW': show. statement as you would a result set from a
*note 'SELECT': select.; see *note connectors-apis::, or your API
documentation for more information.  In addition, you can work in SQL
with results from queries on tables in the 'INFORMATION_SCHEMA'
database, which you cannot easily do with results from *note 'SHOW':
show. statements.  See *note information-schema::.


File: manual.info.tmp,  Node: show-authors,  Next: show-binary-logs,  Prev: show,  Up: show

13.7.5.1 SHOW AUTHORS Statement
...............................

     SHOW AUTHORS

The *note 'SHOW AUTHORS': show-authors. statement displays information
about the people who work on MySQL. For each author, it displays 'Name',
'Location', and 'Comment' values.

This statement is deprecated as of MySQL 5.5.29 and is removed in MySQL
5.6.


File: manual.info.tmp,  Node: show-binary-logs,  Next: show-binlog-events,  Prev: show-authors,  Up: show

13.7.5.2 SHOW BINARY LOGS Statement
...................................

     SHOW BINARY LOGS
     SHOW MASTER LOGS

Lists the binary log files on the server.  This statement is used as
part of the procedure described in *note purge-binary-logs::, that shows
how to determine which logs can be purged.

     mysql> SHOW BINARY LOGS;
     +---------------+-----------+
     | Log_name      | File_size |
     +---------------+-----------+
     | binlog.000015 |    724935 |
     | binlog.000016 |    733481 |
     +---------------+-----------+

*note 'SHOW MASTER LOGS': show-binary-logs. is equivalent to *note 'SHOW
BINARY LOGS': show-binary-logs.

In MySQL 5.5.24 and earlier, the 'SUPER' privilege was required to use
this statement.  Starting with MySQL 5.5.25, a user with the
'REPLICATION CLIENT' privilege may also execute this statement.


File: manual.info.tmp,  Node: show-binlog-events,  Next: show-character-set,  Prev: show-binary-logs,  Up: show

13.7.5.3 SHOW BINLOG EVENTS Statement
.....................................

     SHOW BINLOG EVENTS
        [IN 'LOG_NAME']
        [FROM POS]
        [LIMIT [OFFSET,] ROW_COUNT]

Shows the events in the binary log.  If you do not specify ''LOG_NAME'',
the first binary log is displayed.

The 'LIMIT' clause has the same syntax as for the *note 'SELECT':
select. statement.  See *note select::.

*Note*:

Issuing a *note 'SHOW BINLOG EVENTS': show-binlog-events. with no
'LIMIT' clause could start a very time- and resource-consuming process
because the server returns to the client the complete contents of the
binary log (which includes all statements executed by the server that
modify data).  As an alternative to *note 'SHOW BINLOG EVENTS':
show-binlog-events, use the *note 'mysqlbinlog': mysqlbinlog. utility to
save the binary log to a text file for later examination and analysis.
See *note mysqlbinlog::.

*note 'SHOW BINLOG EVENTS': show-binlog-events. displays the following
fields for each event in the binary log:

   * 'Log_name'

     The name of the file that is being listed.

   * 'Pos'

     The position at which the event occurs.

   * 'Event_type'

     An identifier that describes the event type.

   * 'Server_id'

     The server ID of the server on which the event originated.

   * 'End_log_pos'

     The position at which the next event begins, which is equal to
     'Pos' plus the size of the event.

   * 'Info'

     More detailed information about the event type.  The format of this
     information depends on the event type.

*Note*:

Some events relating to the setting of user and system variables are not
included in the output from *note 'SHOW BINLOG EVENTS':
show-binlog-events.  To get complete coverage of events within a binary
log, use *note 'mysqlbinlog': mysqlbinlog.

*Note*:

*note 'SHOW BINLOG EVENTS': show-binlog-events. does _not_ work with
relay log files.  You can use *note 'SHOW RELAYLOG EVENTS':
show-relaylog-events. for this purpose.


File: manual.info.tmp,  Node: show-character-set,  Next: show-collation,  Prev: show-binlog-events,  Up: show

13.7.5.4 SHOW CHARACTER SET Statement
.....................................

     SHOW CHARACTER SET
         [LIKE 'PATTERN' | WHERE EXPR]

The *note 'SHOW CHARACTER SET': show-character-set. statement shows all
available character sets.  The 'LIKE' clause, if present, indicates
which character set names to match.  The 'WHERE' clause can be given to
select rows using more general conditions, as discussed in *note
extended-show::.  For example:

     mysql> SHOW CHARACTER SET LIKE 'latin%';
     +---------+-----------------------------+-------------------+--------+
     | Charset | Description                 | Default collation | Maxlen |
     +---------+-----------------------------+-------------------+--------+
     | latin1  | cp1252 West European        | latin1_swedish_ci |      1 |
     | latin2  | ISO 8859-2 Central European | latin2_general_ci |      1 |
     | latin5  | ISO 8859-9 Turkish          | latin5_turkish_ci |      1 |
     | latin7  | ISO 8859-13 Baltic          | latin7_general_ci |      1 |
     +---------+-----------------------------+-------------------+--------+

*note 'SHOW CHARACTER SET': show-character-set. output has these
columns:

   * 'Charset'

     The character set name.

   * 'Description'

     A description of the character set.

   * 'Default collation'

     The default collation for the character set.

   * 'Maxlen'

     The maximum number of bytes required to store one character.

The 'filename' character set is for internal use only; consequently,
*note 'SHOW CHARACTER SET': show-character-set. does not display it.

Character set information is also available from the
'INFORMATION_SCHEMA' *note 'CHARACTER_SETS': character-sets-table.
table.  See *note character-sets-table::.


File: manual.info.tmp,  Node: show-collation,  Next: show-columns,  Prev: show-character-set,  Up: show

13.7.5.5 SHOW COLLATION Statement
.................................

     SHOW COLLATION
         [LIKE 'PATTERN' | WHERE EXPR]

This statement lists collations supported by the server.  By default,
the output from *note 'SHOW COLLATION': show-collation. includes all
available collations.  The 'LIKE' clause, if present, indicates which
collation names to match.  The 'WHERE' clause can be given to select
rows using more general conditions, as discussed in *note
extended-show::.  For example:

     mysql> SHOW COLLATION WHERE Charset = 'latin1';
     +-------------------+---------+----+---------+----------+---------+
     | Collation         | Charset | Id | Default | Compiled | Sortlen |
     +-------------------+---------+----+---------+----------+---------+
     | latin1_german1_ci | latin1  |  5 |         | Yes      |       1 |
     | latin1_swedish_ci | latin1  |  8 | Yes     | Yes      |       1 |
     | latin1_danish_ci  | latin1  | 15 |         | Yes      |       1 |
     | latin1_german2_ci | latin1  | 31 |         | Yes      |       2 |
     | latin1_bin        | latin1  | 47 |         | Yes      |       1 |
     | latin1_general_ci | latin1  | 48 |         | Yes      |       1 |
     | latin1_general_cs | latin1  | 49 |         | Yes      |       1 |
     | latin1_spanish_ci | latin1  | 94 |         | Yes      |       1 |
     +-------------------+---------+----+---------+----------+---------+

*note 'SHOW COLLATION': show-collation. output has these columns:

   * 'Collation'

     The collation name.

   * 'Charset'

     The name of the character set with which the collation is
     associated.

   * 'Id'

     The collation ID.

   * 'Default'

     Whether the collation is the default for its character set.

   * 'Compiled'

     Whether the character set is compiled into the server.

   * 'Sortlen'

     This is related to the amount of memory required to sort strings
     expressed in the character set.

To see the default collation for each character set, use the following
statement.  'Default' is a reserved word, so to use it as an identifier,
it must be quoted as such:

     mysql> SHOW COLLATION WHERE `Default` = 'Yes';
     +---------------------+----------+----+---------+----------+---------+
     | Collation           | Charset  | Id | Default | Compiled | Sortlen |
     +---------------------+----------+----+---------+----------+---------+
     | big5_chinese_ci     | big5     |  1 | Yes     | Yes      |       1 |
     | dec8_swedish_ci     | dec8     |  3 | Yes     | Yes      |       1 |
     | cp850_general_ci    | cp850    |  4 | Yes     | Yes      |       1 |
     | hp8_english_ci      | hp8      |  6 | Yes     | Yes      |       1 |
     | koi8r_general_ci    | koi8r    |  7 | Yes     | Yes      |       1 |
     | latin1_swedish_ci   | latin1   |  8 | Yes     | Yes      |       1 |
     ...

Collation information is also available from the 'INFORMATION_SCHEMA'
*note 'COLLATIONS': collations-table. table.  See *note
collations-table::.


File: manual.info.tmp,  Node: show-columns,  Next: show-contributors,  Prev: show-collation,  Up: show

13.7.5.6 SHOW COLUMNS Statement
...............................

     SHOW [FULL] {COLUMNS | FIELDS}
         {FROM | IN} TBL_NAME
         [{FROM | IN} DB_NAME]
         [LIKE 'PATTERN' | WHERE EXPR]

*note 'SHOW COLUMNS': show-columns. displays information about the
columns in a given table.  It also works for views.  *note 'SHOW
COLUMNS': show-columns. displays information only for those columns for
which you have some privilege.

     mysql> SHOW COLUMNS FROM City;
     +-------------+----------+------+-----+---------+----------------+
     | Field       | Type     | Null | Key | Default | Extra          |
     +-------------+----------+------+-----+---------+----------------+
     | ID          | int(11)  | NO   | PRI | NULL    | auto_increment |
     | Name        | char(35) | NO   |     |         |                |
     | CountryCode | char(3)  | NO   | MUL |         |                |
     | District    | char(20) | NO   |     |         |                |
     | Population  | int(11)  | NO   |     | 0       |                |
     +-------------+----------+------+-----+---------+----------------+

An alternative to 'TBL_NAME FROM DB_NAME' syntax is DB_NAME.TBL_NAME.
These two statements are equivalent:

     SHOW COLUMNS FROM mytable FROM mydb;
     SHOW COLUMNS FROM mydb.mytable;

The optional 'FULL' keyword causes the output to include the column
collation and comments, as well as the privileges you have for each
column.

The 'LIKE' clause, if present, indicates which column names to match.
The 'WHERE' clause can be given to select rows using more general
conditions, as discussed in *note extended-show::.

The data types may differ from what you expect them to be based on a
*note 'CREATE TABLE': create-table. statement because MySQL sometimes
changes data types when you create or alter a table.  The conditions
under which this occurs are described in *note silent-column-changes::.

*note 'SHOW COLUMNS': show-columns. displays the following values for
each table column:

   * 'Field'

     The column name.

   * 'Type'

     The column data type.

   * 'Collation'

     The collation for nonbinary string columns, or 'NULL' for other
     columns.  This value is displayed only if you use the 'FULL'
     keyword.

   * 'Null'

     The column nullability.  The value is 'YES' if 'NULL' values can be
     stored in the column, 'NO' if not.

   * 'Key'

     Whether the column is indexed:

        * If 'Key' is empty, the column either is not indexed or is
          indexed only as a secondary column in a multiple-column,
          nonunique index.

        * If 'Key' is 'PRI', the column is a 'PRIMARY KEY' or is one of
          the columns in a multiple-column 'PRIMARY KEY'.

        * If 'Key' is 'UNI', the column is the first column of a
          'UNIQUE' index.  (A 'UNIQUE' index permits multiple 'NULL'
          values, but you can tell whether the column permits 'NULL' by
          checking the 'Null' field.)

        * If 'Key' is 'MUL', the column is the first column of a
          nonunique index in which multiple occurrences of a given value
          are permitted within the column.

     If more than one of the 'Key' values applies to a given column of a
     table, 'Key' displays the one with the highest priority, in the
     order 'PRI', 'UNI', 'MUL'.

     A 'UNIQUE' index may be displayed as 'PRI' if it cannot contain
     'NULL' values and there is no 'PRIMARY KEY' in the table.  A
     'UNIQUE' index may display as 'MUL' if several columns form a
     composite 'UNIQUE' index; although the combination of the columns
     is unique, each column can still hold multiple occurrences of a
     given value.

   * 'Default'

     The default value for the column.  This is 'NULL' if the column has
     an explicit default of 'NULL', or if the column definition includes
     no 'DEFAULT' clause.

   * 'Extra'

     Any additional information that is available about a given column.
     The value is nonempty in these cases: 'auto_increment' for columns
     that have the 'AUTO_INCREMENT' attribute; 'on update
     CURRENT_TIMESTAMP' for *note 'TIMESTAMP': datetime. columns that
     have the 'ON UPDATE CURRENT_TIMESTAMP' attribute.

   * 'Privileges'

     The privileges you have for the column.  This value is displayed
     only if you use the 'FULL' keyword.

   * 'Comment'

     Any comment included in the column definition.  This value is
     displayed only if you use the 'FULL' keyword.

Table column information is also available from the 'INFORMATION_SCHEMA'
*note 'COLUMNS': columns-table. table.  See *note columns-table::.

You can list a table's columns with the *note 'mysqlshow DB_NAME
TBL_NAME': mysqlshow. command.

The *note 'DESCRIBE': describe. statement provides information similar
to *note 'SHOW COLUMNS': show-columns.  See *note describe::.

The *note 'SHOW CREATE TABLE': show-create-table, *note 'SHOW TABLE
STATUS': show-table-status, and *note 'SHOW INDEX': show-index.
statements also provide information about tables.  See *note show::.


File: manual.info.tmp,  Node: show-contributors,  Next: show-create-database,  Prev: show-columns,  Up: show

13.7.5.7 SHOW CONTRIBUTORS Statement
....................................

     SHOW CONTRIBUTORS

The *note 'SHOW CONTRIBUTORS': show-contributors. statement displays
information about the people who contribute to MySQL source or to causes
that we support.  For each contributor, it displays 'Name', 'Location',
and 'Comment' values.

This statement is deprecated as of MySQL 5.5.29 and is removed in MySQL
5.6.


File: manual.info.tmp,  Node: show-create-database,  Next: show-create-event,  Prev: show-contributors,  Up: show

13.7.5.8 SHOW CREATE DATABASE Statement
.......................................

     SHOW CREATE {DATABASE | SCHEMA} [IF NOT EXISTS] DB_NAME

Shows the *note 'CREATE DATABASE': create-database. statement that
creates the named database.  If the 'SHOW' statement includes an 'IF NOT
EXISTS' clause, the output too includes such a clause.  *note 'SHOW
CREATE SCHEMA': show-create-database. is a synonym for *note 'SHOW
CREATE DATABASE': show-create-database.

     mysql> SHOW CREATE DATABASE test\G
     *************************** 1. row ***************************
            Database: test
     Create Database: CREATE DATABASE `test`
                      /*!40100 DEFAULT CHARACTER SET latin1 */

     mysql> SHOW CREATE SCHEMA test\G
     *************************** 1. row ***************************
            Database: test
     Create Database: CREATE DATABASE `test`
                      /*!40100 DEFAULT CHARACTER SET latin1 */

*note 'SHOW CREATE DATABASE': show-create-database. quotes table and
column names according to the value of the 'sql_quote_show_create'
option.  See *note server-system-variables::.


File: manual.info.tmp,  Node: show-create-event,  Next: show-create-function,  Prev: show-create-database,  Up: show

13.7.5.9 SHOW CREATE EVENT Statement
....................................

     SHOW CREATE EVENT EVENT_NAME

This statement displays the *note 'CREATE EVENT': create-event.
statement needed to re-create a given event.  It requires the 'EVENT'
privilege for the database from which the event is to be shown.  For
example (using the same event 'e_daily' defined and then altered in
*note show-events::):

     mysql> SHOW CREATE EVENT myschema.e_daily\G
     *************************** 1. row ***************************
                    Event: e_daily
                 sql_mode:
                time_zone: SYSTEM
             Create Event: CREATE DEFINER=`jon`@`ghidora` EVENT `e_daily`
                             ON SCHEDULE EVERY 1 DAY
                             STARTS CURRENT_TIMESTAMP + INTERVAL 6 HOUR
                             ON COMPLETION NOT PRESERVE
                             ENABLE
                             COMMENT 'Saves total number of sessions then
                                     clears the table each day'
                             DO BEGIN
                               INSERT INTO site_activity.totals (time, total)
                                 SELECT CURRENT_TIMESTAMP, COUNT(*)
                                   FROM site_activity.sessions;
                               DELETE FROM site_activity.sessions;
                             END
     character_set_client: utf8
     collation_connection: utf8_general_ci
       Database Collation: latin1_swedish_ci

'character_set_client' is the session value of the
'character_set_client' system variable when the event was created.
'collation_connection' is the session value of the
'collation_connection' system variable when the event was created.
'Database Collation' is the collation of the database with which the
event is associated.

Note that the output reflects the current status of the event ('ENABLE')
rather than the status with which it was created.


File: manual.info.tmp,  Node: show-create-function,  Next: show-create-procedure,  Prev: show-create-event,  Up: show

13.7.5.10 SHOW CREATE FUNCTION Statement
........................................

     SHOW CREATE FUNCTION FUNC_NAME

This statement is similar to *note 'SHOW CREATE PROCEDURE':
show-create-procedure. but for stored functions.  See *note
show-create-procedure::.


File: manual.info.tmp,  Node: show-create-procedure,  Next: show-create-table,  Prev: show-create-function,  Up: show

13.7.5.11 SHOW CREATE PROCEDURE Statement
.........................................

     SHOW CREATE PROCEDURE PROC_NAME

This statement is a MySQL extension.  It returns the exact string that
can be used to re-create the named stored procedure.  A similar
statement, *note 'SHOW CREATE FUNCTION': show-create-function, displays
information about stored functions (see *note show-create-function::).

To use either statement, you must be the user named in the routine
'DEFINER' clause or have *note 'SELECT': select. access to the
'mysql.proc' table.  If you do not have privileges for the routine
itself, the value displayed for the 'Create Procedure' or 'Create
Function' field will be 'NULL'.

     mysql> SHOW CREATE PROCEDURE test.simpleproc\G
     *************************** 1. row ***************************
                Procedure: simpleproc
                 sql_mode:
         Create Procedure: CREATE PROCEDURE `simpleproc`(OUT param1 INT)
                           BEGIN
                           SELECT COUNT(*) INTO param1 FROM t;
                           END
     character_set_client: utf8
     collation_connection: utf8_general_ci
       Database Collation: latin1_swedish_ci

     mysql> SHOW CREATE FUNCTION test.hello\G
     *************************** 1. row ***************************
                 Function: hello
                 sql_mode:
          Create Function: CREATE FUNCTION `hello`(s CHAR(20))
                           RETURNS CHAR(50)
                           RETURN CONCAT('Hello, ',s,'!')
     character_set_client: utf8
     collation_connection: utf8_general_ci
       Database Collation: latin1_swedish_ci

'character_set_client' is the session value of the
'character_set_client' system variable when the routine was created.
'collation_connection' is the session value of the
'collation_connection' system variable when the routine was created.
'Database Collation' is the collation of the database with which the
routine is associated.


File: manual.info.tmp,  Node: show-create-table,  Next: show-create-trigger,  Prev: show-create-procedure,  Up: show

13.7.5.12 SHOW CREATE TABLE Statement
.....................................

     SHOW CREATE TABLE TBL_NAME

Shows the *note 'CREATE TABLE': create-table. statement that creates the
named table.  To use this statement, you must have some privilege for
the table.  This statement also works with views.

     mysql> SHOW CREATE TABLE t\G
     *************************** 1. row ***************************
            Table: t
     Create Table: CREATE TABLE `t` (
       `id` int(11) NOT NULL AUTO_INCREMENT,
       `s` char(60) DEFAULT NULL,
       PRIMARY KEY (`id`)
     ) ENGINE=InnoDB DEFAULT CHARSET=latin1

*note 'SHOW CREATE TABLE': show-create-table. quotes table and column
names according to the value of the 'sql_quote_show_create' option.  See
*note server-system-variables::.

For information about how *note 'CREATE TABLE': create-table. statements
are stored by MySQL, see *note create-table-statement-retention::.


File: manual.info.tmp,  Node: show-create-trigger,  Next: show-create-view,  Prev: show-create-table,  Up: show

13.7.5.13 SHOW CREATE TRIGGER Statement
.......................................

     SHOW CREATE TRIGGER TRIGGER_NAME

This statement shows the *note 'CREATE TRIGGER': create-trigger.
statement that creates the named trigger.  This statement requires the
'TRIGGER' privilege for the table associated with the trigger.

     mysql> SHOW CREATE TRIGGER ins_sum\G
     *************************** 1. row ***************************
                    Trigger: ins_sum
                   sql_mode:
     SQL Original Statement: CREATE DEFINER=`me`@`localhost` TRIGGER ins_sum
                             BEFORE INSERT ON account
                             FOR EACH ROW SET @sum = @sum + NEW.amount
       character_set_client: utf8
       collation_connection: utf8_general_ci
         Database Collation: latin1_swedish_ci

*note 'SHOW CREATE TRIGGER': show-create-trigger. output has these
columns:

   * 'Trigger': The trigger name.

   * 'sql_mode': The SQL mode in effect when the trigger executes.

   * 'SQL Original Statement': The *note 'CREATE TRIGGER':
     create-trigger. statement that defines the trigger.

   * 'character_set_client': The session value of the
     'character_set_client' system variable when the trigger was
     created.

   * 'collation_connection': The session value of the
     'collation_connection' system variable when the trigger was
     created.

   * 'Database Collation': The collation of the database with which the
     trigger is associated.

Trigger information is also available from the 'INFORMATION_SCHEMA'
*note 'TRIGGERS': triggers-table. table.  See *note triggers-table::.


File: manual.info.tmp,  Node: show-create-view,  Next: show-databases,  Prev: show-create-trigger,  Up: show

13.7.5.14 SHOW CREATE VIEW Statement
....................................

     SHOW CREATE VIEW VIEW_NAME

This statement shows the *note 'CREATE VIEW': create-view. statement
that creates the named view.

     mysql> SHOW CREATE VIEW v\G
     *************************** 1. row ***************************
                     View: v
              Create View: CREATE ALGORITHM=UNDEFINED
                           DEFINER=`bob`@`localhost`
                           SQL SECURITY DEFINER VIEW
                           `v` AS select 1 AS `a`,2 AS `b`
     character_set_client: utf8
     collation_connection: utf8_general_ci

'character_set_client' is the session value of the
'character_set_client' system variable when the view was created.
'collation_connection' is the session value of the
'collation_connection' system variable when the view was created.

Use of *note 'SHOW CREATE VIEW': show-create-view. requires the 'SHOW
VIEW' privilege, and the 'SELECT' privilege for the view in question.

View information is also available from the 'INFORMATION_SCHEMA' *note
'VIEWS': views-table. table.  See *note views-table::.

MySQL lets you use different 'sql_mode' settings to tell the server the
type of SQL syntax to support.  For example, you might use the 'ANSI'
SQL mode to ensure MySQL correctly interprets the standard SQL
concatenation operator, the double bar ('||'), in your queries.  If you
then create a view that concatenates items, you might worry that
changing the 'sql_mode' setting to a value different from 'ANSI' could
cause the view to become invalid.  But this is not the case.  No matter
how you write out a view definition, MySQL always stores it the same
way, in a canonical form.  Here is an example that shows how the server
changes a double bar concatenation operator to a 'CONCAT()' function:

     mysql> SET sql_mode = 'ANSI';
     Query OK, 0 rows affected (0.00 sec)

     mysql> CREATE VIEW test.v AS SELECT 'a' || 'b' as col1;
     Query OK, 0 rows affected (0.01 sec)

     mysql> SHOW CREATE VIEW test.v\G
     *************************** 1. row ***************************
                     View: v
              Create View: CREATE VIEW "v" AS select concat('a','b') AS "col1"
     ...
     1 row in set (0.00 sec)

The advantage of storing a view definition in canonical form is that
changes made later to the value of 'sql_mode' will not affect the
results from the view.  However an additional consequence is that
comments prior to *note 'SELECT': select. are stripped from the
definition by the server.


File: manual.info.tmp,  Node: show-databases,  Next: show-engine,  Prev: show-create-view,  Up: show

13.7.5.15 SHOW DATABASES Statement
..................................

     SHOW {DATABASES | SCHEMAS}
         [LIKE 'PATTERN' | WHERE EXPR]

*note 'SHOW DATABASES': show-databases. lists the databases on the MySQL
server host.  *note 'SHOW SCHEMAS': show-databases. is a synonym for
*note 'SHOW DATABASES': show-databases.  The 'LIKE' clause, if present,
indicates which database names to match.  The 'WHERE' clause can be
given to select rows using more general conditions, as discussed in
*note extended-show::.

You see only those databases for which you have some kind of privilege,
unless you have the global *note 'SHOW DATABASES': show-databases.
privilege.  You can also get this list using the *note 'mysqlshow':
mysqlshow. command.

If the server was started with the '--skip-show-database' option, you
cannot use this statement at all unless you have the 'SHOW DATABASES'
privilege.

MySQL implements databases as directories in the data directory, so this
statement simply lists directories in that location.  However, the
output may include names of directories that do not correspond to actual
databases.

Database information is also available from the 'INFORMATION_SCHEMA'
*note 'SCHEMATA': schemata-table. table.  See *note schemata-table::.

*Caution*:

Because a global privilege is considered a privilege for all databases,
_any_ global privilege enables a user to see all database names with
*note 'SHOW DATABASES': show-databases. or by examining the
'INFORMATION_SCHEMA' *note 'SCHEMATA': schemata-table. table.


File: manual.info.tmp,  Node: show-engine,  Next: show-engines,  Prev: show-databases,  Up: show

13.7.5.16 SHOW ENGINE Statement
...............................

     SHOW ENGINE ENGINE_NAME {STATUS | MUTEX}

*note 'SHOW ENGINE': show-engine. displays operational information about
a storage engine.  It requires the 'PROCESS' privilege.  The statement
has these variants:

     SHOW ENGINE INNODB STATUS
     SHOW ENGINE INNODB MUTEX
     SHOW ENGINE {NDB | NDBCLUSTER} STATUS
     SHOW ENGINE PERFORMANCE_SCHEMA STATUS

*note 'SHOW ENGINE INNODB STATUS': show-engine. displays extensive
information from the standard 'InnoDB' Monitor about the state of the
'InnoDB' storage engine.  For information about the standard monitor and
other 'InnoDB' Monitors that provide information about 'InnoDB'
processing, see *note innodb-monitors::.

*note 'SHOW ENGINE INNODB MUTEX': show-engine. displays 'InnoDB' mutex
and rw-lock statistics.  Statement output has the following columns:

   * 'Type'

     Always 'InnoDB'.

   * 'Name'

     The source file where the mutex is implemented, and the line number
     in the file where the mutex is created.  The line number is
     specific to your version of MySQL.

   * 'Status'

     The mutex status.  This field displays several values if
     'UNIV_DEBUG' was defined at MySQL compilation time (for example, in
     'include/univ.i' in the 'InnoDB' part of the MySQL source tree).
     If 'UNIV_DEBUG' was not defined, the statement displays only the
     'os_waits' value.  In the latter case (without 'UNIV_DEBUG'), the
     information on which the output is based is insufficient to
     distinguish regular mutexes and mutexes that protect rw-locks
     (which permit multiple readers or a single writer).  Consequently,
     the output may appear to contain multiple rows for the same mutex.

        * 'count' indicates how many times the mutex was requested.

        * 'spin_waits' indicates how many times the spinlock had to run.

        * 'spin_rounds' indicates the number of spinlock rounds.
          ('spin_rounds' divided by 'spin_waits' provides the average
          round count.)

        * 'os_waits' indicates the number of operating system waits.
          This occurs when the spinlock did not work (the mutex was not
          locked during the spinlock and it was necessary to yield to
          the operating system and wait).

        * 'os_yields' indicates the number of times a thread trying to
          lock a mutex gave up its timeslice and yielded to the
          operating system (on the presumption that permitting other
          threads to run will free the mutex so that it can be locked).

        * 'os_wait_times' indicates the amount of time (in ms) spent in
          operating system waits.  In MySQL 5.5 timing is disabled and
          this value is always 0.

'SHOW ENGINE INNODB MUTEX' skips the mutexes and rw-locks of buffer pool
blocks, as the amount of output can be overwhelming on systems with a
large buffer pool.  (There is one mutex and one rw-lock in each 16K
buffer pool block, and there are 65,536 blocks per gigabyte.)  'SHOW
ENGINE INNODB MUTEX' also does not list any mutexes or rw-locks that
have never been waited on ('os_waits=0').  Thus, 'SHOW ENGINE INNODB
MUTEX' only displays information about mutexes and rw-locks outside of
the buffer pool that have caused at least one OS-level wait.

'SHOW ENGINE INNODB MUTEX' information can be used to diagnose system
problems.  For example, large values of 'spin_waits' and 'spin_rounds'
may indicate scalability problems.

Use *note 'SHOW ENGINE PERFORMANCE_SCHEMA STATUS': show-engine. to
inspect the internal operation of the Performance Schema code:

     mysql> SHOW ENGINE PERFORMANCE_SCHEMA STATUS\G
     ...
     *************************** 3. row ***************************
       Type: performance_schema
       Name: events_waits_history.row_size
     Status: 76
     *************************** 4. row ***************************
       Type: performance_schema
       Name: events_waits_history.row_count
     Status: 10000
     *************************** 5. row ***************************
       Type: performance_schema
       Name: events_waits_history.memory
     Status: 760000
     ...
     *************************** 57. row ***************************
       Type: performance_schema
       Name: performance_schema.memory
     Status: 26459600
     ...

This statement is intended to help the DBA understand the effects that
different Performance Schema options have on memory requirements.

'Name' values consist of two parts, which name an internal buffer and a
buffer attribute, respectively.  Interpret buffer names as follows:

   * An internal buffer that is not exposed as a table is named within
     parentheses.  Examples: '(pfs_cond_class).row_size',
     '(pfs_mutex_class).memory'.

   * An internal buffer that is exposed as a table in the
     'performance_schema' database is named after the table, without
     parentheses.  Examples: 'events_waits_history.row_size',
     'mutex_instances.row_count'.

   * A value that applies to the Performance Schema as a whole begins
     with 'performance_schema'.  Example: 'performance_schema.memory'.

Buffer attributes have these meanings:

   * 'row_size' is the size of the internal record used by the
     implementation, such as the size of a row in a table.  'row_size'
     values cannot be changed.

   * 'row_count' is the number of internal records, such as the number
     of rows in a table.  'row_count' values can be changed using
     Performance Schema configuration options.

   * For a table, 'TBL_NAME.memory' is the product of 'row_size' and
     'row_count'.  For the Performance Schema as a whole,
     'performance_schema.memory' is the sum of all the memory used (the
     sum of all other 'memory' values).

In some cases, there is a direct relationship between a Performance
Schema configuration parameter and a 'SHOW ENGINE' value.  For example,
'events_waits_history_long.row_count' corresponds to
'performance_schema_events_waits_history_long_size'.  In other cases,
the relationship is more complex.  For example,
'events_waits_history.row_count' corresponds to
'performance_schema_events_waits_history_size' (the number of rows per
thread) multiplied by 'performance_schema_max_thread_instances' ( the
number of threads).

If the server has the *note 'NDB': mysql-cluster. storage engine
enabled, *note 'SHOW ENGINE NDB STATUS': show-engine. displays cluster
status information such as the number of connected data nodes, the
cluster connectstring, and cluster binary log epochs, as well as counts
of various Cluster API objects created by the MySQL Server when
connected to the cluster.  Sample output from this statement is shown
here:

     mysql> SHOW ENGINE NDB STATUS;
     +------------+-----------------------+--------------------------------------------------+
     | Type       | Name                  | Status                                           |
     +------------+-----------------------+--------------------------------------------------+
     | ndbcluster | connection            | cluster_node_id=7,
       connected_host=198.51.100.103, connected_port=1186, number_of_data_nodes=4,
       number_of_ready_data_nodes=3, connect_count=0                                         |
     | ndbcluster | NdbTransaction        | created=6, free=0, sizeof=212                    |
     | ndbcluster | NdbOperation          | created=8, free=8, sizeof=660                    |
     | ndbcluster | NdbIndexScanOperation | created=1, free=1, sizeof=744                    |
     | ndbcluster | NdbIndexOperation     | created=0, free=0, sizeof=664                    |
     | ndbcluster | NdbRecAttr            | created=1285, free=1285, sizeof=60               |
     | ndbcluster | NdbApiSignal          | created=16, free=16, sizeof=136                  |
     | ndbcluster | NdbLabel              | created=0, free=0, sizeof=196                    |
     | ndbcluster | NdbBranch             | created=0, free=0, sizeof=24                     |
     | ndbcluster | NdbSubroutine         | created=0, free=0, sizeof=68                     |
     | ndbcluster | NdbCall               | created=0, free=0, sizeof=16                     |
     | ndbcluster | NdbBlob               | created=1, free=1, sizeof=264                    |
     | ndbcluster | NdbReceiver           | created=4, free=0, sizeof=68                     |
     | ndbcluster | binlog                | latest_epoch=155467, latest_trans_epoch=148126,
       latest_received_binlog_epoch=0, latest_handled_binlog_epoch=0,
       latest_applied_binlog_epoch=0                                                         |
     +------------+-----------------------+--------------------------------------------------+

The 'Status' column in each of these rows provides information about the
MySQL server's connection to the cluster and about the cluster binary
log's status, respectively.  The 'Status' information is in the form of
comma-delimited set of name/value pairs.

The 'connection' row's 'Status' column contains the name/value pairs
described in the following table.

Name                          Value
                              
'cluster_node_id'             The node ID of the MySQL server in the
                              cluster
                              
'connected_host'              The host name or IP address of the
                              cluster management server to which the
                              MySQL server is connected
                              
'connected_port'              The port used by the MySQL server to
                              connect to the management server
                              ('connected_host')
                              
'number_of_data_nodes'        The number of data nodes configured for
                              the cluster (that is, the number of
                              '[ndbd]' sections in the cluster
                              'config.ini' file)
                              
'number_of_ready_data_nodes'  The number of data nodes in the cluster
                              that are actually running
                              
'connect_count'               The number of times this
                              *note 'mysqld': mysqld. has connected or
                              reconnected to cluster data nodes

The 'binlog' row's 'Status' column contains information relating to NDB
Cluster Replication.  The name/value pairs it contains are described in
the following table.

Name                          Value
                              
'latest_epoch'                The most recent epoch most recently run
                              on this MySQL server (that is, the
                              sequence number of the most recent
                              transaction run on the server)
                              
'latest_trans_epoch'          The most recent epoch processed by the
                              cluster's data nodes
                              
'latest_received_binlog_epoch'The most recent epoch received by the
                              binary log thread
                              
'latest_handled_binlog_epoch' The most recent epoch processed by the
                              binary log thread (for writing to the
                              binary log)
                              
'latest_applied_binlog_epoch' The most recent epoch actually written to
                              the binlog

See *note mysql-cluster-replication::, for more information.

The remaining rows from the output of *note 'SHOW ENGINE NDB STATUS':
show-engine. which are most likely to prove useful in monitoring the
cluster are listed here by 'Name':

   * 'NdbTransaction': The number and size of 'NdbTransaction' objects
     that have been created.  An 'NdbTransaction' is created each time a
     table schema operation (such as *note 'CREATE TABLE': create-table.
     or *note 'ALTER TABLE': alter-table.) is performed on an *note
     'NDB': mysql-cluster. table.

   * 'NdbOperation': The number and size of 'NdbOperation' objects that
     have been created.

   * 'NdbIndexScanOperation': The number and size of
     'NdbIndexScanOperation' objects that have been created.

   * 'NdbIndexOperation': The number and size of 'NdbIndexOperation'
     objects that have been created.

   * 'NdbRecAttr': The number and size of 'NdbRecAttr' objects that have
     been created.  In general, one of these is created each time a data
     manipulation statement is performed by an SQL node.

   * 'NdbBlob': The number and size of 'NdbBlob' objects that have been
     created.  An 'NdbBlob' is created for each new operation involving
     a *note 'BLOB': blob. column in an *note 'NDB': mysql-cluster.
     table.

   * 'NdbReceiver': The number and size of any 'NdbReceiver' object that
     have been created.  The number in the 'created' column is the same
     as the number of data nodes in the cluster to which the MySQL
     server has connected.

*Note*:

*note 'SHOW ENGINE NDB STATUS': show-engine. returns an empty result if
no operations involving *note 'NDB': mysql-cluster. tables have been
performed during the current session by the MySQL client accessing the
SQL node on which this statement is run.


File: manual.info.tmp,  Node: show-engines,  Next: show-errors,  Prev: show-engine,  Up: show

13.7.5.17 SHOW ENGINES Statement
................................

     SHOW [STORAGE] ENGINES

*note 'SHOW ENGINES': show-engines. displays status information about
the server's storage engines.  This is particularly useful for checking
whether a storage engine is supported, or to see what the default engine
is.

For information about MySQL storage engines, see *note
innodb-storage-engine::, and *note storage-engines::.

     mysql> SHOW ENGINES\G
     *************************** 1. row ***************************
           Engine: FEDERATED
          Support: NO
          Comment: Federated MySQL storage engine
     Transactions: NULL
               XA: NULL
       Savepoints: NULL
     *************************** 2. row ***************************
           Engine: MRG_MYISAM
          Support: YES
          Comment: Collection of identical MyISAM tables
     Transactions: NO
               XA: NO
       Savepoints: NO
     *************************** 3. row ***************************
           Engine: MyISAM
          Support: YES
          Comment: MyISAM storage engine
     Transactions: NO
               XA: NO
       Savepoints: NO
     *************************** 4. row ***************************
           Engine: BLACKHOLE
          Support: YES
          Comment: /dev/null storage engine (anything you write to it disappears)
     Transactions: NO
               XA: NO
       Savepoints: NO
     *************************** 5. row ***************************
           Engine: CSV
          Support: YES
          Comment: CSV storage engine
     Transactions: NO
               XA: NO
       Savepoints: NO
     *************************** 6. row ***************************
           Engine: MEMORY
          Support: YES
          Comment: Hash based, stored in memory, useful for temporary tables
     Transactions: NO
               XA: NO
       Savepoints: NO
     *************************** 7. row ***************************
           Engine: ARCHIVE
          Support: YES
          Comment: Archive storage engine
     Transactions: NO
               XA: NO
       Savepoints: NO
     *************************** 8. row ***************************
           Engine: InnoDB
          Support: DEFAULT
          Comment: Supports transactions, row-level locking, and foreign keys
     Transactions: YES
               XA: YES
       Savepoints: YES
     *************************** 9. row ***************************
           Engine: PERFORMANCE_SCHEMA
          Support: YES
          Comment: Performance Schema
     Transactions: NO
               XA: NO
       Savepoints: NO

The output from *note 'SHOW ENGINES': show-engines. may vary according
to the MySQL version used and other factors.

*note 'SHOW ENGINES': show-engines. output has these columns:

   * 'Engine'

     The name of the storage engine.

   * 'Support'

     The server's level of support for the storage engine, as shown in
     the following table.

     Value       Meaning
                 
     'YES'       The engine is supported and is active
                 
     'DEFAULT'   Like 'YES', plus this is the default engine
                 
     'NO'        The engine is not supported
                 
     'DISABLED'  The engine is supported but has been disabled

     A value of 'NO' means that the server was compiled without support
     for the engine, so it cannot be enabled at runtime.

     A value of 'DISABLED' occurs either because the server was started
     with an option that disables the engine, or because not all options
     required to enable it were given.  In the latter case, the error
     log should contain a reason indicating why the option is disabled.
     See *note error-log::.

     You might also see 'DISABLED' for a storage engine if the server
     was compiled to support it, but was started with a
     '--skip-ENGINE_NAME' option.  For the *note 'NDB': mysql-cluster.
     storage engine, 'DISABLED' means the server was compiled with
     support for NDB Cluster, but was not started with the
     '--ndbcluster' option.

     All MySQL servers support 'MyISAM' tables.  It is not possible to
     disable 'MyISAM'.

   * 'Comment'

     A brief description of the storage engine.

   * 'Transactions'

     Whether the storage engine supports transactions.

   * 'XA'

     Whether the storage engine supports XA transactions.

   * 'Savepoints'

     Whether the storage engine supports savepoints.

Storage engine information is also available from the
'INFORMATION_SCHEMA' *note 'ENGINES': engines-table. table.  See *note
engines-table::.


File: manual.info.tmp,  Node: show-errors,  Next: show-events,  Prev: show-engines,  Up: show

13.7.5.18 SHOW ERRORS Statement
...............................

     SHOW ERRORS [LIMIT [OFFSET,] ROW_COUNT]
     SHOW COUNT(*) ERRORS

*note 'SHOW ERRORS': show-errors. is a diagnostic statement that is
similar to *note 'SHOW WARNINGS': show-warnings, except that it displays
information only for errors, rather than for errors, warnings, and
notes.

The 'LIMIT' clause has the same syntax as for the *note 'SELECT':
select. statement.  See *note select::.

The *note 'SHOW COUNT(*) ERRORS': show-errors. statement displays the
number of errors.  You can also retrieve this number from the
'error_count' variable:

     SHOW COUNT(*) ERRORS;
     SELECT @@error_count;

*note 'SHOW ERRORS': show-errors. and 'error_count' apply only to
errors, not warnings or notes.  In other respects, they are similar to
*note 'SHOW WARNINGS': show-warnings. and 'warning_count'.  In
particular, *note 'SHOW ERRORS': show-errors. cannot display information
for more than 'max_error_count' messages, and 'error_count' can exceed
the value of 'max_error_count' if the number of errors exceeds
'max_error_count'.

For more information, see *note show-warnings::.


File: manual.info.tmp,  Node: show-events,  Next: show-function-code,  Prev: show-errors,  Up: show

13.7.5.19 SHOW EVENTS Statement
...............................

     SHOW EVENTS
         [{FROM | IN} SCHEMA_NAME]
         [LIKE 'PATTERN' | WHERE EXPR]

This statement displays information about Event Manager events, which
are discussed in *note event-scheduler::.  It requires the 'EVENT'
privilege for the database from which the events are to be shown.

In its simplest form, *note 'SHOW EVENTS': show-events. lists all of the
events in the current schema:

     mysql> SELECT CURRENT_USER(), SCHEMA();
     +----------------+----------+
     | CURRENT_USER() | SCHEMA() |
     +----------------+----------+
     | jon@ghidora    | myschema |
     +----------------+----------+
     1 row in set (0.00 sec)

     mysql> SHOW EVENTS\G
     *************************** 1. row ***************************
                       Db: myschema
                     Name: e_daily
                  Definer: jon@ghidora
                Time zone: SYSTEM
                     Type: RECURRING
               Execute at: NULL
           Interval value: 1
           Interval field: DAY
                   Starts: 2018-08-08 11:06:34
                     Ends: NULL
                   Status: ENABLED
               Originator: 1
     character_set_client: utf8
     collation_connection: utf8_general_ci
       Database Collation: latin1_swedish_ci

To see events for a specific schema, use the 'FROM' clause.  For
example, to see events for the 'test' schema, use the following
statement:

     SHOW EVENTS FROM test;

The 'LIKE' clause, if present, indicates which event names to match.
The 'WHERE' clause can be given to select rows using more general
conditions, as discussed in *note extended-show::.

*note 'SHOW EVENTS': show-events. output has these columns:

   * 'Db'

     The name of the schema (database) to which the event belongs.

   * 'Name'

     The name of the event.

   * 'Definer'

     The account of the user who created the event, in
     ''USER_NAME'@'HOST_NAME'' format.

   * 'Time zone'

     The event time zone, which is the time zone used for scheduling the
     event and that is in effect within the event as it executes.  The
     default value is 'SYSTEM'.

   * 'Type'

     The event repetition type, either 'ONE TIME' (transient) or
     'RECURRING' (repeating).

   * 'Execute At'

     For a one-time event, this is the *note 'DATETIME': datetime. value
     specified in the 'AT' clause of the *note 'CREATE EVENT':
     create-event. statement used to create the event, or of the last
     *note 'ALTER EVENT': alter-event. statement that modified the
     event.  The value shown in this column reflects the addition or
     subtraction of any 'INTERVAL' value included in the event's 'AT'
     clause.  For example, if an event is created using 'ON SCHEDULE AT
     CURRENT_TIMESTAMP + '1:6' DAY_HOUR', and the event was created at
     2018-02-09 14:05:30, the value shown in this column would be
     ''2018-02-10 20:05:30''.  If the event's timing is determined by an
     'EVERY' clause instead of an 'AT' clause (that is, if the event is
     recurring), the value of this column is 'NULL'.

   * 'Interval Value'

     For a recurring event, the number of intervals to wait between
     event executions.  For a transient event, the value of this column
     is always 'NULL'.

   * 'Interval Field'

     The time units used for the interval which a recurring event waits
     before repeating.  For a transient event, the value of this column
     is always 'NULL'.

   * 'Starts'

     The start date and time for a recurring event.  This is displayed
     as a *note 'DATETIME': datetime. value, and is 'NULL' if no start
     date and time are defined for the event.  For a transient event,
     this column is always 'NULL'.  For a recurring event whose
     definition includes a 'STARTS' clause, this column contains the
     corresponding *note 'DATETIME': datetime. value.  As with the
     'Execute At' column, this value resolves any expressions used.  If
     there is no 'STARTS' clause affecting the timing of the event, this
     column is 'NULL'

   * 'Ends'

     For a recurring event whose definition includes a 'ENDS' clause,
     this column contains the corresponding *note 'DATETIME': datetime.
     value.  As with the 'Execute At' column, this value resolves any
     expressions used.  If there is no 'ENDS' clause affecting the
     timing of the event, this column is 'NULL'.

   * 'Status'

     The event status.  One of 'ENABLED', 'DISABLED', or
     'SLAVESIDE_DISABLED'.  'SLAVESIDE_DISABLED' indicates that the
     creation of the event occurred on another MySQL server acting as a
     replication master and replicated to the current MySQL server which
     is acting as a slave, but the event is not presently being executed
     on the slave.  For more information, see *note
     replication-features-invoked::.  information.

   * 'Originator'

     The server ID of the MySQL server on which the event was created;
     used in replication.  The default value is 0.

   * 'character_set_client'

     The session value of the 'character_set_client' system variable
     when the event was created.

   * 'collation_connection'

     The session value of the 'collation_connection' system variable
     when the event was created.

   * 'Database Collation'

     The collation of the database with which the event is associated.

For more information about 'SLAVESIDE_DISABLED' and the 'Originator'
column, see *note replication-features-invoked::.

Times displayed by *note 'SHOW EVENTS': show-events. are given in the
event time zone, as discussed in *note events-metadata::.

Event information is also available from the 'INFORMATION_SCHEMA' *note
'EVENTS': events-table. table.  See *note events-table::.

The event action statement is not shown in the output of *note 'SHOW
EVENTS': show-events.  Use *note 'SHOW CREATE EVENT': show-create-event.
or the 'INFORMATION_SCHEMA' *note 'EVENTS': events-table. table.


File: manual.info.tmp,  Node: show-function-code,  Next: show-function-status,  Prev: show-events,  Up: show

13.7.5.20 SHOW FUNCTION CODE Statement
......................................

     SHOW FUNCTION CODE FUNC_NAME

This statement is similar to *note 'SHOW PROCEDURE CODE':
show-procedure-code. but for stored functions.  See *note
show-procedure-code::.


File: manual.info.tmp,  Node: show-function-status,  Next: show-grants,  Prev: show-function-code,  Up: show

13.7.5.21 SHOW FUNCTION STATUS Statement
........................................

     SHOW FUNCTION STATUS
         [LIKE 'PATTERN' | WHERE EXPR]

This statement is similar to *note 'SHOW PROCEDURE STATUS':
show-procedure-status. but for stored functions.  See *note
show-procedure-status::.


File: manual.info.tmp,  Node: show-grants,  Next: show-index,  Prev: show-function-status,  Up: show

13.7.5.22 SHOW GRANTS Statement
...............................

     SHOW GRANTS [FOR USER]

This statement displays the privileges that are assigned to a MySQL user
account, in the form of *note 'GRANT': grant. statements that must be
executed to duplicate the privilege assignments.

*note 'SHOW GRANTS': show-grants. requires the 'SELECT' privilege for
the 'mysql' system database, except to display privileges for the
current user.  For output that includes an 'IDENTIFIED BY PASSWORD'
clause displaying an account password hash value, the 'SUPER' privilege
is required to see the actual hash value.  Otherwise, the value displays
as '<secret>'.

To name the account for *note 'SHOW GRANTS': show-grants, use the same
format as for the *note 'GRANT': grant. statement (for example,
''jeffrey'@'localhost''):

     mysql> SHOW GRANTS FOR 'jeffrey'@'localhost';
     +------------------------------------------------------------------+
     | Grants for jeffrey@localhost                                     |
     +------------------------------------------------------------------+
     | GRANT USAGE ON *.* TO `jeffrey`@`localhost`                      |
     | GRANT SELECT, INSERT, UPDATE ON `db1`.* TO `jeffrey`@`localhost` |
     +------------------------------------------------------------------+

The host part, if omitted, defaults to ''%''.  For additional
information about specifying account names, see *note account-names::.

To display the privileges granted to the current user (the account you
are using to connect to the server), you can use any of the following
statements:

     SHOW GRANTS;
     SHOW GRANTS FOR CURRENT_USER;
     SHOW GRANTS FOR CURRENT_USER();

If 'SHOW GRANTS FOR CURRENT_USER' (or any equivalent syntax) is used in
definer context, such as within a stored procedure that executes with
definer rather than invoker privileges, the grants displayed are those
of the definer and not the invoker.

*note 'SHOW GRANTS': show-grants. does not display privileges that are
available to the named account but are granted to a different account.
For example, if an anonymous account exists, the named account might be
able to use its privileges, but *note 'SHOW GRANTS': show-grants. does
not display them.


File: manual.info.tmp,  Node: show-index,  Next: show-master-status,  Prev: show-grants,  Up: show

13.7.5.23 SHOW INDEX Statement
..............................

     SHOW {INDEX | INDEXES | KEYS}
         {FROM | IN} TBL_NAME
         [{FROM | IN} DB_NAME]
         [WHERE EXPR]

*note 'SHOW INDEX': show-index. returns table index information.  The
format resembles that of the 'SQLStatistics' call in ODBC. This
statement requires some privilege for any column in the table.

     mysql> SHOW INDEX FROM City\G
     *************************** 1. row ***************************
             Table: city
        Non_unique: 0
          Key_name: PRIMARY
      Seq_in_index: 1
       Column_name: ID
         Collation: A
       Cardinality: 4321
          Sub_part: NULL
            Packed: NULL
              Null:
        Index_type: BTREE
           Comment:
     Index_comment:
     *************************** 2. row ***************************
             Table: city
        Non_unique: 1
          Key_name: CountryCode
      Seq_in_index: 1
       Column_name: CountryCode
         Collation: A
       Cardinality: 4321
          Sub_part: NULL
            Packed: NULL
              Null:
        Index_type: BTREE
           Comment:
     Index_comment:

An alternative to 'TBL_NAME FROM DB_NAME' syntax is DB_NAME.TBL_NAME.
These two statements are equivalent:

     SHOW INDEX FROM mytable FROM mydb;
     SHOW INDEX FROM mydb.mytable;

The 'WHERE' clause can be given to select rows using more general
conditions, as discussed in *note extended-show::.

*note 'SHOW INDEX': show-index. returns the following fields:

   * 'Table'

     The name of the table.

   * 'Non_unique'

     0 if the index cannot contain duplicates, 1 if it can.

   * 'Key_name'

     The name of the index.  If the index is the primary key, the name
     is always 'PRIMARY'.

   * 'Seq_in_index'

     The column sequence number in the index, starting with 1.

   * 'Column_name'

     The name of the column.

   * 'Collation'

     How the column is sorted in the index.  This can have values 'A'
     (ascending) or 'NULL' (not sorted).

   * 'Cardinality'

     An estimate of the number of unique values in the index.  To update
     this number, run *note 'ANALYZE TABLE': analyze-table. or (for
     'MyISAM' tables) *note 'myisamchk -a': myisamchk.

     'Cardinality' is counted based on statistics stored as integers, so
     the value is not necessarily exact even for small tables.  The
     higher the cardinality, the greater the chance that MySQL uses the
     index when doing joins.

   * 'Sub_part'

     The index prefix.  That is, the number of indexed characters if the
     column is only partly indexed, 'NULL' if the entire column is
     indexed.

     *Note*:

     Prefix _limits_ are measured in bytes.  However, prefix _lengths_
     for index specifications in *note 'CREATE TABLE': create-table,
     *note 'ALTER TABLE': alter-table, and *note 'CREATE INDEX':
     create-index. statements are interpreted as number of characters
     for nonbinary string types (*note 'CHAR': char, *note 'VARCHAR':
     char, *note 'TEXT': blob.) and number of bytes for binary string
     types (*note 'BINARY': binary-varbinary, *note 'VARBINARY':
     binary-varbinary, *note 'BLOB': blob.).  Take this into account
     when specifying a prefix length for a nonbinary string column that
     uses a multibyte character set.

     For additional information about index prefixes, see *note
     column-indexes::, and *note create-index::.

   * 'Packed'

     Indicates how the key is packed.  'NULL' if it is not.

   * 'Null'

     Contains 'YES' if the column may contain 'NULL' values and '''' if
     not.

   * 'Index_type'

     The index method used ('BTREE', 'FULLTEXT', 'HASH', 'RTREE').

   * 'Comment'

     Information about the index not described in its own column, such
     as 'disabled' if the index is disabled.

   * 'Index_comment'

     Any comment provided for the index with a 'COMMENT' attribute when
     the index was created.

Information about table indexes is also available from the
'INFORMATION_SCHEMA' *note 'STATISTICS': statistics-table. table.  See
*note statistics-table::.

You can list a table's indexes with the *note 'mysqlshow -k DB_NAME
TBL_NAME': mysqlshow. command.


File: manual.info.tmp,  Node: show-master-status,  Next: show-open-tables,  Prev: show-index,  Up: show

13.7.5.24 SHOW MASTER STATUS Statement
......................................

     SHOW MASTER STATUS

This statement provides status information about the binary log files of
the master.  It requires either the 'SUPER' or 'REPLICATION CLIENT'
privilege.

Example:

     mysql> SHOW MASTER STATUS;
     +---------------+----------+--------------+------------------+
     | File          | Position | Binlog_Do_DB | Binlog_Ignore_DB |
     +---------------+----------+--------------+------------------+
     | mysql-bin.003 | 73       | test         | manual,mysql     |
     +---------------+----------+--------------+------------------+


File: manual.info.tmp,  Node: show-open-tables,  Next: show-plugins,  Prev: show-master-status,  Up: show

13.7.5.25 SHOW OPEN TABLES Statement
....................................

     SHOW OPEN TABLES
         [{FROM | IN} DB_NAME]
         [LIKE 'PATTERN' | WHERE EXPR]

*note 'SHOW OPEN TABLES': show-open-tables. lists the non-'TEMPORARY'
tables that are currently open in the table cache.  See *note
table-cache::.  The 'FROM' clause, if present, restricts the tables
shown to those present in the DB_NAME database.  The 'LIKE' clause, if
present, indicates which table names to match.  The 'WHERE' clause can
be given to select rows using more general conditions, as discussed in
*note extended-show::.

*note 'SHOW OPEN TABLES': show-open-tables. output has these columns:

   * 'Database'

     The database containing the table.

   * 'Table'

     The table name.

   * 'In_use'

     The number of table locks or lock requests there are for the table.
     For example, if one client acquires a lock for a table using 'LOCK
     TABLE t1 WRITE', 'In_use' will be 1.  If another client issues
     'LOCK TABLE t1 WRITE' while the table remains locked, the client
     will block waiting for the lock, but the lock request causes
     'In_use' to be 2.  If the count is zero, the table is open but not
     currently being used.  'In_use' is also increased by the *note
     'HANDLER ... OPEN': handler. statement and decreased by *note
     'HANDLER ... CLOSE': handler.

   * 'Name_locked'

     Whether the table name is locked.  Name locking is used for
     operations such as dropping or renaming tables.

If you have no privileges for a table, it does not show up in the output
from *note 'SHOW OPEN TABLES': show-open-tables.


File: manual.info.tmp,  Node: show-plugins,  Next: show-privileges,  Prev: show-open-tables,  Up: show

13.7.5.26 SHOW PLUGINS Statement
................................

     SHOW PLUGINS

*note 'SHOW PLUGINS': show-plugins. displays information about server
plugins.

Example of *note 'SHOW PLUGINS': show-plugins. output:

     mysql> SHOW PLUGINS\G
     *************************** 1. row ***************************
        Name: binlog
      Status: ACTIVE
        Type: STORAGE ENGINE
     Library: NULL
     License: GPL
     *************************** 2. row ***************************
        Name: CSV
      Status: ACTIVE
        Type: STORAGE ENGINE
     Library: NULL
     License: GPL
     *************************** 3. row ***************************
        Name: MEMORY
      Status: ACTIVE
        Type: STORAGE ENGINE
     Library: NULL
     License: GPL
     *************************** 4. row ***************************
        Name: MyISAM
      Status: ACTIVE
        Type: STORAGE ENGINE
     Library: NULL
     License: GPL
     ...

*note 'SHOW PLUGINS': show-plugins. output has these columns:

   * 'Name'

     The name used to refer to the plugin in statements such as *note
     'INSTALL PLUGIN': install-plugin. and *note 'UNINSTALL PLUGIN':
     uninstall-plugin.

   * 'Status'

     The plugin status, one of 'ACTIVE', 'INACTIVE', 'DISABLED', or
     'DELETED'.

   * 'Type'

     The type of plugin, such as 'STORAGE ENGINE', 'INFORMATION_SCHEMA',
     or 'AUTHENTICATION'.

   * 'Library'

     The name of the plugin shared library file.  This is the name used
     to refer to the plugin file in statements such as *note 'INSTALL
     PLUGIN': install-plugin. and *note 'UNINSTALL PLUGIN':
     uninstall-plugin.  This file is located in the directory named by
     the 'plugin_dir' system variable.  If the library name is 'NULL',
     the plugin is compiled in and cannot be uninstalled with *note
     'UNINSTALL PLUGIN': uninstall-plugin.

   * 'License'

     How the plugin is licensed (for example, 'GPL').

For plugins installed with *note 'INSTALL PLUGIN': install-plugin, the
'Name' and 'Library' values are also registered in the 'mysql.plugin'
system table.

For information about plugin data structures that form the basis of the
information displayed by *note 'SHOW PLUGINS': show-plugins, see *note
plugin-api::.

Plugin information is also available from the 'INFORMATION_SCHEMA'
'.PLUGINS' table.  See *note plugins-table::.


File: manual.info.tmp,  Node: show-privileges,  Next: show-procedure-code,  Prev: show-plugins,  Up: show

13.7.5.27 SHOW PRIVILEGES Statement
...................................

     SHOW PRIVILEGES

*note 'SHOW PRIVILEGES': show-privileges. shows the list of system
privileges that the MySQL server supports.  The exact list of privileges
depends on the version of your server.

     mysql> SHOW PRIVILEGES\G
     *************************** 1. row ***************************
     Privilege: Alter
       Context: Tables
       Comment: To alter the table
     *************************** 2. row ***************************
     Privilege: Alter routine
       Context: Functions,Procedures
       Comment: To alter or drop stored functions/procedures
     *************************** 3. row ***************************
     Privilege: Create
       Context: Databases,Tables,Indexes
       Comment: To create new databases and tables
     *************************** 4. row ***************************
     Privilege: Create routine
       Context: Databases
       Comment: To use CREATE FUNCTION/PROCEDURE
     *************************** 5. row ***************************
     Privilege: Create temporary tables
       Context: Databases
       Comment: To use CREATE TEMPORARY TABLE
     ...

Privileges belonging to a specific user are displayed by the *note 'SHOW
GRANTS': show-grants. statement.  See *note show-grants::, for more
information.


File: manual.info.tmp,  Node: show-procedure-code,  Next: show-procedure-status,  Prev: show-privileges,  Up: show

13.7.5.28 SHOW PROCEDURE CODE Statement
.......................................

     SHOW PROCEDURE CODE PROC_NAME

This statement is a MySQL extension that is available only for servers
that have been built with debugging support.  It displays a
representation of the internal implementation of the named stored
procedure.  A similar statement, *note 'SHOW FUNCTION CODE':
show-function-code, displays information about stored functions (see
*note show-function-code::).

To use either statement, you must be the owner of the routine or have
*note 'SELECT': select. access to the 'mysql.proc' table.

If the named routine is available, each statement produces a result set.
Each row in the result set corresponds to one 'instruction' in the
routine.  The first column is 'Pos', which is an ordinal number
beginning with 0.  The second column is 'Instruction', which contains an
SQL statement (usually changed from the original source), or a directive
which has meaning only to the stored-routine handler.

     mysql> DELIMITER //
     mysql> CREATE PROCEDURE p1 ()
         -> BEGIN
         ->   DECLARE fanta INT DEFAULT 55;
         ->   DROP TABLE t2;
         ->   LOOP
         ->     INSERT INTO t3 VALUES (fanta);
         ->     END LOOP;
         ->   END//
     Query OK, 0 rows affected (0.00 sec)

     mysql> SHOW PROCEDURE CODE p1//
     +-----+----------------------------------------+
     | Pos | Instruction                            |
     +-----+----------------------------------------+
     |   0 | set fanta@0 55                         |
     |   1 | stmt 9 "DROP TABLE t2"                 |
     |   2 | stmt 5 "INSERT INTO t3 VALUES (fanta)" |
     |   3 | jump 2                                 |
     +-----+----------------------------------------+
     4 rows in set (0.00 sec)

In this example, the nonexecutable 'BEGIN' and 'END' statements have
disappeared, and for the 'DECLARE VARIABLE_NAME' statement, only the
executable part appears (the part where the default is assigned).  For
each statement that is taken from source, there is a code word 'stmt'
followed by a type (9 means 'DROP', 5 means *note 'INSERT': insert, and
so on).  The final row contains an instruction 'jump 2', meaning 'GOTO
instruction #2'.


File: manual.info.tmp,  Node: show-procedure-status,  Next: show-processlist,  Prev: show-procedure-code,  Up: show

13.7.5.29 SHOW PROCEDURE STATUS Statement
.........................................

     SHOW PROCEDURE STATUS
         [LIKE 'PATTERN' | WHERE EXPR]

This statement is a MySQL extension.  It returns characteristics of a
stored procedure, such as the database, name, type, creator, creation
and modification dates, and character set information.  A similar
statement, *note 'SHOW FUNCTION STATUS': show-function-status, displays
information about stored functions (see *note show-function-status::).

The 'LIKE' clause, if present, indicates which procedure or function
names to match.  The 'WHERE' clause can be given to select rows using
more general conditions, as discussed in *note extended-show::.

     mysql> SHOW PROCEDURE STATUS LIKE 'sp1'\G
     *************************** 1. row ***************************
                       Db: test
                     Name: sp1
                     Type: PROCEDURE
                  Definer: testuser@localhost
                 Modified: 2018-08-08 13:54:11
                  Created: 2018-08-08 13:54:11
            Security_type: DEFINER
                  Comment:
     character_set_client: utf8
     collation_connection: utf8_general_ci
       Database Collation: latin1_swedish_ci

'character_set_client' is the session value of the
'character_set_client' system variable when the routine was created.
'collation_connection' is the session value of the
'collation_connection' system variable when the routine was created.
'Database Collation' is the collation of the database with which the
routine is associated.

Stored routine information is also available from the
'INFORMATION_SCHEMA' *note 'PARAMETERS': parameters-table. and *note
'ROUTINES': routines-table. tables.  See *note parameters-table::, and
*note routines-table::.


File: manual.info.tmp,  Node: show-processlist,  Next: show-profile,  Prev: show-procedure-status,  Up: show

13.7.5.30 SHOW PROCESSLIST Statement
....................................

     SHOW [FULL] PROCESSLIST

*note 'SHOW PROCESSLIST': show-processlist. shows which threads are
running.  If you have the 'PROCESS' privilege, you can see all threads.
Otherwise, you can see only your own threads (that is, threads
associated with the MySQL account that you are using).  If you do not
use the 'FULL' keyword, only the first 100 characters of each statement
are shown in the 'Info' field.

The *note 'SHOW PROCESSLIST': show-processlist. statement is very useful
if you get the 'too many connections' error message and want to find out
what is going on.  MySQL reserves one extra connection to be used by
accounts that have the 'SUPER' privilege, to ensure that administrators
should always be able to connect and check the system (assuming that you
are not giving this privilege to all your users).

Threads can be killed with the *note 'KILL': kill. statement.  See *note
kill::.

Example of *note 'SHOW PROCESSLIST': show-processlist. output:

     mysql> SHOW FULL PROCESSLIST\G
     *************************** 1. row ***************************
     Id: 1
     User: system user
     Host:
     db: NULL
     Command: Connect
     Time: 1030455
     State: Waiting for master to send event
     Info: NULL
     *************************** 2. row ***************************
     Id: 2
     User: system user
     Host:
     db: NULL
     Command: Connect
     Time: 1004
     State: Has read all relay log; waiting for the slave
            I/O thread to update it
     Info: NULL
     *************************** 3. row ***************************
     Id: 3112
     User: replikator
     Host: artemis:2204
     db: NULL
     Command: Binlog Dump
     Time: 2144
     State: Has sent all binlog to slave; waiting for binlog to be updated
     Info: NULL
     *************************** 4. row ***************************
     Id: 3113
     User: replikator
     Host: iconnect2:45781
     db: NULL
     Command: Binlog Dump
     Time: 2086
     State: Has sent all binlog to slave; waiting for binlog to be updated
     Info: NULL
     *************************** 5. row ***************************
     Id: 3123
     User: stefan
     Host: localhost
     db: apollon
     Command: Query
     Time: 0
     State: NULL
     Info: SHOW FULL PROCESSLIST
     5 rows in set (0.00 sec)

*note 'SHOW PROCESSLIST': show-processlist. output has these columns:

   * 'Id'

     The connection identifier.  This is the same type of value
     displayed in the 'ID' column of the 'INFORMATION_SCHEMA' *note
     'PROCESSLIST': processlist-table. table and returned by the
     'CONNECTION_ID()' function.

   * 'User'

     The MySQL user who issued the statement.  A value of 'system user'
     refers to a nonclient thread spawned by the server to handle tasks
     internally.  This could be the I/O or SQL thread used on
     replication slaves or a delayed-row handler.  For 'system user',
     there is no host specified in the 'Host' column.  'unauthenticated
     user' refers to a thread that has become associated with a client
     connection but for which authentication of the client user has not
     yet been done.  'event_scheduler' refers to the thread that
     monitors scheduled events (see *note event-scheduler::).

   * 'Host'

     The host name of the client issuing the statement (except for
     'system user', for which there is no host).  The host name for
     TCP/IP connections is reported in 'HOST_NAME:CLIENT_PORT' format to
     make it easier to determine which client is doing what.

   * 'db'

     The default database, if one is selected; otherwise 'NULL'.

   * 'Command'

     The type of command the thread is executing.  For descriptions for
     thread commands, see *note thread-information::.  The value of this
     column corresponds to the 'COM_XXX' commands of the client/server
     protocol and 'Com_XXX' status variables.  See *note
     server-status-variables::.

   * 'Time'

     The time in seconds that the thread has been in its current state.
     For a slave SQL thread, the value is the number of seconds between
     the timestamp of the last replicated event and the real time of the
     slave machine.  See *note replication-implementation-details::.

   * 'State'

     An action, event, or state that indicates what the thread is doing.
     Descriptions for 'State' values can be found at *note
     thread-information::.

     Most states correspond to very quick operations.  If a thread stays
     in a given state for many seconds, there might be a problem that
     needs to be investigated.

     For the *note 'SHOW PROCESSLIST': show-processlist. statement, the
     value of 'State' is 'NULL'.

   * 'Info'

     The statement the thread is executing, or 'NULL' if it is not
     executing any statement.  The statement might be the one sent to
     the server, or an innermost statement if the statement executes
     other statements.  For example, if a 'CALL' statement executes a
     stored procedure that is executing a *note 'SELECT': select.
     statement, the 'Info' value shows the *note 'SELECT': select.
     statement.

Process information is also available from the *note 'mysqladmin
processlist': mysqladmin. command and the 'INFORMATION_SCHEMA' *note
'PROCESSLIST': processlist-table. table (see *note mysqladmin:: and
*note processlist-table::).


File: manual.info.tmp,  Node: show-profile,  Next: show-profiles,  Prev: show-processlist,  Up: show

13.7.5.31 SHOW PROFILE Statement
................................

     SHOW PROFILE [TYPE [, TYPE] ... ]
         [FOR QUERY N]
         [LIMIT ROW_COUNT [OFFSET OFFSET]]

     TYPE: {
         ALL
       | BLOCK IO
       | CONTEXT SWITCHES
       | CPU
       | IPC
       | MEMORY
       | PAGE FAULTS
       | SOURCE
       | SWAPS
     }

The *note 'SHOW PROFILE': show-profile. and *note 'SHOW PROFILES':
show-profiles. statements display profiling information that indicates
resource usage for statements executed during the course of the current
session.

To control profiling, use the 'profiling' session variable, which has a
default value of 0 ('OFF').  Enable profiling by setting 'profiling' to
1 or 'ON':

     mysql> SET profiling = 1;

*note 'SHOW PROFILES': show-profiles. displays a list of the most recent
statements sent to the server.  The size of the list is controlled by
the 'profiling_history_size' session variable, which has a default value
of 15.  The maximum value is 100.  Setting the value to 0 has the
practical effect of disabling profiling.

All statements are profiled except *note 'SHOW PROFILE': show-profile.
and *note 'SHOW PROFILES': show-profiles, so you will find neither of
those statements in the profile list.  Malformed statements are
profiled.  For example, 'SHOW PROFILING' is an illegal statement, and a
syntax error occurs if you try to execute it, but it will show up in the
profiling list.

*note 'SHOW PROFILE': show-profile. displays detailed information about
a single statement.  Without the 'FOR QUERY N' clause, the output
pertains to the most recently executed statement.  If 'FOR QUERY N' is
included, *note 'SHOW PROFILE': show-profile. displays information for
statement N.  The values of N correspond to the 'Query_ID' values
displayed by *note 'SHOW PROFILES': show-profiles.

The 'LIMIT ROW_COUNT' clause may be given to limit the output to
ROW_COUNT rows.  If 'LIMIT' is given, 'OFFSET OFFSET' may be added to
begin the output OFFSET rows into the full set of rows.

By default, *note 'SHOW PROFILE': show-profile. displays 'Status' and
'Duration' columns.  The 'Status' values are like the 'State' values
displayed by *note 'SHOW PROCESSLIST': show-processlist, although there
might be some minor differences in interpretion for the two statements
for some status values (see *note thread-information::).

Optional TYPE values may be specified to display specific additional
types of information:

   * 'ALL' displays all information

   * 'BLOCK IO' displays counts for block input and output operations

   * 'CONTEXT SWITCHES' displays counts for voluntary and involuntary
     context switches

   * 'CPU' displays user and system CPU usage times

   * 'IPC' displays counts for messages sent and received

   * 'MEMORY' is not currently implemented

   * 'PAGE FAULTS' displays counts for major and minor page faults

   * 'SOURCE' displays the names of functions from the source code,
     together with the name and line number of the file in which the
     function occurs

   * 'SWAPS' displays swap counts

Profiling is enabled per session.  When a session ends, its profiling
information is lost.

     mysql> SELECT @@profiling;
     +-------------+
     | @@profiling |
     +-------------+
     |           0 |
     +-------------+
     1 row in set (0.00 sec)

     mysql> SET profiling = 1;
     Query OK, 0 rows affected (0.00 sec)

     mysql> DROP TABLE IF EXISTS t1;
     Query OK, 0 rows affected, 1 warning (0.00 sec)

     mysql> CREATE TABLE T1 (id INT);
     Query OK, 0 rows affected (0.01 sec)

     mysql> SHOW PROFILES;
     +----------+----------+--------------------------+
     | Query_ID | Duration | Query                    |
     +----------+----------+--------------------------+
     |        0 | 0.000088 | SET PROFILING = 1        |
     |        1 | 0.000136 | DROP TABLE IF EXISTS t1  |
     |        2 | 0.011947 | CREATE TABLE t1 (id INT) |
     +----------+----------+--------------------------+
     3 rows in set (0.00 sec)

     mysql> SHOW PROFILE;
     +----------------------+----------+
     | Status               | Duration |
     +----------------------+----------+
     | checking permissions | 0.000040 |
     | creating table       | 0.000056 |
     | After create         | 0.011363 |
     | query end            | 0.000375 |
     | freeing items        | 0.000089 |
     | logging slow query   | 0.000019 |
     | cleaning up          | 0.000005 |
     +----------------------+----------+
     7 rows in set (0.00 sec)

     mysql> SHOW PROFILE FOR QUERY 1;
     +--------------------+----------+
     | Status             | Duration |
     +--------------------+----------+
     | query end          | 0.000107 |
     | freeing items      | 0.000008 |
     | logging slow query | 0.000015 |
     | cleaning up        | 0.000006 |
     +--------------------+----------+
     4 rows in set (0.00 sec)

     mysql> SHOW PROFILE CPU FOR QUERY 2;
     +----------------------+----------+----------+------------+
     | Status               | Duration | CPU_user | CPU_system |
     +----------------------+----------+----------+------------+
     | checking permissions | 0.000040 | 0.000038 |   0.000002 |
     | creating table       | 0.000056 | 0.000028 |   0.000028 |
     | After create         | 0.011363 | 0.000217 |   0.001571 |
     | query end            | 0.000375 | 0.000013 |   0.000028 |
     | freeing items        | 0.000089 | 0.000010 |   0.000014 |
     | logging slow query   | 0.000019 | 0.000009 |   0.000010 |
     | cleaning up          | 0.000005 | 0.000003 |   0.000002 |
     +----------------------+----------+----------+------------+
     7 rows in set (0.00 sec)

*Note*:

Profiling is only partially functional on some architectures.  For
values that depend on the 'getrusage()' system call, 'NULL' is returned
on systems such as Windows that do not support the call.  In addition,
profiling is per process and not per thread.  This means that activity
on threads within the server other than your own may affect the timing
information that you see.

Profiling information is also available from the 'INFORMATION_SCHEMA'
*note 'PROFILING': profiling-table. table.  See *note profiling-table::.
For example, the following queries are equivalent:

     SHOW PROFILE FOR QUERY 2;

     SELECT STATE, FORMAT(DURATION, 6) AS DURATION
     FROM INFORMATION_SCHEMA.PROFILING
     WHERE QUERY_ID = 2 ORDER BY SEQ;


File: manual.info.tmp,  Node: show-profiles,  Next: show-relaylog-events,  Prev: show-profile,  Up: show

13.7.5.32 SHOW PROFILES Statement
.................................

     SHOW PROFILES

The *note 'SHOW PROFILES': show-profiles. statement, together with *note
'SHOW PROFILE': show-profile, displays profiling information that
indicates resource usage for statements executed during the course of
the current session.  For more information, see *note show-profile::.


File: manual.info.tmp,  Node: show-relaylog-events,  Next: show-slave-hosts,  Prev: show-profiles,  Up: show

13.7.5.33 SHOW RELAYLOG EVENTS Statement
........................................

     SHOW RELAYLOG EVENTS
         [IN 'LOG_NAME']
         [FROM POS]
         [LIMIT [OFFSET,] ROW_COUNT]

Shows the events in the relay log of a replication slave.  If you do not
specify ''LOG_NAME'', the first relay log is displayed.  This statement
has no effect on the master.

The 'LIMIT' clause has the same syntax as for the *note 'SELECT':
select. statement.  See *note select::.

*Note*:

Issuing a *note 'SHOW RELAYLOG EVENTS': show-relaylog-events. with no
'LIMIT' clause could start a very time- and resource-consuming process
because the server returns to the client the complete contents of the
relay log (including all statements modifying data that have been
received by the slave).

*note 'SHOW RELAYLOG EVENTS': show-relaylog-events. displays the
following fields for each event in the relay log:

   * 'Log_name'

     The name of the file that is being listed.

   * 'Pos'

     The position at which the event occurs.

   * 'Event_type'

     An identifier that describes the event type.

   * 'Server_id'

     The server ID of the server on which the event originated.

   * 'End_log_pos'

     The value of 'End_log_pos' for this event in the master's binary
     log.

   * 'Info'

     More detailed information about the event type.  The format of this
     information depends on the event type.

*Note*:

Some events relating to the setting of user and system variables are not
included in the output from *note 'SHOW RELAYLOG EVENTS':
show-relaylog-events.  To get complete coverage of events within a relay
log, use *note 'mysqlbinlog': mysqlbinlog.


File: manual.info.tmp,  Node: show-slave-hosts,  Next: show-slave-status,  Prev: show-relaylog-events,  Up: show

13.7.5.34 SHOW SLAVE HOSTS Statement
....................................

     SHOW SLAVE HOSTS

Displays a list of replication slaves currently registered with the
master.

'SHOW SLAVE HOSTS' should be executed on a server that acts as a
replication master.  The statement displays information about servers
that are or have been connected as replication slaves, with each row of
the result corresponding to one slave server, as shown here:

     mysql> SHOW SLAVE HOSTS;
     +------------+-----------+------+-----------+
     | Server_id  | Host      | Port | Master_id |
     +------------+-----------+------+-----------+
     |  192168010 | iconnect2 | 3306 | 192168011 |
     | 1921680101 | athena    | 3306 | 192168011 |
     +------------+-----------+------+-----------+

   * 'Server_id': The unique server ID of the slave server, as
     configured in the slave server's option file, or on the command
     line with '--server-id=VALUE'.

   * 'Host': The host name of the slave server as specified on the slave
     with the '--report-host' option.  This can differ from the machine
     name as configured in the operating system.

   * 'User': The slave server user name as, specified on the slave with
     the '--report-user' option.  Statement output includes this column
     only if the master server is started with the
     '--show-slave-auth-info' option.

   * 'Password': The slave server password as, specified on the slave
     with the '--report-password' option.  Statement output includes
     this column only if the master server is started with the
     '--show-slave-auth-info' option.

   * 'Port': The port on the master to which the slave server is
     listening, as specified on the slave with the '--report-port'
     option.

     In MySQL 5.5.23 and later, a zero in this column means that the
     slave port ('--report-port') was not set.  Prior to MySQL 5.5.23,
     3306 was used as the default in such cases (Bug #13333431).

   * 'Master_id': The unique server ID of the master server that the
     slave server is replicating from.  This is the server ID of the
     server on which 'SHOW SLAVE HOSTS' is executed, so this same value
     is listed for each row in the result.


File: manual.info.tmp,  Node: show-slave-status,  Next: show-status,  Prev: show-slave-hosts,  Up: show

13.7.5.35 SHOW SLAVE STATUS Statement
.....................................

     SHOW SLAVE STATUS

This statement provides status information on essential parameters of
the slave threads.  It requires either the 'SUPER' or 'REPLICATION
CLIENT' privilege.

If you issue this statement using the *note 'mysql': mysql. client, you
can use a '\G' statement terminator rather than a semicolon to obtain a
more readable vertical layout:

     mysql> SHOW SLAVE STATUS\G
     *************************** 1. row ***************************
                    Slave_IO_State: Waiting for master to send event
                       Master_Host: localhost
                       Master_User: root
                       Master_Port: 3306
                     Connect_Retry: 3
                   Master_Log_File: gbichot-bin.005
               Read_Master_Log_Pos: 79
                    Relay_Log_File: gbichot-relay-bin.005
                     Relay_Log_Pos: 548
             Relay_Master_Log_File: gbichot-bin.005
                  Slave_IO_Running: Yes
                 Slave_SQL_Running: Yes
                   Replicate_Do_DB:
               Replicate_Ignore_DB:
                Replicate_Do_Table:
            Replicate_Ignore_Table:
           Replicate_Wild_Do_Table:
       Replicate_Wild_Ignore_Table:
                        Last_Errno: 0
                        Last_Error:
                      Skip_Counter: 0
               Exec_Master_Log_Pos: 79
                   Relay_Log_Space: 552
                   Until_Condition: None
                    Until_Log_File:
                     Until_Log_Pos: 0
                Master_SSL_Allowed: No
                Master_SSL_CA_File:
                Master_SSL_CA_Path:
                   Master_SSL_Cert:
                 Master_SSL_Cipher:
                    Master_SSL_Key:
             Seconds_Behind_Master: 8
     Master_SSL_Verify_Server_Cert: No
                     Last_IO_Errno: 0
                     Last_IO_Error:
                    Last_SQL_Errno: 0
                    Last_SQL_Error:
       Replicate_Ignore_Server_Ids: 0
                  Master_Server_Id: 1

The following list describes the fields returned by *note 'SHOW SLAVE
STATUS': show-slave-status.  For additional information about
interpreting their meanings, see *note slave-io-thread-states::.

   * 'Slave_IO_State'

     A copy of the 'State' field of the *note 'SHOW PROCESSLIST':
     show-processlist. output for the slave I/O thread.  This tells you
     what the thread is doing: trying to connect to the master, waiting
     for events from the master, reconnecting to the master, and so on.
     For a listing of possible states, see *note
     slave-io-thread-states::.

   * 'Master_Host'

     The master host that the slave is connected to.

   * 'Master_User'

     The user name of the account used to connect to the master.

   * 'Master_Port'

     The port used to connect to the master.

   * 'Connect_Retry'

     The number of seconds between connect retries (default 60).  This
     can be set with the *note 'CHANGE MASTER TO': change-master-to.
     statement.

   * 'Master_Log_File'

     The name of the master binary log file from which the I/O thread is
     currently reading.

   * 'Read_Master_Log_Pos'

     The position in the current master binary log file up to which the
     I/O thread has read.

   * 'Relay_Log_File'

     The name of the relay log file from which the SQL thread is
     currently reading and executing.

   * 'Relay_Log_Pos'

     The position in the current relay log file up to which the SQL
     thread has read and executed.

   * 'Relay_Master_Log_File'

     The name of the master binary log file containing the most recent
     event executed by the SQL thread.

   * 'Slave_IO_Running'

     Whether the I/O thread is started and has connected successfully to
     the master.  Internally, the state of this thread is represented by
     one of the following three values:

        * MYSQL_SLAVE_NOT_RUN

          The slave I/O thread is not running.  For this state,
          'Slave_IO_Running' is 'No'.

        * MYSQL_SLAVE_RUN_NOT_CONNECT

          The slave I/O thread is running, but is not connected to a
          replication master.  For this state, 'Slave_IO_Running' is
          'Connecting'.

        * MYSQL_SLAVE_RUN_CONNECT

          The slave I/O thread is running, and is connected to a
          replication master.  For this state, 'Slave_IO_Running' is
          'Yes'.

     The value of the 'Slave_running' system status variable corresponds
     with this value.

   * 'Slave_SQL_Running'

     Whether the SQL thread is started.

   * 'Replicate_Do_DB', 'Replicate_Ignore_DB'

     The lists of databases that were specified with the
     '--replicate-do-db' and '--replicate-ignore-db' options, if any.

   * 'Replicate_Do_Table', 'Replicate_Ignore_Table',
     'Replicate_Wild_Do_Table', 'Replicate_Wild_Ignore_Table'

     The lists of tables that were specified with the
     '--replicate-do-table', '--replicate-ignore-table',
     '--replicate-wild-do-table', and '--replicate-wild-ignore-table'
     options, if any.

   * 'Last_Errno', 'Last_Error'

     These columns are aliases for 'Last_SQL_Errno' and
     'Last_SQL_Error'.

     Issuing *note 'RESET MASTER': reset-master. or *note 'RESET SLAVE':
     reset-slave. resets the values shown in these columns.

     *Note*:

     When the slave SQL thread receives an error, it reports the error
     first, then stops the SQL thread.  This means that there is a small
     window of time during which *note 'SHOW SLAVE STATUS':
     show-slave-status. shows a nonzero value for 'Last_SQL_Errno' even
     though 'Slave_SQL_Running' still displays 'Yes'.

   * 'Skip_Counter'

     The current value of the 'sql_slave_skip_counter' system variable.
     See *note set-global-sql-slave-skip-counter::.

   * 'Exec_Master_Log_Pos'

     The position in the current master binary log file to which the SQL
     thread has read and executed, marking the start of the next
     transaction or event to be processed.  You can use this value with
     the *note 'CHANGE MASTER TO': change-master-to. statement's
     'MASTER_LOG_POS' option when starting a new slave from an existing
     slave, so that the new slave reads from this point.  The
     coordinates given by ('Relay_Master_Log_File',
     'Exec_Master_Log_Pos') in the master's binary log correspond to the
     coordinates given by ('Relay_Log_File', 'Relay_Log_Pos') in the
     relay log.

   * 'Relay_Log_Space'

     The total combined size of all existing relay log files.

   * 'Until_Condition', 'Until_Log_File', 'Until_Log_Pos'

     The values specified in the 'UNTIL' clause of the *note 'START
     SLAVE': start-slave. statement.

     'Until_Condition' has these values:

        * 'None' if no 'UNTIL' clause was specified

        * 'Master' if the slave is reading until a given position in the
          master's binary log

        * 'Relay' if the slave is reading until a given position in its
          relay log

     'Until_Log_File' and 'Until_Log_Pos' indicate the log file name and
     position that define the coordinates at which the SQL thread stops
     executing.

   * 'Master_SSL_Allowed', 'Master_SSL_CA_File', 'Master_SSL_CA_Path',
     'Master_SSL_Cert', 'Master_SSL_Cipher', 'Master_SSL_Key',
     'Master_SSL_Verify_Server_Cert'

     These fields show the SSL parameters used by the slave to connect
     to the master, if any.

     'Master_SSL_Allowed' has these values:

        * 'Yes' if an SSL connection to the master is permitted

        * 'No' if an SSL connection to the master is not permitted

        * 'Ignored' if an SSL connection is permitted but the slave
          server does not have SSL support enabled

     The values of the other SSL-related fields correspond to the values
     of the 'MASTER_SSL_CA', 'MASTER_SSL_CAPATH', 'MASTER_SSL_CERT',
     'MASTER_SSL_CIPHER', 'MASTER_SSL_KEY', and
     'MASTER_SSL_VERIFY_SERVER_CERT' options to the *note 'CHANGE MASTER
     TO': change-master-to. statement.  See *note change-master-to::.

   * 'Seconds_Behind_Master'

     This field is an indication of how 'late' the slave is:

        * When the slave is actively processing updates, this field
          shows the difference between the current timestamp on the
          slave and the original timestamp logged on the master for the
          event currently being processed on the slave.

        * When no event is currently being processed on the slave, this
          value is 0.

     In essence, this field measures the time difference in seconds
     between the slave SQL thread and the slave I/O thread.  If the
     network connection between master and slave is fast, the slave I/O
     thread is very close to the master, so this field is a good
     approximation of how late the slave SQL thread is compared to the
     master.  If the network is slow, this is _not_ a good
     approximation; the slave SQL thread may quite often be caught up
     with the slow-reading slave I/O thread, so 'Seconds_Behind_Master'
     often shows a value of 0, even if the I/O thread is late compared
     to the master.  In other words, _this column is useful only for
     fast networks_.

     This time difference computation works even if the master and slave
     do not have identical clock times, provided that the difference,
     computed when the slave I/O thread starts, remains constant from
     then on.  Any changes--including NTP updates--can lead to clock
     skews that can make calculation of 'Seconds_Behind_Master' less
     reliable.

     This field is 'NULL' (undefined or unknown) if the slave SQL thread
     is not running, or if the slave I/O thread is not running or is not
     connected to the master.  For example, if the slave I/O thread is
     running but is not connected to the master and is sleeping for the
     number of seconds given by the *note 'CHANGE MASTER TO':
     change-master-to. statement or '--master-connect-retry' option
     (default 60) before reconnecting, the value is 'NULL'.  This is
     because the slave cannot know what the master is doing, and so
     cannot say reliably how late it is.

     The value of 'Seconds_Behind_Master' is based on the timestamps
     stored in events, which are preserved through replication.  This
     means that if a master M1 is itself a slave of M0, any event from
     M1's binary log that originates from M0's binary log has M0's
     timestamp for that event.  This enables MySQL to replicate *note
     'TIMESTAMP': datetime. successfully.  However, the problem for
     'Seconds_Behind_Master' is that if M1 also receives direct updates
     from clients, the 'Seconds_Behind_Master' value randomly fluctuates
     because sometimes the last event from M1 originates from M0 and
     sometimes is the result of a direct update on M1.

   * 'Last_IO_Errno', 'Last_IO_Error'

     The error number and error message of the most recent error that
     caused the I/O thread to stop.  An error number of 0 and message of
     the empty string mean 'no error.' If the 'Last_IO_Error' value is
     not empty, the error values also appear in the slave's error log.

     Issuing *note 'RESET MASTER': reset-master. or *note 'RESET SLAVE':
     reset-slave. resets the values shown in these columns.

   * 'Last_SQL_Errno', 'Last_SQL_Error'

     The error number and error message of the most recent error that
     caused the SQL thread to stop.  An error number of 0 and message of
     the empty string mean 'no error.' If the 'Last_SQL_Error' value is
     not empty, the error values also appear in the slave's error log.

     Example:

          Last_SQL_Errno: 1051
          Last_SQL_Error: error 'Unknown table 'z'' on query 'drop table z'

     The message indicates that the table 'z' existed on the master and
     was dropped there, but it did not exist on the slave, so *note
     'DROP TABLE': drop-table. failed on the slave.  (This might occur,
     for example, if you forget to copy the table to the slave when
     setting up replication.)

     Issuing *note 'RESET MASTER': reset-master. or *note 'RESET SLAVE':
     reset-slave. resets the values shown in these columns.

   * 'Replicate_Ignore_Server_Ids'

     You can tell a slave to ignore events from 0 or more masters using
     the 'IGNORE_SERVER_IDS' option of the *note 'CHANGE MASTER TO':
     change-master-to. statement.  This is normally of interest only
     when using a circular or other multi-master replication setup.

     The message shown for 'Replicate_Ignore_Server_Ids' consists of a
     space-delimited list of one or more numbers, the first value
     indicating the number of servers to be ignored; if not 0 (the
     default), this server-count value is followed by the actual server
     IDs.  For example, if a *note 'CHANGE MASTER TO': change-master-to.
     statement containing the 'IGNORE_SERVER_IDS = (2,6,9)' option has
     been issued to tell a slave to ignore masters having the server ID
     2, 6, or 9, that information appears as shown here:

            Replicate_Ignore_Server_Ids: 3 2 6 9

     'Replicate_Ignore_Server_Ids' filtering is performed by the I/O
     thread, rather than by the SQL thread, which means that events
     which are filtered out are not written to the relay log.  This
     differs from the filtering actions taken by server options such
     '--replicate-do-table', which apply to the SQL thread.

   * 'Master_Server_Id'

     The 'server_id' value from the master.


File: manual.info.tmp,  Node: show-status,  Next: show-table-status,  Prev: show-slave-status,  Up: show

13.7.5.36 SHOW STATUS Statement
...............................

     SHOW [GLOBAL | SESSION] STATUS
         [LIKE 'PATTERN' | WHERE EXPR]

*note 'SHOW STATUS': show-status. provides server status information
(see *note server-status-variables::).  This statement does not require
any privilege.  It requires only the ability to connect to the server.

Status variable information is also available from these sources:

   * The *note 'GLOBAL_STATUS': variables-table. and *note
     'SESSION_STATUS': variables-table. tables.  See *note
     status-table::.

   * The *note 'mysqladmin extended-status': mysqladmin. command.  See
     *note mysqladmin::.

For *note 'SHOW STATUS': show-status, a 'LIKE' clause, if present,
indicates which variable names to match.  A 'WHERE' clause can be given
to select rows using more general conditions, as discussed in *note
extended-show::.

*note 'SHOW STATUS': show-status. accepts an optional 'GLOBAL' or
'SESSION' variable scope modifier:

   * With a 'GLOBAL' modifier, the statement displays the global status
     values.  A global status variable may represent status for some
     aspect of the server itself (for example, 'Aborted_connects'), or
     the aggregated status over all connections to MySQL (for example,
     'Bytes_received' and 'Bytes_sent').  If a variable has no global
     value, the session value is displayed.

   * With a 'SESSION' modifier, the statement displays the status
     variable values for the current connection.  If a variable has no
     session value, the global value is displayed.  'LOCAL' is a synonym
     for 'SESSION'.

   * If no modifier is present, the default is 'SESSION'.

The scope for each status variable is listed at *note
server-status-variables::.

Each invocation of the *note 'SHOW STATUS': show-status. statement uses
an internal temporary table and increments the global
'Created_tmp_tables' value.

Partial output is shown here.  The list of names and values may differ
for your server.  The meaning of each variable is given in *note
server-status-variables::.

     mysql> SHOW STATUS;
     +--------------------------+------------+
     | Variable_name            | Value      |
     +--------------------------+------------+
     | Aborted_clients          | 0          |
     | Aborted_connects         | 0          |
     | Bytes_received           | 155372598  |
     | Bytes_sent               | 1176560426 |
     | Connections              | 30023      |
     | Created_tmp_disk_tables  | 0          |
     | Created_tmp_tables       | 8340       |
     | Created_tmp_files        | 60         |
     ...
     | Open_tables              | 1          |
     | Open_files               | 2          |
     | Open_streams             | 0          |
     | Opened_tables            | 44600      |
     | Questions                | 2026873    |
     ...
     | Table_locks_immediate    | 1920382    |
     | Table_locks_waited       | 0          |
     | Threads_cached           | 0          |
     | Threads_created          | 30022      |
     | Threads_connected        | 1          |
     | Threads_running          | 1          |
     | Uptime                   | 80380      |
     +--------------------------+------------+

With a 'LIKE' clause, the statement displays only rows for those
variables with names that match the pattern:

     mysql> SHOW STATUS LIKE 'Key%';
     +--------------------+----------+
     | Variable_name      | Value    |
     +--------------------+----------+
     | Key_blocks_used    | 14955    |
     | Key_read_requests  | 96854827 |
     | Key_reads          | 162040   |
     | Key_write_requests | 7589728  |
     | Key_writes         | 3813196  |
     +--------------------+----------+


File: manual.info.tmp,  Node: show-table-status,  Next: show-tables,  Prev: show-status,  Up: show

13.7.5.37 SHOW TABLE STATUS Statement
.....................................

     SHOW TABLE STATUS
         [{FROM | IN} DB_NAME]
         [LIKE 'PATTERN' | WHERE EXPR]

*note 'SHOW TABLE STATUS': show-table-status. works likes *note 'SHOW
TABLES': show-tables, but provides a lot of information about each
non-'TEMPORARY' table.  You can also get this list using the *note
'mysqlshow --status DB_NAME': mysqlshow. command.  The 'LIKE' clause, if
present, indicates which table names to match.  The 'WHERE' clause can
be given to select rows using more general conditions, as discussed in
*note extended-show::.

This statement also displays information about views.

*note 'SHOW TABLE STATUS': show-table-status. output has these columns:

   * 'Name'

     The name of the table.

   * 'Engine'

     The storage engine for the table.  See *note
     innodb-storage-engine::, and *note storage-engines::.

     For partitioned tables, 'Engine' shows the name of the storage
     engine used by all partitions.

   * 'Version'

     The version number of the table's '.frm' file.

   * 'Row_format'

     The row-storage format ('Fixed', 'Dynamic', 'Compressed',
     'Redundant', 'Compact').  For 'MyISAM' tables, 'Dynamic'
     corresponds to what *note 'myisamchk -dvv': myisamchk. reports as
     'Packed'.  'InnoDB' table format is either 'Redundant' or 'Compact'
     when using the 'Antelope' file format, or 'Compressed' or 'Dynamic'
     when using the 'Barracuda' file format.

   * 'Rows'

     The number of rows.  Some storage engines, such as 'MyISAM', store
     the exact count.  For other storage engines, such as 'InnoDB', this
     value is an approximation, and may vary from the actual value by as
     much as 40% to 50%.  In such cases, use 'SELECT COUNT(*)' to obtain
     an accurate count.

     The 'Rows' value is 'NULL' for 'INFORMATION_SCHEMA' tables.

     For *note 'InnoDB': innodb-storage-engine. tables, the row count is
     only a rough estimate used in SQL optimization.  (This is also true
     if the *note 'InnoDB': innodb-storage-engine. table is
     partitioned.)

   * 'Avg_row_length'

     The average row length.

     Refer to the notes at the end of this section for related
     information.

   * 'Data_length'

     For 'MyISAM', 'Data_length' is the length of the data file, in
     bytes.

     For 'InnoDB', 'Data_length' is the approximate amount of space
     allocated for the clustered index, in bytes.  Specifically, it is
     the clustered index size, in pages, multiplied by the 'InnoDB' page
     size.

     Refer to the notes at the end of this section for information
     regarding other storage engines.

   * 'Max_data_length'

     For 'MyISAM', 'Max_data_length' is maximum length of the data file.
     This is the total number of bytes of data that can be stored in the
     table, given the data pointer size used.

     Unused for 'InnoDB'.

     Refer to the notes at the end of this section for information
     regarding other storage engines.

   * 'Index_length'

     For 'MyISAM', 'Index_length' is the length of the index file, in
     bytes.

     For 'InnoDB', 'Index_length' is the approximate amount of space
     allocated for non-clustered indexes, in bytes.  Specifically, it is
     the sum of non-clustered index sizes, in pages, multiplied by the
     'InnoDB' page size.

     Refer to the notes at the end of this section for information
     regarding other storage engines.

   * 'Data_free'

     The number of allocated but unused bytes.

     'InnoDB' tables report the free space of the tablespace to which
     the table belongs.  For a table located in the shared tablespace,
     this is the free space of the shared tablespace.  If you are using
     multiple tablespaces and the table has its own tablespace, the free
     space is for only that table.  Free space means the number of bytes
     in completely free extents minus a safety margin.  Even if free
     space displays as 0, it may be possible to insert rows as long as
     new extents need not be allocated.

     For NDB Cluster, 'Data_free' shows the space allocated on disk for,
     but not used by, a Disk Data table or fragment on disk.  (In-memory
     data resource usage is reported by the 'Data_length' column.)

     For partitioned tables, this value is only an estimate and may not
     be absolutely correct.  A more accurate method of obtaining this
     information in such cases is to query the 'INFORMATION_SCHEMA'
     *note 'PARTITIONS': partitions-table. table, as shown in this
     example:

          SELECT SUM(DATA_FREE)
              FROM  INFORMATION_SCHEMA.PARTITIONS
              WHERE TABLE_SCHEMA = 'mydb'
              AND   TABLE_NAME   = 'mytable';

     For more information, see *note partitions-table::.

   * 'Auto_increment'

     The next 'AUTO_INCREMENT' value.

   * 'Create_time'

     When the table was created.

     Prior to MySQL 5.5.44, for partitioned *note 'InnoDB':
     innodb-storage-engine. tables, the 'Create_time' column shows
     'NULL'.  This column shows the correct table creation time for such
     tables in MySQL 5.5.44 and later.  (Bug #17299181, Bug #69990)

   * 'Update_time'

     When the data file was last updated.  For some storage engines,
     this value is 'NULL'.  For example, 'InnoDB' stores multiple tables
     in its system tablespace and the data file timestamp does not
     apply.  Even with file-per-table mode with each 'InnoDB' table in a
     separate '.ibd' file, change buffering can delay the write to the
     data file, so the file modification time is different from the time
     of the last insert, update, or delete.  For 'MyISAM', the data file
     timestamp is used; however, on Windows the timestamp is not updated
     by updates, so the value is inaccurate.

     For partitioned *note 'InnoDB': innodb-storage-engine. tables,
     'Update_time' is always 'NULL'.

   * 'Check_time'

     When the table was last checked.  Not all storage engines update
     this time, in which case, the value is always 'NULL'.

     For partitioned *note 'InnoDB': innodb-storage-engine. tables,
     'Check_time' is always 'NULL'.

   * 'Collation'

     The table default collation.  The output does not explicitly list
     the table default character set, but the collation name begins with
     the character set name.

   * 'Checksum'

     The live checksum value, if any.

   * 'Create_options'

     Extra options used with *note 'CREATE TABLE': create-table.  The
     original options from when *note 'CREATE TABLE': create-table. was
     executed are retained and the options reported here may differ from
     the active table settings and options.

     'Create_options' shows 'partitioned' if the table is partitioned.

   * 'Comment'

     The comment used when creating the table (or information as to why
     MySQL could not access the table information).

*Notes*

   * For 'InnoDB' tables, *note 'SHOW TABLE STATUS': show-table-status.
     does not give accurate statistics except for the physical size
     reserved by the table.  The row count is only a rough estimate used
     in SQL optimization.

   * For *note 'NDB': mysql-cluster. tables, the output of this
     statement shows appropriate values for the 'Avg_row_length' and
     'Data_length' columns, with the exception that *note 'BLOB': blob.
     columns are not taken into account.

   * For *note 'NDB': mysql-cluster. tables, 'Data_length' includes data
     stored in main memory only; the 'Max_data_length' and 'Data_free'
     columns apply to Disk Data.

   * For NDB Cluster Disk Data tables, 'Max_data_length' shows the space
     allocated for the disk part of a Disk Data table or fragment.
     (In-memory data resource usage is reported by the 'Data_length'
     column.)

   * For 'MEMORY' tables, the 'Data_length', 'Max_data_length', and
     'Index_length' values approximate the actual amount of allocated
     memory.  The allocation algorithm reserves memory in large amounts
     to reduce the number of allocation operations.

   * For views, all columns displayed by *note 'SHOW TABLE STATUS':
     show-table-status. are 'NULL' except that 'Name' indicates the view
     name and 'Comment' says 'VIEW'.

Table information is also available from the 'INFORMATION_SCHEMA' *note
'TABLES': tables-table. table.  See *note tables-table::.


File: manual.info.tmp,  Node: show-tables,  Next: show-triggers,  Prev: show-table-status,  Up: show

13.7.5.38 SHOW TABLES Statement
...............................

     SHOW [FULL] TABLES
         [{FROM | IN} DB_NAME]
         [LIKE 'PATTERN' | WHERE EXPR]

*note 'SHOW TABLES': show-tables. lists the non-'TEMPORARY' tables in a
given database.  You can also get this list using the *note 'mysqlshow
DB_NAME': mysqlshow. command.  The 'LIKE' clause, if present, indicates
which table names to match.  The 'WHERE' clause can be given to select
rows using more general conditions, as discussed in *note
extended-show::.

Matching performed by the 'LIKE' clause is dependent on the setting of
the 'lower_case_table_names' system variable.

This statement also lists any views in the database.  The optional
'FULL' modifier causes *note 'SHOW TABLES': show-tables. to display a
second output column with values of 'BASE TABLE' for a table, 'VIEW' for
a view, or 'SYSTEM VIEW' for an 'INFORMATION_SCHEMA' table.

If you have no privileges for a base table or view, it does not show up
in the output from *note 'SHOW TABLES': show-tables. or *note 'mysqlshow
db_name': mysqlshow.

Table information is also available from the 'INFORMATION_SCHEMA' *note
'TABLES': tables-table. table.  See *note tables-table::.


File: manual.info.tmp,  Node: show-triggers,  Next: show-variables,  Prev: show-tables,  Up: show

13.7.5.39 SHOW TRIGGERS Statement
.................................

     SHOW TRIGGERS
         [{FROM | IN} DB_NAME]
         [LIKE 'PATTERN' | WHERE EXPR]

*note 'SHOW TRIGGERS': show-triggers. lists the triggers currently
defined for tables in a database (the default database unless a 'FROM'
clause is given).  This statement returns results only for databases and
tables for which you have the 'TRIGGER' privilege.  The 'LIKE' clause,
if present, indicates which table names (not trigger names) to match and
causes the statement to display triggers for those tables.  The 'WHERE'
clause can be given to select rows using more general conditions, as
discussed in *note extended-show::.

For the 'ins_sum' trigger defined in *note triggers::, the output of
*note 'SHOW TRIGGERS': show-triggers. is as shown here:

     mysql> SHOW TRIGGERS LIKE 'acc%'\G
     *************************** 1. row ***************************
                  Trigger: ins_sum
                    Event: INSERT
                    Table: account
                Statement: SET @sum = @sum + NEW.amount
                   Timing: BEFORE
                  Created: NULL
                 sql_mode:
                  Definer: me@localhost
     character_set_client: utf8
     collation_connection: utf8_general_ci
       Database Collation: latin1_swedish_ci

*note 'SHOW TRIGGERS': show-triggers. output has these columns:

   * 'Trigger'

     The name of the trigger.

   * 'Event'

     The trigger event.  This is the type of operation on the associated
     table for which the trigger activates.  The value is 'INSERT' (a
     row was inserted), 'DELETE' (a row was deleted), or 'UPDATE' (a row
     was modified).

   * 'Table'

     The table for which the trigger is defined.

   * 'Statement'

     The trigger body; that is, the statement executed when the trigger
     activates.

   * 'Timing'

     Whether the trigger activates before or after the triggering event.
     The value is 'BEFORE' or 'AFTER'.

   * 'Created'

     The value of this column is always 'NULL'.

   * 'sql_mode'

     The SQL mode in effect when the trigger was created, and under
     which the trigger executes.  For the permitted values, see *note
     sql-mode::.

   * 'Definer'

     The account of the user who created the trigger, in
     ''USER_NAME'@'HOST_NAME'' format.

   * 'character_set_client'

     The session value of the 'character_set_client' system variable
     when the trigger was created.

   * 'collation_connection'

     The session value of the 'collation_connection' system variable
     when the trigger was created.

   * 'Database Collation'

     The collation of the database with which the trigger is associated.

Trigger information is also available from the 'INFORMATION_SCHEMA'
*note 'TRIGGERS': triggers-table. table.  See *note triggers-table::.


File: manual.info.tmp,  Node: show-variables,  Next: show-warnings,  Prev: show-triggers,  Up: show

13.7.5.40 SHOW VARIABLES Statement
..................................

     SHOW [GLOBAL | SESSION] VARIABLES
         [LIKE 'PATTERN' | WHERE EXPR]

*note 'SHOW VARIABLES': show-variables. shows the values of MySQL system
variables (see *note server-system-variables::).  This statement does
not require any privilege.  It requires only the ability to connect to
the server.

System variable information is also available from these sources:

   * The *note 'GLOBAL_VARIABLES': variables-table. and *note
     'SESSION_VARIABLES': variables-table. tables.  See *note
     variables-table::.

   * The *note 'mysqladmin variables': mysqladmin. command.  See *note
     mysqladmin::.

For *note 'SHOW VARIABLES': show-variables, a 'LIKE' clause, if present,
indicates which variable names to match.  A 'WHERE' clause can be given
to select rows using more general conditions, as discussed in *note
extended-show::.

*note 'SHOW VARIABLES': show-variables. accepts an optional 'GLOBAL' or
'SESSION' variable scope modifier:

   * With a 'GLOBAL' modifier, the statement displays global system
     variable values.  These are the values used to initialize the
     corresponding session variables for new connections to MySQL. If a
     variable has no global value, no value is displayed.

   * With a 'SESSION' modifier, the statement displays the system
     variable values that are in effect for the current connection.  If
     a variable has no session value, the global value is displayed.
     'LOCAL' is a synonym for 'SESSION'.

   * If no modifier is present, the default is 'SESSION'.

The scope for each system variable is listed at *note
server-system-variables::.

*note 'SHOW VARIABLES': show-variables. is subject to a
version-dependent display-width limit.  For variables with very long
values that are not completely displayed, use *note 'SELECT': select. as
a workaround.  For example:

     SELECT @@GLOBAL.innodb_data_file_path;

Most system variables can be set at server startup (read-only variables
such as 'version_comment' are exceptions).  Many can be changed at
runtime with the *note 'SET': set-variable. statement.  See *note
using-system-variables::, and *note set-variable::.

Partial output is shown here.  The list of names and values may differ
for your server.  *note server-system-variables::, describes the meaning
of each variable, and *note server-configuration::, provides information
about tuning them.

     mysql> SHOW VARIABLES;
     +-----------------------------------------+---------------------------+
     | Variable_name                           | Value                     |
     +-----------------------------------------+---------------------------+
     | auto_increment_increment                | 1                         |
     | auto_increment_offset                   | 1                         |
     | autocommit                              | ON                        |
     | automatic_sp_privileges                 | ON                        |
     | back_log                                | 50                        |
     | basedir                                 | /home/jon/bin/mysql-5.5   |
     | big_tables                              | OFF                       |
     | binlog_cache_size                       | 32768                     |
     | binlog_direct_non_transactional_updates | OFF                       |
     | binlog_format                           | STATEMENT                 |
     | binlog_stmt_cache_size                  | 32768                     |
     | bulk_insert_buffer_size                 | 8388608                   |
     ...
     | max_allowed_packet                      | 1048576                   |
     | max_binlog_cache_size                   | 18446744073709547520      |
     | max_binlog_size                         | 1073741824                |
     | max_binlog_stmt_cache_size              | 18446744073709547520      |
     | max_connect_errors                      | 10                        |
     | max_connections                         | 151                       |
     | max_delayed_threads                     | 20                        |
     | max_error_count                         | 64                        |
     | max_heap_table_size                     | 16777216                  |
     | max_insert_delayed_threads              | 20                        |
     | max_join_size                           | 18446744073709551615      |
     ...

     | thread_handling                         | one-thread-per-connection |
     | thread_stack                            | 262144                    |
     | time_format                             | %H:%i:%s                  |
     | time_zone                               | SYSTEM                    |
     | timed_mutexes                           | OFF                       |
     | timestamp                               | 1316689732                |
     | tmp_table_size                          | 16777216                  |
     | tmpdir                                  | /tmp                      |
     | transaction_alloc_block_size            | 8192                      |
     | transaction_prealloc_size               | 4096                      |
     | tx_isolation                            | REPEATABLE-READ           |
     | unique_checks                           | ON                        |
     | updatable_views_with_limit              | YES                       |
     | version                                 | 5.5.17-log                |
     | version_comment                         | Source distribution       |
     | version_compile_machine                 | x86_64                    |
     | version_compile_os                      | Linux                     |
     | wait_timeout                            | 28800                     |
     | warning_count                           | 0                         |
     +-----------------------------------------+---------------------------+

With a 'LIKE' clause, the statement displays only rows for those
variables with names that match the pattern.  To obtain the row for a
specific variable, use a 'LIKE' clause as shown:

     SHOW VARIABLES LIKE 'max_join_size';
     SHOW SESSION VARIABLES LIKE 'max_join_size';

To get a list of variables whose name match a pattern, use the '%'
wildcard character in a 'LIKE' clause:

     SHOW VARIABLES LIKE '%size%';
     SHOW GLOBAL VARIABLES LIKE '%size%';

Wildcard characters can be used in any position within the pattern to be
matched.  Strictly speaking, because '_' is a wildcard that matches any
single character, you should escape it as '\_' to match it literally.
In practice, this is rarely necessary.


File: manual.info.tmp,  Node: show-warnings,  Prev: show-variables,  Up: show

13.7.5.41 SHOW WARNINGS Statement
.................................

     SHOW WARNINGS [LIMIT [OFFSET,] ROW_COUNT]
     SHOW COUNT(*) WARNINGS

*note 'SHOW WARNINGS': show-warnings. is a diagnostic statement that
displays information about the conditions (errors, warnings, and notes)
resulting from executing a statement in the current session.  Warnings
are generated for DML statements such as *note 'INSERT': insert, *note
'UPDATE': update, and *note 'LOAD DATA': load-data. as well as DDL
statements such as *note 'CREATE TABLE': create-table. and *note 'ALTER
TABLE': alter-table.

The 'LIMIT' clause has the same syntax as for the *note 'SELECT':
select. statement.  See *note select::.

*note 'SHOW WARNINGS': show-warnings. is also used following *note
'EXPLAIN EXTENDED': explain-extended, to display the extra information
generated by *note 'EXPLAIN': explain. when the 'EXTENDED' keyword is
used.  See *note explain-extended::.

*note 'SHOW WARNINGS': show-warnings. displays information about the
conditions resulting from the most recent statement in the current
session that generated messages.  It shows nothing if the most recent
statement used a table and generated no messages.  (That is, statements
that use a table but generate no messages clear the message list.)
Statements that do not use tables and do not generate messages have no
effect on the message list.

The *note 'SHOW COUNT(*) WARNINGS': show-warnings. diagnostic statement
displays the total number of errors, warnings, and notes.  You can also
retrieve this number from the 'warning_count' system variable:

     SHOW COUNT(*) WARNINGS;
     SELECT @@warning_count;

A related diagnostic statement, *note 'SHOW ERRORS': show-errors, shows
only error conditions (it excludes warnings and notes), and *note 'SHOW
COUNT(*) ERRORS': show-warnings. statement displays the total number of
errors.  See *note show-errors::.

Here is a simple example that shows data-conversion warnings for *note
'INSERT': insert.  The example assumes that strict SQL mode is disabled.
With strict mode enabled, the warnings would become errors and terminate
the *note 'INSERT': insert.

     mysql> CREATE TABLE t1 (a TINYINT NOT NULL, b CHAR(4));
     Query OK, 0 rows affected (0.05 sec)

     mysql> INSERT INTO t1 VALUES(10,'mysql'), (NULL,'test'), (300,'xyz');
     Query OK, 3 rows affected, 3 warnings (0.00 sec)
     Records: 3  Duplicates: 0  Warnings: 3

     mysql> SHOW WARNINGS\G
     *************************** 1. row ***************************
       Level: Warning
        Code: 1265
     Message: Data truncated for column 'b' at row 1
     *************************** 2. row ***************************
       Level: Warning
        Code: 1048
     Message: Column 'a' cannot be null
     *************************** 3. row ***************************
       Level: Warning
        Code: 1264
     Message: Out of range value for column 'a' at row 3
     3 rows in set (0.00 sec)

The 'max_error_count' system variable controls the maximum number of
error, warning, and note messages for which the server stores
information, and thus the number of messages that *note 'SHOW WARNINGS':
show-warnings. displays.  To change the number of messages the server
can store, change the value of 'max_error_count'.  The default is 64.

'max_error_count' controls only how many messages are stored, not how
many are counted.  The value of 'warning_count' is not limited by
'max_error_count', even if the number of messages generated exceeds
'max_error_count'.  The following example demonstrates this.  The *note
'ALTER TABLE': alter-table. statement produces three warning messages
(strict SQL mode is disabled for the example to prevent an error from
occuring after a single conversion issue).  Only one message is stored
and displayed because 'max_error_count' has been set to 1, but all three
are counted (as shown by the value of 'warning_count'):

     mysql> SHOW VARIABLES LIKE 'max_error_count';
     +-----------------+-------+
     | Variable_name   | Value |
     +-----------------+-------+
     | max_error_count | 64    |
     +-----------------+-------+
     1 row in set (0.00 sec)

     mysql> SET max_error_count=1, sql_mode = '';
     Query OK, 0 rows affected (0.00 sec)

     mysql> ALTER TABLE t1 MODIFY b CHAR;
     Query OK, 3 rows affected, 3 warnings (0.00 sec)
     Records: 3  Duplicates: 0  Warnings: 3

     mysql> SHOW WARNINGS;
     +---------+------+----------------------------------------+
     | Level   | Code | Message                                |
     +---------+------+----------------------------------------+
     | Warning | 1263 | Data truncated for column 'b' at row 1 |
     +---------+------+----------------------------------------+
     1 row in set (0.00 sec)

     mysql> SELECT @@warning_count;
     +-----------------+
     | @@warning_count |
     +-----------------+
     |               3 |
     +-----------------+
     1 row in set (0.01 sec)

To disable message storage, set 'max_error_count' to 0.  In this case,
'warning_count' still indicates how many warnings occurred, but messages
are not stored and cannot be displayed.

The 'sql_notes' system variable controls whether note messages increment
'warning_count' and whether the server stores them.  By default,
'sql_notes' is 1, but if set to 0, notes do not increment
'warning_count' and the server does not store them:

     mysql> SET sql_notes = 1;
     mysql> DROP TABLE IF EXISTS test.no_such_table;
     Query OK, 0 rows affected, 1 warning (0.00 sec)
     mysql> SHOW WARNINGS;
     +-------+------+-------------------------------+
     | Level | Code | Message                       |
     +-------+------+-------------------------------+
     | Note  | 1051 | Unknown table 'no_such_table' |
     +-------+------+-------------------------------+
     1 row in set (0.00 sec)

     mysql> SET sql_notes = 0;
     mysql> DROP TABLE IF EXISTS test.no_such_table;
     Query OK, 0 rows affected (0.00 sec)
     mysql> SHOW WARNINGS;
     Empty set (0.00 sec)

The MySQL server sends to each client a count indicating the total
number of errors, warnings, and notes resulting from the most recent
statement executed by that client.  From the C API, this value can be
obtained by calling *note 'mysql_warning_count()': mysql-warning-count.
See *note mysql-warning-count::.

In the *note 'mysql': mysql. client, you can enable and disable
automatic warnings display using the 'warnings' and 'nowarning'
commands, respectively, or their shortcuts, '\W' and '\w' (see *note
mysql-commands::).  For example:

     mysql> \W
     Show warnings enabled.
     mysql> SELECT 1/0;
     +------+
     | 1/0  |
     +------+
     | NULL |
     +------+
     1 row in set, 1 warning (0.03 sec)

     Warning (Code 1365): Division by 0
     mysql> \w
     Show warnings disabled.


File: manual.info.tmp,  Node: other-administrative-statements,  Prev: show,  Up: sql-server-administration-statements

13.7.6 Other Administrative Statements
--------------------------------------

* Menu:

* binlog::                       BINLOG Statement
* cache-index::                  CACHE INDEX Statement
* flush::                        FLUSH Statement
* kill::                         KILL Statement
* load-index::                   LOAD INDEX INTO CACHE Statement
* reset::                        RESET Statement


File: manual.info.tmp,  Node: binlog,  Next: cache-index,  Prev: other-administrative-statements,  Up: other-administrative-statements

13.7.6.1 BINLOG Statement
.........................

     BINLOG 'STR'

*note 'BINLOG': binlog. is an internal-use statement.  It is generated
by the *note 'mysqlbinlog': mysqlbinlog. program as the printable
representation of certain events in binary log files.  (See *note
mysqlbinlog::.)  The ''STR'' value is a base 64-encoded string the that
server decodes to determine the data change indicated by the
corresponding event.  This statement requires the 'SUPER' privilege.


File: manual.info.tmp,  Node: cache-index,  Next: flush,  Prev: binlog,  Up: other-administrative-statements

13.7.6.2 CACHE INDEX Statement
..............................

     CACHE INDEX {
           TBL_INDEX_LIST [, TBL_INDEX_LIST] ...
         | TBL_NAME PARTITION (PARTITION_LIST)
       }
       IN KEY_CACHE_NAME

     TBL_INDEX_LIST:
       TBL_NAME [{INDEX|KEY} (INDEX_NAME[, INDEX_NAME] ...)]

     PARTITION_LIST: {
         PARTITION_NAME[, PARTITION_NAME] ...
       | ALL
     }

The *note 'CACHE INDEX': cache-index. statement assigns table indexes to
a specific key cache.  It applies only to 'MyISAM' tables, including
partitioned 'MyISAM' tables.  After the indexes have been assigned, they
can be preloaded into the cache if desired with *note 'LOAD INDEX INTO
CACHE': load-index.

The following statement assigns indexes from the tables 't1', 't2', and
't3' to the key cache named 'hot_cache':

     mysql> CACHE INDEX t1, t2, t3 IN hot_cache;
     +---------+--------------------+----------+----------+
     | Table   | Op                 | Msg_type | Msg_text |
     +---------+--------------------+----------+----------+
     | test.t1 | assign_to_keycache | status   | OK       |
     | test.t2 | assign_to_keycache | status   | OK       |
     | test.t3 | assign_to_keycache | status   | OK       |
     +---------+--------------------+----------+----------+

The syntax of *note 'CACHE INDEX': cache-index. enables you to specify
that only particular indexes from a table should be assigned to the
cache.  However, the implementation assigns all the table's indexes to
the cache, so there is no reason to specify anything other than the
table name.

The key cache referred to in a *note 'CACHE INDEX': cache-index.
statement can be created by setting its size with a parameter setting
statement or in the server parameter settings.  For example:

     SET GLOBAL keycache1.key_buffer_size=128*1024;

Key cache parameters are accessed as members of a structured system
variable.  See *note structured-system-variables::.

A key cache must exist before you assign indexes to it, or an error
occurs:

     mysql> CACHE INDEX t1 IN non_existent_cache;
     ERROR 1284 (HY000): Unknown key cache 'non_existent_cache'

By default, table indexes are assigned to the main (default) key cache
created at the server startup.  When a key cache is destroyed, all
indexes assigned to it are reassigned to the default key cache.

Index assignment affects the server globally: If one client assigns an
index to a given cache, this cache is used for all queries involving the
index, no matter which client issues the queries.

*note 'CACHE INDEX': cache-index. is supported for partitioned 'MyISAM'
tables.  You can assign one or more indexes for one, several, or all
partitions to a given key cache.  For example, you can do the following:

     CREATE TABLE pt (c1 INT, c2 VARCHAR(50), INDEX i(c1))
         ENGINE=MyISAM
         PARTITION BY HASH(c1)
         PARTITIONS 4;

     SET GLOBAL kc_fast.key_buffer_size = 128 * 1024;
     SET GLOBAL kc_slow.key_buffer_size = 128 * 1024;

     CACHE INDEX pt PARTITION (p0) IN kc_fast;
     CACHE INDEX pt PARTITION (p1, p3) IN kc_slow;

The previous set of statements performs the following actions:

   * Creates a partitioned table with 4 partitions; these partitions are
     automatically named 'p0', ..., 'p3'; this table has an index named
     'i' on column 'c1'.

   * Creates 2 key caches named 'kc_fast' and 'kc_slow'

   * Assigns the index for partition 'p0' to the 'kc_fast' key cache and
     the index for partitions 'p1' and 'p3' to the 'kc_slow' key cache;
     the index for the remaining partition ('p2') uses the server's
     default key cache.

If you wish instead to assign the indexes for all partitions in table
'pt' to a single key cache named 'kc_all', you can use either of the
following two statements:

     CACHE INDEX pt PARTITION (ALL) IN kc_all;

     CACHE INDEX pt IN kc_all;

The two statements just shown are equivalent, and issuing either one has
exactly the same effect.  In other words, if you wish to assign indexes
for all partitions of a partitioned table to the same key cache, the
'PARTITION (ALL)' clause is optional.

When assigning indexes for multiple partitions to a key cache, the
partitions need not be contiguous, and you need not list their names in
any particular order.  Indexes for any partitions not explicitly
assigned to a key cache automatically use the server default key cache.

Index preloading is also supported for partitioned 'MyISAM' tables.  For
more information, see *note load-index::.


File: manual.info.tmp,  Node: flush,  Next: kill,  Prev: cache-index,  Up: other-administrative-statements

13.7.6.3 FLUSH Statement
........................

     FLUSH [NO_WRITE_TO_BINLOG | LOCAL] {
         FLUSH_OPTION [, FLUSH_OPTION] ...
       | TABLES_OPTION
     }

     FLUSH_OPTION: {
         BINARY LOGS
       | DES_KEY_FILE
       | ENGINE LOGS
       | ERROR LOGS
       | GENERAL LOGS
       | HOSTS
       | LOGS
       | MASTER
       | PRIVILEGES
       | QUERY CACHE
       | RELAY LOGS
       | SLAVE
       | SLOW LOGS
       | STATUS
       | USER_RESOURCES
     }

     TABLES_OPTION: {
         TABLES
       | TABLES TBL_NAME [, TBL_NAME] ...
       | TABLES WITH READ LOCK
       | TABLES TBL_NAME [, TBL_NAME] ... WITH READ LOCK
     }

The *note 'FLUSH': flush. statement has several variant forms that clear
or reload various internal caches, flush tables, or acquire locks.  To
execute *note 'FLUSH': flush, you must have the 'RELOAD' privilege.
Specific flush options might require additional privileges, as described
later.

*Note*:

It is not possible to issue *note 'FLUSH': flush. statements within
stored functions or triggers.  However, you may use *note 'FLUSH':
flush. in stored procedures, so long as these are not called from stored
functions or triggers.  See *note stored-program-restrictions::.

By default, the server writes *note 'FLUSH': flush. statements to the
binary log so that they replicate to replication slaves.  To suppress
logging, specify the optional 'NO_WRITE_TO_BINLOG' keyword or its alias
'LOCAL'.

*Note*:

'FLUSH LOGS', 'FLUSH MASTER', 'FLUSH SLAVE', and 'FLUSH TABLES WITH READ
LOCK' (with or without a table list) are not written to the binary log
in any case because they would cause problems if replicated to a slave.

The *note 'FLUSH': flush. statement causes an implicit commit.  See
*note implicit-commit::.

The *note 'mysqladmin': mysqladmin. utility provides a command-line
interface to some flush operations, using commands such as
'flush-hosts', 'flush-logs', 'flush-privileges', 'flush-status', and
'flush-tables'.  See *note mysqladmin::.

Sending a 'SIGHUP' signal to the server causes several flush operations
to occur that are similar to various forms of the *note 'FLUSH': flush.
statement.  Signals can be sent by 'root' or the account that owns the
server process.  They enable the applicable flush operations to be
performed without having to connect to the server (which for these
operations requires an account that has the 'RELOAD' privilege).  See
*note unix-signal-response::.

The *note 'RESET': reset. statement is similar to *note 'FLUSH': flush.
See *note reset::, for information about using the *note 'RESET': reset.
statement with replication.

The following list describes the permitted *note 'FLUSH': flush.
statement FLUSH_OPTION values.  For descriptions of 'FLUSH TABLES'
variants, see *note flush-tables-variants::.

   * 'FLUSH BINARY LOGS'

     Closes and reopens any binary log file to which the server is
     writing.  If binary logging is enabled, the sequence number of the
     binary log file is incremented by one relative to the previous
     file.

   * 'FLUSH DES_KEY_FILE'

     Reloads the DES keys from the file that was specified with the
     '--des-key-file' option at server startup time.

   * 'FLUSH ENGINE LOGS'

     Closes and reopens any flushable logs for installed storage
     engines.  This causes 'InnoDB' to flush its logs to disk.

   * 'FLUSH ERROR LOGS'

     Closes and reopens any error log file to which the server is
     writing.

   * 'FLUSH GENERAL LOGS'

     Closes and reopens any general query log file to which the server
     is writing.

   * 'FLUSH HOSTS'

     Empties the host cache and unblocks any blocked hosts (see *note
     host-cache::).  Flush the host cache if some of your hosts change
     IP address or if the error message 'Host 'HOST_NAME' is blocked'
     occurs for connections from legitimate hosts.  (See *note
     blocked-host::.)  When more than 'max_connect_errors' errors occur
     successively for a given host while connecting to the MySQL server,
     MySQL assumes that something is wrong and blocks the host from
     further connection requests.  Flushing the host cache enables
     further connection attempts from the host.  The default value of
     'max_connect_errors' is 10.  To avoid this error message, start the
     server with 'max_connect_errors' set to a large value.

   * 'FLUSH LOGS'

     Closes and reopens any log file to which the server is writing.  If
     binary logging is enabled, the sequence number of the binary log
     file is incremented by one relative to the previous file.  If relay
     logging is enabled, the sequence number of the relay log file is
     incremented by one relative to the previous file.

     'FLUSH LOGS' has no effect on tables used for the general query log
     or for the slow query log (see *note log-destinations::).

   * 'FLUSH MASTER'

     Deletes all binary logs, resets the binary log index file and
     creates a new binary log.  'FLUSH MASTER' is deprecated in favor of
     *note 'RESET MASTER': reset-master.  'FLUSH MASTER' is still
     accepted in MySQL 5.5 for backward compatibility, but is removed in
     MySQL 5.6.  See *note reset-master::.

   * 'FLUSH PRIVILEGES'

     Reloads the privileges from the grant tables in the 'mysql' system
     database.

     If the '--skip-grant-tables' option was specified at server startup
     to disable the MySQL privilege system, 'FLUSH PRIVILEGES' provides
     a way to enable the privilege system at runtime.

     Frees memory cached by the server as a result of *note 'GRANT':
     grant, *note 'CREATE USER': create-user, *note 'CREATE SERVER':
     create-server, and *note 'INSTALL PLUGIN': install-plugin.
     statements.  This memory is not released by the corresponding *note
     'REVOKE': revoke, *note 'DROP USER': drop-user, *note 'DROP
     SERVER': drop-server, and *note 'UNINSTALL PLUGIN':
     uninstall-plugin. statements, so for a server that executes many
     instances of the statements that cause caching, there will be an
     increase in cached memory use unless it is freed with 'FLUSH
     PRIVILEGES'.

   * 'FLUSH QUERY CACHE'

     Defragment the query cache to better utilize its memory.  'FLUSH
     QUERY CACHE' does not remove any queries from the cache, unlike
     'FLUSH TABLES' or 'RESET QUERY CACHE'.

   * 'FLUSH RELAY LOGS'

     Closes and reopens any relay log file to which the server is
     writing.  If relay logging is enabled, the sequence number of the
     relay log file is incremented by one relative to the previous file.

   * 'FLUSH SLAVE'

     Resets all replication slave parameters, including relay log files
     and replication position in the master's binary logs.  'FLUSH
     SLAVE' is deprecated in favor of *note 'RESET SLAVE': reset-slave.
     'FLUSH SLAVE' is still accepted in MySQL 5.5 for backward
     compatibility, but is removed in MySQL 5.6.  See *note
     reset-slave::.

   * 'FLUSH SLOW LOGS'

     Closes and reopens any slow query log file to which the server is
     writing.

   * 'FLUSH STATUS'

     This option adds the current thread's session status variable
     values to the global values and resets the session values to zero.
     Some global variables may be reset to zero as well.  It also resets
     the counters for key caches (default and named) to zero and sets
     'Max_used_connections' to the current number of open connections.
     This information may be of use when debugging a query.  See *note
     bug-reports::.

   * 'FLUSH USER_RESOURCES'

     Resets all per-hour user resources to zero.  This enables clients
     that have reached their hourly connection, query, or update limits
     to resume activity immediately.  'FLUSH USER_RESOURCES' does not
     apply to the limit on maximum simultaneous connections that is
     controlled by the 'max_user_connections' system variable.  See
     *note user-resources::.

*FLUSH TABLES Syntax*

'FLUSH TABLES' flushes tables, and, depending on the variant used,
acquires locks.  Any 'TABLES' variant used in a *note 'FLUSH': flush.
statement must be the only option used.  'FLUSH TABLE' is a synonym for
'FLUSH TABLES'.

*Note*:

The descriptions here that indicate tables are flushed by closing them
apply differently for 'InnoDB', which flushes table contents to disk but
leaves them open.  This still permits table files to be copied while the
tables are open, as long as other activity does not modify them.

   * 'FLUSH TABLES'

     Closes all open tables, forces all tables in use to be closed, and
     flushes the query cache.  'FLUSH TABLES' also removes all query
     results from the query cache, like the 'RESET QUERY CACHE'
     statement.

     'FLUSH TABLES' is not permitted when there is an active *note 'LOCK
     TABLES ... READ': lock-tables.  To flush and lock tables, use
     'FLUSH TABLES TBL_NAME ... WITH READ LOCK' instead.

   * 'FLUSH TABLES TBL_NAME [, TBL_NAME] ...'

     With a list of one or more comma-separated table names, this
     statement is like 'FLUSH TABLES' with no names except that the
     server flushes only the named tables.  If a named table does not
     exist, no error occurs.

   * 'FLUSH TABLES WITH READ LOCK'

     Closes all open tables and locks all tables for all databases with
     a global read lock.  This is a very convenient way to get backups
     if you have a file system such as Veritas or ZFS that can take
     snapshots in time.  Use *note 'UNLOCK TABLES': lock-tables. to
     release the lock.

     'FLUSH TABLES WITH READ LOCK' acquires a global read lock rather
     than table locks, so it is not subject to the same behavior as
     *note 'LOCK TABLES': lock-tables. and *note 'UNLOCK TABLES':
     lock-tables. with respect to table locking and implicit commits:

        * *note 'UNLOCK TABLES': lock-tables. implicitly commits any
          active transaction only if any tables currently have been
          locked with *note 'LOCK TABLES': lock-tables.  The commit does
          not occur for *note 'UNLOCK TABLES': lock-tables. following
          'FLUSH TABLES WITH READ LOCK' because the latter statement
          does not acquire table locks.

        * Beginning a transaction causes table locks acquired with *note
          'LOCK TABLES': lock-tables. to be released, as though you had
          executed *note 'UNLOCK TABLES': lock-tables.  Beginning a
          transaction does not release a global read lock acquired with
          'FLUSH TABLES WITH READ LOCK'.

     'FLUSH TABLES WITH READ LOCK' is not compatible with XA
     transactions.

     'FLUSH TABLES WITH READ LOCK' does not prevent the server from
     inserting rows into the log tables (see *note log-destinations::).

   * 'FLUSH TABLES TBL_NAME [, TBL_NAME] ... WITH READ LOCK'

     This statement flushes and acquires read locks for the named
     tables.  The statement first acquires exclusive metadata locks for
     the tables, so it waits for transactions that have those tables
     open to complete.  Then the statement flushes the tables from the
     table cache, reopens the tables, acquires table locks (like *note
     'LOCK TABLES ... READ': lock-tables.), and downgrades the metadata
     locks from exclusive to shared.  After the statement acquires locks
     and downgrades the metadata locks, other sessions can read but not
     modify the tables.

     Because this statement acquires table locks, you must have the
     'LOCK TABLES' privilege for each table, in addition to the 'RELOAD'
     privilege that is required to use any *note 'FLUSH': flush.
     statement.

     This statement applies only to existing base (non-'TEMPORARY)
     'tables.  If a name refers to a base table, that table is used.  If
     it refers to a 'TEMPORARY' table, it is ignored.  If a name applies
     to a view, an 'ER_WRONG_OBJECT' error occurs.  Otherwise, an
     'ER_NO_SUCH_TABLE' error occurs.

     Use *note 'UNLOCK TABLES': lock-tables. to release the locks, *note
     'LOCK TABLES': lock-tables. to release the locks and acquire other
     locks, or *note 'START TRANSACTION': commit. to release the locks
     and begin a new transaction.

     This 'FLUSH TABLES' variant enables tables to be flushed and locked
     in a single operation.  It provides a workaround for the
     restriction that 'FLUSH TABLES' is not permitted when there is an
     active *note 'LOCK TABLES ... READ': lock-tables.

     This statement does not perform an implicit *note 'UNLOCK TABLES':
     lock-tables, so an error results if you use the statement while
     there is any active *note 'LOCK TABLES': lock-tables. or use it a
     second time without first releasing the locks acquired.

     If a flushed table was opened with *note 'HANDLER': handler, the
     handler is implicitly flushed and loses its position.


File: manual.info.tmp,  Node: kill,  Next: load-index,  Prev: flush,  Up: other-administrative-statements

13.7.6.4 KILL Statement
.......................

     KILL [CONNECTION | QUERY] PROCESSLIST_ID

Each connection to *note 'mysqld': mysqld. runs in a separate thread.
You can kill a thread with the 'KILL PROCESSLIST_ID' statement.

Thread processlist identifiers can be determined from the 'ID' column of
the 'INFORMATION_SCHEMA' *note 'PROCESSLIST': processlist-table. table,
the 'Id' column of *note 'SHOW PROCESSLIST': show-processlist. output,
and the 'PROCESSLIST_ID' column of the Performance Schema *note
'threads': threads-table. table.  The value for the current thread is
returned by the 'CONNECTION_ID()' function.

*note 'KILL': kill. permits an optional 'CONNECTION' or 'QUERY'
modifier:

   * *note 'KILL CONNECTION': kill. is the same as *note 'KILL': kill.
     with no modifier: It terminates the connection associated with the
     given PROCESSLIST_ID, after terminating any statement the
     connection is executing.

   * *note 'KILL QUERY': kill. terminates the statement the connection
     is currently executing, but leaves the connection itself intact.

The ability to see which threads are available to be killed depends on
the 'PROCESS' privilege:

   * Without 'PROCESS', you can see only your own threads.

   * With 'PROCESS', you can see all threads.

The ability to kill threads and statements depends on the 'SUPER'
privilege:

   * Without 'SUPER', you can kill only your own threads and statements.

   * With 'SUPER', you can kill all threads and statements.

You can also use the *note 'mysqladmin processlist': mysqladmin. and
*note 'mysqladmin kill': mysqladmin. commands to examine and kill
threads.

*Note*:

You cannot use *note 'KILL': kill. with the Embedded MySQL Server
library because the embedded server merely runs inside the threads of
the host application.  It does not create any connection threads of its
own.

When you use *note 'KILL': kill, a thread-specific kill flag is set for
the thread.  In most cases, it might take some time for the thread to
die because the kill flag is checked only at specific intervals:

   * During *note 'SELECT': select. operations, for 'ORDER BY' and
     'GROUP BY' loops, the flag is checked after reading a block of
     rows.  If the kill flag is set, the statement is aborted.

   * *note 'ALTER TABLE': alter-table. operations that make a table copy
     check the kill flag periodically for each few copied rows read from
     the original table.  If the kill flag was set, the statement is
     aborted and the temporary table is deleted.

     The *note 'KILL': kill. statement returns without waiting for
     confirmation, but the kill flag check aborts the operation within a
     reasonably small amount of time.  Aborting the operation to perform
     any necessary cleanup also takes some time.

   * During *note 'UPDATE': update. or *note 'DELETE': delete.
     operations, the kill flag is checked after each block read and
     after each updated or deleted row.  If the kill flag is set, the
     statement is aborted.  If you are not using transactions, the
     changes are not rolled back.

   * 'GET_LOCK()' aborts and returns 'NULL'.

   * An *note 'INSERT DELAYED': insert-delayed. thread quickly flushes
     (inserts) all rows it has in memory and then terminates.

   * If the thread is in the table lock handler (state: 'Locked'), the
     table lock is quickly aborted.

   * If the thread is waiting for free disk space in a write call, the
     write is aborted with a 'disk full' error message.

*Warning*:

Killing a *note 'REPAIR TABLE': repair-table. or *note 'OPTIMIZE TABLE':
optimize-table. operation on a 'MyISAM' table results in a table that is
corrupted and unusable.  Any reads or writes to such a table fail until
you optimize or repair it again (without interruption).


File: manual.info.tmp,  Node: load-index,  Next: reset,  Prev: kill,  Up: other-administrative-statements

13.7.6.5 LOAD INDEX INTO CACHE Statement
........................................

     LOAD INDEX INTO CACHE
       TBL_INDEX_LIST [, TBL_INDEX_LIST] ...

     TBL_INDEX_LIST:
       TBL_NAME
         [PARTITION (PARTITION_LIST)]
         [{INDEX|KEY} (INDEX_NAME[, INDEX_NAME] ...)]
         [IGNORE LEAVES]

     PARTITION_LIST: {
         PARTITION_NAME[, PARTITION_NAME] ...
       | ALL
     }

The *note 'LOAD INDEX INTO CACHE': load-index. statement preloads a
table index into the key cache to which it has been assigned by an
explicit *note 'CACHE INDEX': cache-index. statement, or into the
default key cache otherwise.

*note 'LOAD INDEX INTO CACHE': load-index. applies only to 'MyISAM'
tables, including partitioned 'MyISAM' tables.  In addition, indexes on
partitioned tables can be preloaded for one, several, or all partitions.

The 'IGNORE LEAVES' modifier causes only blocks for the nonleaf nodes of
the index to be preloaded.

'IGNORE LEAVES' is also supported for partitioned 'MyISAM' tables.

The following statement preloads nodes (index blocks) of indexes for the
tables 't1' and 't2':

     mysql> LOAD INDEX INTO CACHE t1, t2 IGNORE LEAVES;
     +---------+--------------+----------+----------+
     | Table   | Op           | Msg_type | Msg_text |
     +---------+--------------+----------+----------+
     | test.t1 | preload_keys | status   | OK       |
     | test.t2 | preload_keys | status   | OK       |
     +---------+--------------+----------+----------+

This statement preloads all index blocks from 't1'.  It preloads only
blocks for the nonleaf nodes from 't2'.

The syntax of *note 'LOAD INDEX INTO CACHE': load-index. enables you to
specify that only particular indexes from a table should be preloaded.
However, the implementation preloads all the table's indexes into the
cache, so there is no reason to specify anything other than the table
name.

It is possible to preload indexes on specific partitions of partitioned
'MyISAM' tables.  For example, of the following 2 statements, the first
preloads indexes for partition 'p0' of a partitioned table 'pt', while
the second preloads the indexes for partitions 'p1' and 'p3' of the same
table:

     LOAD INDEX INTO CACHE pt PARTITION (p0);
     LOAD INDEX INTO CACHE pt PARTITION (p1, p3);

To preload the indexes for all partitions in table 'pt', you can use
either of the following two statements:

     LOAD INDEX INTO CACHE pt PARTITION (ALL);

     LOAD INDEX INTO CACHE pt;

The two statements just shown are equivalent, and issuing either one has
exactly the same effect.  In other words, if you wish to preload indexes
for all partitions of a partitioned table, the 'PARTITION (ALL)' clause
is optional.

When preloading indexes for multiple partitions, the partitions need not
be contiguous, and you need not list their names in any particular
order.

*note 'LOAD INDEX INTO CACHE ... IGNORE LEAVES': load-index. fails
unless all indexes in a table have the same block size.  To determine
index block sizes for a table, use *note 'myisamchk -dv': myisamchk. and
check the 'Blocksize' column.


File: manual.info.tmp,  Node: reset,  Prev: load-index,  Up: other-administrative-statements

13.7.6.6 RESET Statement
........................

     RESET RESET_OPTION [, RESET_OPTION] ...

     RESET_OPTION: {
         MASTER
       | QUERY CACHE
       | SLAVE
     }

The *note 'RESET': reset. statement is used to clear the state of
various server operations.  You must have the 'RELOAD' privilege to
execute *note 'RESET': reset.

*note 'RESET': reset. acts as a stronger version of the *note 'FLUSH':
flush. statement.  See *note flush::.

The *note 'RESET': reset. statement causes an implicit commit.  See
*note implicit-commit::.

The following list describes the permitted *note 'RESET': reset.
statement RESET_OPTION values:

   * 'RESET MASTER'

     Deletes all binary logs listed in the index file, resets the binary
     log index file to be empty, and creates a new binary log file.

   * 'RESET QUERY CACHE'

     Removes all query results from the query cache.

   * 'RESET SLAVE'

     Makes the slave forget its replication position in the master
     binary logs.  Also resets the relay log by deleting any existing
     relay log files and beginning a new one.


File: manual.info.tmp,  Node: sql-utility-statements,  Prev: sql-server-administration-statements,  Up: sql-statements

13.8 Utility Statements
=======================

* Menu:

* describe::                     DESCRIBE Statement
* explain::                      EXPLAIN Statement
* help::                         HELP Statement
* use::                          USE Statement


File: manual.info.tmp,  Node: describe,  Next: explain,  Prev: sql-utility-statements,  Up: sql-utility-statements

13.8.1 DESCRIBE Statement
-------------------------

The *note 'DESCRIBE': describe. and *note 'EXPLAIN': explain. statements
are synonyms, used either to obtain information about table structure or
query execution plans.  For more information, see *note show-columns::,
and *note explain::.


File: manual.info.tmp,  Node: explain,  Next: help,  Prev: describe,  Up: sql-utility-statements

13.8.2 EXPLAIN Statement
------------------------

     {EXPLAIN | DESCRIBE | DESC}
         TBL_NAME [COL_NAME | WILD]

     {EXPLAIN | DESCRIBE | DESC}
         [EXPLAIN_TYPE] SELECT SELECT_OPTIONS

     EXPLAIN_TYPE: {EXTENDED | PARTITIONS}

The *note 'DESCRIBE': describe. and *note 'EXPLAIN': explain. statements
are synonyms.  In practice, the *note 'DESCRIBE': describe. keyword is
more often used to obtain information about table structure, whereas
*note 'EXPLAIN': explain. is used to obtain a query execution plan (that
is, an explanation of how MySQL would execute a query).

The following discussion uses the *note 'DESCRIBE': describe. and *note
'EXPLAIN': explain. keywords in accordance with those uses, but the
MySQL parser treats them as completely synonymous.

   * *note explain-table-structure::

   * *note explain-execution-plan::

*Obtaining Table Structure Information*

*note 'DESCRIBE': describe. provides information about the columns in a
table:

     mysql> DESCRIBE City;
     +------------+----------+------+-----+---------+----------------+
     | Field      | Type     | Null | Key | Default | Extra          |
     +------------+----------+------+-----+---------+----------------+
     | Id         | int(11)  | NO   | PRI | NULL    | auto_increment |
     | Name       | char(35) | NO   |     |         |                |
     | Country    | char(3)  | NO   | UNI |         |                |
     | District   | char(20) | YES  | MUL |         |                |
     | Population | int(11)  | NO   |     | 0       |                |
     +------------+----------+------+-----+---------+----------------+

*note 'DESCRIBE': describe. is a shortcut for *note 'SHOW COLUMNS':
show-columns.  These statements also display information for views.  The
description for *note 'SHOW COLUMNS': show-columns. provides more
information about the output columns.  See *note show-columns::.

By default, *note 'DESCRIBE': describe. displays information about all
columns in the table.  COL_NAME, if given, is the name of a column in
the table.  In this case, the statement displays information only for
the named column.  WILD, if given, is a pattern string.  It can contain
the SQL '%' and '_' wildcard characters.  In this case, the statement
displays output only for the columns with names matching the string.
There is no need to enclose the string within quotation marks unless it
contains spaces or other special characters.

The *note 'DESCRIBE': describe. statement is provided for compatibility
with Oracle.

The *note 'SHOW CREATE TABLE': show-create-table, *note 'SHOW TABLE
STATUS': show-table-status, and *note 'SHOW INDEX': show-index.
statements also provide information about tables.  See *note show::.

*Obtaining Execution Plan Information*

The *note 'EXPLAIN': explain. statement provides information about how
MySQL executes statements:

   * When you precede a *note 'SELECT': select. statement with the
     keyword *note 'EXPLAIN': explain, MySQL displays information from
     the optimizer about the statement execution plan.  That is, MySQL
     explains how it would process the statement, including information
     about how tables are joined and in which order.  For information
     about using *note 'EXPLAIN': explain. to obtain execution plan
     information, see *note explain-output::.

   * *note 'EXPLAIN EXTENDED': explain-extended. produces additional
     execution plan information that can be displayed using *note 'SHOW
     WARNINGS': show-warnings.  See *note explain-extended::.

   * *note 'EXPLAIN PARTITIONS': explain. is useful for examining
     queries involving partitioned tables.  See *note
     partitioning-info::.

*note 'EXPLAIN': explain. requires the 'SELECT' privilege for any tables
or views accessed, including any underlying tables of views.  For views,
*note 'EXPLAIN': explain. also requires the 'SHOW VIEW' privilege.

With the help of *note 'EXPLAIN': explain, you can see where you should
add indexes to tables so that the statement executes faster by using
indexes to find rows.  You can also use *note 'EXPLAIN': explain. to
check whether the optimizer joins the tables in an optimal order.  To
give a hint to the optimizer to use a join order corresponding to the
order in which the tables are named in a *note 'SELECT': select.
statement, begin the statement with 'SELECT STRAIGHT_JOIN' rather than
just *note 'SELECT': select.  (See *note select::.)

If you have a problem with indexes not being used when you believe that
they should be, run *note 'ANALYZE TABLE': analyze-table. to update
table statistics, such as cardinality of keys, that can affect the
choices the optimizer makes.  See *note analyze-table::.

*Note*:

MySQL Workbench has a Visual Explain capability that provides a visual
representation of *note 'EXPLAIN': explain. output.  See Tutorial: Using
Explain to Improve Query Performance
(https://dev.mysql.com/doc/workbench/en/wb-tutorial-visual-explain-dbt3.html).


File: manual.info.tmp,  Node: help,  Next: use,  Prev: explain,  Up: sql-utility-statements

13.8.3 HELP Statement
---------------------

     HELP 'SEARCH_STRING'

The *note 'HELP': help. statement returns online information from the
MySQL Reference Manual.  Its proper operation requires that the help
tables in the 'mysql' database be initialized with help topic
information (see *note server-side-help-support::).

The *note 'HELP': help. statement searches the help tables for the given
search string and displays the result of the search.  The search string
is not case-sensitive.

The search string can contain the wildcard characters '%' and '_'.
These have the same meaning as for pattern-matching operations performed
with the 'LIKE' operator.  For example, 'HELP 'rep%'' returns a list of
topics that begin with 'rep'.

The HELP statement understands several types of search strings:

   * At the most general level, use 'contents' to retrieve a list of the
     top-level help categories:

          HELP 'contents'

   * For a list of topics in a given help category, such as 'Data
     Types', use the category name:

          HELP 'data types'

   * For help on a specific help topic, such as the 'ASCII()' function
     or the *note 'CREATE TABLE': create-table. statement, use the
     associated keyword or keywords:

          HELP 'ascii'
          HELP 'create table'

In other words, the search string matches a category, many topics, or a
single topic.  You cannot necessarily tell in advance whether a given
search string will return a list of items or the help information for a
single help topic.  However, you can tell what kind of response *note
'HELP': help. returned by examining the number of rows and columns in
the result set.

The following descriptions indicate the forms that the result set can
take.  Output for the example statements is shown using the familiar
'tabular' or 'vertical' format that you see when using the *note
'mysql': mysql. client, but note that *note 'mysql': mysql. itself
reformats *note 'HELP': help. result sets in a different way.

   * Empty result set

     No match could be found for the search string.

   * Result set containing a single row with three columns

     This means that the search string yielded a hit for the help topic.
     The result has three columns:

        * 'name': The topic name.

        * 'description': Descriptive help text for the topic.

        * 'example': Usage example or examples.  This column might be
          blank.

     Example: 'HELP 'replace''

     Yields:

          name: REPLACE
          description: Syntax:
          REPLACE(str,from_str,to_str)

          Returns the string str with all occurrences of the string from_str
          replaced by the string to_str. REPLACE() performs a case-sensitive
          match when searching for from_str.
          example: mysql> SELECT REPLACE('www.mysql.com', 'w', 'Ww');
                  -> 'WwWwWw.mysql.com'

   * Result set containing multiple rows with two columns

     This means that the search string matched many help topics.  The
     result set indicates the help topic names:

        * 'name': The help topic name.

        * 'is_it_category': 'Y' if the name represents a help category,
          'N' if it does not.  If it does not, the 'name' value when
          specified as the argument to the *note 'HELP': help. statement
          should yield a single-row result set containing a description
          for the named item.

     Example: 'HELP 'status''

     Yields:

          +-----------------------+----------------+
          | name                  | is_it_category |
          +-----------------------+----------------+
          | SHOW                  | N              |
          | SHOW ENGINE           | N              |
          | SHOW MASTER STATUS    | N              |
          | SHOW PROCEDURE STATUS | N              |
          | SHOW SLAVE STATUS     | N              |
          | SHOW STATUS           | N              |
          | SHOW TABLE STATUS     | N              |
          +-----------------------+----------------+

   * Result set containing multiple rows with three columns

     This means the search string matches a category.  The result set
     contains category entries:

        * 'source_category_name': The help category name.

        * 'name': The category or topic name

        * 'is_it_category': 'Y' if the name represents a help category,
          'N' if it does not.  If it does not, the 'name' value when
          specified as the argument to the *note 'HELP': help. statement
          should yield a single-row result set containing a description
          for the named item.

     Example: 'HELP 'functions''

     Yields:

          +----------------------+-------------------------+----------------+
          | source_category_name | name                    | is_it_category |
          +----------------------+-------------------------+----------------+
          | Functions            | CREATE FUNCTION         | N              |
          | Functions            | DROP FUNCTION           | N              |
          | Functions            | Bit Functions           | Y              |
          | Functions            | Comparison operators    | Y              |
          | Functions            | Control flow functions  | Y              |
          | Functions            | Date and Time Functions | Y              |
          | Functions            | Encryption Functions    | Y              |
          | Functions            | Information Functions   | Y              |
          | Functions            | Logical operators       | Y              |
          | Functions            | Miscellaneous Functions | Y              |
          | Functions            | Numeric Functions       | Y              |
          | Functions            | String Functions        | Y              |
          +----------------------+-------------------------+----------------+


File: manual.info.tmp,  Node: use,  Prev: help,  Up: sql-utility-statements

13.8.4 USE Statement
--------------------

     USE DB_NAME

The *note 'USE': use. statement tells MySQL to use the named database as
the default (current) database for subsequent statements.  This
statement requires some privilege for the database or some object within
it.

The named database remains the default until the end of the session or
another *note 'USE': use. statement is issued:

     USE db1;
     SELECT COUNT(*) FROM mytable;   # selects from db1.mytable
     USE db2;
     SELECT COUNT(*) FROM mytable;   # selects from db2.mytable

The database name must be specified on a single line.  Newlines in
database names are not supported.

Making a particular database the default by means of the *note 'USE':
use. statement does not preclude accessing tables in other databases.
The following example accesses the 'author' table from the 'db1'
database and the 'editor' table from the 'db2' database:

     USE db1;
     SELECT author_name,editor_name FROM author,db2.editor
       WHERE author.editor_id = db2.editor.editor_id;


File: manual.info.tmp,  Node: innodb-storage-engine,  Next: storage-engines,  Prev: sql-statements,  Up: Top

14 The InnoDB Storage Engine
****************************

* Menu:

* innodb-introduction::          Introduction to InnoDB
* innodb-installation::          Installing the InnoDB Storage Engine
* innodb-upgrading::             Upgrading the InnoDB Storage Engine
* innodb-downgrading::           Downgrading the InnoDB Storage Engine
* mysql-acid::                   InnoDB and the ACID Model
* innodb-multi-versioning::      InnoDB Multi-Versioning
* innodb-architecture::          InnoDB Architecture
* innodb-in-memory-structures::  InnoDB In-Memory Structures
* innodb-on-disk-structures::    InnoDB On-Disk Structures
* innodb-locking-transaction-model::  InnoDB Locking and Transaction Model
* innodb-configuration::         InnoDB Configuration
* innodb-compression::           InnoDB Table Compression
* innodb-file-format::           InnoDB File-Format Management
* innodb-row-format::            InnoDB Row Formats
* innodb-disk-management::       InnoDB Disk I/O and File Space Management
* innodb-create-index::          InnoDB Fast Index Creation
* innodb-parameters::            InnoDB Startup Options and System Variables
* innodb-information-schema::    InnoDB INFORMATION_SCHEMA Tables
* innodb-performance-schema::    InnoDB Integration with MySQL Performance Schema
* innodb-monitors::              InnoDB Monitors
* innodb-backup-recovery::       InnoDB Backup and Recovery
* innodb-and-mysql-replication::  InnoDB and MySQL Replication
* innodb-troubleshooting::       InnoDB Troubleshooting
* innodb-limits::                InnoDB Limits
* innodb-restrictions-limitations::  InnoDB Restrictions and Limitations


File: manual.info.tmp,  Node: innodb-introduction,  Next: innodb-installation,  Prev: innodb-storage-engine,  Up: innodb-storage-engine

14.1 Introduction to InnoDB
===========================

* Menu:

* innodb-benefits::              Benefits of Using InnoDB Tables
* innodb-best-practices::        Best Practices for InnoDB Tables
* innodb-check-availability::    Checking InnoDB Availability
* innodb-compatibility::         Upward and Downward Compatibility
* innodb-benchmarking::          Testing and Benchmarking with InnoDB
* innodb-turning-off::           Turning Off InnoDB
* innodb-contrib::               Third-Party Software Contributions

'InnoDB' is a general-purpose storage engine that balances high
reliability and high performance.  Starting from MySQL 5.5.5, the
default storage engine for new tables is 'InnoDB' rather than *note
'MyISAM': myisam-storage-engine.  Unless you have configured a different
default storage engine, issuing a *note 'CREATE TABLE': create-table.
statement without an 'ENGINE=' clause creates an 'InnoDB' table.  Given
this change of default behavior, MySQL 5.5 might be a logical point to
evaluate whether tables that use 'MyISAM' could benefit from switching
to 'InnoDB'.

'InnoDB' includes all the features that were part of the InnoDB Plugin
for MySQL 5.1, plus new features specific to MySQL 5.5 and higher.

*Note*:

The 'mysql' and 'INFORMATION_SCHEMA' databases that implement some of
the MySQL internals still use 'MyISAM'.  In particular, you cannot
switch the grant tables to use 'InnoDB'.

*Key Advantages of InnoDB*

   * Its DML operations follow the ACID model, with transactions
     featuring commit, rollback, and crash-recovery capabilities to
     protect user data.  See *note mysql-acid:: for more information.

   * Row-level locking and Oracle-style consistent reads increase
     multi-user concurrency and performance.  See *note
     innodb-locking-transaction-model:: for more information.

   * 'InnoDB' tables arrange your data on disk to optimize queries based
     on primary keys.  Each 'InnoDB' table has a primary key index
     called the clustered index that organizes the data to minimize I/O
     for primary key lookups.  See *note innodb-index-types:: for more
     information.

   * To maintain data integrity, 'InnoDB' supports 'FOREIGN KEY'
     constraints.  With foreign keys, inserts, updates, and deletes are
     checked to ensure they do not result in inconsistencies across
     different tables.  See *note create-table-foreign-keys:: for more
     information.

*InnoDB Storage Engine Features*

Feature                                     Support
                                            
*B-tree indexes*                            Yes
                                            
*Backup/point-in-time recovery*             Yes
(Implemented in the server, rather than     
in the storage engine.)

*Cluster database support*                  No
                                            
*Clustered indexes*                         Yes
                                            
*Compressed data*                           Yes
                                            
*Data caches*                               Yes
                                            
*Encrypted data*                            Yes (Implemented in the
                                            server via encryption
                                            functions; In MySQL 5.7 and
                                            later, data-at-rest
                                            tablespace encryption is
                                            supported.)
                                            
*Foreign key support*                       Yes
                                            
*Full-text search indexes*                  Yes (InnoDB support for
                                            FULLTEXT indexes is
                                            available in MySQL 5.6 and
                                            later.)
                                            
*Geospatial data type support*              Yes
                                            
*Geospatial indexing support*               Yes (InnoDB support for
                                            geospatial indexing is
                                            available in MySQL 5.7 and
                                            later.)
                                            
*Hash indexes*                              No (InnoDB utilizes hash
                                            indexes internally for its
                                            Adaptive Hash Index
                                            feature.)
                                            
*Index caches*                              Yes
                                            
*Locking granularity*                       Row
                                            
*MVCC*                                      Yes
                                            
*Replication support* (Implemented in the   Yes
server, rather than in the storage          
engine.)

*Storage limits*                            64TB
                                            
*T-tree indexes*                            No
                                            
*Transactions*                              Yes
                                            
*Update statistics for data dictionary*     Yes

To compare the features of 'InnoDB' with other storage engines provided
with MySQL, see the _Storage Engine Features_ table in *note
storage-engines::.

*InnoDB Enhancements and New Features*

The 'InnoDB' storage engine in MySQL 5.5 releases includes a number
performance improvements that in MySQL 5.1 were only available by
installing the 'InnoDB' Plugin.  This latest 'InnoDB' offers new
features, improved performance and scalability, enhanced reliability and
new capabilities for flexibility and ease of use.

For information about 'InnoDB' enhancements and new features, refer to:

   * The 'InnoDB' enhancements list in *note mysql-nutshell::.

   * The Release Notes
     (https://dev.mysql.com/doc/relnotes/mysql/5.5/en/).

*Additional InnoDB Information and Resources*

   * For 'InnoDB'-related terms and definitions, see the *note
     glossary::.

   * For a forum dedicated to the 'InnoDB' storage engine, see MySQL
     Forums::InnoDB (http://forums.mysql.com/list.php?22).

   * 'InnoDB' is published under the same GNU GPL License Version 2 (of
     June 1991) as MySQL. For more information on MySQL licensing, see
     <http://www.mysql.com/company/legal/licensing/>.


File: manual.info.tmp,  Node: innodb-benefits,  Next: innodb-best-practices,  Prev: innodb-introduction,  Up: innodb-introduction

14.1.1 Benefits of Using InnoDB Tables
--------------------------------------

You may find 'InnoDB' tables beneficial for the following reasons:

   * If your server crashes because of a hardware or software issue,
     regardless of what was happening in the database at the time, you
     don't need to do anything special after restarting the database.
     'InnoDB' crash recovery automatically finalizes any changes that
     were committed before the time of the crash, and undoes any changes
     that were in process but not committed.  Just restart and continue
     where you left off.  This process is now much faster than in MySQL
     5.1 and earlier.

   * The 'InnoDB' storage engine maintains its own buffer pool that
     caches table and index data in main memory as data is accessed.
     Frequently used data is processed directly from memory.  This cache
     applies to many types of information and speeds up processing.  On
     dedicated database servers, up to 80% of physical memory is often
     assigned to the buffer pool.

   * If you split up related data into different tables, you can set up
     foreign keys that enforce referential integrity.  Update or delete
     data, and the related data in other tables is updated or deleted
     automatically.  Try to insert data into a secondary table without
     corresponding data in the primary table, and the bad data gets
     kicked out automatically.

   * If data becomes corrupted on disk or in memory, a checksum
     mechanism alerts you to the bogus data before you use it.

   * When you design your database with appropriate primary key columns
     for each table, operations involving those columns are
     automatically optimized.  It is very fast to reference the primary
     key columns in 'WHERE' clauses, 'ORDER BY' clauses, 'GROUP BY'
     clauses, and join operations.

   * Inserts, updates, deletes are optimized by an automatic mechanism
     called change buffering.  'InnoDB' not only allows concurrent read
     and write access to the same table, it caches changed data to
     streamline disk I/O.

   * Performance benefits are not limited to giant tables with
     long-running queries.  When the same rows are accessed over and
     over from a table, a feature called the Adaptive Hash Index takes
     over to make these lookups even faster, as if they came out of a
     hash table.

   * You can freely mix 'InnoDB' tables with tables from other MySQL
     storage engines, even within the same statement.  For example, you
     can use a join operation to combine data from 'InnoDB' and *note
     'MEMORY': memory-storage-engine. tables in a single query.

   * 'InnoDB' has been designed for CPU efficiency and maximum
     performance when processing large data volumes.

   * 'InnoDB' tables can handle large quantities of data, even on
     operating systems where file size is limited to 2GB.

For 'InnoDB'-specific tuning techniques you can apply in your
application code, see *note optimizing-innodb::.


File: manual.info.tmp,  Node: innodb-best-practices,  Next: innodb-check-availability,  Prev: innodb-benefits,  Up: innodb-introduction

14.1.2 Best Practices for InnoDB Tables
---------------------------------------

This section describes best practices when using 'InnoDB' tables.

   * Specify a primary key for every table using the most frequently
     queried column or columns, or an auto-increment value if there is
     no obvious primary key.

   * Using joins wherever data is pulled from multiple tables based on
     identical ID values from those tables.  For fast join performance,
     define foreign keys on the join columns, and declare those columns
     with the same data type in each table.  Adding foreign keys ensures
     that referenced columns are indexed, which can improve performance.
     Foreign keys also propagate deletes or updates to all affected
     tables, and prevent insertion of data in a child table if the
     corresponding IDs are not present in the parent table.

   * Turning off autocommit.  Committing hundreds of times a second puts
     a cap on performance (limited by the write speed of your storage
     device).

   * Grouping sets of related DML operations into transactions, by
     bracketing them with 'START TRANSACTION' and 'COMMIT' statements.
     While you don't want to commit too often, you also don't want to
     issue huge batches of 'INSERT', 'UPDATE', or 'DELETE' statements
     that run for hours without committing.

   * Not using *note 'LOCK TABLES': lock-tables. statements.  'InnoDB'
     can handle multiple sessions all reading and writing to the same
     table at once, without sacrificing reliability or high performance.
     To get exclusive write access to a set of rows, use the 'SELECT ...
     FOR UPDATE' syntax to lock just the rows you intend to update.

   * Enabling the 'innodb_file_per_table' option to put the data and
     indexes for individual tables into separate files, instead of the
     system tablespace.  This setting is required to use some of the
     other features, such as table compression.

   * Evaluating whether your data and access patterns benefit from the
     'InnoDB' table compression feature.  You can compress 'InnoDB'
     tables without sacrificing read/write capability.

   * Running your server with the option
     '--sql_mode=NO_ENGINE_SUBSTITUTION' to prevent tables being created
     with a different storage engine if there is an issue with the
     engine specified in the 'ENGINE=' clause of *note 'CREATE TABLE':
     create-table.


File: manual.info.tmp,  Node: innodb-check-availability,  Next: innodb-compatibility,  Prev: innodb-best-practices,  Up: innodb-introduction

14.1.3 Checking InnoDB Availability
-----------------------------------

To determine whether your server supports 'InnoDB':

   * Issue the *note 'SHOW ENGINES': show-engines. statement to view the
     available MySQL storage engines.  Look for 'DEFAULT' in the
     'InnoDB' line.

          mysql> SHOW ENGINES;

     Alternatively, query the *note 'INFORMATION_SCHEMA.ENGINES':
     engines-table. table.

          mysql> SELECT * FROM INFORMATION_SCHEMA.ENGINES;

     (Now that 'InnoDB' is the default MySQL storage engine, only very
     specialized environments might not support it.)

   * Issue a 'SHOW VARIABLES' statement to confirm that 'InnoDB' is
     available.

          mysql> SHOW VARIABLES LIKE 'have_innodb';

   * If 'InnoDB' is not present, you have a 'mysqld' binary that was
     compiled without 'InnoDB' support and you need to get a different
     one.

   * If 'InnoDB' is present but disabled, go back through your startup
     options and configuration file and get rid of any '--skip-innodb'
     option.


File: manual.info.tmp,  Node: innodb-compatibility,  Next: innodb-benchmarking,  Prev: innodb-check-availability,  Up: innodb-introduction

14.1.4 Upward and Downward Compatibility
----------------------------------------

The ability to use the *note 'InnoDB': innodb-storage-engine. table
compression feature introduced in MySQL 5.5 and the new row format
require the use of a new 'InnoDB' file format called Barracuda.  The
previous file format, used by the built-in InnoDB in MySQL 5.1 and
earlier, is now called Antelope and does not support these features, but
does support the other features introduced with the InnoDB storage
engine.

The InnoDB storage engine is upward compatible from standard 'InnoDB' as
built in to, and distributed with, MySQL. Existing databases can be used
with the InnoDB Storage Engine for MySQL. The new parameter
'innodb_file_format' can help protect upward and downward compatibility
between 'InnoDB' versions and database files, allowing users to enable
or disable use of new features that can only be used with certain
versions of 'InnoDB.'

'InnoDB' since version 5.0.21 has a safety feature that prevents it from
opening tables that are in an unknown format.  However, the system
tablespace may contain references to new-format tables that confuse the
built-in InnoDB in MySQL 5.1 and earlier.  These references are cleared
in a slow shutdown.

With previous versions of 'InnoDB', no error would be returned until you
try to access a table that is in a format 'too new' for the software.
To provide early feedback, 'InnoDB' now checks the system tablespace
before startup to ensure that the file format used in the database is
supported by the storage engine.  See *note
innodb-file-format-compatibility-checking:: for the details.


File: manual.info.tmp,  Node: innodb-benchmarking,  Next: innodb-turning-off,  Prev: innodb-compatibility,  Up: innodb-introduction

14.1.5 Testing and Benchmarking with InnoDB
-------------------------------------------

If 'InnoDB' is not your default storage engine, you can determine if
your database server or applications work correctly with 'InnoDB' by
restarting the server with '--default-storage-engine=InnoDB' defined on
the command line or with 'default-storage-engine=innodb' defined in the
'[mysqld]' section of your MySQL server option file.

Since changing the default storage engine only affects new tables as
they are created, run all your application installation and setup steps
to confirm that everything installs properly.  Then exercise all the
application features to make sure all the data loading, editing, and
querying features work.  If a table relies on a feature that is specific
to another storage engine, you will receive an error; add the
'ENGINE=OTHER_ENGINE_NAME' clause to the 'CREATE TABLE' statement to
avoid the error.

If you did not make a deliberate decision about the storage engine, and
you want to preview how certain tables work when created using 'InnoDB',
issue the command 'ALTER TABLE table_name ENGINE=InnoDB;' for each
table.  Or, to run test queries and other statements without disturbing
the original table, make a copy:

     CREATE TABLE InnoDB_Table (...) ENGINE=InnoDB AS SELECT * FROM OTHER_ENGINE_TABLE;

To assess performance with a full application under a realistic
workload, install the latest MySQL server and run benchmarks.

Test the full application lifecycle, from installation, through heavy
usage, and server restart.  Kill the server process while the database
is busy to simulate a power failure, and verify that the data is
recovered successfully when you restart the server.

Test any replication configurations, especially if you use different
MySQL versions and options on the master and slaves.


File: manual.info.tmp,  Node: innodb-turning-off,  Next: innodb-contrib,  Prev: innodb-benchmarking,  Up: innodb-introduction

14.1.6 Turning Off InnoDB
-------------------------

Oracle recommends 'InnoDB' as the preferred storage engine for typical
database applications, from single-user wikis and blogs running on a
local system, to high-end applications pushing the limits of
performance.  As of MySQL 5.5, 'InnoDB' is the default storage engine
for new tables.

If you do not want to use 'InnoDB' tables:

   * Start the server with the '--innodb=OFF' or '--skip-innodb' option
     to disable the 'InnoDB' storage engine.

   * Because the default storage engine is 'InnoDB', the server will not
     start unless you also change the 'default_storage_engine' system
     variable to set the default to some other available engine.

   * To prevent the server from crashing when the *note 'InnoDB'-related
     'information_schema' tables: innodb-i_s-tables. are queried, also
     disable the plugins associated with those tables.  Specify in the
     '[mysqld]' section of the MySQL configuration file:

          loose-innodb-trx=0
          loose-innodb-locks=0
          loose-innodb-lock-waits=0
          loose-innodb-cmp=0
          loose-innodb-cmp-per-index=0
          loose-innodb-cmp-per-index-reset=0
          loose-innodb-cmp-reset=0
          loose-innodb-cmpmem=0
          loose-innodb-cmpmem-reset=0
          loose-innodb-buffer-page=0
          loose-innodb-buffer-page-lru=0
          loose-innodb-buffer-pool-stats=0


File: manual.info.tmp,  Node: innodb-contrib,  Prev: innodb-turning-off,  Up: innodb-introduction

14.1.7 Third-Party Software Contributions
-----------------------------------------

* Menu:

* innodb-contrib-google::        Performance Patches from Google
* innodb-contrib-percona::       Multiple Background I/O Threads Patch from Percona
* innodb-contrib-sun_microsystems::  Performance Patches from Sun Microsystems

Oracle acknowledges that certain Third Party and Open Source software
has been used to develop or is incorporated in the InnoDB storage
engine.  This appendix includes required third-party license
information.


File: manual.info.tmp,  Node: innodb-contrib-google,  Next: innodb-contrib-percona,  Prev: innodb-contrib,  Up: innodb-contrib

14.1.7.1 Performance Patches from Google
........................................

Oracle gratefully acknowledges the following contributions from Google,
Inc.  to improve InnoDB performance:

   * Replacing InnoDB's use of Pthreads mutexes with calls to GCC atomic
     builtins.  This change means that InnoDB mutex and rw-lock
     operations take less CPU time, and improves throughput on those
     platforms where the atomic operations are available.

   * Controlling InnoDB 'I/O' capacity, as discussed in *note
     innodb-configuring-io-capacity::.  InnoDB performs various tasks in
     the background.  Historically, InnoDB has used a hard coded value
     as the total 'I/O' capacity of the server.  With this change, user
     can control the number of 'I/O' operations that can be performed
     per second based on their own workload.

Changes from the Google contributions were incorporated in the following
source code files: 'btr0cur.c', 'btr0sea.c', 'buf0buf.c', 'buf0buf.ic',
'ha_innodb.cc', 'log0log.c', 'log0log.h', 'os0sync.h', 'row0sel.c',
'srv0srv.c', 'srv0srv.h', 'srv0start.c', 'sync0arr.c', 'sync0rw.c',
'sync0rw.h', 'sync0rw.ic', 'sync0sync.c', 'sync0sync.h', 'sync0sync.ic',
and 'univ.i'.

These contributions are incorporated subject to the conditions contained
in the file 'COPYING.Google', which are reproduced here.

     Copyright (c) 2008, 2009, Google Inc.
     All rights reserved.

     Redistribution and use in source and binary forms, with or without
     modification, are permitted provided that the following conditions
     are met:
         * Redistributions of source code must retain the above copyright
           notice, this list of conditions and the following disclaimer.
         * Redistributions in binary form must reproduce the above
           copyright notice, this list of conditions and the following
           disclaimer in the documentation and/or other materials
           provided with the distribution.
         * Neither the name of the Google Inc. nor the names of its
           contributors may be used to endorse or promote products
           derived from this software without specific prior written
           permission.

     THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
     "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
     LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
     FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
     COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
     INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
     BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
     LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
     CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
     LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
     ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
     POSSIBILITY OF SUCH DAMAGE.


File: manual.info.tmp,  Node: innodb-contrib-percona,  Next: innodb-contrib-sun_microsystems,  Prev: innodb-contrib-google,  Up: innodb-contrib

14.1.7.2 Multiple Background I/O Threads Patch from Percona
...........................................................

Oracle gratefully acknowledges the contribution of Percona, Inc.  to
improve InnoDB performance by implementing configurable background
threads, as discussed in *note innodb-performance-multiple_io_threads::.
InnoDB uses background threads to service various types of I/O requests.
The change provides another way to make InnoDB more scalable on high end
systems.

Changes from the Percona, Inc.  contribution were incorporated in the
following source code files: 'ha_innodb.cc', 'os0file.c', 'os0file.h',
'srv0srv.c', 'srv0srv.h', and 'srv0start.c'.

This contribution is incorporated subject to the conditions contained in
the file 'COPYING.Percona', which are reproduced here.

     Copyright (c) 2008, 2009, Percona Inc.
     All rights reserved.

     Redistribution and use in source and binary forms, with or without
     modification, are permitted provided that the following conditions
     are met:
         * Redistributions of source code must retain the above copyright
           notice, this list of conditions and the following disclaimer.
         * Redistributions in binary form must reproduce the above
           copyright notice, this list of conditions and the following
           disclaimer in the documentation and/or other materials
           provided with the distribution.
         * Neither the name of the Percona Inc. nor the names of its
           contributors may be used to endorse or promote products
           derived from this software without specific prior written
           permission.

     THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
     "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
     LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
     FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
     COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
     INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
     BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
     LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
     CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
     LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
     ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
     POSSIBILITY OF SUCH DAMAGE.


File: manual.info.tmp,  Node: innodb-contrib-sun_microsystems,  Prev: innodb-contrib-percona,  Up: innodb-contrib

14.1.7.3 Performance Patches from Sun Microsystems
..................................................

Oracle gratefully acknowledges the following contributions from Sun
Microsystems, Inc.  to improve InnoDB performance:

   * Introducing the 'PAUSE' instruction inside spin loops.  This change
     increases performance in high concurrency, CPU-bound workloads.

   * Enabling inlining of functions and prefetch with Sun Studio.

Changes from the Sun Microsystems, Inc.  contribution were incorporated
in the following source code files: 'univ.i', 'ut0ut.c', and 'ut0ut.h'.

This contribution is incorporated subject to the conditions contained in
the file 'COPYING.Sun_Microsystems', which are reproduced here.

     Copyright (c) 2009, Sun Microsystems, Inc.
     All rights reserved.

     Redistribution and use in source and binary forms, with or without
     modification, are permitted provided that the following conditions
     are met:
         * Redistributions of source code must retain the above copyright
           notice, this list of conditions and the following disclaimer.
         * Redistributions in binary form must reproduce the above
           copyright notice, this list of conditions and the following
           disclaimer in the documentation and/or other materials
           provided with the distribution.
         * Neither the name of Sun Microsystems, Inc. nor the names of its
           contributors may be used to endorse or promote products
           derived from this software without specific prior written
           permission.

     THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
     "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
     LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
     FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
     COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
     INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
     BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
     LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
     CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
     LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
     ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
     POSSIBILITY OF SUCH DAMAGE.


File: manual.info.tmp,  Node: innodb-installation,  Next: innodb-upgrading,  Prev: innodb-introduction,  Up: innodb-storage-engine

14.2 Installing the InnoDB Storage Engine
=========================================

When you use the InnoDB storage engine 1.1 and above, with MySQL 5.5 and
above, you do not need to do anything special to install: everything
comes configured as part of the MySQL source and binary distributions.
This is a change from earlier releases of the InnoDB Plugin, where you
were required to match up MySQL and InnoDB version numbers and update
your build and configuration processes.

The InnoDB storage engine is included in the MySQL distribution,
starting from MySQL 5.1.38.  From MySQL 5.1.46 and up, this is the only
download location for the InnoDB storage engine; it is not available
from the InnoDB website.

If you used any scripts or configuration files with the earlier InnoDB
storage engine from the InnoDB website, be aware that the filename of
the shared library as supplied by MySQL is 'ha_innodb_plugin.so' or
'ha_innodb_plugin.dll', as opposed to 'ha_innodb.so' or 'ha_innodb.dll'
in the older Plugin downloaded from the InnoDB website.  You might need
to change the applicable file names in your startup or configuration
scripts.

Because the InnoDB storage engine has now replaced the built-in InnoDB,
you no longer need to specify options like '--ignore-builtin-innodb' and
'--plugin-load' during startup.

To take best advantage of current InnoDB features, we recommend
specifying the following options in your configuration file:

     innodb_file_per_table=1
     innodb_file_format=barracuda
     innodb_strict_mode=1

For information about these features, see *note innodb-parameters::,
*note innodb-file-format::, and 'innodb_strict_mode'.  You might need to
continue to use the previous values for these parameters in some
replication and similar configurations involving both new and older
versions of MySQL.


File: manual.info.tmp,  Node: innodb-upgrading,  Next: innodb-downgrading,  Prev: innodb-installation,  Up: innodb-storage-engine

14.3 Upgrading the InnoDB Storage Engine
========================================

Prior to MySQL 5.5, some upgrade scenarios involved upgrading the
separate instance of InnoDB known as the InnoDB Plugin.  In MySQL 5.5
and higher, the features of the InnoDB Plugin have been folded back into
built-in InnoDB, so the upgrade procedure for InnoDB is the same as the
one for the MySQL server.  For details, see *note upgrading::.


File: manual.info.tmp,  Node: innodb-downgrading,  Next: mysql-acid,  Prev: innodb-upgrading,  Up: innodb-storage-engine

14.4 Downgrading the InnoDB Storage Engine
==========================================

Prior to MySQL 5.5, some downgrade scenarios involved switching the
separate instance of InnoDB known as the InnoDB Plugin back to the
built-in InnoDB storage engine.  In MySQL 5.5 and higher, the features
of the InnoDB Plugin have been folded back into built-in InnoDB, so the
downgrade procedure for InnoDB is the same as the one for the MySQL
server.  For details, see *note downgrading::.


File: manual.info.tmp,  Node: mysql-acid,  Next: innodb-multi-versioning,  Prev: innodb-downgrading,  Up: innodb-storage-engine

14.5 InnoDB and the ACID Model
==============================

The ACID model is a set of database design principles that emphasize
aspects of reliability that are important for business data and
mission-critical applications.  MySQL includes components such as the
'InnoDB' storage engine that adhere closely to the ACID model, so that
data is not corrupted and results are not distorted by exceptional
conditions such as software crashes and hardware malfunctions.  When you
rely on ACID-compliant features, you do not need to reinvent the wheel
of consistency checking and crash recovery mechanisms.  In cases where
you have additional software safeguards, ultra-reliable hardware, or an
application that can tolerate a small amount of data loss or
inconsistency, you can adjust MySQL settings to trade some of the ACID
reliability for greater performance or throughput.

The following sections discuss how MySQL features, in particular the
'InnoDB' storage engine, interact with the categories of the ACID model:

   * *A*: atomicity.

   * *C*: consistency.

   * *I:*: isolation.

   * *D*: durability.

*Atomicity*

The *atomicity* aspect of the ACID model mainly involves 'InnoDB'
transactions.  Related MySQL features include:

   * Autocommit setting.

   * *note 'COMMIT': commit. statement.

   * *note 'ROLLBACK': commit. statement.

   * Operational data from the 'INFORMATION_SCHEMA' tables.

*Consistency*

The *consistency* aspect of the ACID model mainly involves internal
'InnoDB' processing to protect data from crashes.  Related MySQL
features include:

   * 'InnoDB' doublewrite buffer.

   * 'InnoDB' crash recovery.

*Isolation*

The *isolation* aspect of the ACID model mainly involves 'InnoDB'
transactions, in particular the isolation level that applies to each
transaction.  Related MySQL features include:

   * Autocommit setting.

   * 'SET ISOLATION LEVEL' statement.

   * The low-level details of 'InnoDB' locking.  During performance
     tuning, you see these details through 'INFORMATION_SCHEMA' tables.

*Durability*

The *durability* aspect of the ACID model involves MySQL software
features interacting with your particular hardware configuration.
Because of the many possibilities depending on the capabilities of your
CPU, network, and storage devices, this aspect is the most complicated
to provide concrete guidelines for.  (And those guidelines might take
the form of buy 'new hardware'.)  Related MySQL features include:

   * 'InnoDB' doublewrite buffer, turned on and off by the
     'innodb_doublewrite' configuration option.

   * Configuration option 'innodb_flush_log_at_trx_commit'.

   * Configuration option 'sync_binlog'.

   * Configuration option 'innodb_file_per_table'.

   * Write buffer in a storage device, such as a disk drive, SSD, or
     RAID array.

   * Battery-backed cache in a storage device.

   * The operating system used to run MySQL, in particular its support
     for the 'fsync()' system call.

   * Uninterruptible power supply (UPS) protecting the electrical power
     to all computer servers and storage devices that run MySQL servers
     and store MySQL data.

   * Your backup strategy, such as frequency and types of backups, and
     backup retention periods.

   * For distributed or hosted data applications, the particular
     characteristics of the data centers where the hardware for the
     MySQL servers is located, and network connections between the data
     centers.


File: manual.info.tmp,  Node: innodb-multi-versioning,  Next: innodb-architecture,  Prev: mysql-acid,  Up: innodb-storage-engine

14.6 InnoDB Multi-Versioning
============================

'InnoDB' is a multi-versioned storage engine: it keeps information about
old versions of changed rows, to support transactional features such as
concurrency and rollback.  This information is stored in the tablespace
in a data structure called a rollback segment (after an analogous data
structure in Oracle).  'InnoDB' uses the information in the rollback
segment to perform the undo operations needed in a transaction rollback.
It also uses the information to build earlier versions of a row for a
consistent read.

Internally, 'InnoDB' adds three fields to each row stored in the
database.  A 6-byte 'DB_TRX_ID' field indicates the transaction
identifier for the last transaction that inserted or updated the row.
Also, a deletion is treated internally as an update where a special bit
in the row is set to mark it as deleted.  Each row also contains a
7-byte 'DB_ROLL_PTR' field called the roll pointer.  The roll pointer
points to an undo log record written to the rollback segment.  If the
row was updated, the undo log record contains the information necessary
to rebuild the content of the row before it was updated.  A 6-byte
'DB_ROW_ID' field contains a row ID that increases monotonically as new
rows are inserted.  If 'InnoDB' generates a clustered index
automatically, the index contains row ID values.  Otherwise, the
'DB_ROW_ID' column does not appear in any index.

Undo logs in the rollback segment are divided into insert and update
undo logs.  Insert undo logs are needed only in transaction rollback and
can be discarded as soon as the transaction commits.  Update undo logs
are used also in consistent reads, but they can be discarded only after
there is no transaction present for which 'InnoDB' has assigned a
snapshot that in a consistent read could need the information in the
update undo log to build an earlier version of a database row.

Commit your transactions regularly, including those transactions that
issue only consistent reads.  Otherwise, 'InnoDB' cannot discard data
from the update undo logs, and the rollback segment may grow too big,
filling up your tablespace.

The physical size of an undo log record in the rollback segment is
typically smaller than the corresponding inserted or updated row.  You
can use this information to calculate the space needed for your rollback
segment.

In the 'InnoDB' multi-versioning scheme, a row is not physically removed
from the database immediately when you delete it with an SQL statement.
'InnoDB' only physically removes the corresponding row and its index
records when it discards the update undo log record written for the
deletion.  This removal operation is called a purge, and it is quite
fast, usually taking the same order of time as the SQL statement that
did the deletion.

If you insert and delete rows in smallish batches at about the same rate
in the table, the purge thread can start to lag behind and the table can
grow bigger and bigger because of all the 'dead' rows, making everything
disk-bound and very slow.  In such a case, throttle new row operations,
and allocate more resources to the purge thread by tuning the
'innodb_max_purge_lag' system variable.  See *note innodb-parameters::
for more information.

*Multi-Versioning and Secondary Indexes*

'InnoDB' multiversion concurrency control (MVCC) treats secondary
indexes differently than clustered indexes.  Records in a clustered
index are updated in-place, and their hidden system columns point undo
log entries from which earlier versions of records can be reconstructed.
Unlike clustered index records, secondary index records do not contain
hidden system columns nor are they updated in-place.

When a secondary index column is updated, old secondary index records
are delete-marked, new records are inserted, and delete-marked records
are eventually purged.  When a secondary index record is delete-marked
or the secondary index page is updated by a newer transaction, 'InnoDB'
looks up the database record in the clustered index.  In the clustered
index, the record's 'DB_TRX_ID' is checked, and the correct version of
the record is retrieved from the undo log if the record was modified
after the reading transaction was initiated.

If a secondary index record is marked for deletion or the secondary
index page is updated by a newer transaction, the covering index
technique is not used.  Instead of returning values from the index
structure, 'InnoDB' looks up the record in the clustered index.


File: manual.info.tmp,  Node: innodb-architecture,  Next: innodb-in-memory-structures,  Prev: innodb-multi-versioning,  Up: innodb-storage-engine

14.7 InnoDB Architecture
========================

The following diagram shows in-memory and on-disk structures that
comprise the 'InnoDB' storage engine architecture.  For information
about each structure, see *note innodb-in-memory-structures::, and *note
innodb-on-disk-structures::.

FIGURE GOES HERE: InnoDB Architecture


File: manual.info.tmp,  Node: innodb-in-memory-structures,  Next: innodb-on-disk-structures,  Prev: innodb-architecture,  Up: innodb-storage-engine

14.8 InnoDB In-Memory Structures
================================

* Menu:

* innodb-buffer-pool::           Buffer Pool
* innodb-change-buffer::         Change Buffer
* innodb-adaptive-hash::         Adaptive Hash Index
* innodb-redo-log-buffer::       Log Buffer

This section describes 'InnoDB' in-memory structures and related topics.


File: manual.info.tmp,  Node: innodb-buffer-pool,  Next: innodb-change-buffer,  Prev: innodb-in-memory-structures,  Up: innodb-in-memory-structures

14.8.1 Buffer Pool
------------------

The buffer pool is an area in main memory where 'InnoDB' caches table
and index data as it is accessed.  The buffer pool permits frequently
used data to be processed directly from memory, which speeds up
processing.  On dedicated servers, up to 80% of physical memory is often
assigned to the buffer pool.

For efficiency of high-volume read operations, the buffer pool is
divided into pages that can potentially hold multiple rows.  For
efficiency of cache management, the buffer pool is implemented as a
linked list of pages; data that is rarely used is aged out of the cache
using a variation of the LRU algorithm.

Knowing how to take advantage of the buffer pool to keep frequently
accessed data in memory is an important aspect of MySQL tuning.

*Buffer Pool LRU Algorithm*

The buffer pool is managed as a list using a variation of the least
recently used (LRU) algorithm.  When room is needed to add a new page to
the buffer pool, the least recently used page is evicted and a new page
is added to the middle of the list.  This midpoint insertion strategy
treats the list as two sublists:

   * At the head, a sublist of new ('young') pages that were accessed
     recently

   * At the tail, a sublist of old pages that were accessed less
     recently

FIGURE GOES HERE: Buffer Pool List

The algorithm keeps heavily pages in the new sublist.  The old sublist
contains less-used pages; these pages are candidates for eviction.

By default, the algorithm operates as follows:

   * 3/8 of the buffer pool is devoted to the old sublist.

   * The midpoint of the list is the boundary where the tail of the new
     sublist meets the head of the old sublist.

   * When 'InnoDB' reads a page into the buffer pool, it initially
     inserts it at the midpoint (the head of the old sublist).  A page
     can be read because it is required for a user-initiated operation
     such as an SQL query, or as part of a read-ahead operation
     performed automatically by 'InnoDB'.

   * Accessing a page in the old sublist makes it 'young', moving it to
     the head of the new sublist.  If the page was read because it was
     required by a user-initiated operation, the first access occurs
     immediately and the page is made young.  If the page was read due
     to a read-ahead operation, the first access does not occur
     immediately, and might not occur at all before the page is evicted.

   * As the database operates, pages in the buffer pool that are not
     accessed 'age' by moving toward the tail of the list.  Pages in
     both the new and old sublists age as other pages are made new.
     Pages in the old sublist also age as pages are inserted at the
     midpoint.  Eventually, a page that remains unused reaches the tail
     of the old sublist and is evicted.

By default, pages read by queries are immediately moved into the new
sublist, meaning they stay in the buffer pool longer.  A table scan,
performed for a *note 'mysqldump': mysqldump. operation or a 'SELECT'
statement with no 'WHERE' clause, for example, can bring a large amount
of data into the buffer pool and evict an equivalent amount of older
data, even if the new data is never used again.  Similarly, pages that
are loaded by the read-ahead background thread and accessed only once
are moved to the head of the new list.  These situations can push
frequently used pages to the old sublist where they become subject to
eviction.  For information about optimizing this behavior, see *note
innodb-performance-midpoint_insertion::, and *note
innodb-performance-read_ahead::.

'InnoDB' Standard Monitor output contains several fields in the 'BUFFER
POOL AND MEMORY' section regarding operation of the buffer pool LRU
algorithm.  For details, see *note innodb-buffer-pool-monitoring::.

*Buffer Pool Configuration*

You can configure the various aspects of the buffer pool to improve
performance.

   * Ideally, you set the size of the buffer pool to as large a value as
     practical, leaving enough memory for other processes on the server
     to run without excessive paging.  The larger the buffer pool, the
     more 'InnoDB' acts like an in-memory database, reading data from
     disk once and then accessing the data from memory during subsequent
     reads.  Buffer pool size is configured using the
     'innodb_buffer_pool_size' configuration option.

   * On 64-bit systems with sufficient memory, you can split the buffer
     pool into multiple parts to minimize contention for memory
     structures among concurrent operations.  For details, see *note
     innodb-multiple-buffer-pools::.

   * You can keep frequently accessed data in memory regardless of
     sudden spikes of activity from operations that would bring large
     amounts of infrequently accessed data into the buffer pool.  For
     details, see *note innodb-performance-midpoint_insertion::.

   * You can control when and how to perform read-ahead requests to
     prefetch pages into the buffer pool asynchronously in anticipation
     that the pages will be needed soon.  For details, see *note
     innodb-performance-read_ahead::.

   * You can control when background flushing occurs and whether or not
     the rate of flushing is dynamically adjusted based on workload.
     For details, see *note innodb-buffer-pool-flushing::.

*Monitoring the Buffer Pool Using the InnoDB Standard Monitor*

'InnoDB' Standard Monitor output, which can be accessed using *note
'SHOW ENGINE INNODB STATUS': innodb-standard-monitor, provides metrics
regarding operation of the buffer pool.  Buffer pool metrics are located
in the 'BUFFER POOL AND MEMORY' section of 'InnoDB' Standard Monitor
output and appear similar to the following:

     ----------------------
     BUFFER POOL AND MEMORY
     ----------------------
     Total memory allocated 2217738240; in additional pool allocated 0
     Dictionary memory allocated 121719
     Buffer pool size   131072
     Free buffers       129937
     Database pages     1134
     Old database pages 211
     Modified db pages  187
     Pending reads 0
     Pending writes: LRU 0, flush list 0, single page 0
     Pages made young 0, not young 0
     0.00 youngs/s, 0.00 non-youngs/s
     Pages read 426, created 708, written 768
     0.00 reads/s, 40.99 creates/s, 50.49 writes/s
     Buffer pool hit rate 1000 / 1000, young-making rate 0 / 1000 not 0 / 1000
     Pages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead
     0.00/s
     LRU len: 1134, unzip_LRU len: 0
     I/O sum[0]:cur[0], unzip sum[0]:cur[0]

The following table describes buffer pool metrics reported by the
'InnoDB' Standard Monitor.

*Note*:

Per second averages provided in 'InnoDB' Standard Monitor output are
based on the elapsed time since 'InnoDB' Standard Monitor output was
last printed.

*InnoDB Buffer Pool Metrics*

Name                      Description
                          
Total memory allocated    The total memory allocated for the buffer
                          pool in bytes.
                          
additional pool           The total memory allocated for the additional
allocated                 pool in bytes.
                          
Dictionary memory         The total memory allocated for the 'InnoDB'
allocated                 data dictionary in bytes.
                          
Buffer pool size          The total size in pages allocated to the
                          buffer pool.
                          
Free buffers              The total size in pages of the buffer pool
                          free list.
                          
Database pages            The total size in pages of the buffer pool
                          LRU list.
                          
Old database pages        The total size in pages of the buffer pool
                          old LRU sublist.
                          
Modified db pages         The current number of pages modified in the
                          buffer pool.
                          
Pending reads             The number of buffer pool pages waiting to be
                          read into the buffer pool.
                          
Pending writes LRU        The number of old dirty pages within the
                          buffer pool to be written from the bottom of
                          the LRU list.
                          
Pending writes flush      The number of buffer pool pages to be flushed
list                      during checkpointing.
                          
Pending writes single     The number of pending independent page writes
page                      within the buffer pool.
                          
Pages made young          The total number of pages made young in the
                          buffer pool LRU list (moved to the head of
                          sublist of 'new' pages).
                          
Pages made not young      The total number of pages not made young in
                          the buffer pool LRU list (pages that have
                          remained in the 'old' sublist without being
                          made young).
                          
youngs/s                  The per second average of accesses to old
                          pages in the buffer pool LRU list that have
                          resulted in making pages young.  See the
                          notes that follow this table for more
                          information.
                          
non-youngs/s              The per second average of accesses to old
                          pages in the buffer pool LRU list that have
                          resulted in not making pages young.  See the
                          notes that follow this table for more
                          information.
                          
Pages read                The total number of pages read from the
                          buffer pool.
                          
Pages created             The total number of pages created within the
                          buffer pool.
                          
Pages written             The total number of pages written from the
                          buffer pool.
                          
reads/s                   The per second average number of buffer pool
                          page reads per second.
                          
creates/s                 The per second average number of buffer pool
                          pages created per second.
                          
writes/s                  The per second average number of buffer pool
                          page writes per second.
                          
Buffer pool hit rate      The buffer pool page hit rate for pages read
                          from the buffer pool memory vs from disk
                          storage.
                          
young-making rate         The average hit rate at which page accesses
                          have resulted in making pages young.  See the
                          notes that follow this table for more
                          information.
                          
not (young-making rate)   The average hit rate at which page accesses
                          have not resulted in making pages young.  See
                          the notes that follow this table for more
                          information.
                          
Pages read ahead          The per second average of read ahead
                          operations.
                          
Pages evicted without     The per second average of the pages evicted
access                    without being accessed from the buffer pool.
                          
Random read ahead         The per second average of random read ahead
                          operations.
                          
LRU len                   The total size in pages of the buffer pool
                          LRU list.
                          
unzip_LRU len             The total size in pages of the buffer pool
                          unzip_LRU list.
                          
I/O sum                   The total number of buffer pool LRU list
                          pages accessed, for the last 50 seconds.
                          
I/O cur                   The total number of buffer pool LRU list
                          pages accessed.
                          
I/O unzip sum             The total number of buffer pool unzip_LRU
                          list pages accessed.
                          
I/O unzip cur             The total number of buffer pool unzip_LRU
                          list pages accessed.

*Notes*:

   * The 'youngs/s' metric is applicable only to old pages.  It is based
     on the number of accesses to pages and not the number of pages.
     There can be multiple accesses to a given page, all of which are
     counted.  If you see very low 'youngs/s' values when there are no
     large scans occurring, you might need to reduce the delay time or
     increase the percentage of the buffer pool used for the old
     sublist.  Increasing the percentage makes the old sublist larger,
     so pages in that sublist take longer to move to the tail, which
     increases the likelihood that those pages will be accessed again
     and made young.

   * The 'non-youngs/s' metric is applicable only to old pages.  It is
     based on the number of accesses to pages and not the number of
     pages.  There can be multiple accesses to a given page, all of
     which are counted.  If you do not see a higher 'non-youngs/s' value
     when performing large table scans (and a higher 'youngs/s' value),
     increase the delay value.

   * The 'young-making' rate accounts for accesses to all buffer pool
     pages, not just accesses to pages in the old sublist.  The
     'young-making' rate and 'not' rate do not normally add up to the
     overall buffer pool hit rate.  Page hits in the old sublist cause
     pages to move to the new sublist, but page hits in the new sublist
     cause pages to move to the head of the list only if they are a
     certain distance from the head.

   * 'not (young-making rate)' is the average hit rate at which page
     accesses have not resulted in making pages young due to the delay
     defined by 'innodb_old_blocks_time' not being met, or due to page
     hits in the new sublist that did not result in pages being moved to
     the head.  This rate accounts for accesses to all buffer pool
     pages, not just accesses to pages in the old sublist.

Buffer pool *note server status variables: server-status-variables. and
the *note 'INNODB_BUFFER_POOL_STATS': innodb-buffer-pool-stats-table.
table provide many of the same buffer pool metrics found in 'InnoDB'
Standard Monitor output.  For more information, see *note
innodb-information-schema-buffer-pool-stats-example::.


File: manual.info.tmp,  Node: innodb-change-buffer,  Next: innodb-adaptive-hash,  Prev: innodb-buffer-pool,  Up: innodb-in-memory-structures

14.8.2 Change Buffer
--------------------

The change buffer is a special data structure that caches changes to
secondary index pages when those pages are not in the buffer pool.  The
buffered changes, which may result from *note 'INSERT': insert, *note
'UPDATE': update, or *note 'DELETE': delete. operations (DML), are
merged later when the pages are loaded into the buffer pool by other
read operations.

FIGURE GOES HERE: Change Buffer

Unlike clustered indexes, secondary indexes are usually nonunique, and
inserts into secondary indexes happen in a relatively random order.
Similarly, deletes and updates may affect secondary index pages that are
not adjacently located in an index tree.  Merging cached changes at a
later time, when affected pages are read into the buffer pool by other
operations, avoids substantial random access I/O that would be required
to read secondary index pages into the buffer pool from disk.

Periodically, the purge operation that runs when the system is mostly
idle, or during a slow shutdown, writes the updated index pages to disk.
The purge operation can write disk blocks for a series of index values
more efficiently than if each value were written to disk immediately.

Change buffer merging may take several hours when there are many
affected rows and numerous secondary indexes to update.  During this
time, disk I/O is increased, which can cause a significant slowdown for
disk-bound queries.  Change buffer merging may also continue to occur
after a transaction is committed, and even after a server shutdown and
restart (see *note forcing-innodb-recovery:: for more information).

In memory, the change buffer occupies part of the buffer pool.  On disk,
the change buffer is part of the system tablespace, where index changes
are buffered when the database server is shut down.

The type of data cached in the change buffer is governed by the
'innodb_change_buffering' variable.  For more information, see *note
innodb-change-buffer-configuration::.

For answers to frequently asked questions about the change buffer, see
*note faqs-innodb-change-buffer::.

*Configuring Change Buffering*

When *note 'INSERT': insert, *note 'UPDATE': update, and *note 'DELETE':
delete. operations are performed on a table, the values of indexed
columns (particularly the values of secondary keys) are often in an
unsorted order, requiring substantial I/O to bring secondary indexes up
to date.  The change buffer caches changes to secondary index entries
when the relevant page is not in the buffer pool, thus avoiding
expensive I/O operations by not immediately reading in the page from
disk.  The buffered changes are merged when the page is loaded into the
buffer pool, and the updated page is later flushed to disk.  The
'InnoDB' main thread merges buffered changes when the server is nearly
idle, and during a slow shutdown.

Because it can result in fewer disk reads and writes, the change buffer
feature is most valuable for workloads that are I/O-bound, for example
applications with a high volume of DML operations such as bulk inserts.

However, the change buffer occupies a part of the buffer pool, reducing
the memory available to cache data pages.  If the working set almost
fits in the buffer pool, or if your tables have relatively few secondary
indexes, it may be useful to disable change buffering.  If the working
data set fits entirely within the buffer pool, change buffering does not
impose extra overhead, because it only applies to pages that are not in
the buffer pool.

You can control the extent to which 'InnoDB' performs change buffering
using the 'innodb_change_buffering' configuration parameter.  You can
enable or disable buffering for inserts, delete operations (when index
records are initially marked for deletion) and purge operations (when
index records are physically deleted).  An update operation is a
combination of an insert and a delete.  The default
'innodb_change_buffering' value is 'all'.

Permitted 'innodb_change_buffering' values include:

   * *'all'*

     The default value: buffer inserts, delete-marking operations, and
     purges.

   * *'none'*

     Do not buffer any operations.

   * *'inserts'*

     Buffer insert operations.

   * *'deletes'*

     Buffer delete-marking operations.

   * *'changes'*

     Buffer both inserts and delete-marking operations.

   * *'purges'*

     Buffer physical deletion operations that happen in the background.

You can set the 'innodb_change_buffering' variable in the MySQL option
file ('my.cnf' or 'my.ini') or change it dynamically with the *note 'SET
GLOBAL': set-variable. statement, which requires privileges sufficient
to set global system variables.  See *note system-variable-privileges::.
Changing the setting affects the buffering of new operations; the
merging of existing buffered entries is not affected.

*Monitoring the Change Buffer*

The following options are available for change buffer monitoring:

   * 'InnoDB' Standard Monitor output includes status information for
     the change buffer.  To view monitor data, issue the 'SHOW ENGINE
     INNODB STATUS' command.

          mysql> SHOW ENGINE INNODB STATUS\G

     Change buffer status information is located under the 'INSERT
     BUFFER AND ADAPTIVE HASH INDEX' heading and appears similar to the
     following:

          -------------------------------------
          INSERT BUFFER AND ADAPTIVE HASH INDEX
          -------------------------------------
          Ibuf: size 1, free list len 0, seg size 2, 0 merges
          merged operations:
           insert 0, delete mark 0, delete 0
          discarded operations:
           insert 0, delete mark 0, delete 0
          Hash table size 276707, node heap has 1 buffer(s)
          15.81 hash searches/s, 46.33 non-hash searches/s

     For more information, see *note innodb-standard-monitor::.

   * The *note 'INFORMATION_SCHEMA.INNODB_BUFFER_PAGE':
     innodb-buffer-page-table. table provides metadata about each page
     in the buffer pool, including change buffer index and change buffer
     bitmap pages.  Change buffer pages are identified by 'PAGE_TYPE'.
     'IBUF_INDEX' is the page type for change buffer index pages, and
     'IBUF_BITMAP' is the page type for change buffer bitmap pages.

     *Warning*:

     Querying the *note 'INNODB_BUFFER_PAGE': innodb-buffer-page-table.
     table can introduce significant performance overhead.  To avoid
     impacting performance, reproduce the issue you want to investigate
     on a test instance and run your queries on the test instance.

     For example, you can query the *note 'INNODB_BUFFER_PAGE':
     innodb-buffer-page-table. table to determine the approximate number
     of 'IBUF_INDEX' and 'IBUF_BITMAP' pages as a percentage of total
     buffer pool pages.

          mysql> SELECT (SELECT COUNT(*) FROM INFORMATION_SCHEMA.INNODB_BUFFER_PAGE
                 WHERE PAGE_TYPE LIKE 'IBUF%') AS change_buffer_pages,
                 (SELECT COUNT(*) FROM INFORMATION_SCHEMA.INNODB_BUFFER_PAGE) AS total_pages,
                 (SELECT ((change_buffer_pages/total_pages)*100))
                 AS change_buffer_page_percentage;
          +---------------------+-------------+-------------------------------+
          | change_buffer_pages | total_pages | change_buffer_page_percentage |
          +---------------------+-------------+-------------------------------+
          |                  25 |        8192 |                        0.3052 |
          +---------------------+-------------+-------------------------------+

     For information about other data provided by the *note
     'INNODB_BUFFER_PAGE': innodb-buffer-page-table. table, see *note
     innodb-buffer-page-table::.  For related usage information, see
     *note innodb-information-schema-buffer-pool-tables::.

   * *note Performance Schema: performance-schema. provides change
     buffer mutex wait instrumentation for advanced performance
     monitoring.  To view change buffer instrumentation, issue the
     following query (Performance Schema must be enabled):

          mysql> SELECT * FROM performance_schema.setup_instruments
                 WHERE NAME LIKE '%wait/synch/mutex/innodb/ibuf%';
          +-------------------------------------------------------+---------+-------+
          | NAME                                                  | ENABLED | TIMED |
          +-------------------------------------------------------+---------+-------+
          | wait/synch/mutex/innodb/ibuf_bitmap_mutex             | YES     | YES   |
          | wait/synch/mutex/innodb/ibuf_mutex                    | YES     | YES   |
          | wait/synch/mutex/innodb/ibuf_pessimistic_insert_mutex | YES     | YES   |
          +-------------------------------------------------------+---------+-------+

     For information about monitoring 'InnoDB' mutex waits, see *note
     monitor-innodb-mutex-waits-performance-schema::.


File: manual.info.tmp,  Node: innodb-adaptive-hash,  Next: innodb-redo-log-buffer,  Prev: innodb-change-buffer,  Up: innodb-in-memory-structures

14.8.3 Adaptive Hash Index
--------------------------

The adaptive hash index feature enables 'InnoDB' to perform more like an
in-memory database on systems with appropriate combinations of workload
and sufficient memory for the buffer pool without sacrificing
transactional features or reliability.  The adaptive hash index feature
is enabled by the 'innodb_adaptive_hash_index' variable, or turned off
at server startup by '--skip-innodb-adaptive-hash-index'.

Based on the observed pattern of searches, a hash index is built using a
prefix of the index key.  The prefix can be any length, and it may be
that only some values in the B-tree appear in the hash index.  Hash
indexes are built on demand for the pages of the index that are accessed
often.

If a table fits almost entirely in main memory, a hash index can speed
up queries by enabling direct lookup of any element, turning the index
value into a sort of pointer.  'InnoDB' has a mechanism that monitors
index searches.  If 'InnoDB' notices that queries could benefit from
building a hash index, it does so automatically.

With some workloads, the speedup from hash index lookups greatly
outweighs the extra work to monitor index lookups and maintain the hash
index structure.  Access to the adaptive hash index can sometimes become
a source of contention under heavy workloads, such as multiple
concurrent joins.  Queries with 'LIKE' operators and '%' wildcards also
tend not to benefit.  For workloads that do not benefit from the
adaptive hash index feature, turning it off reduces unnecessary
performance overhead.  Because it is difficult to predict in advance
whether the adaptive hash index feature is appropriate for a particular
system and workload, consider running benchmarks with it enabled and
disabled.

You can monitor adaptive hash index use and contention in the
'SEMAPHORES' section of *note 'SHOW ENGINE INNODB STATUS': show-engine.
output.  If there are numerous threads waiting on an RW-latch created in
'btr0sea.c', it might be useful to disable the adaptive hash index
feature.

For information about the performance characteristics of hash indexes,
see *note index-btree-hash::.


File: manual.info.tmp,  Node: innodb-redo-log-buffer,  Prev: innodb-adaptive-hash,  Up: innodb-in-memory-structures

14.8.4 Log Buffer
-----------------

The log buffer is the memory area that holds data to be written to the
log files on disk.  Log buffer size is defined by the
'innodb_log_buffer_size' variable.  The default size is 16MB. The
contents of the log buffer are periodically flushed to disk.  A large
log buffer enables large transactions to run without the need to write
redo log data to disk before the transactions commit.  Thus, if you have
transactions that update, insert, or delete many rows, increasing the
size of the log buffer saves disk I/O.

The 'innodb_flush_log_at_trx_commit' variable controls how the contents
of the log buffer are written and flushed to disk.  The
'innodb_flush_log_at_timeout'
(https://dev.mysql.com/doc/refman/5.6/en/innodb-parameters.html#sysvar_innodb_flush_log_at_timeout)
variable controls log flushing frequency.

For related information, see *note
innodb-startup-memory-configuration::, and *note
optimizing-innodb-logging::.


File: manual.info.tmp,  Node: innodb-on-disk-structures,  Next: innodb-locking-transaction-model,  Prev: innodb-in-memory-structures,  Up: innodb-storage-engine

14.9 InnoDB On-Disk Structures
==============================

* Menu:

* innodb-tables::                Tables
* innodb-indexes::               Indexes
* innodb-tablespace::            Tablespaces
* innodb-data-dictionary::       InnoDB Data Dictionary
* innodb-doublewrite-buffer::    Doublewrite Buffer
* innodb-redo-log::              Redo Log
* innodb-undo-logs::             Undo Logs

This section describes 'InnoDB' on-disk structures and related topics.


File: manual.info.tmp,  Node: innodb-tables,  Next: innodb-indexes,  Prev: innodb-on-disk-structures,  Up: innodb-on-disk-structures

14.9.1 Tables
-------------

* Menu:

* using-innodb-tables::          Creating InnoDB Tables
* innodb-migration::             Moving or Copying InnoDB Tables
* converting-tables-to-innodb::  Converting Tables from MyISAM to InnoDB
* innodb-auto-increment-handling::  AUTO_INCREMENT Handling in InnoDB

This section covers topics related to 'InnoDB' tables.


File: manual.info.tmp,  Node: using-innodb-tables,  Next: innodb-migration,  Prev: innodb-tables,  Up: innodb-tables

14.9.1.1 Creating InnoDB Tables
...............................

To create an 'InnoDB' table, use the *note 'CREATE TABLE': create-table.
statement.

     CREATE TABLE t1 (a INT, b CHAR (20), PRIMARY KEY (a)) ENGINE=InnoDB;

You do not need to specify the 'ENGINE=InnoDB' clause if 'InnoDB' is
defined as the default storage engine, which it is by default.  To check
the default storage engine, issue the following statement:

     mysql> SELECT @@default_storage_engine;
     +--------------------------+
     | @@default_storage_engine |
     +--------------------------+
     | InnoDB                   |
     +--------------------------+

You might still use 'ENGINE=InnoDB' clause if you plan to use *note
'mysqldump': mysqldump. or replication to replay the *note 'CREATE
TABLE': create-table. statement on a server where the default storage
engine is not 'InnoDB'.

An 'InnoDB' table and its indexes can be created in the system
tablespace or in a file-per-table tablespace.  When
'innodb_file_per_table' is enabled, an 'InnoDB' table is implicitly
created in an individual file-per-table tablespace.  Conversely, when
'innodb_file_per_table' is disabled, an 'InnoDB' table is implicitly
created in the 'InnoDB' system tablespace.

When you create an 'InnoDB' table, MySQL creates a .frm file in the
database directory under the MySQL data directory.  For more information
about '.frm' files, see *note innodb-frm-file::.  For a table created in
a file-per-table tablespace, MySQL also creates an .ibd tablespace file
in the database directory.  A table created in the 'InnoDB'system
tablespace is created in an existing ibdata file, which resides in the
MySQL data directory.

Internally, 'InnoDB' adds an entry for each table to the 'InnoDB' data
dictionary.  The entry includes the database name.  For example, if
table 't1' is created in the 'test' database, the data dictionary entry
for the database name is ''test/t1''.  This means you can create a table
of the same name ('t1') in a different database, and the table names do
not collide inside 'InnoDB'.

*InnoDB Tables and .frm Files*

MySQL stores data dictionary information for tables in .frm files in
database directories.  Unlike other MySQL storage engines, 'InnoDB' also
encodes information about the table in its own internal data dictionary
inside the system tablespace.  When MySQL drops a table or a database,
it deletes one or more '.frm' files as well as the corresponding entries
inside the 'InnoDB' data dictionary.  You cannot move 'InnoDB' tables
between databases simply by moving the '.frm' files.  For information
about moving 'InnoDB' tables, see *note innodb-migration::.

*InnoDB Tables and Row Formats*

The default row format of an 'InnoDB' table is 'Compact'.  Although this
row format is fine for basic experimentation, consider using the
'Dynamic' or 'Compressed' row format to take advantage of 'InnoDB'
features such as table compression and efficient off-page storage of
long column values.  Using these row formats requires that
'innodb_file_per_table' is enabled and that 'innodb_file_format' is set
to Barracuda:

     SET GLOBAL innodb_file_per_table=1;
     SET GLOBAL innodb_file_format=barracuda;
     CREATE TABLE t3 (a INT, b CHAR (20), PRIMARY KEY (a)) ROW_FORMAT=DYNAMIC;
     CREATE TABLE t4 (a INT, b CHAR (20), PRIMARY KEY (a)) ROW_FORMAT=COMPRESSED;

For more information about 'InnoDB' row formats, see *note
innodb-row-format::.  For how to determine the row format of an 'InnoDB'
table and the physical characteristics of 'InnoDB' row formats, see
*note innodb-row-format::.

*InnoDB Tables and Primary Keys*

Always define a primary key for an 'InnoDB' table, specifying the column
or columns that:

   * Are referenced by the most important queries.

   * Are never left blank.

   * Never have duplicate values.

   * Rarely if ever change value once inserted.

For example, in a table containing information about people, you would
not create a primary key on '(firstname, lastname)' because more than
one person can have the same name, some people have blank last names,
and sometimes people change their names.  With so many constraints,
often there is not an obvious set of columns to use as a primary key, so
you create a new column with a numeric ID to serve as all or part of the
primary key.  You can declare an auto-increment column so that ascending
values are filled in automatically as rows are inserted:

     # The value of ID can act like a pointer between related items in different tables.
     CREATE TABLE t5 (id INT AUTO_INCREMENT, b CHAR (20), PRIMARY KEY (id));

     # The primary key can consist of more than one column. Any autoinc column must come first.
     CREATE TABLE t6 (id INT AUTO_INCREMENT, a INT, b CHAR (20), PRIMARY KEY (id,a));

Although the table works correctly without defining a primary key, the
primary key is involved with many aspects of performance and is a
crucial design aspect for any large or frequently used table.  It is
recommended that you always specify a primary key in the *note 'CREATE
TABLE': create-table. statement.  If you create the table, load data,
and then run *note 'ALTER TABLE': alter-table. to add a primary key
later, that operation is much slower than defining the primary key when
creating the table.

*Viewing InnoDB Table Properties*

To view the properties of an 'InnoDB' table, issue a *note 'SHOW TABLE
STATUS': show-table-status. statement:

     mysql> SHOW TABLE STATUS FROM test LIKE 't%' \G;
     *************************** 1. row ***************************
                Name: t1
              Engine: InnoDB
             Version: 10
          Row_format: Compact
                Rows: 0
      Avg_row_length: 0
         Data_length: 16384
     Max_data_length: 0
        Index_length: 0
           Data_free: 41943040
      Auto_increment: NULL
         Create_time: 2015-03-16 16:42:17
         Update_time: NULL
          Check_time: NULL
           Collation: latin1_swedish_ci
            Checksum: NULL
      Create_options:
             Comment:
     1 row in set (0.00 sec)

For information about *note 'SHOW TABLE STATUS': show-table-status.
output, see *note show-table-status::.


File: manual.info.tmp,  Node: innodb-migration,  Next: converting-tables-to-innodb,  Prev: using-innodb-tables,  Up: innodb-tables

14.9.1.2 Moving or Copying InnoDB Tables
........................................

This section describes techniques for moving or copying some or all
'InnoDB' tables to a different server or instance.  For example, you
might move an entire MySQL instance to a larger, faster server; you
might clone an entire MySQL instance to a new replication slave server;
you might copy individual tables to another instance to develop and test
an application, or to a data warehouse server to produce reports.

On Windows, 'InnoDB' always stores database and table names internally
in lowercase.  To move databases in a binary format from Unix to Windows
or from Windows to Unix, create all databases and tables using lowercase
names.  A convenient way to accomplish this is to add the following line
to the '[mysqld]' section of your 'my.cnf' or 'my.ini' file before
creating any databases or tables:

     [mysqld]
     lower_case_table_names=1

Techniques for moving or copying 'InnoDB' tables include:

   * *note copy-tables-cold-backup::

   * *note copy-tables-logical-backup::

*Copying Data Files (Cold Backup Method)*

You can move an 'InnoDB' database simply by copying all the relevant
files listed under "Cold Backups" in *note innodb-backup::.

'InnoDB' data and log files are binary-compatible on all platforms
having the same floating-point number format.  If the floating-point
formats differ but you have not used *note 'FLOAT':
floating-point-types. or *note 'DOUBLE': floating-point-types. data
types in your tables, then the procedure is the same: simply copy the
relevant files.

*Restoring from a Logical Backup*

You can use a utility such as *note 'mysqldump': mysqldump. to perform a
logical backup, which produces a set of SQL statements that can be
executed to reproduce the original database object definitions and table
data for transfer to another SQL server.  Using this method, it does not
matter whether the formats differ or if your tables contain
floating-point data.

To improve the performance of this method, disable 'autocommit' when
importing data.  Perform a commit only after importing an entire table
or segment of a table.


File: manual.info.tmp,  Node: converting-tables-to-innodb,  Next: innodb-auto-increment-handling,  Prev: innodb-migration,  Up: innodb-tables

14.9.1.3 Converting Tables from MyISAM to InnoDB
................................................

If you have *note 'MyISAM': myisam-storage-engine. tables that you want
to convert to *note 'InnoDB': innodb-storage-engine. for better
reliability and scalability, review the following guidelines and tips
before converting.

   * *note innodb-convert-memory-usage::

   * *note innodb-convert-transactions::

   * *note innodb-convert-deadlock::

   * *note innodb-convert-plan-storage::

   * *note innodb-convert-convert::

   * *note innodb-convert-clone::

   * *note innodb-convert-transfer::

   * *note innodb-convert-storage-requirements::

   * *note innodb-convert-primary-key::

   * *note innodb-convert-application-performance::

   * *note innodb-convert-understand-files::

*Adjusting Memory Usage for MyISAM and InnoDB*

As you transition away from 'MyISAM' tables, lower the value of the
'key_buffer_size' configuration option to free memory no longer needed
for caching results.  Increase the value of the
'innodb_buffer_pool_size' configuration option, which performs a similar
role of allocating cache memory for 'InnoDB' tables.  The 'InnoDB'
buffer pool caches both table data and index data, speeding up lookups
for queries and keeping query results in memory for reuse.  For guidance
regarding buffer pool size configuration, see *note memory-use::.

On a busy server, run benchmarks with the query cache turned off.  The
'InnoDB' buffer pool provides similar benefits, so the query cache might
be tying up memory unnecessarily.  For information about the query
cache, see *note query-cache::.

*Handling Too-Long Or Too-Short Transactions*

Because 'MyISAM' tables do not support transactions, you might not have
paid much attention to the 'autocommit' configuration option and the
*note 'COMMIT': commit. and *note 'ROLLBACK': commit. statements.  These
keywords are important to allow multiple sessions to read and write
'InnoDB' tables concurrently, providing substantial scalability benefits
in write-heavy workloads.

While a transaction is open, the system keeps a snapshot of the data as
seen at the beginning of the transaction, which can cause substantial
overhead if the system inserts, updates, and deletes millions of rows
while a stray transaction keeps running.  Thus, take care to avoid
transactions that run for too long:

   * If you are using a *note 'mysql': mysql. session for interactive
     experiments, always *note 'COMMIT': commit. (to finalize the
     changes) or *note 'ROLLBACK': commit. (to undo the changes) when
     finished.  Close down interactive sessions rather than leave them
     open for long periods, to avoid keeping transactions open for long
     periods by accident.

   * Make sure that any error handlers in your application also *note
     'ROLLBACK': commit. incomplete changes or *note 'COMMIT': commit.
     completed changes.

   * *note 'ROLLBACK': commit. is a relatively expensive operation,
     because *note 'INSERT': insert, *note 'UPDATE': update, and *note
     'DELETE': delete. operations are written to 'InnoDB' tables prior
     to the *note 'COMMIT': commit, with the expectation that most
     changes are committed successfully and rollbacks are rare.  When
     experimenting with large volumes of data, avoid making changes to
     large numbers of rows and then rolling back those changes.

   * When loading large volumes of data with a sequence of *note
     'INSERT': insert. statements, periodically *note 'COMMIT': commit.
     the results to avoid having transactions that last for hours.  In
     typical load operations for data warehousing, if something goes
     wrong, you truncate the table (using *note 'TRUNCATE TABLE':
     truncate-table.) and start over from the beginning rather than
     doing a *note 'ROLLBACK': commit.

The preceding tips save memory and disk space that can be wasted during
too-long transactions.  When transactions are shorter than they should
be, the problem is excessive I/O. With each *note 'COMMIT': commit,
MySQL makes sure each change is safely recorded to disk, which involves
some I/O.

   * For most operations on 'InnoDB' tables, you should use the setting
     'autocommit=0'.  From an efficiency perspective, this avoids
     unnecessary I/O when you issue large numbers of consecutive *note
     'INSERT': insert, *note 'UPDATE': update, or *note 'DELETE':
     delete. statements.  From a safety perspective, this allows you to
     issue a *note 'ROLLBACK': commit. statement to recover lost or
     garbled data if you make a mistake on the *note 'mysql': mysql.
     command line, or in an exception handler in your application.

   * The time when 'autocommit=1' is suitable for 'InnoDB' tables is
     when running a sequence of queries for generating reports or
     analyzing statistics.  In this situation, there is no I/O penalty
     related to *note 'COMMIT': commit. or *note 'ROLLBACK': commit, and
     'InnoDB' can automatically optimize the read-only workload
     (https://dev.mysql.com/doc/refman/5.6/en/innodb-performance-ro-txn.html).

   * If you make a series of related changes, finalize all the changes
     at once with a single *note 'COMMIT': commit. at the end.  For
     example, if you insert related pieces of information into several
     tables, do a single *note 'COMMIT': commit. after making all the
     changes.  Or if you run many consecutive *note 'INSERT': insert.
     statements, do a single *note 'COMMIT': commit. after all the data
     is loaded; if you are doing millions of *note 'INSERT': insert.
     statements, perhaps split up the huge transaction by issuing a
     *note 'COMMIT': commit. every ten thousand or hundred thousand
     records, so the transaction does not grow too large.

   * Remember that even a *note 'SELECT': select. statement opens a
     transaction, so after running some report or debugging queries in
     an interactive *note 'mysql': mysql. session, either issue a *note
     'COMMIT': commit. or close the *note 'mysql': mysql. session.

*Handling Deadlocks*

You might see warning messages referring to 'deadlocks' in the MySQL
error log, or the output of *note 'SHOW ENGINE INNODB STATUS':
show-engine.  Despite the scary-sounding name, a deadlock is not a
serious issue for 'InnoDB' tables, and often does not require any
corrective action.  When two transactions start modifying multiple
tables, accessing the tables in a different order, they can reach a
state where each transaction is waiting for the other and neither can
proceed.  MySQL immediately detects this condition and cancels (rolls
back) the 'smaller' transaction, allowing the other to proceed.

Your applications do need error-handling logic to restart a transaction
that is forcibly cancelled like this.  When you re-issue the same SQL
statements as before, the original timing issue no longer applies.
Either the other transaction has already finished and yours can proceed,
or the other transaction is still in progress and your transaction waits
until it finishes.

If deadlock warnings occur constantly, you might review the application
code to reorder the SQL operations in a consistent way, or to shorten
the transactions.

For more information, see *note innodb-deadlocks::.

*Planning the Storage Layout*

To get the best performance from 'InnoDB' tables, you can adjust a
number of parameters related to storage layout.

When you convert 'MyISAM' tables that are large, frequently accessed,
and hold vital data, investigate and consider the
'innodb_file_per_table' and 'innodb_file_format' configuration options,
and the *note 'ROW_FORMAT' and 'KEY_BLOCK_SIZE' clauses:
innodb-row-format. of the *note 'CREATE TABLE': create-table. statement.

During your initial experiments, the most important setting is
'innodb_file_per_table'.  When this setting is enabled, new 'InnoDB'
tables are implicitly created in file-per-table tablespaces.  In
contrast with the 'InnoDB' system tablespace, file-per-table tablespaces
allow disk space to be reclaimed by the operating system when a table is
truncated or dropped.  File-per-table tablespaces also support the
Barracuda file format and associated features such as table compression
and efficient off-page storage for long variable-length columns.  For
more information, see *note innodb-file-per-table-tablespaces::.

*Converting an Existing Table*

To convert a non-'InnoDB' table to use 'InnoDB' use *note 'ALTER TABLE':
alter-table.:

     ALTER TABLE TABLE_NAME ENGINE=InnoDB;

*Warning*:

Do _not_ convert MySQL system tables in the 'mysql' database from
'MyISAM' to 'InnoDB' tables.  This is an unsupported operation.  If you
do this, MySQL does not restart until you restore the old system tables
from a backup or regenerate them by reinitializing the data directory
(see *note data-directory-initialization::).

*Cloning the Structure of a Table*

You might make an 'InnoDB' table that is a clone of a MyISAM table,
rather than using *note 'ALTER TABLE': alter-table. to perform
conversion, to test the old and new table side-by-side before switching.

Create an empty 'InnoDB' table with identical column and index
definitions.  Use 'SHOW CREATE TABLE TABLE_NAME\G' to see the full *note
'CREATE TABLE': create-table. statement to use.  Change the 'ENGINE'
clause to 'ENGINE=INNODB'.

*Transferring Existing Data*

To transfer a large volume of data into an empty 'InnoDB' table created
as shown in the previous section, insert the rows with 'INSERT INTO
INNODB_TABLE SELECT * FROM MYISAM_TABLE ORDER BY PRIMARY_KEY_COLUMNS'.

You can also create the indexes for the 'InnoDB' table after inserting
the data.  Historically, creating new secondary indexes was a slow
operation for InnoDB, but now you can create the indexes after the data
is loaded with relatively little overhead from the index creation step.

If you have 'UNIQUE' constraints on secondary keys, you can speed up a
table import by turning off the uniqueness checks temporarily during the
import operation:

     SET unique_checks=0;... IMPORT OPERATION ...
     SET unique_checks=1;

For big tables, this saves disk I/O because 'InnoDB' can use its change
buffer to write secondary index records as a batch.  Be certain that the
data contains no duplicate keys.  'unique_checks' permits but does not
require storage engines to ignore duplicate keys.

For better control over the insertion process, you can insert big tables
in pieces:

     INSERT INTO newtable SELECT * FROM oldtable
        WHERE yourkey > SOMETHING AND yourkey <= SOMETHINGELSE;

After all records are inserted, you can rename the tables.

During the conversion of big tables, increase the size of the 'InnoDB'
buffer pool to reduce disk I/O, to a maximum of 80% of physical memory.
You can also increase the size of 'InnoDB' log files.

*Storage Requirements*

If you intend to make several temporary copies of your data in 'InnoDB'
tables during the conversion process, it is recommended that you create
the tables in file-per-table tablespaces so that you can reclaim the
disk space when you drop the tables.  When the 'innodb_file_per_table'
configuration option is enabled (the default), newly created 'InnoDB'
tables are implicitly created in file-per-table tablespaces.

Whether you convert the 'MyISAM' table directly or create a cloned
'InnoDB' table, make sure that you have sufficient disk space to hold
both the old and new tables during the process.  *'InnoDB' tables
require more disk space than 'MyISAM' tables.*  If an *note 'ALTER
TABLE': alter-table. operation runs out of space, it starts a rollback,
and that can take hours if it is disk-bound.  For inserts, 'InnoDB' uses
the insert buffer to merge secondary index records to indexes in
batches.  That saves a lot of disk I/O. For rollback, no such mechanism
is used, and the rollback can take 30 times longer than the insertion.

In the case of a runaway rollback, if you do not have valuable data in
your database, it may be advisable to kill the database process rather
than wait for millions of disk I/O operations to complete.  For the
complete procedure, see *note forcing-innodb-recovery::.

*Defining a PRIMARY KEY for Each Table*

The 'PRIMARY KEY' clause is a critical factor affecting the performance
of MySQL queries and the space usage for tables and indexes.  The
primary key uniquely identifies a row in a table.  Every row in the
table must have a primary key value, and no two rows can have the same
primary key value.

These are guidelines for the primary key, followed by more detailed
explanations.

   * Declare a 'PRIMARY KEY' for each table.  Typically, it is the most
     important column that you refer to in 'WHERE' clauses when looking
     up a single row.

   * Declare the 'PRIMARY KEY' clause in the original *note 'CREATE
     TABLE': create-table. statement, rather than adding it later
     through an *note 'ALTER TABLE': alter-table. statement.

   * Choose the column and its data type carefully.  Prefer numeric
     columns over character or string ones.

   * Consider using an auto-increment column if there is not another
     stable, unique, non-null, numeric column to use.

   * An auto-increment column is also a good choice if there is any
     doubt whether the value of the primary key column could ever
     change.  Changing the value of a primary key column is an expensive
     operation, possibly involving rearranging data within the table and
     within each secondary index.

Consider adding a primary key to any table that does not already have
one.  Use the smallest practical numeric type based on the maximum
projected size of the table.  This can make each row slightly more
compact, which can yield substantial space savings for large tables.
The space savings are multiplied if the table has any secondary indexes,
because the primary key value is repeated in each secondary index entry.
In addition to reducing data size on disk, a small primary key also lets
more data fit into the buffer pool, speeding up all kinds of operations
and improving concurrency.

If the table already has a primary key on some longer column, such as a
'VARCHAR', consider adding a new unsigned 'AUTO_INCREMENT' column and
switching the primary key to that, even if that column is not referenced
in queries.  This design change can produce substantial space savings in
the secondary indexes.  You can designate the former primary key columns
as 'UNIQUE NOT NULL' to enforce the same constraints as the 'PRIMARY
KEY' clause, that is, to prevent duplicate or null values across all
those columns.

If you spread related information across multiple tables, typically each
table uses the same column for its primary key.  For example, a
personnel database might have several tables, each with a primary key of
employee number.  A sales database might have some tables with a primary
key of customer number, and other tables with a primary key of order
number.  Because lookups using the primary key are very fast, you can
construct efficient join queries for such tables.

If you leave the 'PRIMARY KEY' clause out entirely, MySQL creates an
invisible one for you.  It is a 6-byte value that might be longer than
you need, thus wasting space.  Because it is hidden, you cannot refer to
it in queries.

*Application Performance Considerations*

The reliability and scalability features of 'InnoDB' require more disk
storage than equivalent 'MyISAM' tables.  You might change the column
and index definitions slightly, for better space utilization, reduced
I/O and memory consumption when processing result sets, and better query
optimization plans making efficient use of index lookups.

If you do set up a numeric ID column for the primary key, use that value
to cross-reference with related values in any other tables, particularly
for join queries.  For example, rather than accepting a country name as
input and doing queries searching for the same name, do one lookup to
determine the country ID, then do other queries (or a single join query)
to look up relevant information across several tables.  Rather than
storing a customer or catalog item number as a string of digits,
potentially using up several bytes, convert it to a numeric ID for
storing and querying.  A 4-byte unsigned *note 'INT': integer-types.
column can index over 4 billion items (with the US meaning of billion:
1000 million).  For the ranges of the different integer types, see *note
integer-types::.

*Understanding Files Associated with InnoDB Tables*

'InnoDB' files require more care and planning than 'MyISAM' files do.

   * You must not delete the ibdata files that represent the 'InnoDB'
     system tablespace.

   * Methods of moving or copying 'InnoDB' tables to a different server
     are described in *note innodb-migration::.


File: manual.info.tmp,  Node: innodb-auto-increment-handling,  Prev: converting-tables-to-innodb,  Up: innodb-tables

14.9.1.4 AUTO_INCREMENT Handling in InnoDB
..........................................

'InnoDB' provides a configurable locking mechanism that can
significantly improve scalability and performance of SQL statements that
add rows to tables with 'AUTO_INCREMENT' columns.  To use the
'AUTO_INCREMENT' mechanism with an 'InnoDB' table, an 'AUTO_INCREMENT'
column must be defined as part of an index such that it is possible to
perform the equivalent of an indexed 'SELECT MAX(AI_COL)' lookup on the
table to obtain the maximum column value.  Typically, this is achieved
by making the column the first column of some table index.

This section describes the behavior of 'AUTO_INCREMENT' lock modes,
usage implications for different 'AUTO_INCREMENT' lock mode settings,
and how 'InnoDB' initializes the 'AUTO_INCREMENT' counter.

   * *note innodb-auto-increment-lock-modes::

   * *note innodb-auto-increment-lock-mode-usage-implications::

   * *note innodb-auto-increment-initialization::

   * *note innodb-auto-increment-notes::

*InnoDB AUTO_INCREMENT Lock Modes*

This section describes the behavior of 'AUTO_INCREMENT' lock modes used
to generate auto-increment values, and how each lock mode affects
replication.  Auto-increment lock modes are configured at startup using
the 'innodb_autoinc_lock_mode' configuration parameter.

The following terms are used in describing 'innodb_autoinc_lock_mode'
settings:

   * '*note 'INSERT': insert.-like' statements

     All statements that generate new rows in a table, including *note
     'INSERT': insert, *note 'INSERT ... SELECT': insert-select, *note
     'REPLACE': replace, *note 'REPLACE ... SELECT': replace, and *note
     'LOAD DATA': load-data.  Includes 'simple-inserts', 'bulk-inserts',
     and 'mixed-mode' inserts.

   * 'Simple inserts'

     Statements for which the number of rows to be inserted can be
     determined in advance (when the statement is initially processed).
     This includes single-row and multiple-row *note 'INSERT': insert.
     and *note 'REPLACE': replace. statements that do not have a nested
     subquery, but not *note 'INSERT ... ON DUPLICATE KEY UPDATE':
     insert-on-duplicate.

   * 'Bulk inserts'

     Statements for which the number of rows to be inserted (and the
     number of required auto-increment values) is not known in advance.
     This includes *note 'INSERT ... SELECT': insert-select, *note
     'REPLACE ... SELECT': replace, and *note 'LOAD DATA': load-data.
     statements, but not plain 'INSERT'.  'InnoDB' assigns new values
     for the 'AUTO_INCREMENT' column one at a time as each row is
     processed.

   * 'Mixed-mode inserts'

     These are 'simple insert' statements that specify the
     auto-increment value for some (but not all) of the new rows.  An
     example follows, where 'c1' is an 'AUTO_INCREMENT' column of table
     't1':

          INSERT INTO t1 (c1,c2) VALUES (1,'a'), (NULL,'b'), (5,'c'), (NULL,'d');

     Another type of 'mixed-mode insert' is *note 'INSERT ... ON
     DUPLICATE KEY UPDATE': insert-on-duplicate, which in the worst case
     is in effect an *note 'INSERT': insert. followed by a *note
     'UPDATE': update, where the allocated value for the
     'AUTO_INCREMENT' column may or may not be used during the update
     phase.

There are three possible settings for the 'innodb_autoinc_lock_mode'
configuration parameter.  The settings are 0, 1, or 2, for
'traditional', 'consecutive', or 'interleaved' lock mode, respectively.

   * 'innodb_autoinc_lock_mode = 0' ('traditional' lock mode)

     The traditional lock mode provides the same behavior that existed
     before the 'innodb_autoinc_lock_mode' configuration parameter was
     introduced in MySQL 5.1.  The traditional lock mode option is
     provided for backward compatibility, performance testing, and
     working around issues with "mixed-mode inserts", due to possible
     differences in semantics.

     In this lock mode, all 'INSERT-like' statements obtain a special
     table-level 'AUTO-INC' lock for inserts into tables with
     'AUTO_INCREMENT' columns.  This lock is normally held to the end of
     the statement (not to the end of the transaction) to ensure that
     auto-increment values are assigned in a predictable and repeatable
     order for a given sequence of *note 'INSERT': insert. statements,
     and to ensure that auto-increment values assigned by any given
     statement are consecutive.

     In the case of statement-based replication, this means that when an
     SQL statement is replicated on a slave server, the same values are
     used for the auto-increment column as on the master server.  The
     result of execution of multiple *note 'INSERT': insert. statements
     is deterministic, and the slave reproduces the same data as on the
     master.  If auto-increment values generated by multiple *note
     'INSERT': insert. statements were interleaved, the result of two
     concurrent *note 'INSERT': insert. statements would be
     nondeterministic, and could not reliably be propagated to a slave
     server using statement-based replication.

     To make this clear, consider an example that uses this table:

          CREATE TABLE t1 (
            c1 INT(11) NOT NULL AUTO_INCREMENT,
            c2 VARCHAR(10) DEFAULT NULL,
            PRIMARY KEY (c1)
          ) ENGINE=InnoDB;

     Suppose that there are two transactions running, each inserting
     rows into a table with an 'AUTO_INCREMENT' column.  One transaction
     is using an *note 'INSERT ... SELECT': insert-select. statement
     that inserts 1000 rows, and another is using a simple *note
     'INSERT': insert. statement that inserts one row:

          Tx1: INSERT INTO t1 (c2) SELECT 1000 rows from another table ...
          Tx2: INSERT INTO t1 (c2) VALUES ('xxx');

     'InnoDB' cannot tell in advance how many rows are retrieved from
     the *note 'SELECT': select. in the *note 'INSERT': insert.
     statement in Tx1, and it assigns the auto-increment values one at a
     time as the statement proceeds.  With a table-level lock, held to
     the end of the statement, only one *note 'INSERT': insert.
     statement referring to table 't1' can execute at a time, and the
     generation of auto-increment numbers by different statements is not
     interleaved.  The auto-increment value generated by the Tx1 *note
     'INSERT ... SELECT': insert-select. statement is consecutive, and
     the (single) auto-increment value used by the *note 'INSERT':
     insert. statement in Tx2 is either be smaller or larger than all
     those used for Tx1, depending on which statement executes first.

     As long as the SQL statements execute in the same order when
     replayed from the binary log (when using statement-based
     replication, or in recovery scenarios), the results are the same as
     they were when Tx1 and Tx2 first ran.  Thus, table-level locks held
     until the end of a statement make *note 'INSERT': insert.
     statements using auto-increment safe for use with statement-based
     replication.  However, those table-level locks limit concurrency
     and scalability when multiple transactions are executing insert
     statements at the same time.

     In the preceding example, if there were no table-level lock, the
     value of the auto-increment column used for the *note 'INSERT':
     insert. in Tx2 depends on precisely when the statement executes.
     If the *note 'INSERT': insert. of Tx2 executes while the *note
     'INSERT': insert. of Tx1 is running (rather than before it starts
     or after it completes), the specific auto-increment values assigned
     by the two *note 'INSERT': insert. statements are nondeterministic,
     and may vary from run to run.

     Under the consecutive lock mode, 'InnoDB' can avoid using
     table-level 'AUTO-INC' locks for 'simple insert' statements where
     the number of rows is known in advance, and still preserve
     deterministic execution and safety for statement-based replication.

     If you are not using the binary log to replay SQL statements as
     part of recovery or replication, the interleaved lock mode can be
     used to eliminate all use of table-level 'AUTO-INC' locks for even
     greater concurrency and performance, at the cost of permitting gaps
     in auto-increment numbers assigned by a statement and potentially
     having the numbers assigned by concurrently executing statements
     interleaved.

   * 'innodb_autoinc_lock_mode = 1' ('consecutive' lock mode)

     This is the default lock mode.  In this mode, 'bulk inserts' use
     the special 'AUTO-INC' table-level lock and hold it until the end
     of the statement.  This applies to all *note 'INSERT ... SELECT':
     insert-select, *note 'REPLACE ... SELECT': replace, and *note 'LOAD
     DATA': load-data. statements.  Only one statement holding the
     'AUTO-INC' lock can execute at a time.  If the source table of the
     bulk insert operation is different from the target table, the
     'AUTO-INC' lock on the target table is taken after a shared lock is
     taken on the first row selected from the source table.  If the
     source and target of the bulk insert operation are the same table,
     the 'AUTO-INC' lock is taken after shared locks are taken on all
     selected rows.

     'Simple inserts' (for which the number of rows to be inserted is
     known in advance) avoid table-level 'AUTO-INC' locks by obtaining
     the required number of auto-increment values under the control of a
     mutex (a light-weight lock) that is only held for the duration of
     the allocation process, _not_ until the statement completes.  No
     table-level 'AUTO-INC' lock is used unless an 'AUTO-INC' lock is
     held by another transaction.  If another transaction holds an
     'AUTO-INC' lock, a 'simple insert' waits for the 'AUTO-INC' lock,
     as if it were a 'bulk insert'.

     This lock mode ensures that, in the presence of *note 'INSERT':
     insert. statements where the number of rows is not known in advance
     (and where auto-increment numbers are assigned as the statement
     progresses), all auto-increment values assigned by any '*note
     'INSERT': insert.-like' statement are consecutive, and operations
     are safe for statement-based replication.

     Simply put, this lock mode significantly improves scalability while
     being safe for use with statement-based replication.  Further, as
     with 'traditional' lock mode, auto-increment numbers assigned by
     any given statement are _consecutive_.  There is _no change_ in
     semantics compared to 'traditional' mode for any statement that
     uses auto-increment, with one important exception.

     The exception is for 'mixed-mode inserts', where the user provides
     explicit values for an 'AUTO_INCREMENT' column for some, but not
     all, rows in a multiple-row 'simple insert'.  For such inserts,
     'InnoDB' allocates more auto-increment values than the number of
     rows to be inserted.  However, all values automatically assigned
     are consecutively generated (and thus higher than) the
     auto-increment value generated by the most recently executed
     previous statement.  'Excess' numbers are lost.

   * 'innodb_autoinc_lock_mode = 2' ('interleaved' lock mode)

     In this lock mode, no '*note 'INSERT': insert.-like' statements use
     the table-level 'AUTO-INC' lock, and multiple statements can
     execute at the same time.  This is the fastest and most scalable
     lock mode, but it is _not safe_ when using statement-based
     replication or recovery scenarios when SQL statements are replayed
     from the binary log.

     In this lock mode, auto-increment values are guaranteed to be
     unique and monotonically increasing across all concurrently
     executing '*note 'INSERT': insert.-like' statements.  However,
     because multiple statements can be generating numbers at the same
     time (that is, allocation of numbers is _interleaved_ across
     statements), the values generated for the rows inserted by any
     given statement may not be consecutive.

     If the only statements executing are 'simple inserts' where the
     number of rows to be inserted is known ahead of time, there are no
     gaps in the numbers generated for a single statement, except for
     'mixed-mode inserts'.  However, when 'bulk inserts' are executed,
     there may be gaps in the auto-increment values assigned by any
     given statement.

*InnoDB AUTO_INCREMENT Lock Mode Usage Implications*

   * Using auto-increment with replication

     If you are using statement-based replication, set
     'innodb_autoinc_lock_mode' to 0 or 1 and use the same value on the
     master and its slaves.  Auto-increment values are not ensured to be
     the same on the slaves as on the master if you use
     'innodb_autoinc_lock_mode' = 2 ('interleaved') or configurations
     where the master and slaves do not use the same lock mode.

     If you are using row-based or mixed-format replication, all of the
     auto-increment lock modes are safe, since row-based replication is
     not sensitive to the order of execution of the SQL statements (and
     the mixed format uses row-based replication for any statements that
     are unsafe for statement-based replication).

   * 'Lost' auto-increment values and sequence gaps

     In all lock modes (0, 1, and 2), if a transaction that generated
     auto-increment values rolls back, those auto-increment values are
     'lost'.  Once a value is generated for an auto-increment column, it
     cannot be rolled back, whether or not the '*note 'INSERT':
     insert.-like' statement is completed, and whether or not the
     containing transaction is rolled back.  Such lost values are not
     reused.  Thus, there may be gaps in the values stored in an
     'AUTO_INCREMENT' column of a table.

   * Specifying NULL or 0 for the 'AUTO_INCREMENT' column

     In all lock modes (0, 1, and 2), if a user specifies NULL or 0 for
     the 'AUTO_INCREMENT' column in an *note 'INSERT': insert, 'InnoDB'
     treats the row as if the value was not specified and generates a
     new value for it.

   * Assigning a negative value to the 'AUTO_INCREMENT' column

     In all lock modes (0, 1, and 2), the behavior of the auto-increment
     mechanism is not defined if you assign a negative value to the
     'AUTO_INCREMENT' column.

   * If the 'AUTO_INCREMENT' value becomes larger than the maximum
     integer for the specified integer type

     In all lock modes (0, 1, and 2), the behavior of the auto-increment
     mechanism is not defined if the value becomes larger than the
     maximum integer that can be stored in the specified integer type.

   * Gaps in auto-increment values for 'bulk inserts'

     With 'innodb_autoinc_lock_mode' set to 0 ('traditional') or 1
     ('consecutive'), the auto-increment values generated by any given
     statement are consecutive, without gaps, because the table-level
     'AUTO-INC' lock is held until the end of the statement, and only
     one such statement can execute at a time.

     With 'innodb_autoinc_lock_mode' set to 2 ('interleaved'), there may
     be gaps in the auto-increment values generated by 'bulk inserts,'
     but only if there are concurrently executing '*note 'INSERT':
     insert.-like' statements.

     For lock modes 1 or 2, gaps may occur between successive statements
     because for bulk inserts the exact number of auto-increment values
     required by each statement may not be known and overestimation is
     possible.

   * Auto-increment values assigned by 'mixed-mode inserts'

     Consider a 'mixed-mode insert,' where a 'simple insert' specifies
     the auto-increment value for some (but not all) resulting rows.
     Such a statement behaves differently in lock modes 0, 1, and 2.
     For example, assume 'c1' is an 'AUTO_INCREMENT' column of table
     't1', and that the most recent automatically generated sequence
     number is 100.

          mysql> CREATE TABLE t1 (
              -> c1 INT UNSIGNED NOT NULL AUTO_INCREMENT PRIMARY KEY,
              -> c2 CHAR(1)
              -> ) ENGINE = INNODB;

     Now, consider the following 'mixed-mode insert' statement:

          mysql> INSERT INTO t1 (c1,c2) VALUES (1,'a'), (NULL,'b'), (5,'c'), (NULL,'d');

     With 'innodb_autoinc_lock_mode' set to 0 ('traditional'), the four
     new rows are:

          mysql> SELECT c1, c2 FROM t1 ORDER BY c2;
          +-----+------+
          | c1  | c2   |
          +-----+------+
          |   1 | a    |
          | 101 | b    |
          |   5 | c    |
          | 102 | d    |
          +-----+------+

     The next available auto-increment value is 103 because the
     auto-increment values are allocated one at a time, not all at once
     at the beginning of statement execution.  This result is true
     whether or not there are concurrently executing '*note 'INSERT':
     insert.-like' statements (of any type).

     With 'innodb_autoinc_lock_mode' set to 1 ('consecutive'), the four
     new rows are also:

          mysql> SELECT c1, c2 FROM t1 ORDER BY c2;
          +-----+------+
          | c1  | c2   |
          +-----+------+
          |   1 | a    |
          | 101 | b    |
          |   5 | c    |
          | 102 | d    |
          +-----+------+

     However, in this case, the next available auto-increment value is
     105, not 103 because four auto-increment values are allocated at
     the time the statement is processed, but only two are used.  This
     result is true whether or not there are concurrently executing
     '*note 'INSERT': insert.-like' statements (of any type).

     With 'innodb_autoinc_lock_mode' set to mode 2 ('interleaved'), the
     four new rows are:

          mysql> SELECT c1, c2 FROM t1 ORDER BY c2;
          +-----+------+
          | c1  | c2   |
          +-----+------+
          |   1 | a    |
          |   X | b    |
          |   5 | c    |
          |   Y | d    |
          +-----+------+

     The values of X and Y are unique and larger than any previously
     generated rows.  However, the specific values of X and Y depend on
     the number of auto-increment values generated by concurrently
     executing statements.

     Finally, consider the following statement, issued when the
     most-recently generated sequence number is 100:

          mysql> INSERT INTO t1 (c1,c2) VALUES (1,'a'), (NULL,'b'), (101,'c'), (NULL,'d');

     With any 'innodb_autoinc_lock_mode' setting, this statement
     generates a duplicate-key error 23000 ('Can't write; duplicate key
     in table') because 101 is allocated for the row '(NULL, 'b')' and
     insertion of the row '(101, 'c')' fails.

   * Modifying 'AUTO_INCREMENT' column values in the middle of a
     sequence of *note 'INSERT': insert. statements

     In all lock modes (0, 1, and 2), modifying an 'AUTO_INCREMENT'
     column value in the middle of a sequence of *note 'INSERT': insert.
     statements could lead to 'Duplicate entry' errors.  For example, if
     you perform an *note 'UPDATE': update. operation that changes an
     'AUTO_INCREMENT' column value to a value larger than the current
     maximum auto-increment value, subsequent *note 'INSERT': insert.
     operations that do not specify an unused auto-increment value could
     encounter 'Duplicate entry' errors.  This behavior is demonstrated
     in the following example.

          mysql> CREATE TABLE t1 (
              -> c1 INT NOT NULL AUTO_INCREMENT,
              -> PRIMARY KEY (c1)
              ->  ) ENGINE = InnoDB;

          mysql> INSERT INTO t1 VALUES(0), (0), (3);

          mysql> SELECT c1 FROM t1;
          +----+
          | c1 |
          +----+
          |  1 |
          |  2 |
          |  3 |
          +----+

          mysql> UPDATE t1 SET c1 = 4 WHERE c1 = 1;

          mysql> SELECT c1 FROM t1;
          +----+
          | c1 |
          +----+
          |  2 |
          |  3 |
          |  4 |
          +----+

          mysql> INSERT INTO t1 VALUES(0);
          ERROR 1062 (23000): Duplicate entry '4' for key 'PRIMARY'

*InnoDB AUTO_INCREMENT Counter Initialization*

This section describes how 'InnoDB' initializes 'AUTO_INCREMENT'
counters.

If you specify an 'AUTO_INCREMENT' column for an 'InnoDB' table, the
table handle in the 'InnoDB' data dictionary contains a special counter
called the auto-increment counter that is used in assigning new values
for the column.  This counter is stored only in main memory, not on
disk.

To initialize an auto-increment counter after a server restart, 'InnoDB'
executes the equivalent of the following statement on the first insert
into a table containing an 'AUTO_INCREMENT' column.

     SELECT MAX(ai_col) FROM TABLE_NAME FOR UPDATE;

'InnoDB' increments the value retrieved by the statement and assigns it
to the column and to the auto-increment counter for the table.  By
default, the value is incremented by 1.  This default can be overridden
by the 'auto_increment_increment' configuration setting.

If the table is empty, 'InnoDB' uses the value '1'.  This default can be
overridden by the 'auto_increment_offset' configuration setting.

If a *note 'SHOW TABLE STATUS': show-table-status. statement examines
the table before the auto-increment counter is initialized, 'InnoDB'
initializes but does not increment the value.  The value is stored for
use by later inserts.  This initialization uses a normal
exclusive-locking read on the table and the lock lasts to the end of the
transaction.  'InnoDB' follows the same procedure for initializing the
auto-increment counter for a newly created table.

After the auto-increment counter has been initialized, if you do not
explicitly specify a value for an 'AUTO_INCREMENT' column, 'InnoDB'
increments the counter and assigns the new value to the column.  If you
insert a row that explicitly specifies the column value, and the value
is greater than the current counter value, the counter is set to the
specified column value.

'InnoDB' uses the in-memory auto-increment counter as long as the server
runs.  When the server is stopped and restarted, 'InnoDB' reinitializes
the counter for each table for the first *note 'INSERT': insert. to the
table, as described earlier.

A server restart also cancels the effect of the 'AUTO_INCREMENT = N'
table option in *note 'CREATE TABLE': create-table. and *note 'ALTER
TABLE': alter-table. statements, which you can use with 'InnoDB' tables
to set the initial counter value or alter the current counter value.

*Notes*

   * When an 'AUTO_INCREMENT' integer column runs out of values, a
     subsequent 'INSERT' operation returns a duplicate-key error.  This
     is general MySQL behavior.

   * When you restart the MySQL server, 'InnoDB' may reuse an old value
     that was generated for an 'AUTO_INCREMENT' column but never stored
     (that is, a value that was generated during an old transaction that
     was rolled back).


File: manual.info.tmp,  Node: innodb-indexes,  Next: innodb-tablespace,  Prev: innodb-tables,  Up: innodb-on-disk-structures

14.9.2 Indexes
--------------

* Menu:

* innodb-index-types::           Clustered and Secondary Indexes
* innodb-physical-structure::    The Physical Structure of an InnoDB Index

This section covers topics related to 'InnoDB' indexes.


File: manual.info.tmp,  Node: innodb-index-types,  Next: innodb-physical-structure,  Prev: innodb-indexes,  Up: innodb-indexes

14.9.2.1 Clustered and Secondary Indexes
........................................

Every 'InnoDB' table has a special index called the clustered index
where the data for the rows is stored.  Typically, the clustered index
is synonymous with the primary key.  To get the best performance from
queries, inserts, and other database operations, you must understand how
'InnoDB' uses the clustered index to optimize the most common lookup and
DML operations for each table.

   * When you define a 'PRIMARY KEY' on your table, 'InnoDB' uses it as
     the clustered index.  Define a primary key for each table that you
     create.  If there is no logical unique and non-null column or set
     of columns, add a new auto-increment column, whose values are
     filled in automatically.

   * If you do not define a 'PRIMARY KEY' for your table, MySQL locates
     the first 'UNIQUE' index where all the key columns are 'NOT NULL'
     and 'InnoDB' uses it as the clustered index.

   * If the table has no 'PRIMARY KEY' or suitable 'UNIQUE' index,
     'InnoDB' internally generates a hidden clustered index named
     'GEN_CLUST_INDEX' on a synthetic column containing row ID values.
     The rows are ordered by the ID that 'InnoDB' assigns to the rows in
     such a table.  The row ID is a 6-byte field that increases
     monotonically as new rows are inserted.  Thus, the rows ordered by
     the row ID are physically in insertion order.

*How the Clustered Index Speeds Up Queries*

Accessing a row through the clustered index is fast because the index
search leads directly to the page with all the row data.  If a table is
large, the clustered index architecture often saves a disk I/O operation
when compared to storage organizations that store row data using a
different page from the index record.

*How Secondary Indexes Relate to the Clustered Index*

All indexes other than the clustered index are known as secondary
indexes.  In 'InnoDB', each record in a secondary index contains the
primary key columns for the row, as well as the columns specified for
the secondary index.  'InnoDB' uses this primary key value to search for
the row in the clustered index.

If the primary key is long, the secondary indexes use more space, so it
is advantageous to have a short primary key.

For guidelines to take advantage of 'InnoDB' clustered and secondary
indexes, see *note optimization-indexes::.


File: manual.info.tmp,  Node: innodb-physical-structure,  Prev: innodb-index-types,  Up: innodb-indexes

14.9.2.2 The Physical Structure of an InnoDB Index
..................................................

All 'InnoDB' indexes are B-trees where the index records are stored in
the leaf pages of the tree.  The default size of an index page is 16KB.

When new records are inserted into an 'InnoDB' clustered index, 'InnoDB'
tries to leave 1/16 of the page free for future insertions and updates
of the index records.  If index records are inserted in a sequential
order (ascending or descending), the resulting index pages are about
15/16 full.  If records are inserted in a random order, the pages are
from 1/2 to 15/16 full.  If the fill factor of an index page drops below
1/2, 'InnoDB' tries to contract the index tree to free the page.

Changing the 'InnoDB' page size is not a supported operation and there
is no guarantee that 'InnoDB' functions normally with a page size other
than 16KB. Problems compiling or running 'InnoDB' may occur.  In
particular, 'ROW_FORMAT=COMPRESSED' in the Barracuda file format assumes
that the page size is at most 16KB and uses 14-bit pointers.

An instance using a particular 'InnoDB' page size cannot use data files
or log files from an instance that uses a different page size.


File: manual.info.tmp,  Node: innodb-tablespace,  Next: innodb-data-dictionary,  Prev: innodb-indexes,  Up: innodb-on-disk-structures

14.9.3 Tablespaces
------------------

* Menu:

* innodb-system-tablespace::     The System Tablespace
* innodb-file-per-table-tablespaces::  File-Per-Table Tablespaces

This section covers topics related to 'InnoDB' tablespaces.


File: manual.info.tmp,  Node: innodb-system-tablespace,  Next: innodb-file-per-table-tablespaces,  Prev: innodb-tablespace,  Up: innodb-tablespace

14.9.3.1 The System Tablespace
..............................

The system tablespace is the storage area for the 'InnoDB' data
dictionary, the doublewrite buffer, the change buffer, and undo logs.
It may also contain table and index data if tables are created in the
system tablespace rather than file-per-table tablespaces.

The system tablespace can have one or more data files.  By default, a
single system tablespace data file, named 'ibdata1', is created in the
data directory.  The size and number of system tablespace data files is
defined by the 'innodb_data_file_path' startup option.  For
configuration information, see *note
innodb-startup-data-file-configuration::.

Additional information about the system tablespace is provided under the
following topics in the section:

   * *note innodb-resize-system-tablespace::

   * *note innodb-raw-devices::

*Resizing the System Tablespace*

This section describes how to increase or decrease the size of the
system tablespace.

*Increasing the Size of the System Tablespace*

The easiest way to increase the size of the system tablespace is to
configure it to be auto-extending.  To do so, specify the 'autoextend'
attribute for the last data file in the 'innodb_data_file_path' setting,
and restart the server.  For example:

     innodb_data_file_path=ibdata1:10MB:autoextend

When the 'autoextend' attribute is specified, the data file
automatically increases in size by 8MB increments as space is required.
The 'innodb_autoextend_increment' variable controls the increment size.

You can also increase system tablespace size by adding another data
file.  To do so:

  1. Stop the MySQL server.

  2. If the last data file in the 'innodb_data_file_path' setting is
     defined with the 'autoextend' attribute, remove it, and modify the
     size attribute to reflect the current data file size.  To determine
     the appropriate data file size to specify, check your file system
     for the file size, and round that value down to the closest MB
     value, where a MB is equal to 1024 x 1024.

  3. Append a new data file to the 'innodb_data_file_path' setting,
     optionally specifying the 'autoextend' attribute.  The 'autoextend'
     attribute can be specified only for the last data file in the
     'innodb_data_file_path' setting.

  4. Start the MySQL server.

For example, this tablespace has one auto-extending data file:

     innodb_data_home_dir =
     innodb_data_file_path = /ibdata/ibdata1:10M:autoextend

Suppose that the data file has grown to 988MB over time.  This is the
'innodb_data_file_path' setting after modifying the size attribute to
reflect the current data file size, and after specifying a new 50MB
auto-extending data file:

     innodb_data_home_dir =
     innodb_data_file_path = /ibdata/ibdata1:988M;/disk2/ibdata2:50M:autoextend

When adding a new data file, do not specify an existing file name.
'InnoDB' creates and initializes the new data file when you start the
server.

*Note*:

You cannot increase the size of an existing system tablespace data file
by changing its size attribute.  For example, changing the
'innodb_data_file_path' setting from 'ibdata1:10M:autoextend' to
'ibdata1:12M:autoextend' produces the following error when starting the
server:

     [ERROR] [MY-012263] [InnoDB] The Auto-extending innodb_system
     data file './ibdata1' is of a different size 640 pages (rounded down to MB) than
     specified in the .cnf file: initial 768 pages, max 0 (relevant if non-zero) pages!

The error indicates that the existing data file size (expressed in
'InnoDB' pages) is different from the size specified in the
configuration file.  If you encounter this error, restore the previous
'innodb_data_file_path' setting, and refer to the system tablespace
resizing instructions.

'InnoDB' page size is defined by the 'innodb_page_size'
(https://dev.mysql.com/doc/refman/5.6/en/innodb-parameters.html#sysvar_innodb_page_size)
variable.  The default is 16384 bytes.

*Decreasing the Size of the InnoDB System Tablespace*

You cannot remove a data file from the system tablespace.  To decrease
the system tablespace size, use this procedure:

  1. Use *note 'mysqldump': mysqldump. to dump all of your 'InnoDB'
     tables.

  2. Stop the server.

  3. Remove all of the existing tablespace files, including the 'ibdata'
     and 'ib_log' files.  If you want to keep a backup copy of the
     information, then copy all the 'ib*' files to another location
     before the removing the files in your MySQL installation.

  4. Remove any '.frm' files for 'InnoDB' tables.

  5. Configure the data files for the new system tablespace.  See *note
     innodb-startup-data-file-configuration::.

  6. Restart the server.

  7. Import the dump files.

To avoid large system tablespaces, consider using file-per-table
tablespaces for your data.  File-per-table tablespaces are the default
tablespace type and are used implicitly when creating an 'InnoDB' table.
Unlike the system tablespace, disk space is returned to the operating
system after truncating or dropping a table created in a file-per-table
tablespace.  For more information, see *note
innodb-file-per-table-tablespaces::.

*Using Raw Disk Partitions for the System Tablespace*

You can use raw disk partitions as data files in the 'InnoDB' system
tablespace.  This technique enables nonbuffered I/O on Windows and on
some Linux and Unix systems without file system overhead.  Perform tests
with and without raw partitions to verify whether this change actually
improves performance on your system.

When you use a raw disk partition, ensure that the user ID that runs the
MySQL server has read and write privileges for that partition.  For
example, if you run the server as the 'mysql' user, the partition must
be readable and writeable by 'mysql'.  If you run the server with the
'--memlock' option, the server must be run as 'root', so the partition
must be readable and writeable by 'root'.

The procedures described below involve option file modification.  For
additional information, see *note option-files::.

*Allocating a Raw Disk Partition on Linux and Unix Systems*

  1. When you create a new data file, specify the keyword 'newraw'
     immediately after the data file size for the
     'innodb_data_file_path' option.  The partition must be at least as
     large as the size that you specify.  Note that 1MB in 'InnoDB' is
     1024 x 1024 bytes, whereas 1MB in disk specifications usually means
     1,000,000 bytes.

          [mysqld]
          innodb_data_home_dir=
          innodb_data_file_path=/dev/hdd1:3Gnewraw;/dev/hdd2:2Gnewraw

  2. Restart the server.  'InnoDB' notices the 'newraw' keyword and
     initializes the new partition.  However, do not create or change
     any 'InnoDB' tables yet.  Otherwise, when you next restart the
     server, 'InnoDB' reinitializes the partition and your changes are
     lost.  (As a safety measure 'InnoDB' prevents users from modifying
     data when any partition with 'newraw' is specified.)

  3. After 'InnoDB' has initialized the new partition, stop the server,
     change 'newraw' in the data file specification to 'raw':

          [mysqld]
          innodb_data_home_dir=
          innodb_data_file_path=/dev/hdd1:3Graw;/dev/hdd2:2Graw

  4. Restart the server.  'InnoDB' now permits changes to be made.

*Allocating a Raw Disk Partition on Windows*

On Windows systems, the same steps and accompanying guidelines described
for Linux and Unix systems apply except that the 'innodb_data_file_path'
setting differs slightly on Windows.

  1. When you create a new data file, specify the keyword 'newraw'
     immediately after the data file size for the
     'innodb_data_file_path' option:

          [mysqld]
          innodb_data_home_dir=
          innodb_data_file_path=//./D::10Gnewraw

     The '//./' corresponds to the Windows syntax of '\\.\' for
     accessing physical drives.  In the example above, 'D:' is the drive
     letter of the partition.

  2. Restart the server.  'InnoDB' notices the 'newraw' keyword and
     initializes the new partition.

  3. After 'InnoDB' has initialized the new partition, stop the server,
     change 'newraw' in the data file specification to 'raw':

          [mysqld]
          innodb_data_home_dir=
          innodb_data_file_path=//./D::10Graw

  4. Restart the server.  'InnoDB' now permits changes to be made.


File: manual.info.tmp,  Node: innodb-file-per-table-tablespaces,  Prev: innodb-system-tablespace,  Up: innodb-tablespace

14.9.3.2 File-Per-Table Tablespaces
...................................

A file-per-table tablespace contains data and indexes for a single
'InnoDB' table, and is stored on the file system in its own data file.

File-per-table tablespace characteristics are described under the
following topics in this section:

   * *note innodb-file-per-table-configuration::

   * *note innodb-file-per-table-data-files::

   * *note innodb-file-per-table-advantages::

   * *note innodb-file-per-table-disadvantages::

*File-Per-Table Tablespace Configuration*

'InnoDB' creates tables in the shared system tablespace by default.  To
have 'InnoDB' create tables in file-per-table tablespaces instead, you
can enable the 'innodb_file_per_table' variable.

An 'innodb_file_per_table' setting can be specified in an option file or
configured at runtime using a *note 'SET GLOBAL': set-variable.
statement.  Changing the setting at runtime requires privileges
sufficient to set global system variables.  See *note
system-variable-privileges::.

Option file:

     [mysqld]
     innodb_file_per_table=ON

Using *note 'SET GLOBAL': set-variable. at runtime:

     mysql> SET GLOBAL innodb_file_per_table=ON;

*Warning*:

Enabling 'innodb_file_per_table' causes a table-copying *note 'ALTER
TABLE': alter-table. operation to implicitly move a table that resides
in the system tablespace to a file-per-table tablespace.  A
table-copying *note 'ALTER TABLE': alter-table. operation recreates the
table using the current 'innodb_file_per_table' setting.  This behavior
does not apply when adding or dropping secondary indexes.  To perform a
table-copying 'ALTER TABLE' operation on a table that resides in the
system tablespace without moving the table to a file-per-table
tablespace, ensure that 'innodb_file_per_table' is disabled before
executing the operation.

*File-Per-Table Tablespace Data Files*

A file-per-table tablespace is created in an '.idb' data file in a
schema directory under the MySQL data directory.  The '.ibd' file is
named for the table ('TABLE_NAME.ibd').  For example, the data file for
table 'test.t1' is created in the 'test' directory under the MySQL data
directory:

     mysql> USE test;

     mysql> CREATE TABLE t1 (
        id INT PRIMARY KEY AUTO_INCREMENT,
        name VARCHAR(100)
      ) ENGINE = InnoDB;

     shell> cd /PATH/TO/MYSQL/data/test
     shell> ls
     t1.ibd

*File-Per-Table Tablespace Advantages*

File-per-table tablespaces have the following advantages over the shared
system tablespace.

   * Disk space is returned to the operating system after truncating or
     dropping a table created in a file-per-table tablespace.
     Truncating or dropping a table stored in the system tablespace
     creates free space within the system tablespace, which can only be
     used for 'InnoDB' data.  In other words, a system tablespace does
     not shrink in size after a table is truncated or dropped.

   * A table-copying *note 'ALTER TABLE': alter-table. operation on a
     table that resides in the system tablespace can increase the amount
     of disk space occupied by the tablespace.  Such operations may
     require as much additional space as the data in the table plus
     indexes.  This space is not released back to the operating system
     as it is for file-per-table tablespaces.

   * *note 'TRUNCATE TABLE': truncate-table. performance is better when
     executed on tables that reside in file-per-table tablespaces.

   * Tables created in file-per-table tablespaces use the Barracuda file
     format.  See *note innodb-file-format::.  The Barracuda file format
     enables features associated with 'DYNAMIC' and 'COMPRESSED' row
     formats.  See *note innodb-row-format::.

   * Tables stored in individual tablespace data files can save time and
     improve chances for a successful recovery when data corruption
     occurs, when backups or binary logs are unavailable, or when the
     MySQL server instance cannot be restarted.

   * You can backup or restore tables created in file-per-table
     tablespaces quickly using MySQL Enterprise Backup, without
     interrupting the use of other 'InnoDB' tables.  This is beneficial
     for tables on varying backup schedules or that require backup less
     frequently.  See Making a Partial Backup
     (https://dev.mysql.com/doc/mysql-enterprise-backup/3.12/en/partial.html)
     for details.

   * File-per-table tablespaces permit monitoring table size on the file
     system by monitoring the size of the tablespace data file.

   * Common Linux file systems do not permit concurrent writes to a
     single file such as a system tablespace data file when
     'innodb_flush_method' is set to 'O_DIRECT'.  As a result, there are
     possible performance improvements when using file-per-table
     tablespaces in conjunction with this setting.

   * Tables in the shared system tablespace, which contains other
     structures such as the 'InnoDB' data dictionary and undo logs, are
     limited in size by the 64TB tablespace size limit.  By comparison,
     each file-per-table tablespace has a 64TB size limit, which
     provides plenty of room for individual tables to grow in size.

*File-Per-Table Tablespace Disadvantages*

File-per-table tablespaces have the following disadvantages compared to
the shared system tablespace.

   * With file-per-table tablespaces, each table may have unused space
     that can only be utilized by rows of the same table, which can lead
     to wasted space if not properly managed.

   * 'fsync' operations are performed on multiple file-per-table data
     files instead of a shared system tablespace data file.  Because
     'fsync' operations are per file, write operations for multiple
     tables cannot be combined, which can result in a higher total
     number of 'fsync' operations.

   * *note 'mysqld': mysqld. must keep an open file handle for each
     file-per-table tablespace, which may impact performance if you have
     numerous tables in file-per-table tablespaces.

   * More file descriptors are required when each table has its own data
     file.

   * There is potential for more fragmentation, which can impede *note
     'DROP TABLE': drop-table. and table scan performance.  However, if
     fragmentation is managed, file-per-table tablespaces can improve
     performance for these operations.

   * The buffer pool is scanned when dropping a table that resides in a
     file-per-table tablespace, which can take several seconds for large
     buffer pools.  The scan is performed with a broad internal lock,
     which may delay other operations.

   * The 'innodb_autoextend_increment' variable, which defines the
     increment size for extending the size of an auto-extending system
     tablespace file when it becomes full, does not apply to
     file-per-table tablespace files, which are auto-extending
     regardless of the 'innodb_autoextend_increment' setting.  Initial
     file-per-table tablespace extensions are by small amounts, after
     which extensions occur in increments of 4MB.


File: manual.info.tmp,  Node: innodb-data-dictionary,  Next: innodb-doublewrite-buffer,  Prev: innodb-tablespace,  Up: innodb-on-disk-structures

14.9.4 InnoDB Data Dictionary
-----------------------------

The 'InnoDB' data dictionary is comprised of internal system tables that
contain metadata used to keep track of objects such as tables, indexes,
and table columns.  The metadata is physically located in the 'InnoDB'
system tablespace.  For historical reasons, data dictionary metadata
overlaps to some degree with information stored in 'InnoDB' table
metadata files ('.frm' files).


File: manual.info.tmp,  Node: innodb-doublewrite-buffer,  Next: innodb-redo-log,  Prev: innodb-data-dictionary,  Up: innodb-on-disk-structures

14.9.5 Doublewrite Buffer
-------------------------

The doublewrite buffer is a storage area in the system tablespace where
'InnoDB' writes pages that are flushed from the buffer pool before
writing them to their proper positions in the data file.  Only after
flushing and writing pages to the doublewrite buffer does 'InnoDB' write
pages to their proper positions.  If there is an operating system,
storage subsystem, or *note 'mysqld': mysqld. process crash in the
middle of a page write, 'InnoDB' can find a good copy of the page from
the doublewrite buffer during crash recovery.

Although data is always written twice, the doublewrite buffer does not
require twice as much I/O overhead or twice as many I/O operations.
Data is written to the doublewrite buffer as a large sequential chunk,
with a single 'fsync()' call to the operating system.

The doublewrite buffer is enabled by default.  To disable the
doublewrite buffer, set 'innodb_doublewrite' to 0.


File: manual.info.tmp,  Node: innodb-redo-log,  Next: innodb-undo-logs,  Prev: innodb-doublewrite-buffer,  Up: innodb-on-disk-structures

14.9.6 Redo Log
---------------

The redo log is a disk-based data structure used during crash recovery
to correct data written by incomplete transactions.  During normal
operations, the redo log encodes requests to change table data that
result from SQL statements or low-level API calls.  Modifications that
did not finish updating the data files before an unexpected shutdown are
replayed automatically during initialization, and before the connections
are accepted.  For information about the role of the redo log in crash
recovery, see *note innodb-recovery::.

By default, the redo log is physically represented on disk by two files
named 'ib_logfile0' and 'ib_logfile1'.  MySQL writes to the redo log
files in a circular fashion.  Data in the redo log is encoded in terms
of records affected; this data is collectively referred to as redo.  The
passage of data through the redo log is represented by an
ever-increasing LSN value.

For related information, see *note
innodb-startup-log-file-configuration::, and *note
optimizing-innodb-logging::.

*Changing the Number or Size of Redo Log Files*

To change the number or the size of your 'InnoDB' redo log files,
perform the following steps:

  1. If 'innodb_fast_shutdown' is set to 2, set 'innodb_fast_shutdown'
     to 1:

          mysql> SET GLOBAL innodb_fast_shutdown = 1;

  2. After ensuring that 'innodb_fast_shutdown' is not set to 2, stop
     the MySQL server and make sure that it shuts down without errors
     (to ensure that there is no information for outstanding
     transactions in the log).

  3. Copy the old log files into a safe place in case something went
     wrong during the shutdown and you need them to recover the
     tablespace.

  4. Delete the old log files from the log file directory.

  5. Edit 'my.cnf' to change the log file configuration.

  6. Start the MySQL server again.  *note 'mysqld': mysqld. sees that no
     *note 'InnoDB': innodb-storage-engine. log files exist at startup
     and creates new ones.

*Group Commit for Redo Log Flushing*

'InnoDB', like any other ACID-compliant database engine, flushes the
redo log of a transaction before it is committed.  'InnoDB' uses group
commit functionality to group multiple such flush requests together to
avoid one flush for each commit.  With group commit, 'InnoDB' issues a
single write to the redo log file to perform the commit action for
multiple user transactions that commit at about the same time,
significantly improving throughput.

Group commit in 'InnoDB' worked in earlier releases of MySQL and works
once again with MySQL 5.1 with the 'InnoDB' Plugin, and MySQL 5.5 and
higher.  The introduction of support for the distributed transactions
and Two Phase Commit (2PC) in MySQL 5.0 interfered with the 'InnoDB'
group commit functionality.  This issue is now resolved.

The group commit functionality inside 'InnoDB' works with the Two Phase
Commit protocol in MySQL. Re-enabling of the group commit functionality
fully ensures that the ordering of commit in the MySQL binary log and
the 'InnoDB' logfile is the same as it was before.  It means it is safe
to use the MySQL Enterprise Backup product with 'InnoDB' 1.0.4 (that is,
the 'InnoDB' Plugin with MySQL 5.1) and above.

For more information about performance of 'COMMIT' and other
transactional operations, see *note
optimizing-innodb-transaction-management::.


File: manual.info.tmp,  Node: innodb-undo-logs,  Prev: innodb-redo-log,  Up: innodb-on-disk-structures

14.9.7 Undo Logs
----------------

An undo log is a collection of undo log records associated with a single
read-write transaction.  An undo log record contains information about
how to undo the latest change by a transaction to a clustered index
record.  If another transaction needs to see the original data as part
of a consistent read operation, the unmodified data is retrieved from
undo log records.  Undo logs exist within undo log segments, which are
contained within rollback segments.  Rollback segments are physically
part of the system tablespace.

'InnoDB' supports 128 rollback segments.  The 'innodb_rollback_segments'
variable defines the number of rollback segments used by 'InnoDB'.

Each rollback segment supports up to 1024 transactions, depending on the
number of undo logs assigned to each transaction.  (The 1024 value is
the number of undo slots in a rollback segment.)  A transaction is
assigned up to two undo logs, one for each of the following operation
types:

  1. *note 'INSERT': insert. operations

  2. *note 'UPDATE': update. and *note 'DELETE': delete. operations

Undo logs are assigned as needed.  For example, a transaction that
performs *note 'INSERT': insert, *note 'UPDATE': update, and *note
'DELETE': delete. operations is assigned two undo logs.  A transaction
that performs only *note 'INSERT': insert. operations is assigned a
single undo log.  Undo logs are assigned to a transaction from a single
rollback segment that is also assigned to the transaction.

An undo log assigned to a transaction remains tied to the transaction
for its duration.  For example, an undo log assigned to a transaction
for an *note 'INSERT': insert. operation is used for all *note 'INSERT':
insert. operations performed by that transaction.

Given the factors described above, the following formulas can be used to
estimate the number of concurrent read-write transactions that 'InnoDB'
is capable of supporting.

*Note*:

A transaction can encounter a concurrent transaction limit error before
reaching the number of concurrent read-write transactions that 'InnoDB'
is capable of supporting.  This occurs when the rollback segment
assigned to a transaction runs out of undo slots.  In such cases, try
rerunning the transaction.

   * If each transaction performs either an *note 'INSERT': insert. *or*
     an *note 'UPDATE': update. or *note 'DELETE': delete. operation,
     the number of concurrent read-write transactions that 'InnoDB' is
     capable of supporting is:

          1024 * innodb_rollback_segments

   * If each transaction performs an *note 'INSERT': insert. *and* an
     *note 'UPDATE': update. or *note 'DELETE': delete. operation, the
     number of concurrent read-write transactions that 'InnoDB' is
     capable of supporting is:

          (1024 / 2) * innodb_rollback_segments


File: manual.info.tmp,  Node: innodb-locking-transaction-model,  Next: innodb-configuration,  Prev: innodb-on-disk-structures,  Up: innodb-storage-engine

14.10 InnoDB Locking and Transaction Model
==========================================

* Menu:

* innodb-locking::               InnoDB Locking
* innodb-transaction-model::     InnoDB Transaction Model
* innodb-locks-set::             Locks Set by Different SQL Statements in InnoDB
* innodb-next-key-locking::      Phantom Rows
* innodb-deadlocks::             Deadlocks in InnoDB

To implement a large-scale, busy, or highly reliable database
application, to port substantial code from a different database system,
or to tune MySQL performance, it is important to understand 'InnoDB'
locking and the 'InnoDB' transaction model.

This section discusses several topics related to 'InnoDB' locking and
the 'InnoDB' transaction model with which you should be familiar.

   * *note innodb-locking:: describes lock types used by 'InnoDB'.

   * *note innodb-transaction-model:: describes transaction isolation
     levels and the locking strategies used by each.  It also discusses
     the use of 'autocommit', consistent non-locking reads, and locking
     reads.

   * *note innodb-locks-set:: discusses specific types of locks set in
     'InnoDB' for various statements.

   * *note innodb-next-key-locking:: describes how 'InnoDB' uses
     next-key locking to avoid phantom rows.

   * *note innodb-deadlocks:: provides a deadlock example, discusses
     deadlock detection and rollback, and provides tips for minimizing
     and handling deadlocks in 'InnoDB'.


File: manual.info.tmp,  Node: innodb-locking,  Next: innodb-transaction-model,  Prev: innodb-locking-transaction-model,  Up: innodb-locking-transaction-model

14.10.1 InnoDB Locking
----------------------

This section describes lock types used by 'InnoDB'.

   * *note innodb-shared-exclusive-locks::

   * *note innodb-intention-locks::

   * *note innodb-record-locks::

   * *note innodb-gap-locks::

   * *note innodb-next-key-locks::

   * *note innodb-insert-intention-locks::

   * *note innodb-auto-inc-locks::

*Shared and Exclusive Locks*

'InnoDB' implements standard row-level locking where there are two types
of locks, shared ('S') locks and exclusive ('X') locks.

   * A shared ('S') lock permits the transaction that holds the lock to
     read a row.

   * An exclusive ('X') lock permits the transaction that holds the lock
     to update or delete a row.

If transaction 'T1' holds a shared ('S') lock on row 'r', then requests
from some distinct transaction 'T2' for a lock on row 'r' are handled as
follows:

   * A request by 'T2' for an 'S' lock can be granted immediately.  As a
     result, both 'T1' and 'T2' hold an 'S' lock on 'r'.

   * A request by 'T2' for an 'X' lock cannot be granted immediately.

If a transaction 'T1' holds an exclusive ('X') lock on row 'r', a
request from some distinct transaction 'T2' for a lock of either type on
'r' cannot be granted immediately.  Instead, transaction 'T2' has to
wait for transaction 'T1' to release its lock on row 'r'.

*Intention Locks*

'InnoDB' supports _multiple granularity locking_ which permits
coexistence of row locks and table locks.  For example, a statement such
as *note 'LOCK TABLES ... WRITE': lock-tables. takes an exclusive lock
(an 'X' lock) on the specified table.  To make locking at multiple
granularity levels practical, 'InnoDB' uses intention locks.  Intention
locks are table-level locks that indicate which type of lock (shared or
exclusive) a transaction requires later for a row in a table.  There are
two types of intention locks:

   * An intention shared lock ('IS') indicates that a transaction
     intends to set a _shared_ lock on individual rows in a table.

   * An intention exclusive lock ('IX') indicates that a transaction
     intends to set an exclusive lock on individual rows in a table.

For example, *note 'SELECT ... LOCK IN SHARE MODE': select. sets an 'IS'
lock, and *note 'SELECT ... FOR UPDATE': select. sets an 'IX' lock.

The intention locking protocol is as follows:

   * Before a transaction can acquire a shared lock on a row in a table,
     it must first acquire an 'IS' lock or stronger on the table.

   * Before a transaction can acquire an exclusive lock on a row in a
     table, it must first acquire an 'IX' lock on the table.

Table-level lock type compatibility is summarized in the following
matrix.

               'X'            'IX'           'S'            'IS'
                                                            
'X'            Conflict       Conflict       Conflict       Conflict
                                                            
'IX'           Conflict       Compatible     Conflict       Compatible
                                                            
'S'            Conflict       Conflict       Compatible     Compatible
                                                            
'IS'           Conflict       Compatible     Compatible     Compatible
                                             

A lock is granted to a requesting transaction if it is compatible with
existing locks, but not if it conflicts with existing locks.  A
transaction waits until the conflicting existing lock is released.  If a
lock request conflicts with an existing lock and cannot be granted
because it would cause deadlock, an error occurs.

Intention locks do not block anything except full table requests (for
example, *note 'LOCK TABLES ... WRITE': lock-tables.).  The main purpose
of intention locks is to show that someone is locking a row, or going to
lock a row in the table.

Transaction data for an intention lock appears similar to the following
in *note 'SHOW ENGINE INNODB STATUS': show-engine. and *note InnoDB
monitor: innodb-standard-monitor. output:

     TABLE LOCK table `test`.`t` trx id 10080 lock mode IX

*Record Locks*

A record lock is a lock on an index record.  For example, 'SELECT c1
FROM t WHERE c1 = 10 FOR UPDATE;' prevents any other transaction from
inserting, updating, or deleting rows where the value of 't.c1' is '10'.

Record locks always lock index records, even if a table is defined with
no indexes.  For such cases, 'InnoDB' creates a hidden clustered index
and uses this index for record locking.  See *note innodb-index-types::.

Transaction data for a record lock appears similar to the following in
*note 'SHOW ENGINE INNODB STATUS': show-engine. and *note InnoDB
monitor: innodb-standard-monitor. output:

     RECORD LOCKS space id 58 page no 3 n bits 72 index `PRIMARY` of table `test`.`t`
     trx id 10078 lock_mode X locks rec but not gap
     Record lock, heap no 2 PHYSICAL RECORD: n_fields 3; compact format; info bits 0
      0: len 4; hex 8000000a; asc     ;;
      1: len 6; hex 00000000274f; asc     'O;;
      2: len 7; hex b60000019d0110; asc        ;;

*Gap Locks*

A gap lock is a lock on a gap between index records, or a lock on the
gap before the first or after the last index record.  For example,
'SELECT c1 FROM t WHERE c1 BETWEEN 10 and 20 FOR UPDATE;' prevents other
transactions from inserting a value of '15' into column 't.c1', whether
or not there was already any such value in the column, because the gaps
between all existing values in the range are locked.

A gap might span a single index value, multiple index values, or even be
empty.

Gap locks are part of the tradeoff between performance and concurrency,
and are used in some transaction isolation levels and not others.

Gap locking is not needed for statements that lock rows using a unique
index to search for a unique row.  (This does not include the case that
the search condition includes only some columns of a multiple-column
unique index; in that case, gap locking does occur.)  For example, if
the 'id' column has a unique index, the following statement uses only an
index-record lock for the row having 'id' value 100 and it does not
matter whether other sessions insert rows in the preceding gap:

     SELECT * FROM child WHERE id = 100;

If 'id' is not indexed or has a nonunique index, the statement does lock
the preceding gap.

It is also worth noting here that conflicting locks can be held on a gap
by different transactions.  For example, transaction A can hold a shared
gap lock (gap S-lock) on a gap while transaction B holds an exclusive
gap lock (gap X-lock) on the same gap.  The reason conflicting gap locks
are allowed is that if a record is purged from an index, the gap locks
held on the record by different transactions must be merged.

Gap locks in 'InnoDB' are 'purely inhibitive', which means that their
only purpose is to prevent other transactions from inserting to the gap.
Gap locks can co-exist.  A gap lock taken by one transaction does not
prevent another transaction from taking a gap lock on the same gap.
There is no difference between shared and exclusive gap locks.  They do
not conflict with each other, and they perform the same function.

Gap locking can be disabled explicitly.  This occurs if you change the
transaction isolation level to 'READ COMMITTED' or enable the
'innodb_locks_unsafe_for_binlog' system variable.  Under these
circumstances, gap locking is disabled for searches and index scans and
is used only for foreign-key constraint checking and duplicate-key
checking.

There are also other effects of using the 'READ COMMITTED' isolation
level or enabling 'innodb_locks_unsafe_for_binlog'.  Record locks for
nonmatching rows are released after MySQL has evaluated the 'WHERE'
condition.  For 'UPDATE' statements, 'InnoDB' does a 'semi-consistent'
read, such that it returns the latest committed version to MySQL so that
MySQL can determine whether the row matches the 'WHERE' condition of the
*note 'UPDATE': update.

*Next-Key Locks*

A next-key lock is a combination of a record lock on the index record
and a gap lock on the gap before the index record.

'InnoDB' performs row-level locking in such a way that when it searches
or scans a table index, it sets shared or exclusive locks on the index
records it encounters.  Thus, the row-level locks are actually
index-record locks.  A next-key lock on an index record also affects the
'gap' before that index record.  That is, a next-key lock is an
index-record lock plus a gap lock on the gap preceding the index record.
If one session has a shared or exclusive lock on record 'R' in an index,
another session cannot insert a new index record in the gap immediately
before 'R' in the index order.

Suppose that an index contains the values 10, 11, 13, and 20.  The
possible next-key locks for this index cover the following intervals,
where a round bracket denotes exclusion of the interval endpoint and a
square bracket denotes inclusion of the endpoint:

     (negative infinity, 10]
     (10, 11]
     (11, 13]
     (13, 20]
     (20, positive infinity)

For the last interval, the next-key lock locks the gap above the largest
value in the index and the 'supremum' pseudo-record having a value
higher than any value actually in the index.  The supremum is not a real
index record, so, in effect, this next-key lock locks only the gap
following the largest index value.

By default, 'InnoDB' operates in 'REPEATABLE READ' transaction isolation
level and with the 'innodb_locks_unsafe_for_binlog' system variable
disabled.  In this case, 'InnoDB' uses next-key locks for searches and
index scans, which prevents phantom rows (see *note
innodb-next-key-locking::).

Transaction data for a next-key lock appears similar to the following in
*note 'SHOW ENGINE INNODB STATUS': show-engine. and *note InnoDB
monitor: innodb-standard-monitor. output:

     RECORD LOCKS space id 58 page no 3 n bits 72 index `PRIMARY` of table `test`.`t`
     trx id 10080 lock_mode X
     Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0
      0: len 8; hex 73757072656d756d; asc supremum;;

     Record lock, heap no 2 PHYSICAL RECORD: n_fields 3; compact format; info bits 0
      0: len 4; hex 8000000a; asc     ;;
      1: len 6; hex 00000000274f; asc     'O;;
      2: len 7; hex b60000019d0110; asc        ;;

*Insert Intention Locks*

An insert intention lock is a type of gap lock set by *note 'INSERT':
insert. operations prior to row insertion.  This lock signals the intent
to insert in such a way that multiple transactions inserting into the
same index gap need not wait for each other if they are not inserting at
the same position within the gap.  Suppose that there are index records
with values of 4 and 7.  Separate transactions that attempt to insert
values of 5 and 6, respectively, each lock the gap between 4 and 7 with
insert intention locks prior to obtaining the exclusive lock on the
inserted row, but do not block each other because the rows are
nonconflicting.

The following example demonstrates a transaction taking an insert
intention lock prior to obtaining an exclusive lock on the inserted
record.  The example involves two clients, A and B.

Client A creates a table containing two index records (90 and 102) and
then starts a transaction that places an exclusive lock on index records
with an ID greater than 100.  The exclusive lock includes a gap lock
before record 102:

     mysql> CREATE TABLE child (id int(11) NOT NULL, PRIMARY KEY(id)) ENGINE=InnoDB;
     mysql> INSERT INTO child (id) values (90),(102);

     mysql> START TRANSACTION;
     mysql> SELECT * FROM child WHERE id > 100 FOR UPDATE;
     +-----+
     | id  |
     +-----+
     | 102 |
     +-----+

Client B begins a transaction to insert a record into the gap.  The
transaction takes an insert intention lock while it waits to obtain an
exclusive lock.

     mysql> START TRANSACTION;
     mysql> INSERT INTO child (id) VALUES (101);

Transaction data for an insert intention lock appears similar to the
following in *note 'SHOW ENGINE INNODB STATUS': show-engine. and *note
InnoDB monitor: innodb-standard-monitor. output:

     RECORD LOCKS space id 31 page no 3 n bits 72 index `PRIMARY` of table `test`.`child`
     trx id 8731 lock_mode X locks gap before rec *insert intention* waiting
     Record lock, heap no 3 PHYSICAL RECORD: n_fields 3; compact format; info bits 0
      0: len 4; hex 80000066; asc    f;;
      1: len 6; hex 000000002215; asc     " ;;
      2: len 7; hex 9000000172011c; asc     r  ;;...

*AUTO-INC Locks*

An 'AUTO-INC' lock is a special table-level lock taken by transactions
inserting into tables with 'AUTO_INCREMENT' columns.  In the simplest
case, if one transaction is inserting values into the table, any other
transactions must wait to do their own inserts into that table, so that
rows inserted by the first transaction receive consecutive primary key
values.

The 'innodb_autoinc_lock_mode' configuration option controls the
algorithm used for auto-increment locking.  It allows you to choose how
to trade off between predictable sequences of auto-increment values and
maximum concurrency for insert operations.

For more information, see *note innodb-auto-increment-handling::.


File: manual.info.tmp,  Node: innodb-transaction-model,  Next: innodb-locks-set,  Prev: innodb-locking,  Up: innodb-locking-transaction-model

14.10.2 InnoDB Transaction Model
--------------------------------

* Menu:

* innodb-transaction-isolation-levels::  Transaction Isolation Levels
* innodb-autocommit-commit-rollback::  autocommit, Commit, and Rollback
* innodb-consistent-read::       Consistent Nonlocking Reads
* innodb-locking-reads::         Locking Reads

In the 'InnoDB' transaction model, the goal is to combine the best
properties of a multi-versioning database with traditional two-phase
locking.  'InnoDB' performs locking at the row level and runs queries as
nonlocking consistent reads by default, in the style of Oracle.  The
lock information in 'InnoDB' is stored space-efficiently so that lock
escalation is not needed.  Typically, several users are permitted to
lock every row in 'InnoDB' tables, or any random subset of the rows,
without causing 'InnoDB' memory exhaustion.


File: manual.info.tmp,  Node: innodb-transaction-isolation-levels,  Next: innodb-autocommit-commit-rollback,  Prev: innodb-transaction-model,  Up: innodb-transaction-model

14.10.2.1 Transaction Isolation Levels
......................................

Transaction isolation is one of the foundations of database processing.
Isolation is the I in the acronym ACID; the isolation level is the
setting that fine-tunes the balance between performance and reliability,
consistency, and reproducibility of results when multiple transactions
are making changes and performing queries at the same time.

'InnoDB' offers all four transaction isolation levels described by the
SQL:1992 standard: 'READ UNCOMMITTED', 'READ COMMITTED', 'REPEATABLE
READ', and 'SERIALIZABLE'.  The default isolation level for 'InnoDB' is
'REPEATABLE READ'.

A user can change the isolation level for a single session or for all
subsequent connections with the *note 'SET TRANSACTION':
set-transaction. statement.  To set the server's default isolation level
for all connections, use the '--transaction-isolation' option on the
command line or in an option file.  For detailed information about
isolation levels and level-setting syntax, see *note set-transaction::.

'InnoDB' supports each of the transaction isolation levels described
here using different locking strategies.  You can enforce a high degree
of consistency with the default 'REPEATABLE READ' level, for operations
on crucial data where ACID compliance is important.  Or you can relax
the consistency rules with 'READ COMMITTED' or even 'READ UNCOMMITTED',
in situations such as bulk reporting where precise consistency and
repeatable results are less important than minimizing the amount of
overhead for locking.  'SERIALIZABLE' enforces even stricter rules than
'REPEATABLE READ', and is used mainly in specialized situations, such as
with XA transactions and for troubleshooting issues with concurrency and
deadlocks.

The following list describes how MySQL supports the different
transaction levels.  The list goes from the most commonly used level to
the least used.

   * 
     'REPEATABLE READ'

     This is the default isolation level for 'InnoDB'.  Consistent reads
     within the same transaction read the snapshot established by the
     first read.  This means that if you issue several plain
     (nonlocking) *note 'SELECT': select. statements within the same
     transaction, these *note 'SELECT': select. statements are
     consistent also with respect to each other.  See *note
     innodb-consistent-read::.

     For locking reads (*note 'SELECT': select. with 'FOR UPDATE' or
     'LOCK IN SHARE MODE'), *note 'UPDATE': update, and *note 'DELETE':
     delete. statements, locking depends on whether the statement uses a
     unique index with a unique search condition, or a range-type search
     condition.

        * For a unique index with a unique search condition, 'InnoDB'
          locks only the index record found, not the gap before it.

        * For other search conditions, 'InnoDB' locks the index range
          scanned, using gap locks or next-key locks to block insertions
          by other sessions into the gaps covered by the range.  For
          information about gap locks and next-key locks, see *note
          innodb-locking::.

   * 
     'READ COMMITTED'

     Each consistent read, even within the same transaction, sets and
     reads its own fresh snapshot.  For information about consistent
     reads, see *note innodb-consistent-read::.

     For locking reads (*note 'SELECT': select. with 'FOR UPDATE' or
     'LOCK IN SHARE MODE'), *note 'UPDATE': update. statements, and
     *note 'DELETE': delete. statements, 'InnoDB' locks only index
     records, not the gaps before them, and thus permits the free
     insertion of new records next to locked records.  Gap locking is
     only used for foreign-key constraint checking and duplicate-key
     checking.

     Because gap locking is disabled, phantom problems may occur, as
     other sessions can insert new rows into the gaps.  For information
     about phantoms, see *note innodb-next-key-locking::.

     Only row-based binary logging is supported with the 'READ
     COMMITTED' isolation level.  If you use 'READ COMMITTED' with
     'binlog_format=MIXED', the server automatically uses row-based
     logging.

     Using 'READ COMMITTED' has additional effects:

        * For *note 'UPDATE': update. or *note 'DELETE': delete.
          statements, 'InnoDB' holds locks only for rows that it updates
          or deletes.  Record locks for nonmatching rows are released
          after MySQL has evaluated the 'WHERE' condition.  This greatly
          reduces the probability of deadlocks, but they can still
          happen.

        * For *note 'UPDATE': update. statements, if a row is already
          locked, 'InnoDB' performs a 'semi-consistent' read, returning
          the latest committed version to MySQL so that MySQL can
          determine whether the row matches the 'WHERE' condition of the
          *note 'UPDATE': update.  If the row matches (must be updated),
          MySQL reads the row again and this time 'InnoDB' either locks
          it or waits for a lock on it.

     Consider the following example, beginning with this table:

          CREATE TABLE t (a INT NOT NULL, b INT) ENGINE = InnoDB;
          INSERT INTO t VALUES (1,2),(2,3),(3,2),(4,3),(5,2);
          COMMIT;

     In this case, the table has no indexes, so searches and index scans
     use the hidden clustered index for record locking (see *note
     innodb-index-types::) rather than indexed columns.

     Suppose that one session performs an *note 'UPDATE': update. using
     these statements:

          # Session A
          START TRANSACTION;
          UPDATE t SET b = 5 WHERE b = 3;

     Suppose also that a second session performs an *note 'UPDATE':
     update. by executing this statement following those of the first
     session:

          # Session B
          UPDATE t SET b = 4 WHERE b = 2;

     As *note 'InnoDB': innodb-storage-engine. executes each *note
     'UPDATE': update, it first acquires an exclusive lock for each row
     that it reads, and then determines whether to modify it.  If *note
     'InnoDB': innodb-storage-engine. does not modify the row, it
     releases the lock.  Otherwise, *note 'InnoDB':
     innodb-storage-engine. retains the lock until the end of the
     transaction.  This affects transaction processing as follows.

     When using the default 'REPEATABLE READ' isolation level, the first
     *note 'UPDATE': update. acquires an x-lock on each row that it
     reads and does not release any of them:

          x-lock(1,2); retain x-lock
          x-lock(2,3); update(2,3) to (2,5); retain x-lock
          x-lock(3,2); retain x-lock
          x-lock(4,3); update(4,3) to (4,5); retain x-lock
          x-lock(5,2); retain x-lock

     The second *note 'UPDATE': update. blocks as soon as it tries to
     acquire any locks (because first update has retained locks on all
     rows), and does not proceed until the first *note 'UPDATE': update.
     commits or rolls back:

          x-lock(1,2); block and wait for first UPDATE to commit or roll back

     If 'READ COMMITTED' is used instead, the first *note 'UPDATE':
     update. acquires an x-lock on each row that it reads and releases
     those for rows that it does not modify:

          x-lock(1,2); unlock(1,2)
          x-lock(2,3); update(2,3) to (2,5); retain x-lock
          x-lock(3,2); unlock(3,2)
          x-lock(4,3); update(4,3) to (4,5); retain x-lock
          x-lock(5,2); unlock(5,2)

     For the second 'UPDATE', 'InnoDB' does a 'semi-consistent' read,
     returning the latest committed version of each row that it reads to
     MySQL so that MySQL can determine whether the row matches the
     'WHERE' condition of the *note 'UPDATE': update.:

          x-lock(1,2); update(1,2) to (1,4); retain x-lock
          x-lock(2,3); unlock(2,3)
          x-lock(3,2); update(3,2) to (3,4); retain x-lock
          x-lock(4,3); unlock(4,3)
          x-lock(5,2); update(5,2) to (5,4); retain x-lock

     However, if the 'WHERE' condition includes an indexed column, and
     'InnoDB' uses the index, only the indexed column is considered when
     taking and retaining record locks.  In the following example, the
     first *note 'UPDATE': update. takes and retains an x-lock on each
     row where b = 2.  The second *note 'UPDATE': update. blocks when it
     tries to acquire x-locks on the same records, as it also uses the
     index defined on column b.

          CREATE TABLE t (a INT NOT NULL, b INT, c INT, INDEX (b)) ENGINE = InnoDB;
          INSERT INTO t VALUES (1,2,3),(2,2,4);
          COMMIT;

          # Session A
          START TRANSACTION;
          UPDATE t SET b = 3 WHERE b = 2 AND c = 3;

          # Session B
          UPDATE t SET b = 4 WHERE b = 2 AND c = 4;

     The effects of using the 'READ COMMITTED' isolation level are the
     same as enabling the 'innodb_locks_unsafe_for_binlog' configuration
     option, with these exceptions:

        * Enabling 'innodb_locks_unsafe_for_binlog' is a global setting
          and affects all sessions, whereas the isolation level can be
          set globally for all sessions, or individually per session.

        * 'innodb_locks_unsafe_for_binlog' can be set only at server
          startup, whereas the isolation level can be set at startup or
          changed at runtime.

     'READ COMMITTED' therefore offers finer and more flexible control
     than 'innodb_locks_unsafe_for_binlog'.

   * 
     'READ UNCOMMITTED'

     *note 'SELECT': select. statements are performed in a nonlocking
     fashion, but a possible earlier version of a row might be used.
     Thus, using this isolation level, such reads are not consistent.
     This is also called a 'dirty read.' Otherwise, this isolation level
     works like 'READ COMMITTED'.

   * 
     'SERIALIZABLE'

     This level is like 'REPEATABLE READ', but 'InnoDB' implicitly
     converts all plain *note 'SELECT': select. statements to *note
     'SELECT ... LOCK IN SHARE MODE': select. if 'autocommit' is
     disabled.  If 'autocommit' is enabled, the *note 'SELECT': select.
     is its own transaction.  It therefore is known to be read only and
     can be serialized if performed as a consistent (nonlocking) read
     and need not block for other transactions.  (To force a plain *note
     'SELECT': select. to block if other transactions have modified the
     selected rows, disable 'autocommit'.)


File: manual.info.tmp,  Node: innodb-autocommit-commit-rollback,  Next: innodb-consistent-read,  Prev: innodb-transaction-isolation-levels,  Up: innodb-transaction-model

14.10.2.2 autocommit, Commit, and Rollback
..........................................

In 'InnoDB', all user activity occurs inside a transaction.  If
'autocommit' mode is enabled, each SQL statement forms a single
transaction on its own.  By default, MySQL starts the session for each
new connection with 'autocommit' enabled, so MySQL does a commit after
each SQL statement if that statement did not return an error.  If a
statement returns an error, the commit or rollback behavior depends on
the error.  See *note innodb-error-handling::.

A session that has 'autocommit' enabled can perform a multiple-statement
transaction by starting it with an explicit *note 'START TRANSACTION':
commit. or *note 'BEGIN': commit. statement and ending it with a *note
'COMMIT': commit. or *note 'ROLLBACK': commit. statement.  See *note
commit::.

If 'autocommit' mode is disabled within a session with 'SET autocommit =
0', the session always has a transaction open.  A *note 'COMMIT':
commit. or *note 'ROLLBACK': commit. statement ends the current
transaction and a new one starts.

If a session that has 'autocommit' disabled ends without explicitly
committing the final transaction, MySQL rolls back that transaction.

Some statements implicitly end a transaction, as if you had done a *note
'COMMIT': commit. before executing the statement.  For details, see
*note implicit-commit::.

A *note 'COMMIT': commit. means that the changes made in the current
transaction are made permanent and become visible to other sessions.  A
*note 'ROLLBACK': commit. statement, on the other hand, cancels all
modifications made by the current transaction.  Both *note 'COMMIT':
commit. and *note 'ROLLBACK': commit. release all 'InnoDB' locks that
were set during the current transaction.

*Grouping DML Operations with Transactions*

By default, connection to the MySQL server begins with autocommit mode
enabled, which automatically commits every SQL statement as you execute
it.  This mode of operation might be unfamiliar if you have experience
with other database systems, where it is standard practice to issue a
sequence of DML statements and commit them or roll them back all
together.

To use multiple-statement transactions, switch autocommit off with the
SQL statement 'SET autocommit = 0' and end each transaction with *note
'COMMIT': commit. or *note 'ROLLBACK': commit. as appropriate.  To leave
autocommit on, begin each transaction with *note 'START TRANSACTION':
commit. and end it with *note 'COMMIT': commit. or *note 'ROLLBACK':
commit.  The following example shows two transactions.  The first is
committed; the second is rolled back.

     shell> mysql test

     mysql> CREATE TABLE customer (a INT, b CHAR (20), INDEX (a));
     Query OK, 0 rows affected (0.00 sec)
     mysql> -- Do a transaction with autocommit turned on.
     mysql> START TRANSACTION;
     Query OK, 0 rows affected (0.00 sec)
     mysql> INSERT INTO customer VALUES (10, 'Heikki');
     Query OK, 1 row affected (0.00 sec)
     mysql> COMMIT;
     Query OK, 0 rows affected (0.00 sec)
     mysql> -- Do another transaction with autocommit turned off.
     mysql> SET autocommit=0;
     Query OK, 0 rows affected (0.00 sec)
     mysql> INSERT INTO customer VALUES (15, 'John');
     Query OK, 1 row affected (0.00 sec)
     mysql> INSERT INTO customer VALUES (20, 'Paul');
     Query OK, 1 row affected (0.00 sec)
     mysql> DELETE FROM customer WHERE b = 'Heikki';
     Query OK, 1 row affected (0.00 sec)
     mysql> -- Now we undo those last 2 inserts and the delete.
     mysql> ROLLBACK;
     Query OK, 0 rows affected (0.00 sec)
     mysql> SELECT * FROM customer;
     +------+--------+
     | a    | b      |
     +------+--------+
     |   10 | Heikki |
     +------+--------+
     1 row in set (0.00 sec)
     mysql>

*Transactions in Client-Side Languages*

In APIs such as PHP, Perl DBI, JDBC, ODBC, or the standard C call
interface of MySQL, you can send transaction control statements such as
*note 'COMMIT': commit. to the MySQL server as strings just like any
other SQL statements such as *note 'SELECT': select. or *note 'INSERT':
insert.  Some APIs also offer separate special transaction commit and
rollback functions or methods.


File: manual.info.tmp,  Node: innodb-consistent-read,  Next: innodb-locking-reads,  Prev: innodb-autocommit-commit-rollback,  Up: innodb-transaction-model

14.10.2.3 Consistent Nonlocking Reads
.....................................

A consistent read means that 'InnoDB' uses multi-versioning to present
to a query a snapshot of the database at a point in time.  The query
sees the changes made by transactions that committed before that point
of time, and no changes made by later or uncommitted transactions.  The
exception to this rule is that the query sees the changes made by
earlier statements within the same transaction.  This exception causes
the following anomaly: If you update some rows in a table, a *note
'SELECT': select. sees the latest version of the updated rows, but it
might also see older versions of any rows.  If other sessions
simultaneously update the same table, the anomaly means that you might
see the table in a state that never existed in the database.

If the transaction isolation level is 'REPEATABLE READ' (the default
level), all consistent reads within the same transaction read the
snapshot established by the first such read in that transaction.  You
can get a fresher snapshot for your queries by committing the current
transaction and after that issuing new queries.

With 'READ COMMITTED' isolation level, each consistent read within a
transaction sets and reads its own fresh snapshot.

Consistent read is the default mode in which 'InnoDB' processes *note
'SELECT': select. statements in 'READ COMMITTED' and 'REPEATABLE READ'
isolation levels.  A consistent read does not set any locks on the
tables it accesses, and therefore other sessions are free to modify
those tables at the same time a consistent read is being performed on
the table.

Suppose that you are running in the default 'REPEATABLE READ' isolation
level.  When you issue a consistent read (that is, an ordinary *note
'SELECT': select. statement), 'InnoDB' gives your transaction a
timepoint according to which your query sees the database.  If another
transaction deletes a row and commits after your timepoint was assigned,
you do not see the row as having been deleted.  Inserts and updates are
treated similarly.

*Note*:

The snapshot of the database state applies to *note 'SELECT': select.
statements within a transaction, not necessarily to DML statements.  If
you insert or modify some rows and then commit that transaction, a *note
'DELETE': delete. or *note 'UPDATE': update. statement issued from
another concurrent 'REPEATABLE READ' transaction could affect those
just-committed rows, even though the session could not query them.  If a
transaction does update or delete rows committed by a different
transaction, those changes do become visible to the current transaction.
For example, you might encounter a situation like the following:

     SELECT COUNT(c1) FROM t1 WHERE c1 = 'xyz';
     -- Returns 0: no rows match.
     DELETE FROM t1 WHERE c1 = 'xyz';
     -- Deletes several rows recently committed by other transaction.

     SELECT COUNT(c2) FROM t1 WHERE c2 = 'abc';
     -- Returns 0: no rows match.
     UPDATE t1 SET c2 = 'cba' WHERE c2 = 'abc';
     -- Affects 10 rows: another txn just committed 10 rows with 'abc' values.
     SELECT COUNT(c2) FROM t1 WHERE c2 = 'cba';
     -- Returns 10: this txn can now see the rows it just updated.

You can advance your timepoint by committing your transaction and then
doing another *note 'SELECT': select. or *note 'START TRANSACTION WITH
CONSISTENT SNAPSHOT': commit.

This is called _multi-versioned concurrency control_.

In the following example, session A sees the row inserted by B only when
B has committed the insert and A has committed as well, so that the
timepoint is advanced past the commit of B.

                  Session A              Session B

                SET autocommit=0;      SET autocommit=0;
     time
     |          SELECT * FROM t;
     |          empty set
     |                                 INSERT INTO t VALUES (1, 2);
     |
     v          SELECT * FROM t;
                empty set
                                       COMMIT;

                SELECT * FROM t;
                empty set

                COMMIT;

                SELECT * FROM t;
                ---------------------
                |    1    |    2    |
                ---------------------

If you want to see the 'freshest' state of the database, use either the
'READ COMMITTED' isolation level or a locking read:

     SELECT * FROM t LOCK IN SHARE MODE;

With 'READ COMMITTED' isolation level, each consistent read within a
transaction sets and reads its own fresh snapshot.  With 'LOCK IN SHARE
MODE', a locking read occurs instead: A 'SELECT' blocks until the
transaction containing the freshest rows ends (see *note
innodb-locking-reads::).

Consistent read does not work over certain DDL statements:

   * Consistent read does not work over *note 'DROP TABLE': drop-table,
     because MySQL cannot use a table that has been dropped and 'InnoDB'
     destroys the table.

   * Consistent read does not work over *note 'ALTER TABLE':
     alter-table, because that statement makes a temporary copy of the
     original table and deletes the original table when the temporary
     copy is built.  When you reissue a consistent read within a
     transaction, rows in the new table are not visible because those
     rows did not exist when the transaction's snapshot was taken.

The type of read varies for selects in clauses like *note 'INSERT INTO
... SELECT': insert, *note 'UPDATE ... (SELECT)': update, and *note
'CREATE TABLE ... SELECT': create-table. that do not specify 'FOR
UPDATE' or 'LOCK IN SHARE MODE':

   * By default, 'InnoDB' uses stronger locks and the *note 'SELECT':
     select. part acts like 'READ COMMITTED', where each consistent
     read, even within the same transaction, sets and reads its own
     fresh snapshot.

   * To use a consistent read in such cases, enable the
     'innodb_locks_unsafe_for_binlog' option and set the isolation level
     of the transaction to 'READ UNCOMMITTED', 'READ COMMITTED', or
     'REPEATABLE READ' (that is, anything other than 'SERIALIZABLE').
     In this case, no locks are set on rows read from the selected
     table.


File: manual.info.tmp,  Node: innodb-locking-reads,  Prev: innodb-consistent-read,  Up: innodb-transaction-model

14.10.2.4 Locking Reads
.......................

If you query data and then insert or update related data within the same
transaction, the regular 'SELECT' statement does not give enough
protection.  Other transactions can update or delete the same rows you
just queried.  'InnoDB' supports two types of locking reads that offer
extra safety:

   * *note 'SELECT ... LOCK IN SHARE MODE': select.

     Sets a shared mode lock on any rows that are read.  Other sessions
     can read the rows, but cannot modify them until your transaction
     commits.  If any of these rows were changed by another transaction
     that has not yet committed, your query waits until that transaction
     ends and then uses the latest values.

   * *note 'SELECT ... FOR UPDATE': select.

     For index records the search encounters, locks the rows and any
     associated index entries, the same as if you issued an 'UPDATE'
     statement for those rows.  Other transactions are blocked from
     updating those rows, from doing 'SELECT ... LOCK IN SHARE MODE', or
     from reading the data in certain transaction isolation levels.
     Consistent reads ignore any locks set on the records that exist in
     the read view.  (Old versions of a record cannot be locked; they
     are reconstructed by applying undo logs on an in-memory copy of the
     record.)

These clauses are primarily useful when dealing with tree-structured or
graph-structured data, either in a single table or split across multiple
tables.  You traverse edges or tree branches from one place to another,
while reserving the right to come back and change any of these 'pointer'
values.

All locks set by 'LOCK IN SHARE MODE' and 'FOR UPDATE' queries are
released when the transaction is committed or rolled back.

*Note*:

Locking reads are only possible when autocommit is disabled (either by
beginning transaction with *note 'START TRANSACTION': commit. or by
setting 'autocommit' to 0.

A locking read clause in an outer statement does not lock the rows of a
table in a nested subquery unless a locking read clause is also
specified in the subquery.  For example, the following statement does
not lock rows in table 't2'.

     SELECT * FROM t1 WHERE c1 = (SELECT c1 FROM t2) FOR UPDATE;

To lock rows in table 't2', add a locking read clause to the subquery:

     SELECT * FROM t1 WHERE c1 = (SELECT c1 FROM t2 FOR UPDATE) FOR UPDATE;

*Locking Read Examples*

Suppose that you want to insert a new row into a table 'child', and make
sure that the child row has a parent row in table 'parent'.  Your
application code can ensure referential integrity throughout this
sequence of operations.

First, use a consistent read to query the table 'PARENT' and verify that
the parent row exists.  Can you safely insert the child row to table
'CHILD'?  No, because some other session could delete the parent row in
the moment between your 'SELECT' and your 'INSERT', without you being
aware of it.

To avoid this potential issue, perform the *note 'SELECT': select. using
'LOCK IN SHARE MODE':

     SELECT * FROM parent WHERE NAME = 'Jones' LOCK IN SHARE MODE;

After the 'LOCK IN SHARE MODE' query returns the parent ''Jones'', you
can safely add the child record to the 'CHILD' table and commit the
transaction.  Any transaction that tries to acquire an exclusive lock in
the applicable row in the 'PARENT' table waits until you are finished,
that is, until the data in all tables is in a consistent state.

For another example, consider an integer counter field in a table
'CHILD_CODES', used to assign a unique identifier to each child added to
table 'CHILD'.  Do not use either consistent read or a shared mode read
to read the present value of the counter, because two users of the
database could see the same value for the counter, and a duplicate-key
error occurs if two transactions attempt to add rows with the same
identifier to the 'CHILD' table.

Here, 'LOCK IN SHARE MODE' is not a good solution because if two users
read the counter at the same time, at least one of them ends up in
deadlock when it attempts to update the counter.

To implement reading and incrementing the counter, first perform a
locking read of the counter using 'FOR UPDATE', and then increment the
counter.  For example:

     SELECT counter_field FROM child_codes FOR UPDATE;
     UPDATE child_codes SET counter_field = counter_field + 1;

A *note 'SELECT ... FOR UPDATE': select. reads the latest available
data, setting exclusive locks on each row it reads.  Thus, it sets the
same locks a searched SQL *note 'UPDATE': update. would set on the rows.

The preceding description is merely an example of how *note 'SELECT ...
FOR UPDATE': select. works.  In MySQL, the specific task of generating a
unique identifier actually can be accomplished using only a single
access to the table:

     UPDATE child_codes SET counter_field = LAST_INSERT_ID(counter_field + 1);
     SELECT LAST_INSERT_ID();

The *note 'SELECT': select. statement merely retrieves the identifier
information (specific to the current connection).  It does not access
any table.


File: manual.info.tmp,  Node: innodb-locks-set,  Next: innodb-next-key-locking,  Prev: innodb-transaction-model,  Up: innodb-locking-transaction-model

14.10.3 Locks Set by Different SQL Statements in InnoDB
-------------------------------------------------------

A locking read, an *note 'UPDATE': update, or a *note 'DELETE': delete.
generally set record locks on every index record that is scanned in the
processing of the SQL statement.  It does not matter whether there are
'WHERE' conditions in the statement that would exclude the row.
'InnoDB' does not remember the exact 'WHERE' condition, but only knows
which index ranges were scanned.  The locks are normally next-key locks
that also block inserts into the 'gap' immediately before the record.
However, gap locking can be disabled explicitly, which causes next-key
locking not to be used.  For more information, see *note
innodb-locking::.  The transaction isolation level also can affect which
locks are set; see *note innodb-transaction-isolation-levels::.

If a secondary index is used in a search and index record locks to be
set are exclusive, 'InnoDB' also retrieves the corresponding clustered
index records and sets locks on them.

If you have no indexes suitable for your statement and MySQL must scan
the entire table to process the statement, every row of the table
becomes locked, which in turn blocks all inserts by other users to the
table.  It is important to create good indexes so that your queries do
not unnecessarily scan many rows.

'InnoDB' sets specific types of locks as follows.

   * *note 'SELECT ... FROM': select. is a consistent read, reading a
     snapshot of the database and setting no locks unless the
     transaction isolation level is set to 'SERIALIZABLE'.  For
     'SERIALIZABLE' level, the search sets shared next-key locks on the
     index records it encounters.  However, only an index record lock is
     required for statements that lock rows using a unique index to
     search for a unique row.

   * For locking reads (*note 'SELECT': select. with 'FOR UPDATE' or
     'LOCK IN SHARE MODE'), *note 'UPDATE': update, and *note 'DELETE':
     delete. statements, the locks that are taken depend on whether the
     statement uses a unique index with a unique search condition, or a
     range-type search condition.

        * For a unique index with a unique search condition, 'InnoDB'
          locks only the index record found, not the gap before it.

        * For other search conditions, and for non-unique indexes,
          'InnoDB' locks the index range scanned, using gap locks or
          next-key locks to block insertions by other sessions into the
          gaps covered by the range.  For information about gap locks
          and next-key locks, see *note innodb-locking::.

   * For index records the search encounters, *note 'SELECT ... FOR
     UPDATE': select. blocks other sessions from doing *note 'SELECT ...
     LOCK IN SHARE MODE': select. or from reading in certain transaction
     isolation levels.  Consistent reads ignore any locks set on the
     records that exist in the read view.

   * *note 'UPDATE ... WHERE ...': update. sets an exclusive next-key
     lock on every record the search encounters.  However, only an index
     record lock is required for statements that lock rows using a
     unique index to search for a unique row.

   * When *note 'UPDATE': update. modifies a clustered index record,
     implicit locks are taken on affected secondary index records.  The
     *note 'UPDATE': update. operation also takes shared locks on
     affected secondary index records when performing duplicate check
     scans prior to inserting new secondary index records, and when
     inserting new secondary index records.

   * *note 'DELETE FROM ... WHERE ...': delete. sets an exclusive
     next-key lock on every record the search encounters.  However, only
     an index record lock is required for statements that lock rows
     using a unique index to search for a unique row.

   * *note 'INSERT': insert. sets an exclusive lock on the inserted row.
     This lock is an index-record lock, not a next-key lock (that is,
     there is no gap lock) and does not prevent other sessions from
     inserting into the gap before the inserted row.

     Prior to inserting the row, a type of gap lock called an insert
     intention gap lock is set.  This lock signals the intent to insert
     in such a way that multiple transactions inserting into the same
     index gap need not wait for each other if they are not inserting at
     the same position within the gap.  Suppose that there are index
     records with values of 4 and 7.  Separate transactions that attempt
     to insert values of 5 and 6 each lock the gap between 4 and 7 with
     insert intention locks prior to obtaining the exclusive lock on the
     inserted row, but do not block each other because the rows are
     nonconflicting.

     If a duplicate-key error occurs, a shared lock on the duplicate
     index record is set.  This use of a shared lock can result in
     deadlock should there be multiple sessions trying to insert the
     same row if another session already has an exclusive lock.  This
     can occur if another session deletes the row.  Suppose that an
     'InnoDB' table 't1' has the following structure:

          CREATE TABLE t1 (i INT, PRIMARY KEY (i)) ENGINE = InnoDB;

     Now suppose that three sessions perform the following operations in
     order:

     Session 1:

          START TRANSACTION;
          INSERT INTO t1 VALUES(1);

     Session 2:

          START TRANSACTION;
          INSERT INTO t1 VALUES(1);

     Session 3:

          START TRANSACTION;
          INSERT INTO t1 VALUES(1);

     Session 1:

          ROLLBACK;

     The first operation by session 1 acquires an exclusive lock for the
     row.  The operations by sessions 2 and 3 both result in a
     duplicate-key error and they both request a shared lock for the
     row.  When session 1 rolls back, it releases its exclusive lock on
     the row and the queued shared lock requests for sessions 2 and 3
     are granted.  At this point, sessions 2 and 3 deadlock: Neither can
     acquire an exclusive lock for the row because of the shared lock
     held by the other.

     A similar situation occurs if the table already contains a row with
     key value 1 and three sessions perform the following operations in
     order:

     Session 1:

          START TRANSACTION;
          DELETE FROM t1 WHERE i = 1;

     Session 2:

          START TRANSACTION;
          INSERT INTO t1 VALUES(1);

     Session 3:

          START TRANSACTION;
          INSERT INTO t1 VALUES(1);

     Session 1:

          COMMIT;

     The first operation by session 1 acquires an exclusive lock for the
     row.  The operations by sessions 2 and 3 both result in a
     duplicate-key error and they both request a shared lock for the
     row.  When session 1 commits, it releases its exclusive lock on the
     row and the queued shared lock requests for sessions 2 and 3 are
     granted.  At this point, sessions 2 and 3 deadlock: Neither can
     acquire an exclusive lock for the row because of the shared lock
     held by the other.

   * *note 'INSERT ... ON DUPLICATE KEY UPDATE': insert-on-duplicate.
     differs from a simple *note 'INSERT': insert. in that an exclusive
     lock rather than a shared lock is placed on the row to be updated
     when a duplicate-key error occurs.  An exclusive index-record lock
     is taken for a duplicate primary key value.  An exclusive next-key
     lock is taken for a duplicate unique key value.

   * *note 'REPLACE': replace. is done like an *note 'INSERT': insert.
     if there is no collision on a unique key.  Otherwise, an exclusive
     next-key lock is placed on the row to be replaced.

   * 'INSERT INTO T SELECT ... FROM S WHERE ...' sets an exclusive index
     record lock (without a gap lock) on each row inserted into 'T'.  If
     the transaction isolation level is 'READ COMMITTED', or
     'innodb_locks_unsafe_for_binlog' is enabled and the transaction
     isolation level is not 'SERIALIZABLE', 'InnoDB' does the search on
     'S' as a consistent read (no locks).  Otherwise, 'InnoDB' sets
     shared next-key locks on rows from 'S'.  'InnoDB' has to set locks
     in the latter case: During roll-forward recovery using a
     statement-based binary log, every SQL statement must be executed in
     exactly the same way it was done originally.

     *note 'CREATE TABLE ... SELECT ...': create-table. performs the
     *note 'SELECT': select. with shared next-key locks or as a
     consistent read, as for *note 'INSERT ... SELECT': insert-select.

     When a 'SELECT' is used in the constructs 'REPLACE INTO t SELECT
     ... FROM s WHERE ...' or 'UPDATE t ... WHERE col IN (SELECT ...
     FROM s ...)', 'InnoDB' sets shared next-key locks on rows from
     table 's'.

   * 'InnoDB' sets an exclusive lock on the end of the index associated
     with the 'AUTO_INCREMENT' column while initializing a previously
     specified 'AUTO_INCREMENT' column on a table.

     With 'innodb_autoinc_lock_mode=0', 'InnoDB' uses a special
     'AUTO-INC' table lock mode where the lock is obtained and held to
     the end of the current SQL statement (not to the end of the entire
     transaction) while accessing the auto-increment counter.  Other
     clients cannot insert into the table while the 'AUTO-INC' table
     lock is held.  The same behavior occurs for 'bulk inserts' with
     'innodb_autoinc_lock_mode=1'.  Table-level 'AUTO-INC' locks are not
     used with 'innodb_autoinc_lock_mode=2'.  For more information, See
     *note innodb-auto-increment-handling::.

     'InnoDB' fetches the value of a previously initialized
     'AUTO_INCREMENT' column without setting any locks.

   * If a 'FOREIGN KEY' constraint is defined on a table, any insert,
     update, or delete that requires the constraint condition to be
     checked sets shared record-level locks on the records that it looks
     at to check the constraint.  'InnoDB' also sets these locks in the
     case where the constraint fails.

   * *note 'LOCK TABLES': lock-tables. sets table locks, but it is the
     higher MySQL layer above the 'InnoDB' layer that sets these locks.
     'InnoDB' is aware of table locks if 'innodb_table_locks = 1' (the
     default) and 'autocommit = 0', and the MySQL layer above 'InnoDB'
     knows about row-level locks.

     Otherwise, 'InnoDB''s automatic deadlock detection cannot detect
     deadlocks where such table locks are involved.  Also, because in
     this case the higher MySQL layer does not know about row-level
     locks, it is possible to get a table lock on a table where another
     session currently has row-level locks.  However, this does not
     endanger transaction integrity, as discussed in *note
     innodb-deadlock-detection::.

   * *note 'LOCK TABLES': lock-tables. acquires two locks on each table
     if 'innodb_table_locks=1' (the default).  In addition to a table
     lock on the MySQL layer, it also acquires an 'InnoDB' table lock.
     Versions of MySQL before 4.1.2 did not acquire 'InnoDB' table
     locks; the old behavior can be selected by setting
     'innodb_table_locks=0'.  If no 'InnoDB' table lock is acquired,
     *note 'LOCK TABLES': lock-tables. completes even if some records of
     the tables are being locked by other transactions.

     As of MySQL 5.5.3, 'innodb_table_locks=0' has no effect for tables
     locked explicitly with *note 'LOCK TABLES ... WRITE': lock-tables.
     It still has an effect for tables locked for read or write by *note
     'LOCK TABLES ... WRITE': lock-tables. implicitly (for example,
     through triggers) or by *note 'LOCK TABLES ... READ': lock-tables.

   * All 'InnoDB' locks held by a transaction are released when the
     transaction is committed or aborted.  Thus, it does not make much
     sense to invoke *note 'LOCK TABLES': lock-tables. on 'InnoDB'
     tables in 'autocommit=1' mode because the acquired 'InnoDB' table
     locks would be released immediately.

   * You cannot lock additional tables in the middle of a transaction
     because *note 'LOCK TABLES': lock-tables. performs an implicit
     *note 'COMMIT': commit. and *note 'UNLOCK TABLES': lock-tables.


File: manual.info.tmp,  Node: innodb-next-key-locking,  Next: innodb-deadlocks,  Prev: innodb-locks-set,  Up: innodb-locking-transaction-model

14.10.4 Phantom Rows
--------------------

The so-called _phantom_ problem occurs within a transaction when the
same query produces different sets of rows at different times.  For
example, if a *note 'SELECT': select. is executed twice, but returns a
row the second time that was not returned the first time, the row is a
'phantom' row.

Suppose that there is an index on the 'id' column of the 'child' table
and that you want to read and lock all rows from the table having an
identifier value larger than 100, with the intention of updating some
column in the selected rows later:

     SELECT * FROM child WHERE id > 100 FOR UPDATE;

The query scans the index starting from the first record where 'id' is
bigger than 100.  Let the table contain rows having 'id' values of 90
and 102.  If the locks set on the index records in the scanned range do
not lock out inserts made in the gaps (in this case, the gap between 90
and 102), another session can insert a new row into the table with an
'id' of 101.  If you were to execute the same *note 'SELECT': select.
within the same transaction, you would see a new row with an 'id' of 101
(a 'phantom') in the result set returned by the query.  If we regard a
set of rows as a data item, the new phantom child would violate the
isolation principle of transactions that a transaction should be able to
run so that the data it has read does not change during the transaction.

To prevent phantoms, 'InnoDB' uses an algorithm called _next-key
locking_ that combines index-row locking with gap locking.  'InnoDB'
performs row-level locking in such a way that when it searches or scans
a table index, it sets shared or exclusive locks on the index records it
encounters.  Thus, the row-level locks are actually index-record locks.
In addition, a next-key lock on an index record also affects the 'gap'
before that index record.  That is, a next-key lock is an index-record
lock plus a gap lock on the gap preceding the index record.  If one
session has a shared or exclusive lock on record 'R' in an index,
another session cannot insert a new index record in the gap immediately
before 'R' in the index order.

When 'InnoDB' scans an index, it can also lock the gap after the last
record in the index.  Just that happens in the preceding example: To
prevent any insert into the table where 'id' would be bigger than 100,
the locks set by 'InnoDB' include a lock on the gap following 'id' value
102.

You can use next-key locking to implement a uniqueness check in your
application: If you read your data in share mode and do not see a
duplicate for a row you are going to insert, then you can safely insert
your row and know that the next-key lock set on the successor of your
row during the read prevents anyone meanwhile inserting a duplicate for
your row.  Thus, the next-key locking enables you to 'lock' the
nonexistence of something in your table.

Gap locking can be disabled as discussed in *note innodb-locking::.
This may cause phantom problems because other sessions can insert new
rows into the gaps when gap locking is disabled.


File: manual.info.tmp,  Node: innodb-deadlocks,  Prev: innodb-next-key-locking,  Up: innodb-locking-transaction-model

14.10.5 Deadlocks in InnoDB
---------------------------

* Menu:

* innodb-deadlock-example::      An InnoDB Deadlock Example
* innodb-deadlock-detection::    Deadlock Detection and Rollback
* innodb-deadlocks-handling::    How to Minimize and Handle Deadlocks

A deadlock is a situation where different transactions are unable to
proceed because each holds a lock that the other needs.  Because both
transactions are waiting for a resource to become available, neither
ever release the locks it holds.

A deadlock can occur when transactions lock rows in multiple tables
(through statements such as *note 'UPDATE': update. or *note 'SELECT ...
FOR UPDATE': select.), but in the opposite order.  A deadlock can also
occur when such statements lock ranges of index records and gaps, with
each transaction acquiring some locks but not others due to a timing
issue.  For a deadlock example, see *note innodb-deadlock-example::.

To reduce the possibility of deadlocks, use transactions rather than
*note 'LOCK TABLES': lock-tables. statements; keep transactions that
insert or update data small enough that they do not stay open for long
periods of time; when different transactions update multiple tables or
large ranges of rows, use the same order of operations (such as *note
'SELECT ... FOR UPDATE': select.) in each transaction; create indexes on
the columns used in *note 'SELECT ... FOR UPDATE': select. and *note
'UPDATE ... WHERE': update. statements.  The possibility of deadlocks is
not affected by the isolation level, because the isolation level changes
the behavior of read operations, while deadlocks occur because of write
operations.  For more information about avoiding and recovering from
deadlock conditions, see *note innodb-deadlocks-handling::.

If a deadlock does occur, 'InnoDB' detects the condition and rolls back
one of the transactions (the victim).  Thus, even if your application
logic is correct, you must still handle the case where a transaction
must be retried.  To see the last deadlock in an 'InnoDB' user
transaction, use the *note 'SHOW ENGINE INNODB STATUS': show-engine.
command.  If frequent deadlocks highlight a problem with transaction
structure or application error handling, run with the
'innodb_print_all_deadlocks' setting enabled to print information about
all deadlocks to the *note 'mysqld': mysqld. error log.  For more
information about how deadlocks are automatically detected and handled,
see *note innodb-deadlock-detection::.


File: manual.info.tmp,  Node: innodb-deadlock-example,  Next: innodb-deadlock-detection,  Prev: innodb-deadlocks,  Up: innodb-deadlocks

14.10.5.1 An InnoDB Deadlock Example
....................................

The following example illustrates how an error can occur when a lock
request would cause a deadlock.  The example involves two clients, A and
B.

First, client A creates a table containing one row, and then begins a
transaction.  Within the transaction, A obtains an 'S' lock on the row
by selecting it in share mode:

     mysql> CREATE TABLE t (i INT) ENGINE = InnoDB;
     Query OK, 0 rows affected (1.07 sec)

     mysql> INSERT INTO t (i) VALUES(1);
     Query OK, 1 row affected (0.09 sec)

     mysql> START TRANSACTION;
     Query OK, 0 rows affected (0.00 sec)

     mysql> SELECT * FROM t WHERE i = 1 LOCK IN SHARE MODE;
     +------+
     | i    |
     +------+
     |    1 |
     +------+

Next, client B begins a transaction and attempts to delete the row from
the table:

     mysql> START TRANSACTION;
     Query OK, 0 rows affected (0.00 sec)

     mysql> DELETE FROM t WHERE i = 1;

The delete operation requires an 'X' lock.  The lock cannot be granted
because it is incompatible with the 'S' lock that client A holds, so the
request goes on the queue of lock requests for the row and client B
blocks.

Finally, client A also attempts to delete the row from the table:

     mysql> DELETE FROM t WHERE i = 1;
     ERROR 1213 (40001): Deadlock found when trying to get lock;
     try restarting transaction

Deadlock occurs here because client A needs an 'X' lock to delete the
row.  However, that lock request cannot be granted because client B
already has a request for an 'X' lock and is waiting for client A to
release its 'S' lock.  Nor can the 'S' lock held by A be upgraded to an
'X' lock because of the prior request by B for an 'X' lock.  As a
result, 'InnoDB' generates an error for one of the clients and releases
its locks.  The client returns this error:

     ERROR 1213 (40001): Deadlock found when trying to get lock;
     try restarting transaction

At that point, the lock request for the other client can be granted and
it deletes the row from the table.


File: manual.info.tmp,  Node: innodb-deadlock-detection,  Next: innodb-deadlocks-handling,  Prev: innodb-deadlock-example,  Up: innodb-deadlocks

14.10.5.2 Deadlock Detection and Rollback
.........................................

'InnoDB' automatically detects transaction deadlocks and rolls back a
transaction or transactions to break the deadlock.  'InnoDB' tries to
pick small transactions to roll back, where the size of a transaction is
determined by the number of rows inserted, updated, or deleted.

'InnoDB' is aware of table locks if 'innodb_table_locks = 1' (the
default) and 'autocommit = 0', and the MySQL layer above it knows about
row-level locks.  Otherwise, 'InnoDB' cannot detect deadlocks where a
table lock set by a MySQL *note 'LOCK TABLES': lock-tables. statement or
a lock set by a storage engine other than 'InnoDB' is involved.  Resolve
these situations by setting the value of the 'innodb_lock_wait_timeout'
system variable.

When 'InnoDB' performs a complete rollback of a transaction, all locks
set by the transaction are released.  However, if just a single SQL
statement is rolled back as a result of an error, some of the locks set
by the statement may be preserved.  This happens because 'InnoDB' stores
row locks in a format such that it cannot know afterward which lock was
set by which statement.

If a *note 'SELECT': select. calls a stored function in a transaction,
and a statement within the function fails, that statement rolls back.
Furthermore, if *note 'ROLLBACK': commit. is executed after that, the
entire transaction rolls back.

If the 'LATEST DETECTED DEADLOCK' section of 'InnoDB' Monitor output
includes a message stating, ''TOO DEEP OR LONG SEARCH IN THE LOCK TABLE
WAITS-FOR GRAPH, WE WILL ROLL BACK FOLLOWING TRANSACTION',' this
indicates that the number of transactions on the wait-for list has
reached a limit of 200.  A wait-for list that exceeds 200 transactions
is treated as a deadlock and the transaction attempting to check the
wait-for list is rolled back.  The same error may also occur if the
locking thread must look at more than 1,000,000 locks owned by
transactions on the wait-for list.

For techniques to organize database operations to avoid deadlocks, see
*note innodb-deadlocks::.


File: manual.info.tmp,  Node: innodb-deadlocks-handling,  Prev: innodb-deadlock-detection,  Up: innodb-deadlocks

14.10.5.3 How to Minimize and Handle Deadlocks
..............................................

This section builds on the conceptual information about deadlocks in
*note innodb-deadlock-detection::.  It explains how to organize database
operations to minimize deadlocks and the subsequent error handling
required in applications.

Deadlocks are a classic problem in transactional databases, but they are
not dangerous unless they are so frequent that you cannot run certain
transactions at all.  Normally, you must write your applications so that
they are always prepared to re-issue a transaction if it gets rolled
back because of a deadlock.

'InnoDB' uses automatic row-level locking.  You can get deadlocks even
in the case of transactions that just insert or delete a single row.
That is because these operations are not really 'atomic'; they
automatically set locks on the (possibly several) index records of the
row inserted or deleted.

You can cope with deadlocks and reduce the likelihood of their
occurrence with the following techniques:

   * At any time, issue the *note 'SHOW ENGINE INNODB STATUS':
     show-engine. command to determine the cause of the most recent
     deadlock.  That can help you to tune your application to avoid
     deadlocks.

   * If frequent deadlock warnings cause concern, collect more extensive
     debugging information by enabling the 'innodb_print_all_deadlocks'
     configuration option.  Information about each deadlock, not just
     the latest one, is recorded in the MySQL error log.  Disable this
     option when you are finished debugging.

   * Always be prepared to re-issue a transaction if it fails due to
     deadlock.  Deadlocks are not dangerous.  Just try again.

   * Keep transactions small and short in duration to make them less
     prone to collision.

   * Commit transactions immediately after making a set of related
     changes to make them less prone to collision.  In particular, do
     not leave an interactive *note 'mysql': mysql. session open for a
     long time with an uncommitted transaction.

   * If you use locking reads (*note 'SELECT ... FOR UPDATE': select. or
     'SELECT ... LOCK IN SHARE MODE'), try using a lower isolation level
     such as 'READ COMMITTED'.

   * When modifying multiple tables within a transaction, or different
     sets of rows in the same table, do those operations in a consistent
     order each time.  Then transactions form well-defined queues and do
     not deadlock.  For example, organize database operations into
     functions within your application, or call stored routines, rather
     than coding multiple similar sequences of 'INSERT', 'UPDATE', and
     'DELETE' statements in different places.

   * Add well-chosen indexes to your tables.  Then your queries need to
     scan fewer index records and consequently set fewer locks.  Use
     *note 'EXPLAIN SELECT': explain. to determine which indexes the
     MySQL server regards as the most appropriate for your queries.

   * Use less locking.  If you can afford to permit a *note 'SELECT':
     select. to return data from an old snapshot, do not add the clause
     'FOR UPDATE' or 'LOCK IN SHARE MODE' to it.  Using the 'READ
     COMMITTED' isolation level is good here, because each consistent
     read within the same transaction reads from its own fresh snapshot.

   * If nothing else helps, serialize your transactions with table-level
     locks.  The correct way to use *note 'LOCK TABLES': lock-tables.
     with transactional tables, such as 'InnoDB' tables, is to begin a
     transaction with 'SET autocommit = 0' (not *note 'START
     TRANSACTION': commit.) followed by *note 'LOCK TABLES':
     lock-tables, and to not call *note 'UNLOCK TABLES': lock-tables.
     until you commit the transaction explicitly.  For example, if you
     need to write to table 't1' and read from table 't2', you can do
     this:

          SET autocommit=0;
          LOCK TABLES t1 WRITE, t2 READ, ...;... DO SOMETHING WITH TABLES T1 AND T2 HERE ...
          COMMIT;
          UNLOCK TABLES;

     Table-level locks prevent concurrent updates to the table, avoiding
     deadlocks at the expense of less responsiveness for a busy system.

   * Another way to serialize transactions is to create an auxiliary
     'semaphore' table that contains just a single row.  Have each
     transaction update that row before accessing other tables.  In that
     way, all transactions happen in a serial fashion.  Note that the
     'InnoDB' instant deadlock detection algorithm also works in this
     case, because the serializing lock is a row-level lock.  With MySQL
     table-level locks, the timeout method must be used to resolve
     deadlocks.


File: manual.info.tmp,  Node: innodb-configuration,  Next: innodb-compression,  Prev: innodb-locking-transaction-model,  Up: innodb-storage-engine

14.11 InnoDB Configuration
==========================

* Menu:

* innodb-init-startup-configuration::  InnoDB Startup Configuration
* innodb-performance-buffer-pool::  InnoDB Buffer Pool Configuration
* innodb-performance-use_sys_malloc::  Configuring the Memory Allocator for InnoDB
* innodb-performance-thread_concurrency::  Configuring Thread Concurrency for InnoDB
* innodb-performance-multiple_io_threads::  Configuring the Number of Background InnoDB I/O Threads
* innodb-linux-native-aio::      Using Asynchronous I/O on Linux
* innodb-configuring-io-capacity::  Configuring InnoDB I/O Capacity
* innodb-performance-spin_lock_polling::  Configuring Spin Lock Polling
* innodb-purge-configuration::   Purge Configuration
* innodb-statistics-estimation::  Configuring Optimizer Statistics for InnoDB

This section provides configuration information and procedures for
'InnoDB' initialization, startup, and various components and features of
the 'InnoDB' storage engine.  For information about optimizing database
operations for 'InnoDB' tables, see *note optimizing-innodb::.


File: manual.info.tmp,  Node: innodb-init-startup-configuration,  Next: innodb-performance-buffer-pool,  Prev: innodb-configuration,  Up: innodb-configuration

14.11.1 InnoDB Startup Configuration
------------------------------------

The first decisions to make about 'InnoDB' configuration involve the
configuration of data files, log files, and memory buffers.  It is
recommended that you define data file, log file, and page size
configuration before creating the 'InnoDB' instance.  Modifying data
file or log file configuration after the 'InnoDB' instance is created
may involve a non-trivial procedure.

In addition to these topics, this section provides information about
specifying 'InnoDB' options in a configuration file, viewing 'InnoDB'
initialization information, and important storage considerations.

   * *note innodb-startup-mysql-configuration-file::

   * *note innodb-startup-initialization-information::

   * *note innodb-startup-storage-considerations::

   * *note innodb-startup-data-file-configuration::

   * *note innodb-startup-log-file-configuration::

   * *note innodb-startup-memory-configuration::

*Specifying Options in a MySQL Configuration File*

Because MySQL uses data file and log file configuration settings to
initialize the 'InnoDB' instance, it is recommended that you define
these settings in a configuration file that MySQL reads at startup,
prior to initializing 'InnoDB' for the first time.  'InnoDB' is
initialized when the MySQL server is started, and the first
initialization of 'InnoDB' normally occurs the first time you start the
MySQL server.

You can place 'InnoDB' options in the '[mysqld]' group of any option
file that your server reads when it starts.  The locations of MySQL
option files are described in *note option-files::.

To make sure that *note 'mysqld': mysqld. reads options only from a
specific file, use the '--defaults-file' option as the first option on
the command line when starting the server:

     mysqld --defaults-file=PATH_TO_CONFIGURATION_FILE

*Viewing InnoDB Initialization Information*

To view 'InnoDB' initialization information during startup, start *note
'mysqld': mysqld. from a command prompt.  When *note 'mysqld': mysqld.
is started from a command prompt, initialization information is printed
to the console.

For example, on Windows, if *note 'mysqld': mysqld. is located in
'C:\Program Files\MySQL\MySQL Server 5.5\bin', start the MySQL server
like this:

     C:\> "C:\Program Files\MySQL\MySQL Server 5.5\bin\mysqld" --console

On Unix-like systems, *note 'mysqld': mysqld. is located in the 'bin'
directory of your MySQL installation:

     shell> bin/mysqld --user=mysql &

If you do not send server output to the console, check the error log
after startup to see the initialization information 'InnoDB' printed
during the startup process.

For information about starting MySQL using other methods, see *note
automatic-start::.

*Note*:

'InnoDB' does not open all user tables and associated data files at
startup.  However, 'InnoDB' does check for the existence of tablespace
files ('*.ibd' files) that are referenced in the data dictionary.  If a
tablespace file is not found, 'InnoDB' logs an error and continues the
startup sequence.  Tablespace files that are referenced in the redo log
may be opened during crash recovery for redo application.

*Important Storage Considerations*

Review the following storage-related considerations before proceeding
with your startup configuration.

   * In some cases, database performance improves if the data is not all
     placed on the same physical disk.  Putting log files on a different
     disk from data is very often beneficial for performance.  For
     example, you can place system tablespace data files and log files
     on different disks.  You can also use raw disk partitions (raw
     devices) for 'InnoDB' data files, which may speed up I/O. See *note
     innodb-raw-devices::.

   * 'InnoDB' is a transaction-safe (ACID compliant) storage engine for
     MySQL that has commit, rollback, and crash-recovery capabilities to
     protect user data.  *However, it cannot do so* if the underlying
     operating system or hardware does not work as advertised.  Many
     operating systems or disk subsystems may delay or reorder write
     operations to improve performance.  On some operating systems, the
     very 'fsync()' system call that should wait until all unwritten
     data for a file has been flushed might actually return before the
     data has been flushed to stable storage.  Because of this, an
     operating system crash or a power outage may destroy recently
     committed data, or in the worst case, even corrupt the database
     because of write operations having been reordered.  If data
     integrity is important to you, perform some 'pull-the-plug' tests
     before using anything in production.  On macOS, 'InnoDB' uses a
     special 'fcntl()' file flush method.  Under Linux, it is advisable
     to *disable the write-back cache*.

     On ATA/SATA disk drives, a command such 'hdparm -W0 /dev/hda' may
     work to disable the write-back cache.  *Beware that some drives or
     disk controllers may be unable to disable the write-back cache.*

   * With regard to 'InnoDB' recovery capabilities that protect user
     data, 'InnoDB' uses a file flush technique involving a structure
     called the doublewrite buffer, which is enabled by default
     ('innodb_doublewrite=ON').  The doublewrite buffer adds safety to
     recovery following a crash or power outage, and improves
     performance on most varieties of Unix by reducing the need for
     'fsync()' operations.  It is recommended that the
     'innodb_doublewrite' option remains enabled if you are concerned
     with data integrity or possible failures.  For additional
     information about the doublewrite buffer, see *note
     innodb-disk-io::.

   * Before using NFS with 'InnoDB', review potential issues outlined in
     *note disk-issues-nfs::.

   * Running MySQL server on a 4K sector hard drive on Windows is not
     supported with 'innodb_flush_method=async_unbuffered', which is the
     default setting.  The workaround is to use
     'innodb_flush_method=normal'.

*System Tablespace Data File Configuration*

The 'innodb_data_file_path' startup option defines the name, size, and
attributes of 'InnoDB' system tablespace data files.  If you do not
configure this option prior to initializing the MySQL server, the
default behavior is to create a single auto-extending data file,
slightly larger than 10MB, named 'ibdata1':

     mysql> SHOW VARIABLES LIKE 'innodb_data_file_path';
     +-----------------------+------------------------+
     | Variable_name         | Value                  |
     +-----------------------+------------------------+
     | innodb_data_file_path | ibdata1:10M:autoextend |
     +-----------------------+------------------------+

The full data file specification syntax includes the file name, file
size, 'autoextend' attribute, and 'max' attribute:

     FILE_NAME:FILE_SIZE[:autoextend[:max:MAX_FILE_SIZE]]

File sizes are specified in kilobytes, megabytes, or gigabytes by
appending 'K', 'M' or 'G' to the size value.  If specifying the data
file size in kilobytes, do so in multiples of 1024.  Otherwise, kilobyte
values are rounded to nearest megabyte (MB) boundary.  The sum of file
sizes must be, at a minimum, slightly larger than 10MB.

You can specify more than one data file using a semicolon-separated
list.  For example:

     [mysqld]
     innodb_data_file_path=ibdata1:50M;ibdata2:50M:autoextend

The 'autoextend' and 'max' attributes can be used only for the data file
that is specified last.

When the 'autoextend' attribute is specified, the data file
automatically increases in size by 8MB increments as space is required.
The 'innodb_autoextend_increment' variable controls the increment size.

To specify a maximum size for an auto-extending data file, use the 'max'
attribute following the 'autoextend' attribute.  Use the 'max' attribute
only in cases where constraining disk usage is of critical importance.
The following configuration permits 'ibdata1' to grow to a limit of
500MB:

     [mysqld]
     innodb_data_file_path=ibdata1:10M:autoextend:max:500M

If your disk becomes full, you can add a data file on another disk.  For
instructions, see *note innodb-resize-system-tablespace::.

The size limit for individual files is determined by your operating
system.  You can set the file size to more than 4GB on operating systems
that support large files.  You can also use raw disk partitions as data
files.  See *note innodb-raw-devices::.

'InnoDB' is not aware of the file system maximum file size, so be
cautious on file systems where the maximum file size is a small value
such as 2GB.

System tablespace files are created in the data directory by default
('datadir').  To specify an alternate location, use the
'innodb_data_home_dir' option.  For example, to create a system
tablespace data file in a directory named 'myibdata', use this
configuration:

     [mysqld]
     innodb_data_home_dir = /myibdata/
     innodb_data_file_path=ibdata1:50M:autoextend

A trailing slash is required when specifying a value for
'innodb_data_home_dir'.  'InnoDB' does not create directories, so ensure
that the specified directory exists before you start the server.  Also,
ensure sure that the MySQL server has the proper access rights to create
files in the directory.

'InnoDB' forms the directory path for each data file by textually
concatenating the value of 'innodb_data_home_dir' to the data file name.
If 'innodb_data_home_dir' is not defined, the default value is './',
which is the data directory.  (The MySQL server changes its current
working directory to the data directory when it begins executing.)

If you specify 'innodb_data_home_dir' as an empty string, you can
specify absolute paths for data files listed in the
'innodb_data_file_path' value.  The following configuration is
equivalent to the preceding one:

     [mysqld]
     innodb_data_home_dir =
     innodb_data_file_path=/myibdata/ibdata1:50M:autoextend

*Redo Log File Configuration*

By default, 'InnoDB' creates two 5MB redo log files in the data
directory named 'ib_logfile0' and 'ib_logfile1'.

The following options can be used to modify the default configuration:

   * 'innodb_log_group_home_dir' defines directory path to the 'InnoDB'
     log files (the redo logs).  If this option is not configured,
     'InnoDB' log files are created in the MySQL data directory
     ('datadir').

     You might use this option to place 'InnoDB' log files in a
     different physical storage location than 'InnoDB' data files to
     avoid potential I/O resource conflicts.  For example:

          [mysqld]
          innodb_log_group_home_dir = /dr3/iblogs

     *Note*:

     'InnoDB' does not create directories, so make sure that the log
     directory exists before you start the server.  Use the Unix or DOS
     'mkdir' command to create any necessary directories.

     Make sure that the MySQL server has the proper access rights to
     create files in the log directory.  More generally, the server must
     have access rights in any directory where it needs to create log
     files.

   * 'innodb_log_files_in_group' defines the number of log files in the
     log group.  The default and recommended value is 2.

   * 'innodb_log_file_size' defines the size in bytes of each log file
     in the log group.  The combined size of log files
     ('innodb_log_file_size' * 'innodb_log_files_in_group') cannot
     exceed a maximum value that is slightly less than 4GB. A pair of
     2047 MB log files, for example, approaches the limit but does not
     exceed it.  The default log file size is 5MB. Generally, the
     combined size of the log files should be large enough that the
     server can smooth out peaks and troughs in workload activity, which
     often means that there is enough redo log space to handle more than
     an hour of write activity.  The larger the value, the less
     checkpoint flush activity is needed in the buffer pool, saving disk
     I/O. For additional information, see *note
     optimizing-innodb-logging::.

*Memory Configuration*

MySQL allocates memory to various caches and buffers to improve
performance of database operations.  When allocating memory for
'InnoDB', always consider memory required by the operating system,
memory allocated to other applications, and memory allocated for other
MySQL buffers and caches.  For example, if you use 'MyISAM' tables,
consider the amount of memory allocated for the key buffer
('key_buffer_size').  For an overview of MySQL buffers and caches, see
*note memory-use::.

Buffers specific to 'InnoDB' are configured using the following
parameters:

   * 'innodb_buffer_pool_size' defines size of the buffer pool, which is
     the memory area that holds cached data for 'InnoDB' tables,
     indexes, and other auxiliary buffers.  The size of the buffer pool
     is important for system performance, and it is typically
     recommended that 'innodb_buffer_pool_size' is configured to 50 to
     75 percent of system memory.  The default buffer pool size is
     128MB. For additional guidance, see *note memory-use::.  For
     information about how to configure 'InnoDB' buffer pool size, see
     Configuring InnoDB Buffer Pool Size
     (https://dev.mysql.com/doc/refman/5.7/en/innodb-buffer-pool-resize.html).
     Buffer pool size can be configured at startup.

     On systems with a large amount of memory, you can improve
     concurrency by dividing the buffer pool into multiple buffer pool
     instances.  The number of buffer pool instances is controlled by
     the by 'innodb_buffer_pool_instances' option.  By default, 'InnoDB'
     creates one buffer pool instance.  The number of buffer pool
     instances can be configured at startup.  For more information, see
     *note innodb-multiple-buffer-pools::.

   * 'innodb_additional_mem_pool_size' defines size in bytes of a memory
     pool 'InnoDB' uses to store data dictionary information and other
     internal data structures.  The more tables you have in your
     application, the more memory you allocate here.  If 'InnoDB' runs
     out of memory in this pool, it starts to allocate memory from the
     operating system and writes warning messages to the MySQL error
     log.  The default value is 8MB.

   * 'innodb_log_buffer_size' defines the size in bytes of the buffer
     that 'InnoDB' uses to write to the log files on disk.  The default
     size is 8MB. A large log buffer enables large transactions to run
     without a need to write the log to disk before the transactions
     commit.  If you have transactions that update, insert, or delete
     many rows, you might consider increasing the size of the log buffer
     to save disk I/O. 'innodb_log_buffer_size' can be configured at
     startup.  For related information, see *note
     optimizing-innodb-logging::.

*Warning*:

On 32-bit GNU/Linux x86, be careful not to set memory usage too high.
'glibc' may permit the process heap to grow over thread stacks, which
crashes your server.  It is a risk if the memory allocated to the *note
'mysqld': mysqld. process for global and per-thread buffers and caches
is close to or exceeds 2GB.

A formula similar to the following that calculates global and per-thread
memory allocation for MySQL can be used to estimate MySQL memory usage.
You may need to modify the formula to account for buffers and caches in
your MySQL version and configuration.  For an overview of MySQL buffers
and caches, see *note memory-use::.

     innodb_buffer_pool_size
     + key_buffer_size
     + max_connections*(sort_buffer_size+read_buffer_size+binlog_cache_size)
     + max_connections*2MB

Each thread uses a stack (often 2MB, but only 256KB in MySQL binaries
provided by Oracle Corporation.)  and in the worst case also uses
'sort_buffer_size + read_buffer_size' additional memory.

On Linux, if the kernel is enabled for large page support, 'InnoDB' can
use large pages to allocate memory for its buffer pool and additional
memory pool.  See *note large-page-support::.


File: manual.info.tmp,  Node: innodb-performance-buffer-pool,  Next: innodb-performance-use_sys_malloc,  Prev: innodb-init-startup-configuration,  Up: innodb-configuration

14.11.2 InnoDB Buffer Pool Configuration
----------------------------------------

* Menu:

* innodb-multiple-buffer-pools::  Configuring Multiple Buffer Pool Instances
* innodb-performance-midpoint_insertion::  Making the Buffer Pool Scan Resistant
* innodb-performance-read_ahead::  Configuring InnoDB Buffer Pool Prefetching (Read-Ahead)
* innodb-buffer-pool-flushing::  Configuring Buffer Pool Flushing

This section provides configuration and tuning information for the
'InnoDB' buffer pool.


File: manual.info.tmp,  Node: innodb-multiple-buffer-pools,  Next: innodb-performance-midpoint_insertion,  Prev: innodb-performance-buffer-pool,  Up: innodb-performance-buffer-pool

14.11.2.1 Configuring Multiple Buffer Pool Instances
....................................................

For systems with buffer pools in the multi-gigabyte range, dividing the
buffer pool into separate instances can improve concurrency, by reducing
contention as different threads read and write to cached pages.  This
feature is typically intended for systems with a buffer pool size in the
multi-gigabyte range.  Multiple buffer pool instances are configured
using the 'innodb_buffer_pool_instances' configuration option, and you
might also adjust the 'innodb_buffer_pool_size' value.

When the 'InnoDB' buffer pool is large, many data requests can be
satisfied by retrieving from memory.  You might encounter bottlenecks
from multiple threads trying to access the buffer pool at once.  You can
enable multiple buffer pools to minimize this contention.  Each page
that is stored in or read from the buffer pool is assigned to one of the
buffer pools randomly, using a hashing function.  Each buffer pool
manages its own free lists, flush lists, LRUs, and all other data
structures connected to a buffer pool, and is protected by its own
buffer pool mutex.

To enable multiple buffer pool instances, set the
'innodb_buffer_pool_instances' configuration option to a value greater
than 1 (the default) up to 64 (the maximum).  This option takes effect
only when you set 'innodb_buffer_pool_size' to a size of 1GB or more.
The total size you specify is divided among all the buffer pools.  For
best efficiency, specify a combination of 'innodb_buffer_pool_instances'
and 'innodb_buffer_pool_size' so that each buffer pool instance is at
least 1GB.


File: manual.info.tmp,  Node: innodb-performance-midpoint_insertion,  Next: innodb-performance-read_ahead,  Prev: innodb-multiple-buffer-pools,  Up: innodb-performance-buffer-pool

14.11.2.2 Making the Buffer Pool Scan Resistant
...............................................

Rather than using a strict LRU algorithm, 'InnoDB' uses a technique to
minimize the amount of data that is brought into the buffer pool and
never accessed again.  The goal is to make sure that frequently accessed
('hot') pages remain in the buffer pool, even as read-ahead and full
table scans bring in new blocks that might or might not be accessed
afterward.

Newly read blocks are inserted into the middle of the LRU list.  All
newly read pages are inserted at a location that by default is '3/8'
from the tail of the LRU list.  The pages are moved to the front of the
list (the most-recently used end) when they are accessed in the buffer
pool for the first time.  Thus, pages that are never accessed never make
it to the front portion of the LRU list, and 'age out' sooner than with
a strict LRU approach.  This arrangement divides the LRU list into two
segments, where the pages downstream of the insertion point are
considered 'old' and are desirable victims for LRU eviction.

For an explanation of the inner workings of the 'InnoDB' buffer pool and
specifics about the LRU algorithm, see *note innodb-buffer-pool::.

You can control the insertion point in the LRU list and choose whether
'InnoDB' applies the same optimization to blocks brought into the buffer
pool by table or index scans.  The configuration parameter
'innodb_old_blocks_pct' controls the percentage of 'old' blocks in the
LRU list.  The default value of 'innodb_old_blocks_pct' is '37',
corresponding to the original fixed ratio of 3/8.  The value range is
'5' (new pages in the buffer pool age out very quickly) to '95' (only 5%
of the buffer pool is reserved for hot pages, making the algorithm close
to the familiar LRU strategy).

The optimization that keeps the buffer pool from being churned by
read-ahead can avoid similar problems due to table or index scans.  In
these scans, a data page is typically accessed a few times in quick
succession and is never touched again.  The configuration parameter
'innodb_old_blocks_time' specifies the time window (in milliseconds)
after the first access to a page during which it can be accessed without
being moved to the front (most-recently used end) of the LRU list.  The
default value of 'innodb_old_blocks_time' is '0', corresponding to the
original behavior of moving a page to the most-recently used end of the
buffer pool list when it is first accessed in the buffer pool.
Increasing this value makes more and more blocks likely to age out
faster from the buffer pool.

Both 'innodb_old_blocks_pct' and 'innodb_old_blocks_time' can be
specified in the MySQL option file ('my.cnf' or 'my.ini') or changed at
runtime with the *note 'SET GLOBAL': set-variable. statement.  Changing
the value at runtime requires privileges sufficient to set global system
variables.  See *note system-variable-privileges::.

To help you gauge the effect of setting these parameters, the 'SHOW
ENGINE INNODB STATUS' command reports buffer pool statistics.  For
details, see *note innodb-buffer-pool-monitoring::.

Because the effects of these parameters can vary widely based on your
hardware configuration, your data, and the details of your workload,
always benchmark to verify the effectiveness before changing these
settings in any performance-critical or production environment.

In mixed workloads where most of the activity is OLTP type with periodic
batch reporting queries which result in large scans, setting the value
of 'innodb_old_blocks_time' during the batch runs can help keep the
working set of the normal workload in the buffer pool.

When scanning large tables that cannot fit entirely in the buffer pool,
setting 'innodb_old_blocks_pct' to a small value keeps the data that is
only read once from consuming a significant portion of the buffer pool.
For example, setting 'innodb_old_blocks_pct=5' restricts this data that
is only read once to 5% of the buffer pool.

When scanning small tables that do fit into memory, there is less
overhead for moving pages around within the buffer pool, so you can
leave 'innodb_old_blocks_pct' at its default value, or even higher, such
as 'innodb_old_blocks_pct=50'.

The effect of the 'innodb_old_blocks_time' parameter is harder to
predict than the 'innodb_old_blocks_pct' parameter, is relatively small,
and varies more with the workload.  To arrive at an optimal value,
conduct your own benchmarks if the performance improvement from
adjusting 'innodb_old_blocks_pct' is not sufficient.


File: manual.info.tmp,  Node: innodb-performance-read_ahead,  Next: innodb-buffer-pool-flushing,  Prev: innodb-performance-midpoint_insertion,  Up: innodb-performance-buffer-pool

14.11.2.3 Configuring InnoDB Buffer Pool Prefetching (Read-Ahead)
.................................................................

A read-ahead request is an I/O request to prefetch multiple pages in the
buffer pool asynchronously, in anticipation that these pages will be
needed soon.  The requests bring in all the pages in one extent.
'InnoDB' uses two read-ahead algorithms to improve I/O performance:

_Linear_ read-ahead is a technique that predicts what pages might be
needed soon based on pages in the buffer pool being accessed
sequentially.  You control when 'InnoDB' performs a read-ahead operation
by adjusting the number of sequential page accesses required to trigger
an asynchronous read request, using the configuration parameter
'innodb_read_ahead_threshold'.  Before this parameter was added,
'InnoDB' would only calculate whether to issue an asynchronous prefetch
request for the entire next extent when it read the last page of the
current extent.

The configuration parameter 'innodb_read_ahead_threshold' controls how
sensitive 'InnoDB' is in detecting patterns of sequential page access.
If the number of pages read sequentially from an extent is greater than
or equal to 'innodb_read_ahead_threshold', 'InnoDB' initiates an
asynchronous read-ahead operation of the entire following extent.
'innodb_read_ahead_threshold' can be set to any value from 0-64.  The
default value is 56.  The higher the value, the more strict the access
pattern check.  For example, if you set the value to 48, 'InnoDB'
triggers a linear read-ahead request only when 48 pages in the current
extent have been accessed sequentially.  If the value is 8, 'InnoDB'
triggers an asynchronous read-ahead even if as few as 8 pages in the
extent are accessed sequentially.  You can set the value of this
parameter in the MySQL configuration file, or change it dynamically with
the *note 'SET GLOBAL': set-variable. statement, which requires
privileges sufficient to set global system variables.  See *note
system-variable-privileges::.

_Random_ read-ahead is a technique that predicts when pages might be
needed soon based on pages already in the buffer pool, regardless of the
order in which those pages were read.  If 13 consecutive pages from the
same extent are found in the buffer pool, 'InnoDB' asynchronously issues
a request to prefetch the remaining pages of the extent.  To enable this
feature, set the configuration variable 'innodb_random_read_ahead' to
'ON'.

Random read-ahead functionality was removed from the 'InnoDB Plugin'
(version 1.0.4) and was therefore not included in MySQL 5.5.0 when
'InnoDB Plugin' became the 'built-in' version of 'InnoDB'.  Random
read-ahead was reintroduced in MySQL 5.1.59 and 5.5.16 and higher along
with the 'innodb_random_read_ahead' configuration option, which is
disabled by default.  To enable this feature, set the configuration
variable 'innodb_random_read_ahead' to 'ON'.

The 'SHOW ENGINE INNODB STATUS' command displays statistics to help you
evaluate the effectiveness of the read-ahead algorithm.  Statistics
include counter information for the following global status variables:

   * 'Innodb_buffer_pool_read_ahead'

   * 'Innodb_buffer_pool_read_ahead_evicted'

   * 'Innodb_buffer_pool_read_ahead_rnd'

This information can be useful when fine-tuning the
'innodb_random_read_ahead' setting.

For more information about I/O performance, see *note
optimizing-innodb-diskio:: and *note disk-issues::.


File: manual.info.tmp,  Node: innodb-buffer-pool-flushing,  Prev: innodb-performance-read_ahead,  Up: innodb-performance-buffer-pool

14.11.2.4 Configuring Buffer Pool Flushing
..........................................

'InnoDB' performs certain tasks in the background, including flushing of
dirty pages from the buffer pool, a task performed by the master thread.
Dirty pages are those that have been modified but are not yet written to
the data files on disk.  'InnoDB' aggressively flushes buffer pool pages
if the percentage of dirty pages in the buffer pool reaches the
'innodb_max_dirty_pages_pct' threshold.

*Adaptive Flushing*

'InnoDB' uses an adaptive flushing algorithm to dynamically adjust the
rate of flushing based on the speed of redo log generation and the
current rate of flushing.  The intent is to smooth overall performance
by ensuring that flushing activity keeps pace with the current workload.
Automatically adjusting the flushing rate helps avoid sudden dips in
throughput that can occur when bursts of I/O activity due to buffer pool
flushing affects the I/O capacity available for ordinary read and write
activity.

Sharp checkpoints, which are typically associated with write-intensive
workloads that generate a lot of redo entries, can cause a sudden change
in throughput, for example.  A sharp checkpoint occurs when 'InnoDB'
wants to reuse a portion of a log file.  Before doing so, all dirty
pages with redo entries in that portion of the log file must be flushed.
If log files become full, a sharp checkpoint occurs, causing a temporary
reduction in throughput.  This scenario can occur even if
'innodb_max_dirty_pages_pct' threshold is not reached.

The adaptive flushing algorithm helps avoid such scenarios by tracking
the number of dirty pages in the buffer pool and the rate at which redo
log records are being generated.  Based on this information, it decides
how many dirty pages to flush from the buffer pool each second, which
permits it to manage sudden changes in workload.

Internal benchmarking has shown that the algorithm not only maintains
throughput over time, but can also improve overall throughput
significantly.  However, adaptive flushing can affect the I/O pattern of
a workload significantly and may not be appropriate in all cases.  If
adaptive flushing is not appropriate to the characteristics of your
workload, you can disable it.  Adaptive flushing controlled by the
'innodb_adaptive_flushing' variable, which is enabled by default.

Be aware that if flushing falls behind, the rate of buffer pool flushing
can exceed the I/O capacity available to 'InnoDB', as defined by
'innodb_io_capacity' setting.

The 'innodb_io_capacity' setting is applicable to all buffer pool
instances.  When dirty pages are flushed, I/O capacity is divided
equally among buffer pool instances.


File: manual.info.tmp,  Node: innodb-performance-use_sys_malloc,  Next: innodb-performance-thread_concurrency,  Prev: innodb-performance-buffer-pool,  Up: innodb-configuration

14.11.3 Configuring the Memory Allocator for InnoDB
---------------------------------------------------

When 'InnoDB' was developed, the memory allocators supplied with
operating systems and run-time libraries were often lacking in
performance and scalability.  At that time, there were no memory
allocator libraries tuned for multi-core CPUs.  Therefore, 'InnoDB'
implemented its own memory allocator in the 'mem' subsystem.  This
allocator is guarded by a single mutex, which may become a bottleneck.
'InnoDB' also implements a wrapper interface around the system allocator
('malloc' and 'free') that is likewise guarded by a single mutex.

Today, as multi-core systems have become more widely available, and as
operating systems have matured, significant improvements have been made
in the memory allocators provided with operating systems.  These new
memory allocators perform better and are more scalable than they were in
the past.  Most workloads, especially those where memory is frequently
allocated and released (such as multi-table joins), benefit from using a
more highly tuned memory allocator as opposed to the internal,
'InnoDB'-specific memory allocator.

You can control whether 'InnoDB' uses its own memory allocator or an
allocator of the operating system, by setting the value of the system
configuration parameter 'innodb_use_sys_malloc' in the MySQL option file
('my.cnf' or 'my.ini').  If set to 'ON' or '1' (the default), InnoDB
uses the 'malloc' and 'free' functions of the underlying system rather
than manage memory pools itself.  This parameter is not dynamic, and
takes effect only when the system is started.  To continue to use the
InnoDB memory allocator, set 'innodb_use_sys_malloc' to '0'.

*Note*:

When the 'InnoDB' memory allocator is disabled, 'InnoDB' ignores the
value of the parameter 'innodb_additional_mem_pool_size'.  The 'InnoDB'
memory allocator uses an additional memory pool for satisfying
allocation requests without having to fall back to the system memory
allocator.  When the 'InnoDB' memory allocator is disabled, all such
allocation requests are fulfilled by the system memory allocator.

On Unix-like systems that use dynamic linking, replacing the memory
allocator may be as easy as making the environment variable 'LD_PRELOAD'
or 'LD_LIBRARY_PATH' point to the dynamic library that implements the
allocator.  On other systems, some relinking may be necessary.  Please
refer to the documentation of the memory allocator library of your
choice.

Since 'InnoDB' cannot track all memory use when the system memory
allocator is used ('innodb_use_sys_malloc' is 'ON'), the section 'BUFFER
POOL AND MEMORY' in the output of the 'SHOW ENGINE INNODB STATUS'
command only includes the buffer pool statistics in the 'Total memory
allocated'.  Any memory allocated using the 'mem' subsystem or using
'ut_malloc' is excluded.

For more information about the performance implications of 'InnoDB'
memory usage, see *note buffering-caching::.


File: manual.info.tmp,  Node: innodb-performance-thread_concurrency,  Next: innodb-performance-multiple_io_threads,  Prev: innodb-performance-use_sys_malloc,  Up: innodb-configuration

14.11.4 Configuring Thread Concurrency for InnoDB
-------------------------------------------------

'InnoDB' uses operating system threads to process requests from user
transactions.  (Transactions may issue many requests to 'InnoDB' before
they commit or roll back.)  On modern operating systems and servers with
multi-core processors, where context switching is efficient, most
workloads run well without any limit on the number of concurrent
threads.  Scalability improvements in MySQL 5.5 and up reduce the need
to limit the number of concurrently executing threads inside 'InnoDB'.

In situations where it is helpful to minimize context switching between
threads, 'InnoDB' can use a number of techniques to limit the number of
concurrently executing operating system threads (and thus the number of
requests that are processed at any one time).  When 'InnoDB' receives a
new request from a user session, if the number of threads concurrently
executing is at a pre-defined limit, the new request sleeps for a short
time before it tries again.  A request that cannot be rescheduled after
the sleep is put in a first-in/first-out queue and eventually is
processed.  Threads waiting for locks are not counted in the number of
concurrently executing threads.

You can limit the number of concurrent threads by setting the
configuration parameter 'innodb_thread_concurrency'.  Once the number of
executing threads reaches this limit, additional threads sleep for a
number of microseconds, set by the configuration parameter
'innodb_thread_sleep_delay', before being placed into the queue.

The default value for 'innodb_thread_concurrency' and the implied
default limit on the number of concurrent threads has been changed in
various releases of MySQL and 'InnoDB'.  The default value of
'innodb_thread_concurrency' is '0', so that by default there is no limit
on the number of concurrently executing threads, as shown in *note
innodb-thread-concurrency-table::.

*Changes to innodb_thread_concurrency*

InnoDB         MySQL Version      Default     Default        Value to
Version                           value       limit of       allow
                                              concurrent     unlimited
                                              threads        threads
                                                             
Built-in       Earlier than       20          No limit       20 or higher
               5.1.11                                        
               
Built-in       5.1.11 and newer   8           8              0
                                                             
InnoDB         (corresponding     8           8              0
before 1.0.3   to Plugin)                                    
               
InnoDB 1.0.3   (corresponding     0           No limit       0
and newer      to Plugin)                     
               

'InnoDB' causes threads to sleep only when the number of concurrent
threads is limited.  When there is no limit on the number of threads,
all contend equally to be scheduled.  That is, if
'innodb_thread_concurrency' is '0', the value of
'innodb_thread_sleep_delay' is ignored.

When there is a limit on the number of threads (when
'innodb_thread_concurrency' is > 0), 'InnoDB' reduces context switching
overhead by permitting multiple requests made during the execution of a
_single SQL statement_ to enter 'InnoDB' without observing the limit set
by 'innodb_thread_concurrency'.  Since an SQL statement (such as a join)
may comprise multiple row operations within 'InnoDB', 'InnoDB' assigns a
specified number of 'tickets' that allow a thread to be scheduled
repeatedly with minimal overhead.

When a new SQL statement starts, a thread has no tickets, and it must
observe 'innodb_thread_concurrency'.  Once the thread is entitled to
enter 'InnoDB', it is assigned a number of tickets that it can use for
subsequently entering 'InnoDB' to perform row operations.  If the
tickets run out, the thread is evicted, and 'innodb_thread_concurrency'
is observed again which may place the thread back into the
first-in/first-out queue of waiting threads.  When the thread is once
again entitled to enter 'InnoDB', tickets are assigned again.  The
number of tickets assigned is specified by the global option
'innodb_concurrency_tickets', which is 500 by default.  A thread that is
waiting for a lock is given one ticket once the lock becomes available.

The correct values of these variables depend on your environment and
workload.  Try a range of different values to determine what value works
for your applications.  Before limiting the number of concurrently
executing threads, review configuration options that may improve the
performance of InnoDB on multi-core and multi-processor computers, such
as 'innodb_use_sys_malloc' and 'innodb_adaptive_hash_index'.

For general performance information about MySQL thread handling, see
*note client-connections::.


File: manual.info.tmp,  Node: innodb-performance-multiple_io_threads,  Next: innodb-linux-native-aio,  Prev: innodb-performance-thread_concurrency,  Up: innodb-configuration

14.11.5 Configuring the Number of Background InnoDB I/O Threads
---------------------------------------------------------------

'InnoDB' uses background threads to service various types of I/O
requests.  You can configure the number of background threads that
service read and write I/O on data pages using the
'innodb_read_io_threads' and 'innodb_write_io_threads' configuration
parameters.  These parameters signify the number of background threads
used for read and write requests, respectively.  They are effective on
all supported platforms.  You can set values for these parameters in the
MySQL option file ('my.cnf' or 'my.ini'); you cannot change values
dynamically.  The default value for these parameters is '4' and
permissible values range from '1-64'.

These parameters replace 'innodb_file_io_threads' from earlier versions
of MySQL. If you try to set a value for this obsolete parameter, a
warning is written to the log file and the value is ignored.  This
parameter only applied to Windows platforms.  (On non-Windows platforms,
there was only one thread each for read and write.)

The purpose of these configuration options to make 'InnoDB' more
scalable on high end systems.  Each background thread can handle up to
256 pending I/O requests.  A major source of background I/O is
read-ahead requests.  'InnoDB' tries to balance the load of incoming
requests in such way that most background threads share work equally.
'InnoDB' also attempts to allocate read requests from the same extent to
the same thread, to increase the chances of coalescing the requests.  If
you have a high end I/O subsystem and you see more than 64 x
'innodb_read_io_threads' pending read requests in 'SHOW ENGINE INNODB
STATUS' output, you might improve performance by increasing the value of
'innodb_read_io_threads'.

On Linux systems, 'InnoDB' uses the asynchronous I/O subsystem by
default to perform read-ahead and write requests for data file pages,
which changes the way that 'InnoDB' background threads service these
types of I/O requests.  For more information, see *note
innodb-linux-native-aio::.

For more information about 'InnoDB' I/O performance, see *note
optimizing-innodb-diskio::.


File: manual.info.tmp,  Node: innodb-linux-native-aio,  Next: innodb-configuring-io-capacity,  Prev: innodb-performance-multiple_io_threads,  Up: innodb-configuration

14.11.6 Using Asynchronous I/O on Linux
---------------------------------------

'InnoDB' uses the asynchronous I/O subsystem (native AIO) on Linux to
perform read-ahead and write requests for data file pages.  This
behavior is controlled by the 'innodb_use_native_aio' configuration
option, which applies to Linux systems only and is enabled by default.
On other Unix-like systems, 'InnoDB' uses synchronous I/O only.
Historically, 'InnoDB' only used asynchronous I/O on Windows systems.
Using the asynchronous I/O subsystem on Linux requires the 'libaio'
library.

With synchronous I/O, query threads queue I/O requests, and 'InnoDB'
background threads retrieve the queued requests one at a time, issuing a
synchronous I/O call for each.  When an I/O request is completed and the
I/O call returns, the 'InnoDB' background thread that is handling the
request calls an I/O completion routine and returns to process the next
request.  The number of requests that can be processed in parallel is N,
where N is the number of 'InnoDB' background threads.  The number of
'InnoDB' background threads is controlled by 'innodb_read_io_threads'
and 'innodb_write_io_threads'.  See *note
innodb-performance-multiple_io_threads::.

With native AIO, query threads dispatch I/O requests directly to the
operating system, thereby removing the limit imposed by the number of
background threads.  'InnoDB' background threads wait for I/O events to
signal completed requests.  When a request is completed, a background
thread calls an I/O completion routine and resumes waiting for I/O
events.

The advantage of native AIO is scalability for heavily I/O-bound systems
that typically show many pending reads/writes in 'SHOW ENGINE INNODB
STATUS\G' output.  The increase in parallel processing when using native
AIO means that the type of I/O scheduler or properties of the disk array
controller have a greater influence on I/O performance.

A potential disadvantage of native AIO for heavily I/O-bound systems is
lack of control over the number of I/O write requests dispatched to the
operating system at once.  Too many I/O write requests dispatched to the
operating system for parallel processing could, in some cases, result in
I/O read starvation, depending on the amount of I/O activity and system
capabilities.

If a problem with the asynchronous I/O subsystem in the OS prevents
'InnoDB' from starting, you can start the server with
'innodb_use_native_aio=0'.  This option may also be disabled
automatically during startup if 'InnoDB' detects a potential problem
such as a combination of 'tmpdir' location, 'tmpfs' file system, and
Linux kernel that does not support asynchronous I/O on 'tmpfs'.


File: manual.info.tmp,  Node: innodb-configuring-io-capacity,  Next: innodb-performance-spin_lock_polling,  Prev: innodb-linux-native-aio,  Up: innodb-configuration

14.11.7 Configuring InnoDB I/O Capacity
---------------------------------------

The 'InnoDB' master thread and other threads perform various tasks in
the background, most of which are I/O related, such as flushing dirty
pages from the buffer pool and writing changes from the change buffer to
the appropriate secondary indexes.  'InnoDB' attempts to perform these
tasks in a way that does not adversely affect the normal working of the
server.  It tries to estimate the available I/O bandwidth and tune its
activities to take advantage of available capacity.

The 'innodb_io_capacity' variable defines the overall I/O capacity
available to 'InnoDB'.  It should be set to approximately the number of
I/O operations that the system can perform per second (IOPS). When
'innodb_io_capacity' is set, 'InnoDB' estimates the I/O bandwidth
available for background tasks based on the set value.

You can set 'innodb_io_capacity' to a value of 100 or greater.  The
default value is '200'.  Typically, values around 100 are appropriate
for consumer-level storage devices, such as hard drives up to 7200 RPMs.
Faster hard drives, RAID configurations, and solid state drives (SSDs)
benefit from higher values.

Ideally, keep the setting as low as practical, but not so low that
background activities fall behind.  If the value is too high, data is
removed from the buffer pool and change buffer too quickly for caching
to provide a significant benefit.  For busy systems capable of higher
I/O rates, you can set a higher value to help the server handle the
background maintenance work associated with a high rate of row changes.
Generally, you can increase the value as a function of the number of
drives used for 'InnoDB' I/O. For example, you can increase the value on
systems that use multiple disks or SSDs.

The default setting of 200 is generally sufficient for a lower-end SSD.
For a higher-end, bus-attached SSD, consider a higher setting such as
1000, for example.  For systems with individual 5400 RPM or 7200 RPM
drives, you might lower the value to 100, which represents an estimated
proportion of the I/O operations per second (IOPS) available to
older-generation disk drives that can perform about 100 IOPS.

Although you can specify a high value such as a million, in practice
such large values have little benefit.  Generally, a value higher than
20000 is not recommended unless you are certain that lower values are
insufficient for your workload.

Consider write workload when tuning 'innodb_io_capacity'.  Systems with
large write workloads are likely to benefit from a higher setting.  A
lower setting may be sufficient for systems with a small write workload.

The 'innodb_io_capacity' setting is not a per buffer pool instance
setting.  Available I/O capacity is distributed equally among buffer
pool instances for flushing activities.

You can set the 'innodb_io_capacity' value in the MySQL option file
('my.cnf' or 'my.ini') or modify it at runtime using a *note 'SET
GLOBAL': set-variable. statement, which requires privileges sufficient
to set global system variables.  See *note system-variable-privileges::.


File: manual.info.tmp,  Node: innodb-performance-spin_lock_polling,  Next: innodb-purge-configuration,  Prev: innodb-configuring-io-capacity,  Up: innodb-configuration

14.11.8 Configuring Spin Lock Polling
-------------------------------------

'InnoDB' mutexes and rw-locks are typically reserved for short
intervals.  On a multi-core system, it can be more efficient for a
thread to continuously check if it can acquire a mutex or rw-lock for a
period of time before it sleeps.  If the mutex or rw-lock becomes
available during this period, the thread can continue immediately, in
the same time slice.  However, too-frequent polling of a shared object
such as a mutex or rw-lock by multiple threads can cause 'cache ping
pong', which results in processors invalidating portions of each other's
cache.  'InnoDB' minimizes this issue by forcing a random delay between
polls to desychronize polling activity.  The random delay is implemented
as a spin-wait loop.

The duration of a spin-wait loop is determined by the number of PAUSE
instructions that occur in the loop.  That number is generated by
randomly selecting an integer ranging from 0 up to but not including the
'innodb_spin_wait_delay' value, and multiplying that value by 50.  For
example, an integer is randomly selected from the following range for an
'innodb_spin_wait_delay' setting of 6:

     {0,1,2,3,4,5}

The selected integer is multiplied by 50, resulting in one of six
possible PAUSE instruction values:

     {0,50,100,150,200,250}

For that set of values, 250 is the maximum number of PAUSE instructions
that can occur in a spin-wait loop.  An 'innodb_spin_wait_delay' setting
of 5 results in a set of five possible values '{0,50,100,150,200}',
where 200 is the maximum number of PAUSE instructions, and so on.  In
this way, the 'innodb_spin_wait_delay' setting controls the maximum
delay between spin lock polls.

The duration of the delay loop depends on the C compiler and the target
processor.  In the 100MHz Pentium era, an 'innodb_spin_wait_delay' unit
was calibrated to be equivalent to one microsecond.  That time
equivalence did not hold, but PAUSE instruction duration has remained
fairly constant in terms of processor cycles relative to other CPU
instructions on most processor architectures.

On a system where all processor cores share a fast cache memory, you
might reduce the maximum delay or disable the busy loop altogether by
setting 'innodb_spin_wait_delay=0'.  On a system with multiple processor
chips, the effect of cache invalidation can be more significant and you
might increase the maximum delay.

The 'innodb_spin_wait_delay' variable is dynamic.  It can be specified
in a MySQL option file or modified at runtime using a *note 'SET
GLOBAL': set-variable. statement.  Runtime modification requires
privileges sufficient to set global system variables.  See *note
system-variable-privileges::.


File: manual.info.tmp,  Node: innodb-purge-configuration,  Next: innodb-statistics-estimation,  Prev: innodb-performance-spin_lock_polling,  Up: innodb-configuration

14.11.9 Purge Configuration
---------------------------

'InnoDB' does not physically remove a row from the database immediately
when you delete it with an SQL statement.  A row and its index records
are only physically removed when 'InnoDB' discards the undo log record
written for the deletion.  This removal operation, which only occurs
after the row is no longer required for multi-version concurrency
control (MVCC) or rollback, is called a purge.

Purge runs on a periodic schedule.  It parses and processes undo log
pages from the history list, which is a list of undo log pages for
committed transactions that is maintained by the 'InnoDB' transaction
system.  Purge frees the undo log pages from the history list after
processing them.

*Configuring a Dedicated Purge Thread*

By default, purge operations are performed by the 'InnoDB' master
thread.  Starting with MySQL 5.5, purge operations can performed in the
background by a dedicated purge thread rather than as part of the
'InnoDB' master thread.  The use of a dedicated purge thread may improve
scalability by allowing the main database operations to run
independently from maintenance work happening in the background.

You can enable a dedicated purge thread by the setting
'innodb_purge_threads' to 1.  The default value is 0, which means that
the purge operation is performed by the 'InnoDB' master thread.

*Configuring Purge Batch Size*

The 'innodb_purge_batch_size' variable defines the number of undo log
pages that purge parses and processes in one batch from the history
list.  The default value is 20.

The purge system also frees the undo log pages that are no longer
required.  It does so every 128 iterations through the undo logs.  In
addition to defining the number of undo log pages parsed and processed
in a batch, the 'innodb_purge_batch_size' variable defines the number of
undo log pages that purge frees every 128 iterations through the undo
logs.

The 'innodb_purge_batch_size' variable is intended for advanced
performance tuning and experimentation.  Most users need not change
'innodb_purge_batch_size' from its default value.

*Configuring the Maximum Purge Lag*

The 'innodb_max_purge_lag' variable defines the desired maximum purge
lag.  When the purge lag exceeds the 'innodb_max_purge_lag' threshold, a
delay is imposed on *note 'INSERT': insert, *note 'UPDATE': update, and
*note 'DELETE': delete. operations to allow time for purge operations to
catch up.  The default value is 0, which means there is no maximum purge
lag and no delay.

The 'InnoDB' transaction system maintains a list of transactions that
have index records delete-marked by *note 'UPDATE': update. or *note
'DELETE': delete. operations.  The length of the list is the purge lag.
The purge lag delay is calculated by the following formula, which
results in a minimum delay of 5000 microseconds:

     (purge lag/innodb_max_purge_lag - 0.5) * 10000

The delay is calculated at the beginning of a purge batch, every ten
seconds.  The operations are not delayed if purge cannot run because of
an old consistent read view that could see the rows to be purged.

A typical 'innodb_max_purge_lag' setting for a problematic workload
might be 1000000 (1 million), assuming that transactions are small, only
100 bytes in size, and it is permissible to have 100MB of unpurged table
rows.

The purge lag is presented as the 'History list length' value in the
'TRANSACTIONS' section of *note 'SHOW ENGINE INNODB STATUS':
show-engine. output.

     mysql> SHOW ENGINE INNODB STATUS;
     ...
     ------------
     TRANSACTIONS
     ------------
     Trx id counter 0 290328385
     Purge done for trx's n:o < 0 290315608 undo n:o < 0 17
     History list length 20

The 'History list length' is typically a low value, usually less than a
few thousand, but a write-heavy workload or long running transactions
can cause it to increase, even for transactions that are read only.  The
reason that a long running transaction can cause the 'History list
length' to increase is that under a consistent read transaction
isolation level such as 'REPEATABLE READ', a transaction must return the
same result as when the read view for that transaction was created.
Consequently, the 'InnoDB' multi-version concurrency control (MVCC)
system must keep a copy of the data in the undo log until all
transactions that depend on that data have completed.  The following are
examples of long running transactions that could cause the 'History list
length' to increase:

   * A *note 'mysqldump': mysqldump. operation that uses the
     '--single-transaction' option while there is a significant amount
     of concurrent DML.

   * Running a *note 'SELECT': select. query after disabling
     'autocommit', and forgetting to issue an explicit 'COMMIT' or
     'ROLLBACK'.


File: manual.info.tmp,  Node: innodb-statistics-estimation,  Prev: innodb-purge-configuration,  Up: innodb-configuration

14.11.10 Configuring Optimizer Statistics for InnoDB
----------------------------------------------------

* Menu:

* innodb-analyze-table-complexity::  Estimating ANALYZE TABLE Complexity for InnoDB Tables

The MySQL query optimizer uses estimated statistics about key
distributions to choose the indexes for an execution plan, based on the
relative selectivity of the index.  Certain operations cause InnoDB to
sample random pages from each index on a table to estimate the
cardinality of the index.  (This technique is known as random dives.)
These operations include the *note 'ANALYZE TABLE': analyze-table.
statement, the *note 'SHOW TABLE STATUS': show-table-status. statement,
and accessing the table for the first time after a restart.

To give you control over the quality of the statistics estimate (and
thus better information for the query optimizer), you can now change the
number of sampled pages using the parameter 'innodb_stats_sample_pages'.
Previously, the number of sampled pages was always 8, which could be
insufficient to produce an accurate estimate, leading to poor index
choices by the query optimizer.  This technique is especially important
for large tables and tables used in joins.  Unnecessary full table scans
for such tables can be a substantial performance issue.

You can set the global parameter 'innodb_stats_sample_pages', at
runtime.  The default value for this parameter is 8, preserving the same
behavior as in past releases.

*Note*:

The value of 'innodb_stats_sample_pages' affects the index sampling for
_all_ tables and indexes.  There are the following potentially
significant impacts when you change the index sample size:

   * Small values like 1 or 2 can result in very inaccurate estimates of
     cardinality.

   * Increasing the 'innodb_stats_sample_pages' value might require more
     disk reads.  Values much larger than 8 (say, 100), can cause a big
     slowdown in the time it takes to open a table or execute 'SHOW
     TABLE STATUS'.

   * The optimizer might choose very different query plans based on
     different estimates of index selectivity.

To disable the cardinality estimation for metadata statements such as
*note 'SHOW TABLE STATUS': show-table-status. or *note 'SHOW INDEX':
show-index, or when accessing the *note 'INFORMATION_SCHEMA.TABLES':
tables-table. or *note 'INFORMATION_SCHEMA.STATISTICS':
statistics-table. tables, execute the statement 'SET GLOBAL
innodb_stats_on_metadata=OFF'.  The ability to set this option
dynamically is also relatively new.

All 'InnoDB' tables are opened, and the statistics are re-estimated for
all associated indexes, when the *note 'mysql': mysql. client starts
with the '--auto-rehash' setting on (the default).  To improve the start
up time of the *note 'mysql': mysql. client, you can turn auto-rehash
off using the '--disable-auto-rehash' option.  The 'auto-rehash' feature
enables automatic name completion of database, table, and column names
for interactive users.

Whatever value of 'innodb_stats_sample_pages' works best for a system,
set the option and leave it at that value.  Choose a value that results
in reasonably accurate estimates for all tables in your database without
requiring excessive I/O. Because the statistics are automatically
recalculated at various times other than on execution of *note 'ANALYZE
TABLE': analyze-table, it does not make sense to increase the index
sample size, run *note 'ANALYZE TABLE': analyze-table, then decrease
sample size again.  The more accurate statistics calculated by 'ANALYZE'
running with a high value of 'innodb_stats_sample_pages' can be wiped
away later.

Although it is not possible to specify the sample size on a per-table
basis, smaller tables generally require fewer index samples than larger
tables do.  If your database has many large tables, consider using a
higher value for 'innodb_stats_sample_pages' than if you have mostly
smaller tables.


File: manual.info.tmp,  Node: innodb-analyze-table-complexity,  Prev: innodb-statistics-estimation,  Up: innodb-statistics-estimation

14.11.10.1 Estimating ANALYZE TABLE Complexity for InnoDB Tables
................................................................

'ANALYZE TABLE' complexity for 'InnoDB' tables is dependent on:

   * The number of pages sampled, as defined by
     'innodb_stats_sample_pages'.

   * The number of indexed columns in a table

   * The number of partitions.  If a table has no partitions, the number
     of partitions is considered to be 1.

Using these parameters, an approximate formula for estimating 'ANALYZE
TABLE' complexity would be:

'innodb_stats_sample_pages' * number of indexed columns in a table *
number of partitions

Typically, the greater the resulting value, the greater the execution
time for *note 'ANALYZE TABLE': analyze-table.

For more information about the 'innodb_stats_sample_pages' configuration
parameter, see *note innodb-statistics-estimation::.


File: manual.info.tmp,  Node: innodb-compression,  Next: innodb-file-format,  Prev: innodb-configuration,  Up: innodb-storage-engine

14.12 InnoDB Table Compression
==============================

* Menu:

* innodb-compression-background::  Overview of Table Compression
* innodb-compression-usage::     Enabling Compression for a Table
* innodb-compression-tuning::    Tuning Compression for InnoDB Tables
* innodb-compression-tuning-monitoring::  Monitoring InnoDB Table Compression at Runtime
* innodb-compression-internals::  How Compression Works for InnoDB Tables
* innodb-compression-syntax-warnings::  SQL Compression Syntax Warnings and Errors

By using the SQL syntax and InnoDB configuration options for
compression, you can create tables where the data is stored in
compressed form.  Compression can help to improve both raw performance
and scalability.  The compression means less data is transferred between
disk and memory, and takes up less space on disk and in memory.  The
benefits are amplified for tables with secondary indexes, because index
data is compressed also.  Compression can be especially important for
SSD storage devices, because they tend to have lower capacity than HDD
devices.


File: manual.info.tmp,  Node: innodb-compression-background,  Next: innodb-compression-usage,  Prev: innodb-compression,  Up: innodb-compression

14.12.1 Overview of Table Compression
-------------------------------------

Because processors and cache memories have increased in speed more than
disk storage devices, many workloads are disk-bound.  Data compression
enables smaller database size, reduced I/O, and improved throughput, at
the small cost of increased CPU utilization.  Compression is especially
valuable for read-intensive applications, on systems with enough RAM to
keep frequently used data in memory.

An InnoDB table created with 'ROW_FORMAT=COMPRESSED' can use a smaller
page size on disk than the usual 16KB default.  Smaller pages require
less I/O to read from and write to disk, which is especially valuable
for SSD devices.

The page size is specified through the 'KEY_BLOCK_SIZE' parameter.  The
different page size means the table must be in its own '.ibd' file
rather than in the system tablespace, which requires enabling the
'innodb_file_per_table' option.  The level of compression is the same
regardless of the 'KEY_BLOCK_SIZE' value.  As you specify smaller values
for 'KEY_BLOCK_SIZE', you get the I/O benefits of increasingly smaller
pages.  But if you specify a value that is too small, there is
additional overhead to reorganize the pages when data values cannot be
compressed enough to fit multiple rows in each page.  There is a hard
limit on how small 'KEY_BLOCK_SIZE' can be for a table, based on the
lengths of the key columns for each of its indexes.  Specify a value
that is too small, and the *note 'CREATE TABLE': create-table. or *note
'ALTER TABLE': alter-table. statement fails.

In the buffer pool, the compressed data is held in small pages, with a
page size based on the 'KEY_BLOCK_SIZE' value.  For extracting or
updating the column values, MySQL also creates a 16KB page in the buffer
pool with the uncompressed data.  Within the buffer pool, any updates to
the uncompressed page are also re-written back to the equivalent
compressed page.  You might need to size your buffer pool to accommodate
the additional data of both compressed and uncompressed pages, although
the uncompressed pages are evicted from the buffer pool when space is
needed, and then uncompressed again on the next access.


File: manual.info.tmp,  Node: innodb-compression-usage,  Next: innodb-compression-tuning,  Prev: innodb-compression-background,  Up: innodb-compression

14.12.2 Enabling Compression for a Table
----------------------------------------

Before creating a compressed table, make sure the
'innodb_file_per_table' configuration option is enabled, and
'innodb_file_format' is set to 'Barracuda'.  You can set these
parameters in the MySQL configuration file 'my.cnf' or 'my.ini', or with
the 'SET' statement without shutting down the MySQL server.

To enable compression for a table, you use the clauses
'ROW_FORMAT=COMPRESSED', 'KEY_BLOCK_SIZE', or both in a *note 'CREATE
TABLE': create-table. or *note 'ALTER TABLE': alter-table. statement.

To create a compressed table, you might use statements like these:

     SET GLOBAL innodb_file_per_table=1;
     SET GLOBAL innodb_file_format=Barracuda;
     CREATE TABLE t1
      (c1 INT PRIMARY KEY)
      ROW_FORMAT=COMPRESSED
      KEY_BLOCK_SIZE=8;

   * If you specify 'ROW_FORMAT=COMPRESSED', you can omit
     'KEY_BLOCK_SIZE'; the default compressed page size of 8KB is used.

   * If you specify 'KEY_BLOCK_SIZE', you can omit
     'ROW_FORMAT=COMPRESSED'; compression is enabled automatically.

   * To determine the best value for 'KEY_BLOCK_SIZE', typically you
     create several copies of the same table with different values for
     this clause, then measure the size of the resulting '.ibd' files
     and see how well each performs with a realistic workload.

   * For additional performance-related configuration options, see *note
     innodb-compression-tuning::.

The default uncompressed size of InnoDB data pages is 16KB. Depending on
the combination of option values, MySQL uses a page size of 1KB, 2KB,
4KB, 8KB, or 16KB for the '.ibd' file of the table.  The actual
compression algorithm is not affected by the 'KEY_BLOCK_SIZE' value; the
value determines how large each compressed chunk is, which in turn
affects how many rows can be packed into each compressed page.

Setting 'KEY_BLOCK_SIZE=16' typically does not result in much
compression, since the normal InnoDB page size is 16KB. This setting may
still be useful for tables with many long *note 'BLOB': blob, *note
'VARCHAR': char. or *note 'TEXT': blob. columns, because such values
often do compress well, and might therefore require fewer overflow pages
as described in *note innodb-compression-internals::.

All indexes of a table (including the clustered index) are compressed
using the same page size, as specified in the *note 'CREATE TABLE':
create-table. or *note 'ALTER TABLE': alter-table. statement.  Table
attributes such as 'ROW_FORMAT' and 'KEY_BLOCK_SIZE' are not part of the
*note 'CREATE INDEX': create-index. syntax, and are ignored if they are
specified (although you see them in the output of the 'SHOW CREATE
TABLE' statement).

*Restrictions on Compressed Tables*

Because MySQL versions prior to 5.1 cannot process compressed tables,
using compression requires specifying the configuration parameter
'innodb_file_format=Barracuda', to avoid accidentally introducing
compatibility issues.

Table compression is also not available for the InnoDB system
tablespace.  The system tablespace (space 0, the 'ibdata*' files) can
contain user data, but it also contains internal system information, and
therefore is never compressed.  Thus, compression applies only to tables
(and indexes) stored in their own tablespaces, that is, created with the
'innodb_file_per_table' option enabled.

Compression applies to an entire table and all its associated indexes,
not to individual rows, despite the clause name 'ROW_FORMAT'.


File: manual.info.tmp,  Node: innodb-compression-tuning,  Next: innodb-compression-tuning-monitoring,  Prev: innodb-compression-usage,  Up: innodb-compression

14.12.3 Tuning Compression for InnoDB Tables
--------------------------------------------

Most often, the internal optimizations described in *note
innodb-compression-internals-storage:: ensure that the system runs well
with compressed data.  However, because the efficiency of compression
depends on the nature of your data, you can make decisions that affect
the performance of compressed tables:

   * Which tables to compress.

   * What compressed page size to use.

   * Whether to adjust the size of the buffer pool based on run-time
     performance characteristics, such as the amount of time the system
     spends compressing and uncompressing data.  Whether the workload is
     more like a data warehouse (primarily queries) or an OLTP system
     (mix of queries and DML).

   * If the system performs DML operations on compressed tables, and the
     way the data is distributed leads to expensive compression failures
     at runtime, you might adjust additional advanced configuration
     options.

Use the guidelines in this section to help make those architectural and
configuration choices.  When you are ready to conduct long-term testing
and put compressed tables into production, see *note
innodb-compression-tuning-monitoring:: for ways to verify the
effectiveness of those choices under real-world conditions.

*When to Use Compression*

In general, compression works best on tables that include a reasonable
number of character string columns and where the data is read far more
often than it is written.  Because there are no guaranteed ways to
predict whether or not compression benefits a particular situation,
always test with a specific workload and data set running on a
representative configuration.  Consider the following factors when
deciding which tables to compress.

*Data Characteristics and Compression*

A key determinant of the efficiency of compression in reducing the size
of data files is the nature of the data itself.  Recall that compression
works by identifying repeated strings of bytes in a block of data.
Completely randomized data is the worst case.  Typical data often has
repeated values, and so compresses effectively.  Character strings often
compress well, whether defined in 'CHAR', 'VARCHAR', 'TEXT' or 'BLOB'
columns.  On the other hand, tables containing mostly binary data
(integers or floating point numbers) or data that is previously
compressed (for example JPEG or PNG images) may not generally compress
well, significantly or at all.

You choose whether to turn on compression for each InnoDB table.  A
table and all of its indexes use the same (compressed) page size.  It
might be that the primary key (clustered) index, which contains the data
for all columns of a table, compresses more effectively than the
secondary indexes.  For those cases where there are long rows, the use
of compression might result in long column values being stored
'off-page', as discussed in *note innodb-row-format-dynamic::.  Those
overflow pages may compress well.  Given these considerations, for many
applications, some tables compress more effectively than others, and you
might find that your workload performs best only with a subset of tables
compressed.

To determine whether or not to compress a particular table, conduct
experiments.  You can get a rough estimate of how efficiently your data
can be compressed by using a utility that implements LZ77 compression
(such as 'gzip' or WinZip) on a copy of the .ibd file for an
uncompressed table.  You can expect less compression from a MySQL
compressed table than from file-based compression tools, because MySQL
compresses data in chunks based on the page size, 16KB by default.  In
addition to user data, the page format includes some internal system
data that is not compressed.  File-based compression utilities can
examine much larger chunks of data, and so might find more repeated
strings in a huge file than MySQL can find in an individual page.

Another way to test compression on a specific table is to copy some data
from your uncompressed table to a similar, compressed table (having all
the same indexes) and look at the size of the resulting '.ibd' file.
For example:

     USE test;
     SET GLOBAL innodb_file_per_table=1;
     SET GLOBAL innodb_file_format=Barracuda;
     SET GLOBAL autocommit=0;

     -- Create an uncompressed table with a million or two rows.
     CREATE TABLE big_table AS SELECT * FROM information_schema.columns;
     INSERT INTO big_table SELECT * FROM big_table;
     INSERT INTO big_table SELECT * FROM big_table;
     INSERT INTO big_table SELECT * FROM big_table;
     INSERT INTO big_table SELECT * FROM big_table;
     INSERT INTO big_table SELECT * FROM big_table;
     INSERT INTO big_table SELECT * FROM big_table;
     INSERT INTO big_table SELECT * FROM big_table;
     INSERT INTO big_table SELECT * FROM big_table;
     INSERT INTO big_table SELECT * FROM big_table;
     INSERT INTO big_table SELECT * FROM big_table;
     COMMIT;
     ALTER TABLE big_table ADD id int unsigned NOT NULL PRIMARY KEY auto_increment;

     SHOW CREATE TABLE big_table\G

     select count(id) from big_table;

     -- Check how much space is needed for the uncompressed table.
     \! ls -l data/test/big_table.ibd

     CREATE TABLE key_block_size_4 LIKE big_table;
     ALTER TABLE key_block_size_4 key_block_size=4 row_format=compressed;

     INSERT INTO key_block_size_4 SELECT * FROM big_table;
     commit;

     -- Check how much space is needed for a compressed table
     -- with particular compression settings.
     \! ls -l data/test/key_block_size_4.ibd

This experiment produced the following numbers, which of course could
vary considerably depending on your table structure and data:

     -rw-rw----  1 cirrus  staff  310378496 Jan  9 13:44 data/test/big_table.ibd
     -rw-rw----  1 cirrus  staff  83886080 Jan  9 15:10 data/test/key_block_size_4.ibd

To see whether compression is efficient for your particular workload,
use a MySQL instance with no other compressed tables and run queries
against the *note 'INFORMATION_SCHEMA.INNODB_CMP': innodb-cmp-table.
table.  For exmaple, you examine the ratio of successful compression
operations to overall compression operations.  (In the 'INNODB_CMP'
table, compare 'COMPRESS_OPS' to 'COMPRESS_OPS_OK'.  See *note
'INNODB_CMP': innodb-information-schema-innodb_cmp. for more
information.)  If a high percentage of compression operations complete
successfully, the table might be a good candidate for compression.

*Compression and Application and Schema Design*

Decide whether to compress data in your application or in the table; do
not use both types of compression for the same data.  When you compress
the data in the application and store the results in a compressed table,
extra space savings are extremely unlikely, and the double compression
just wastes CPU cycles.

*Compressing in the Database*

The InnoDB table compression is automatic and applies to all columns and
index values.  The columns can still be tested with operators such as
'LIKE', and sort operations can still use indexes even when the index
values are compressed.  Because indexes are often a significant fraction
of the total size of a database, compression could result in significant
savings in storage, I/O or processor time.  The compression and
decompression operations happen on the database server, which likely is
a powerful system that is sized to handle the expected load.

*Compressing in the Application*

If you compress data such as text in your application, before it is
inserted into the database, You might save overhead for data that does
not compress well by compressing some columns and not others.  This
approach uses CPU cycles for compression and uncompression on the client
machine rather than the database server, which might be appropriate for
a distributed application with many clients, or where the client machine
has spare CPU cycles.

*Hybrid Approach*

Of course, it is possible to combine these approaches.  For some
applications, it may be appropriate to use some compressed tables and
some uncompressed tables.  It may be best to externally compress some
data (and store it in uncompressed InnoDB tables) and allow InnoDB to
compress (some of) the other tables in the application.  As always,
up-front design and real-life testing are valuable in reaching the right
decision.

*Workload Characteristics and Compression*

In addition to choosing which tables to compress (and the page size),
the workload is another key determinant of performance.  If the
application is dominated by reads, rather than updates, fewer pages need
to be reorganized and recompressed after the index page runs out of room
for the per-page 'modification log' that InnoDB maintains for compressed
data.  If the updates predominantly change non-indexed columns or those
containing 'BLOB's or large strings that happen to be stored 'off-page',
the overhead of compression may be acceptable.  If the only changes to a
table are 'INSERT's that use a monotonically increasing primary key, and
there are few secondary indexes, there is little need to reorganize and
recompress index pages.  Since InnoDB can 'delete-mark' and delete rows
on compressed pages 'in place' by modifying uncompressed data, 'DELETE'
operations on a table are relatively efficient.

For some environments, the time it takes to load data can be as
important as run-time retrieval.  Especially in data warehouse
environments, many tables may be read-only or read-mostly.  In those
cases, it might or might not be acceptable to pay the price of
compression in terms of increased load time, unless the resulting
savings in fewer disk reads or in storage cost is significant.

Fundamentally, compression works best when the CPU time is available for
compressing and uncompressing data.  Thus, if your workload is I/O
bound, rather than CPU-bound, you might find that compression can
improve overall performance.  When you test your application performance
with different compression configurations, test on a platform similar to
the planned configuration of the production system.

*Configuration Characteristics and Compression*

Reading and writing database pages from and to disk is the slowest
aspect of system performance.  Compression attempts to reduce I/O by
using CPU time to compress and uncompress data, and is most effective
when I/O is a relatively scarce resource compared to processor cycles.

This is often especially the case when running in a multi-user
environment with fast, multi-core CPUs.  When a page of a compressed
table is in memory, InnoDB often uses an additional 16K in the buffer
pool for an uncompressed copy of the page.  The adaptive LRU algorithm
in the InnoDB storage engine attempts to balance the use of memory
between compressed and uncompressed pages to take into account whether
the workload is running in an I/O-bound or CPU-bound manner.  Still, a
configuration with more memory dedicated to the InnoDB buffer pool tends
to run better when using compressed tables than a configuration where
memory is highly constrained.

*Choosing the Compressed Page Size*

The optimal setting of the compressed page size depends on the type and
distribution of data that the table and its indexes contain.  The
compressed page size should always be bigger than the maximum record
size, or operations may fail as noted in *note
innodb-compression-internals-storage-btree::.

Setting the compressed page size too large wastes some space, but the
pages do not have to be compressed as often.  If the compressed page
size is set too small, inserts or updates may require time-consuming
recompression, and the B-tree nodes may have to be split more
frequently, leading to bigger data files and less efficient indexing.

Typically, you set the compressed page size to 8K or 4K bytes.  Given
that the maximum row size for an InnoDB table is around 8K,
'KEY_BLOCK_SIZE=8' is usually a safe choice.


File: manual.info.tmp,  Node: innodb-compression-tuning-monitoring,  Next: innodb-compression-internals,  Prev: innodb-compression-tuning,  Up: innodb-compression

14.12.4 Monitoring InnoDB Table Compression at Runtime
------------------------------------------------------

Overall application performance, CPU and I/O utilization and the size of
disk files are good indicators of how effective compression is for your
application.  This section builds on the performance tuning advice from
*note innodb-compression-tuning::, and shows how to find problems that
might not turn up during initial testing.

To dig deeper into performance considerations for compressed tables, you
can monitor compression performance at runtime using the Information
Schema tables described in *note
innodb-information-schema-examples-compression::.  These tables reflect
the internal use of memory and the rates of compression used overall.

The *note 'INNODB_CMP': innodb-cmp-table. table reports information
about compression activity for each compressed page size
('KEY_BLOCK_SIZE') in use.  The information in these tables is
system-wide: it summarizes the compression statistics across all
compressed tables in your database.  You can use this data to help
decide whether or not to compress a table by examining these tables when
no other compressed tables are being accessed.  It involves relatively
low overhead on the server, so you might query it periodically on a
production server to check the overall efficiency of the compression
feature.

The key statistics to consider are the number of, and amount of time
spent performing, compression and uncompression operations.  Since
InnoDB must split B-tree nodes when they are too full to contain the
compressed data following a modification, compare the number of
'successful' compression operations with the number of such operations
overall.  Based on the information in the 'INNODB_CMP' tables and
overall application performance and hardware resource utilization, you
might make changes in your hardware configuration, adjust the size of
the InnoDB buffer pool, choose a different page size, or select a
different set of tables to compress.

If the amount of CPU time required for compressing and uncompressing is
high, changing to faster CPUs, or those with more cores, can help
improve performance with the same data, application workload and set of
compressed tables.  Increasing the size of the InnoDB buffer pool might
also help performance, so that more uncompressed pages can stay in
memory, reducing the need to uncompress pages that exist in memory only
in compressed form.

A large number of compression operations overall (compared to the number
of 'INSERT', 'UPDATE' and 'DELETE' operations in your application and
the size of the database) could indicate that some of your compressed
tables are being updated too heavily for effective compression.  If so,
choose a larger page size, or be more selective about which tables you
compress.

If the number of 'successful' compression operations ('COMPRESS_OPS_OK')
is a high percentage of the total number of compression operations
('COMPRESS_OPS'), then the system is likely performing well.  If the
ratio is low, then InnoDB is reorganizing, recompressing, and splitting
B-tree nodes more often than is desirable.  In this case, avoid
compressing some tables, or increase 'KEY_BLOCK_SIZE' for some of the
compressed tables.  You might turn off compression for tables that cause
the number of 'compression failures' in your application to be more than
1% or 2% of the total.  (Such a failure ratio might be acceptable during
a temporary operation such as a data load).


File: manual.info.tmp,  Node: innodb-compression-internals,  Next: innodb-compression-syntax-warnings,  Prev: innodb-compression-tuning-monitoring,  Up: innodb-compression

14.12.5 How Compression Works for InnoDB Tables
-----------------------------------------------

This section describes some internal implementation details about
compression for InnoDB tables.  The information presented here may be
helpful in tuning for performance, but is not necessary to know for
basic use of compression.

*Compression Algorithms*

Some operating systems implement compression at the file system level.
Files are typically divided into fixed-size blocks that are compressed
into variable-size blocks, which easily leads into fragmentation.  Every
time something inside a block is modified, the whole block is
recompressed before it is written to disk.  These properties make this
compression technique unsuitable for use in an update-intensive database
system.

InnoDB implements compression with the help of the well-known zlib
library (http://www.zlib.net/), which implements the LZ77 compression
algorithm.  This compression algorithm is mature, robust, and efficient
in both CPU utilization and in reduction of data size.  The algorithm is
'lossless', so that the original uncompressed data can always be
reconstructed from the compressed form.  LZ77 compression works by
finding sequences of data that are repeated within the data to be
compressed.  The patterns of values in your data determine how well it
compresses, but typical user data often compresses by 50% or more.

*Note*:

Prior to MySQL 5.5.62, 'InnoDB' supports the 'zlib' library up to
version 1.2.3.  In MySQL 5.5.62 and later, 'InnoDB' supports the 'zlib'
library up to version 1.2.11.

Unlike compression performed by an application, or compression features
of some other database management systems, InnoDB compression applies
both to user data and to indexes.  In many cases, indexes can constitute
40-50% or more of the total database size, so this difference is
significant.  When compression is working well for a data set, the size
of the InnoDB data files (the '.ibd' files) is 25% to 50% of the
uncompressed size or possibly smaller.  Depending on the workload, this
smaller database can in turn lead to a reduction in I/O, and an increase
in throughput, at a modest cost in terms of increased CPU utilization.

*InnoDB Data Storage and Compression*

All user data in InnoDB tables is stored in pages comprising a B-tree
index (the clustered index).  In some other database systems, this type
of index is called an 'index-organized table'.  Each row in the index
node contains the values of the (user-specified or system-generated)
primary key and all the other columns of the table.

Secondary indexes in InnoDB tables are also B-trees, containing pairs of
values: the index key and a pointer to a row in the clustered index.
The pointer is in fact the value of the primary key of the table, which
is used to access the clustered index if columns other than the index
key and primary key are required.  Secondary index records must always
fit on a single B-tree page.

The compression of B-tree nodes (of both clustered and secondary
indexes) is handled differently from compression of overflow pages used
to store long 'VARCHAR', 'BLOB', or 'TEXT' columns, as explained in the
following sections.

*Compression of B-Tree Pages*

Because they are frequently updated, B-tree pages require special
treatment.  It is important to minimize the number of times B-tree nodes
are split, as well as to minimize the need to uncompress and recompress
their content.

One technique InnoDB uses is to maintain some system information in the
B-tree node in uncompressed form, thus facilitating certain in-place
updates.  For example, this allows rows to be delete-marked and deleted
without any compression operation.

In addition, InnoDB attempts to avoid unnecessary uncompression and
recompression of index pages when they are changed.  Within each B-tree
page, the system keeps an uncompressed 'modification log' to record
changes made to the page.  Updates and inserts of small records may be
written to this modification log without requiring the entire page to be
completely reconstructed.

When the space for the modification log runs out, InnoDB uncompresses
the page, applies the changes and recompresses the page.  If
recompression fails (a situation known as a compression failure), the
B-tree nodes are split and the process is repeated until the update or
insert succeeds.

Generally, InnoDB requires that each B-tree page can accommodate at
least two records.  For compressed tables, this requirement has been
relaxed.  Leaf pages of B-tree nodes (whether of the primary key or
secondary indexes) only need to accommodate one record, but that record
must fit in uncompressed form, in the per-page modification log.
Starting with InnoDB storage engine version 1.0.2, and if
'innodb_strict_mode' is 'ON', the InnoDB storage engine checks the
maximum row size during *note 'CREATE TABLE': create-table. or *note
'CREATE INDEX': create-index.  If the row does not fit, the following
error message is issued: 'ERROR HY000: Too big row'.

If you create a table when 'innodb_strict_mode' is OFF, and a subsequent
'INSERT' or 'UPDATE' statement attempts to create an index entry that
does not fit in the size of the compressed page, the operation fails
with 'ERROR 42000: Row size too large'.  (This error message does not
name the index for which the record is too large, or mention the length
of the index record or the maximum record size on that particular index
page.)  To solve this problem, rebuild the table with *note 'ALTER
TABLE': alter-table. and select a larger compressed page size
('KEY_BLOCK_SIZE'), shorten any column prefix indexes, or disable
compression entirely with 'ROW_FORMAT=DYNAMIC' or 'ROW_FORMAT=COMPACT'.

*Compressing BLOB, VARCHAR, and TEXT Columns*

In an InnoDB table, *note 'BLOB': blob, *note 'VARCHAR': char, and *note
'TEXT': blob. columns that are not part of the primary key may be stored
on separately allocated overflow pages.  We refer to these columns as
off-page columns.  Their values are stored on singly-linked lists of
overflow pages.

For tables created in 'ROW_FORMAT=DYNAMIC' or 'ROW_FORMAT=COMPRESSED',
the values of *note 'BLOB': blob, *note 'TEXT': blob, or *note
'VARCHAR': char. columns may be stored fully off-page, depending on
their length and the length of the entire row.  For columns that are
stored off-page, the clustered index record only contains 20-byte
pointers to the overflow pages, one per column.  Whether any columns are
stored off-page depends on the page size and the total size of the row.
When the row is too long to fit entirely within the page of the
clustered index, MySQL chooses the longest columns for off-page storage
until the row fits on the clustered index page.  As noted above, if a
row does not fit by itself on a compressed page, an error occurs.

*Note*:

For tables created in 'ROW_FORMAT=DYNAMIC' or 'ROW_FORMAT=COMPRESSED',
*note 'TEXT': blob. and *note 'BLOB': blob. columns that are less than
or equal to 40 bytes are always stored in-line.

Tables created in older versions of InnoDB use the Antelope file format,
which supports only 'ROW_FORMAT=REDUNDANT' and 'ROW_FORMAT=COMPACT'.  In
these formats, MySQL stores the first 768 bytes of *note 'BLOB': blob,
*note 'VARCHAR': char, and *note 'TEXT': blob. columns in the clustered
index record along with the primary key.  The 768-byte prefix is
followed by a 20-byte pointer to the overflow pages that contain the
rest of the column value.

When a table is in 'COMPRESSED' format, all data written to overflow
pages is compressed 'as is'; that is, InnoDB applies the zlib
compression algorithm to the entire data item.  Other than the data,
compressed overflow pages contain an uncompressed header and trailer
comprising a page checksum and a link to the next overflow page, among
other things.  Therefore, very significant storage savings can be
obtained for longer 'BLOB', 'TEXT', or 'VARCHAR' columns if the data is
highly compressible, as is often the case with text data.  Image data,
such as 'JPEG', is typically already compressed and so does not benefit
much from being stored in a compressed table; the double compression can
waste CPU cycles for little or no space savings.

The overflow pages are of the same size as other pages.  A row
containing ten columns stored off-page occupies ten overflow pages, even
if the total length of the columns is only 8K bytes.  In an uncompressed
table, ten uncompressed overflow pages occupy 160K bytes.  In a
compressed table with an 8K page size, they occupy only 80K bytes.
Thus, it is often more efficient to use compressed table format for
tables with long column values.

Using a 16K compressed page size can reduce storage and I/O costs for
*note 'BLOB': blob, *note 'VARCHAR': char, or *note 'TEXT': blob.
columns, because such data often compress well, and might therefore
require fewer overflow pages, even though the B-tree nodes themselves
take as many pages as in the uncompressed form.

*Compression and the InnoDB Buffer Pool*

In a compressed InnoDB table, every compressed page (whether 1K, 2K, 4K
or 8K) corresponds to an uncompressed page of 16K bytes.  To access the
data in a page, InnoDB reads the compressed page from disk if it is not
already in the buffer pool, then uncompresses the page to its original
form.  This section describes how InnoDB manages the buffer pool with
respect to pages of compressed tables.

To minimize I/O and to reduce the need to uncompress a page, at times
the buffer pool contains both the compressed and uncompressed form of a
database page.  To make room for other required database pages, InnoDB
may evict from the buffer pool an uncompressed page, while leaving the
compressed page in memory.  Or, if a page has not been accessed in a
while, the compressed form of the page might be written to disk, to free
space for other data.  Thus, at any given time, the buffer pool might
contain both the compressed and uncompressed forms of the page, or only
the compressed form of the page, or neither.

InnoDB keeps track of which pages to keep in memory and which to evict
using a least-recently-used (LRU) list, so that hot (frequently
accessed) data tends to stay in memory.  When compressed tables are
accessed, MySQL uses an adaptive LRU algorithm to achieve an appropriate
balance of compressed and uncompressed pages in memory.  This adaptive
algorithm is sensitive to whether the system is running in an I/O-bound
or CPU-bound manner.  The goal is to avoid spending too much processing
time uncompressing pages when the CPU is busy, and to avoid doing excess
I/O when the CPU has spare cycles that can be used for uncompressing
compressed pages (that may already be in memory).  When the system is
I/O-bound, the algorithm prefers to evict the uncompressed copy of a
page rather than both copies, to make more room for other disk pages to
become memory resident.  When the system is CPU-bound, MySQL prefers to
evict both the compressed and uncompressed page, so that more memory can
be used for 'hot' pages and reducing the need to uncompress data in
memory only in compressed form.

*Compression and the InnoDB Redo Log Files*

Before a compressed page is written to a data file, MySQL writes a copy
of the page to the redo log (if it has been recompressed since the last
time it was written to the database).  This is done to ensure that redo
logs are usable for crash recovery, even in the unlikely case that the
'zlib' library is upgraded and that change introduces a compatibility
problem with the compressed data.  Therefore, some increase in the size
of log files, or a need for more frequent checkpoints, can be expected
when using compression.  The amount of increase in the log file size or
checkpoint frequency depends on the number of times compressed pages are
modified in a way that requires reorganization and recompression.

Note that compressed tables use a different file format for the redo log
and the per-table tablespaces than in MySQL 5.1 and earlier.  The MySQL
Enterprise Backup product supports this latest Barracuda file format for
compressed InnoDB tables.


File: manual.info.tmp,  Node: innodb-compression-syntax-warnings,  Prev: innodb-compression-internals,  Up: innodb-compression

14.12.6 SQL Compression Syntax Warnings and Errors
--------------------------------------------------

Specifying 'ROW_FORMAT=COMPRESSED' or 'KEY_BLOCK_SIZE' in *note 'CREATE
TABLE': create-table. or *note 'ALTER TABLE': alter-table. statements
produces the following warnings if the Barracuda file format is not
enabled.  You can view them with the 'SHOW WARNINGS' statement.

Level          Code           Message
                              
Warning        1478           'InnoDB: KEY_BLOCK_SIZE requires
                              innodb_file_per_table.'
                              
Warning        1478           'InnoDB: KEY_BLOCK_SIZE requires
                              innodb_file_format=1'
                              
Warning        1478           'InnoDB: ignoring KEY_BLOCK_SIZE=4.'
                              
Warning        1478           'InnoDB: ROW_FORMAT=COMPRESSED requires
                              innodb_file_per_table.'
                              
Warning        1478           'InnoDB: assuming ROW_FORMAT=COMPACT.'
               

Notes:

   * By default, these messages are only warnings, not errors, and the
     table is created without compression, as if the options were not
     specified.

   * When 'innodb_strict_mode' is enabled, MySQL generates an error, not
     a warning, for these cases.  The table is not created if the
     current configuration does not permit using compressed tables.

The 'non-strict' behavior lets you import a 'mysqldump' file into a
database that does not support compressed tables, even if the source
database contained compressed tables.  In that case, MySQL creates the
table in 'ROW_FORMAT=COMPACT' instead of preventing the operation.

To import the dump file into a new database, and have the tables
re-created as they exist in the original database, ensure the server has
the proper settings for the configuration parameters
'innodb_file_format' and 'innodb_file_per_table'.

The attribute 'KEY_BLOCK_SIZE' is permitted only when 'ROW_FORMAT' is
specified as 'COMPRESSED' or is omitted.  Specifying a 'KEY_BLOCK_SIZE'
with any other 'ROW_FORMAT' generates a warning that you can view with
'SHOW WARNINGS'.  However, the table is non-compressed; the specified
'KEY_BLOCK_SIZE' is ignored).

Level                                       Code                          Message
                                                                          
Warning                                     1478                          ' InnoDB: ignoring
                                                                          KEY_BLOCK_SIZE=N unless
                                                                          ROW_FORMAT=COMPRESSED. '

If you are running with 'innodb_strict_mode' enabled, the combination of
a 'KEY_BLOCK_SIZE' with any 'ROW_FORMAT' other than 'COMPRESSED'
generates an error, not a warning, and the table is not created.

*note innodb-compression-create-and-alter-options-table:: provides an
overview the 'ROW_FORMAT' and 'KEY_BLOCK_SIZE' options that are used
with *note 'CREATE TABLE': create-table. or *note 'ALTER TABLE':
alter-table.

*ROW_FORMAT and KEY_BLOCK_SIZE Options*

Option         Usage Notes                   Description
                                             
'ROW_FORMAT=REDUNDANT'Storage format used priorLess efficient than
               to MySQL 5.0.3                'ROW_FORMAT=COMPACT'; for
                                             backward compatibility
                                             
'ROW_FORMAT=COMPACT'Default storage format  Stores a prefix of 768
               since MySQL 5.0.3             bytes of long column values
                                             in the clustered index
                                             page, with the remaining
                                             bytes stored in an overflow
                                             page
                                             
'ROW_FORMAT=DYNAMIC'Available only with     Store values within the
               'innodb_file_format=Barracuda'clustered index page if
                                             they fit; if not, stores
                                             only a 20-byte pointer to
                                             an overflow page (no
                                             prefix)
                                             
'ROW_FORMAT=COMPRESSED'Available only with  Compresses the table and
               'innodb_file_format=Barracuda'indexes using zlib to
                                             default compressed page
                                             size of 8K bytes
                                             
'KEY_BLOCK_SIZE=N'Available only with       Specifies compressed page
               'innodb_file_format=Barracuda'size of 1, 2, 4, 8 or 16
                                             kilobytes; implies
                                             'ROW_FORMAT=COMPRESSED'

*note innodb-compression-create-and-alter-errors-table:: summarizes
error conditions that occur with certain combinations of configuration
parameters and options on the *note 'CREATE TABLE': create-table. or
*note 'ALTER TABLE': alter-table. statements, and how the options appear
in the output of 'SHOW TABLE STATUS'.

When 'innodb_strict_mode' is 'OFF', InnoDB creates or alters the table,
but ignores certain settings as shown below.  You can see the warning
messages in the MySQL error log.  When 'innodb_strict_mode' is 'ON',
these specified combinations of options generate errors, and the table
is not created or altered.  To see the full description of the error
condition, issue the 'SHOW ERRORS' statement: example:

     mysql> CREATE TABLE x (id INT PRIMARY KEY, c INT)

     -> ENGINE=INNODB KEY_BLOCK_SIZE=33333;

     ERROR 1005 (HY000): Can't create table 'test.x' (errno: 1478)

     mysql> SHOW ERRORS;
     +-------+------+-------------------------------------------+
     | Level | Code | Message                                   |
     +-------+------+-------------------------------------------+
     | Error | 1478 | InnoDB: invalid KEY_BLOCK_SIZE=33333.     |
     | Error | 1005 | Can't create table 'test.x' (errno: 1478) |
     +-------+------+-------------------------------------------+

*CREATE/ALTER TABLE Warnings and Errors when InnoDB Strict Mode is OFF*

Syntax                   Warning or Error         Resulting
                         Condition                'ROW_FORMAT', as shown
                                                  in 'SHOW TABLE STATUS'
                                                  
'ROW_FORMAT=REDUNDANT'   None                     'REDUNDANT'
                                                  
'ROW_FORMAT=COMPACT'     None                     'COMPACT'
                                                  
'ROW_FORMAT=COMPRESSED'  Ignored unless both      'COMPACT'
or                       'innodb_file_format''=Barracuda'
'ROW_FORMAT=DYNAMIC'     and
or 'KEY_BLOCK_SIZE' is   'innodb_file_per_table'
specified                are enabled
                         
Invalid                  'KEY_BLOCK_SIZE' is      the specified row
'KEY_BLOCK_SIZE' is      ignored                  format, or 'COMPACT'
specified (not 1, 2,                              by default
4, 8 or 16)                                       

'ROW_FORMAT=COMPRESSED'  None; 'KEY_BLOCK_SIZE'   'COMPRESSED'
and valid                specified is used, not   
'KEY_BLOCK_SIZE' are     the 8K default
specified                

'KEY_BLOCK_SIZE' is      'KEY_BLOCK_SIZE' is      'REDUNDANT', 'COMPACT'
specified with           ignored                  or 'DYNAMIC'
'REDUNDANT', 'COMPACT'                            
or 'DYNAMIC' row
format

'ROW_FORMAT' is not      Ignored if recognized    'COMPACT' or N/A
one of 'REDUNDANT',      by the MySQL parser.
'COMPACT', 'DYNAMIC'     Otherwise, an error is
or 'COMPRESSED'          issued.
                         

When 'innodb_strict_mode' is 'ON', the InnoDB storage engine rejects
invalid 'ROW_FORMAT' or 'KEY_BLOCK_SIZE' parameters.  For compatibility
with earlier versions of MySQL, strict mode is not enabled by default;
instead, MySQL issues warnings (not errors) for ignored invalid
parameters.

Note that it is not possible to see the chosen 'KEY_BLOCK_SIZE' using
'SHOW TABLE STATUS'.  The statement 'SHOW CREATE TABLE' displays the
'KEY_BLOCK_SIZE' (even if it was ignored when creating the table).  The
real compressed page size of the table cannot be displayed by MySQL.


File: manual.info.tmp,  Node: innodb-file-format,  Next: innodb-row-format,  Prev: innodb-compression,  Up: innodb-storage-engine

14.13 InnoDB File-Format Management
===================================

* Menu:

* innodb-file-format-enabling::  Enabling File Formats
* innodb-file-format-compatibility::  Verifying File Format Compatibility
* innodb-file-format-identifying::  Identifying the File Format in Use
* innodb-file-format-downgrading::  Downgrading the File Format

As 'InnoDB' evolves, data file formats that are not compatible with
prior versions of 'InnoDB' are sometimes required to support new
features.  To help manage compatibility in upgrade and downgrade
situations, and systems that run different versions of MySQL, 'InnoDB'
uses named file formats.  'InnoDB' currently supports two named file
formats, Antelope and Barracuda.

   * Antelope is the original 'InnoDB' file format, which previously did
     not have a name.  It supports COMPACT and REDUNDANT row formats for
     'InnoDB' tables and is the default file format in MySQL 5.5 to
     ensure maximum compatibility with earlier MySQL versions that do
     not support the Barracuda file format.

   * Barracuda is the newest file format.  It supports all 'InnoDB' row
     formats including the newer COMPRESSED and DYNAMIC row formats.
     The features associated with COMPRESSED and DYNAMIC row formats
     include compressed tables and efficient storage of off-page
     columns.  See *note innodb-row-format::.

This section discusses enabling 'InnoDB' file formats, verifying
compatibility of different file formats between MySQL releases,
identifying the file format in use, and downgrading the file format.


File: manual.info.tmp,  Node: innodb-file-format-enabling,  Next: innodb-file-format-compatibility,  Prev: innodb-file-format,  Up: innodb-file-format

14.13.1 Enabling File Formats
-----------------------------

The 'innodb_file_format' configuration option enables an 'InnoDB' file
format for file-per-table tablespaces.

'Antelope' is the default 'innodb_file_format'.

To preclude the use of features supported by the Barracuda file that
make your database inaccessible to the built-in 'InnoDB' in MySQL 5.1
and prior releases, set 'innodb_file_format' to 'Antelope'.
Alternatively, you can disable 'innodb_file_per_table' to have new
tables created in the system tablespace.  The system tablespace is
stored in the original Antelope file format.

You can set the value of 'innodb_file_format' on the command line when
you start *note 'mysqld': mysqld, or in the option file ('my.cnf' on
Unix, 'my.ini' on Windows).  You can also change it dynamically with a
'SET GLOBAL' statement.

     SET GLOBAL innodb_file_format=Barracuda;

Although Oracle recommends using the Barracuda format for new tables
where practical, in MySQL 5.5 the default file format is Antelope, for
maximum compatibility with replication configurations containing earlier
MySQL releases.


File: manual.info.tmp,  Node: innodb-file-format-compatibility,  Next: innodb-file-format-identifying,  Prev: innodb-file-format-enabling,  Up: innodb-file-format

14.13.2 Verifying File Format Compatibility
-------------------------------------------

* Menu:

* innodb-file-format-compatibility-checking::  Compatibility Check When InnoDB Is Started
* innodb-file-format-compatibility-checking-table-access::  Compatibility Check When a Table Is Opened

InnoDB incorporates several checks to guard against the possible crashes
and data corruptions that might occur if you run an old release of the
MySQL server on InnoDB data files that use a newer file format.  These
checks take place when the server is started, and when you first access
a table.  This section describes these checks, how you can control them,
and error and warning conditions that might arise.

*Backward Compatibility*

You only need to consider backward file format compatibility when using
a recent version of InnoDB (the InnoDB Plugin, or MySQL 5.5 and higher
with InnoDB) alongside an older version (MySQL 5.1 or earlier, with the
built-in InnoDB rather than the InnoDB Plugin).  To minimize the chance
of compatibility issues, you can standardize on the InnoDB Plugin for
all your MySQL 5.1 and earlier database servers.

In general, a newer version of InnoDB may create a table or index that
cannot safely be read or written with an older version of InnoDB without
risk of crashes, hangs, wrong results or corruptions.  MySQL 5.5 and
higher with InnoDB includes a mechanism to guard against these
conditions, and to help preserve compatibility among database files and
versions of InnoDB. This mechanism lets you take advantage of some new
features of an InnoDB release (such as performance improvements and bug
fixes), and still preserve the option of using your database with a
prior version of InnoDB, by preventing accidental use of new features
that create downward-incompatible disk files.

If a version of InnoDB supports a particular file format (whether or not
that format is the default), you can query and update any table that
requires that format or an earlier format.  Only the creation of new
tables using new features is limited based on the particular file format
enabled.  Conversely, if a tablespace contains a table or index that
uses a file format that is not supported, it cannot be accessed at all,
even for read access.

The only way to 'downgrade' an InnoDB tablespace to the earlier Antelope
file format is to copy the data to a new table, in a tablespace that
uses the earlier format.  This can be done with the *note 'ALTER TABLE':
alter-table. statement, as described in *note
innodb-file-format-downgrading::.

The easiest way to determine the file format of an existing InnoDB
tablespace is to examine the properties of the table it contains, using
the 'SHOW TABLE STATUS' command or querying the table
'INFORMATION_SCHEMA.TABLES'.  If the 'Row_format' of the table is
reported as ''Compressed'' or ''Dynamic'', the tablespace containing the
table uses the Barracuda format.  Otherwise, it uses the prior InnoDB
file format, Antelope.

*Internal Details*

Every InnoDB file-per-table tablespace (represented by a '*.ibd' file)
file is labeled with a file format identifier.  The system tablespace
(represented by the 'ibdata' files) is tagged with the 'highest' file
format in use in a group of InnoDB database files, and this tag is
checked when the files are opened.

Creating a compressed table, or a table with 'ROW_FORMAT=DYNAMIC',
updates the file header of the corresponding file-per-table '.ibd' file
and the table type in the InnoDB data dictionary with the identifier for
the Barracuda file format.  From that point forward, the table cannot be
used with a version of InnoDB that does not support the Barracuda file
format.  To protect against anomalous behavior, InnoDB version 5.0.21
and later performs a compatibility check when the table is opened.  (In
many cases, the *note 'ALTER TABLE': alter-table. statement recreates a
table and thus changes its properties.  The special case of adding or
dropping indexes without rebuilding the table is described in *note
innodb-create-index::.)

*Definition of ib-file set*

To avoid confusion, for the purposes of this discussion we define the
term 'ib-file set' to mean the set of operating system files that InnoDB
manages as a unit.  The ib-file set includes the following files:

   * The system tablespace (one or more 'ibdata' files) that contain
     internal system information (including internal catalogs and undo
     information) and may include user data and indexes.

   * Zero or more single-table tablespaces (also called 'file per table'
     files, named '*.ibd' files).

   * InnoDB log files; usually two, 'ib_logfile0' and 'ib_logfile1'.
     Used for crash recovery and in backups.

An 'ib-file set' does not include the corresponding '.frm' files that
contain metadata about InnoDB tables.  The '.frm' files are created and
managed by MySQL, and can sometimes get out of sync with the internal
metadata in InnoDB.

Multiple tables, even from more than one database, can be stored in a
single 'ib-file set'.  (In MySQL, a 'database' is a logical collection
of tables, what other systems refer to as a 'schema' or 'catalog'.)


File: manual.info.tmp,  Node: innodb-file-format-compatibility-checking,  Next: innodb-file-format-compatibility-checking-table-access,  Prev: innodb-file-format-compatibility,  Up: innodb-file-format-compatibility

14.13.2.1 Compatibility Check When InnoDB Is Started
....................................................

To prevent possible crashes or data corruptions when InnoDB opens an
ib-file set, it checks that it can fully support the file formats in use
within the ib-file set.  If the system is restarted following a crash,
or a 'fast shutdown' (i.e., 'innodb_fast_shutdown' is greater than
zero), there may be on-disk data structures (such as redo or undo
entries, or doublewrite pages) that are in a 'too-new' format for the
current software.  During the recovery process, serious damage can be
done to your data files if these data structures are accessed.  The
startup check of the file format occurs before any recovery process
begins, thereby preventing consistency issues with the new tables or
startup problems for the MySQL server.

Beginning with version InnoDB 1.0.1, the system tablespace records an
identifier or tag for the 'highest' file format used by any table in any
of the tablespaces that is part of the ib-file set.  Checks against this
file format tag are controlled by the configuration parameter
'innodb_file_format_check', which is 'ON' by default.

If the file format tag in the system tablespace is newer or higher than
the highest version supported by the particular currently executing
software and if 'innodb_file_format_check' is 'ON', the following error
is issued when the server is started:

     InnoDB: Error: the system tablespace is in a
     file format that this version doesn't support

You can also set 'innodb_file_format' to a file format name.  Doing so
prevents InnoDB from starting if the current software does not support
the file format specified.  It also sets the 'high water mark' to the
value you specify.  The ability to set 'innodb_file_format_check' is
useful (with future releases) if you manually 'downgrade' all of the
tables in an ib-file set (as described in *note innodb-downgrading::).
You can then rely on the file format check at startup if you
subsequently use an older version of InnoDB to access the ib-file set.

In some limited circumstances, you might want to start the server and
use an ib-file set that is in a new file format that is not supported by
the software you are using.  If you set the configuration parameter
'innodb_file_format_check' to 'OFF', InnoDB opens the database, but
issues this warning message in the error log:

     InnoDB: Warning: the system tablespace is in a
     file format that this version doesn't support

*Note*:

This is a dangerous setting, as it permits the recovery process to run,
possibly corrupting your database if the previous shutdown was a crash
or 'fast shutdown'.  You should only set 'innodb_file_format_check' to
'OFF' if you are sure that the previous shutdown was done with
'innodb_fast_shutdown=0', so that essentially no recovery process
occurs.

The parameter 'innodb_file_format_check' affects only what happens when
a database is opened, not subsequently.  Conversely, the parameter
'innodb_file_format' (which enables a specific format) only determines
whether or not a new table can be created in the enabled format and has
no effect on whether or not a database can be opened.

The file format tag is a 'high water mark', and as such it is increased
after the server is started, if a table in a 'higher' format is created
or an existing table is accessed for read or write (assuming its format
is supported).  If you access an existing table in a format higher than
the format the running software supports, the system tablespace tag is
not updated, but table-level compatibility checking applies (and an
error is issued), as described in *note
innodb-file-format-compatibility-checking-table-access::.  Any time the
high water mark is updated, the value of 'innodb_file_format_check' is
updated as well, so the command 'SELECT @@innodb_file_format_check;'
displays the name of the latest file format known to be used by tables
in the currently open ib-file set and supported by the currently
executing software.


File: manual.info.tmp,  Node: innodb-file-format-compatibility-checking-table-access,  Prev: innodb-file-format-compatibility-checking,  Up: innodb-file-format-compatibility

14.13.2.2 Compatibility Check When a Table Is Opened
....................................................

When a table is first accessed, InnoDB (including some releases prior to
InnoDB 1.0) checks that the file format of the tablespace in which the
table is stored is fully supported.  This check prevents crashes or
corruptions that would otherwise occur when tables using a 'too new'
data structure are encountered.

All tables using any file format supported by a release can be read or
written (assuming the user has sufficient privileges).  The setting of
the system configuration parameter 'innodb_file_format' can prevent
creating a new table that uses a specific file format, even if the file
format is supported by a given release.  Such a setting might be used to
preserve backward compatibility, but it does not prevent accessing any
table that uses a supported format.

Versions of MySQL older than 5.0.21 cannot reliably use database files
created by newer versions if a new file format was used when a table was
created.  To prevent various error conditions or corruptions, InnoDB
checks file format compatibility when it opens a file (for example, upon
first access to a table).  If the currently running version of InnoDB
does not support the file format identified by the table type in the
InnoDB data dictionary, MySQL reports the following error:

     ERROR 1146 (42S02): Table 'TEST.T1' doesn't exist

InnoDB also writes a message to the error log:

     InnoDB: table TEST/T1: unknown table type 33

The table type should be equal to the tablespace flags, which contains
the file format version as discussed in *note
innodb-file-format-identifying::.

Versions of InnoDB prior to MySQL 4.1 did not include table format
identifiers in the database files, and versions prior to MySQL 5.0.21
did not include a table format compatibility check.  Therefore, there is
no way to ensure proper operations if a table in a newer file format is
used with versions of InnoDB prior to 5.0.21.

The file format management capability in InnoDB 1.0 and higher
(tablespace tagging and run-time checks) allows InnoDB to verify as soon
as possible that the running version of software can properly process
the tables existing in the database.

If you permit InnoDB to open a database containing files in a format it
does not support (by setting the parameter 'innodb_file_format_check' to
'OFF'), the table-level checking described in this section still
applies.

Users are _strongly_ urged not to use database files that contain
Barracuda file format tables with releases of InnoDB older than the
MySQL 5.1 with the InnoDB Plugin.  It is possible to 'downgrade' such
tables to the Antelope format with the procedure described in *note
innodb-file-format-downgrading::.


File: manual.info.tmp,  Node: innodb-file-format-identifying,  Next: innodb-file-format-downgrading,  Prev: innodb-file-format-compatibility,  Up: innodb-file-format

14.13.3 Identifying the File Format in Use
------------------------------------------

If you enable a different file format using the 'innodb_file_format'
configuration option, the change only applies to newly created tables.
Also, when you create a new table, the tablespace containing the table
is tagged with the 'earliest' or 'simplest' file format that is required
to support the table's features.  For example, if you enable the
'Barracuda' file format, and create a new table that does not use the
Dynamic or Compressed row format, the new tablespace that contains the
table is tagged as using the 'Antelope' file format .

It is easy to identify the file format used by a given table.  The table
uses the 'Antelope' file format if the row format reported by 'SHOW
TABLE STATUS' is either 'Compact' or 'Redundant'.  The table uses the
'Barracuda' file format if the row format reported by 'SHOW TABLE
STATUS' is either 'Compressed' or 'Dynamic'.

     mysql> SHOW TABLE STATUS\G
     *************************** 1. row ***************************
                Name: t1
              Engine: InnoDB
             Version: 10
          Row_format: Compact
                Rows: 0
      Avg_row_length: 0
         Data_length: 16384
     Max_data_length: 0
        Index_length: 16384
           Data_free: 0
      Auto_increment: 1
         Create_time: 2014-11-03 13:32:10
         Update_time: NULL
          Check_time: NULL
           Collation: latin1_swedish_ci
            Checksum: NULL
      Create_options:
             Comment:


File: manual.info.tmp,  Node: innodb-file-format-downgrading,  Prev: innodb-file-format-identifying,  Up: innodb-file-format

14.13.4 Downgrading the File Format
-----------------------------------

Each InnoDB tablespace file (with a name matching '*.ibd') is tagged
with the file format used to create its table and indexes.  The way to
downgrade the tablespace is to re-create the table and its indexes.  The
easiest way to recreate a table and its indexes is to use the command:

     ALTER TABLE T ROW_FORMAT=COMPACT;

on each table that you want to downgrade.  The 'COMPACT' row format uses
the file format Antelope.  It was introduced in MySQL 5.0.3.


File: manual.info.tmp,  Node: innodb-row-format,  Next: innodb-disk-management,  Prev: innodb-file-format,  Up: innodb-storage-engine

14.14 InnoDB Row Formats
========================

The row format of a table determines how its rows are physically stored,
which in turn can affect the performance of queries and DML operations.
As more rows fit into a single disk page, queries and index lookups can
work faster, less cache memory is required in the buffer pool, and less
I/O is required to write out updated values.

The data in each table is divided into pages.  The pages that make up
each table are arranged in a tree data structure called a B-tree index.
Table data and secondary indexes both use this type of structure.  The
B-tree index that represents an entire table is known as the clustered
index, which is organized according to the primary key columns.  The
nodes of a clustered index data structure contain the values of all
columns in the row.  The nodes of a secondary index structure contain
the values of index columns and primary key columns.

Variable-length columns are an exception to the rule that column values
are stored in B-tree index nodes.  Variable-length columns that are too
long to fit on a B-tree page are stored on separately allocated disk
pages called overflow pages.  Such columns are referred to as off-page
columns.  The values of off-page columns are stored in singly-linked
lists of overflow pages, with each such column having its own list of
one or more overflow pages.  Depending on column length, all or a prefix
of variable-length column values are stored in the B-tree to avoid
wasting storage and having to read a separate page.

The 'InnoDB' storage engine supports four row formats: 'REDUNDANT',
'COMPACT', 'DYNAMIC', and 'COMPRESSED'.

*InnoDB Row Format Overview*

Row     Compact     Enhanced    Large       Compression Supported   Required
Format  Storage     Variable-LengthIndex KeySupport     Tablespace  File
        CharacteristicsColumn   Prefix                  Types       Format
                    Storage     Support                             
                                
'REDUNDANT'No       No          No          No          system,     Antelope
                                                        file-per-tableor
                                                                    Barracuda
                                                                    
'COMPACT'Yes        No          No          No          system,     Antelope
                                                        file-per-tableor
                                                                    Barracuda
                                                                    
'DYNAMIC'Yes        Yes         Yes         No          file-per-tableBarracuda
                                                                    
'COMPRESSED'Yes     Yes         Yes         Yes         file-per-tableBarracuda
                                                        

The topics that follow describe row format storage characteristics and
how to define and determine the row format of a table.

   * *note innodb-row-format-redundant::

   * *note innodb-row-format-compact::

   * *note innodb-row-format-dynamic::

   * *note innodb-row-format-compressed::

   * *note innodb-row-format-defining::

   * *note innodb-row-format-detrmining::

*REDUNDANT Row Format*

The 'REDUNDANT' format provides compatibility with older versions of
MySQL.

The 'REDUNDANT' row format is supported by both 'InnoDB' file formats
('Antelope' and 'Barracuda').  For more information, see *note
innodb-file-format::.

Tables that use the 'REDUNDANT' row format store the first 768 bytes of
variable-length column values (*note 'VARCHAR': char, *note 'VARBINARY':
binary-varbinary, and *note 'BLOB': blob. and *note 'TEXT': blob. types)
in the index record within the B-tree node, with the remainder stored on
overflow pages.  Fixed-length columns greater than or equal to 768 bytes
are encoded as variable-length columns, which can be stored off-page.
For example, a 'CHAR(255)' column can exceed 768 bytes if the maximum
byte length of the character set is greater than 3, as it is with
'utf8mb4'.

If the value of a column is 768 bytes or less, an overflow page is not
used, and some savings in I/O may result, since the value is stored
entirely in the B-tree node.  This works well for relatively short
'BLOB' column values, but may cause B-tree nodes to fill with data
rather than key values, reducing their efficiency.  Tables with many
'BLOB' columns could cause B-tree nodes to become too full, and contain
too few rows, making the entire index less efficient than if rows were
shorter or column values were stored off-page.

*REDUNDANT Row Format Storage Characteristics*

The 'REDUNDANT' row format has the following storage characteristics:

   * Each index record contains a 6-byte header.  The header is used to
     link together consecutive records, and for row-level locking.

   * Records in the clustered index contain fields for all user-defined
     columns.  In addition, there is a 6-byte transaction ID field and a
     7-byte roll pointer field.

   * If no primary key is defined for a table, each clustered index
     record also contains a 6-byte row ID field.

   * Each secondary index record contains all the primary key columns
     defined for the clustered index key that are not in the secondary
     index.

   * A record contains a pointer to each field of the record.  If the
     total length of the fields in a record is less than 128 bytes, the
     pointer is one byte; otherwise, two bytes.  The array of pointers
     is called the record directory.  The area where the pointers point
     is the data part of the record.

   * Internally, fixed-length character columns such as *note
     'CHAR(10)': char. are stored in fixed-length format.  Trailing
     spaces are not truncated from *note 'VARCHAR': char. columns.

   * Fixed-length columns greater than or equal to 768 bytes are encoded
     as variable-length columns, which can be stored off-page.  For
     example, a 'CHAR(255)' column can exceed 768 bytes if the maximum
     byte length of the character set is greater than 3, as it is with
     'utf8mb4'.

   * An SQL 'NULL' value reserves one or two bytes in the record
     directory.  An SQL 'NULL' value reserves zero bytes in the data
     part of the record if stored in a variable-length column.  For a
     fixed-length column, the fixed length of the column is reserved in
     the data part of the record.  Reserving fixed space for 'NULL'
     values permits columns to be updated in place from 'NULL' to
     non-'NULL' values without causing index page fragmentation.

*COMPACT Row Format*

The 'COMPACT' row format reduces row storage space by about 20% compared
to the 'REDUNDANT' row format, at the cost of increasing CPU use for
some operations.  If your workload is a typical one that is limited by
cache hit rates and disk speed, 'COMPACT' format is likely to be faster.
If the workload is limited by CPU speed, compact format might be slower.

The 'COMPACT' row format is supported by both 'InnoDB' file formats
('Antelope' and 'Barracuda').  For more information, see *note
innodb-file-format::.

Tables that use the 'COMPACT' row format store the first 768 bytes of
variable-length column values (*note 'VARCHAR': char, *note 'VARBINARY':
binary-varbinary, and *note 'BLOB': blob. and *note 'TEXT': blob. types)
in the index record within the B-tree node, with the remainder stored on
overflow pages.  Fixed-length columns greater than or equal to 768 bytes
are encoded as variable-length columns, which can be stored off-page.
For example, a 'CHAR(255)' column can exceed 768 bytes if the maximum
byte length of the character set is greater than 3, as it is with
'utf8mb4'.

If the value of a column is 768 bytes or less, an overflow page is not
used, and some savings in I/O may result, since the value is stored
entirely in the B-tree node.  This works well for relatively short
'BLOB' column values, but may cause B-tree nodes to fill with data
rather than key values, reducing their efficiency.  Tables with many
'BLOB' columns could cause B-tree nodes to become too full, and contain
too few rows, making the entire index less efficient than if rows were
shorter or column values were stored off-page.

*COMPACT Row Format Storage Characteristics*

The 'COMPACT' row format has the following storage characteristics:

   * Each index record contains a 5-byte header that may be preceded by
     a variable-length header.  The header is used to link together
     consecutive records, and for row-level locking.

   * The variable-length part of the record header contains a bit vector
     for indicating 'NULL' columns.  If the number of columns in the
     index that can be 'NULL' is N, the bit vector occupies
     'CEILING(N/8)' bytes.  (For example, if there are anywhere from 9
     to 16 columns that can be 'NULL', the bit vector uses two bytes.)
     Columns that are 'NULL' do not occupy space other than the bit in
     this vector.  The variable-length part of the header also contains
     the lengths of variable-length columns.  Each length takes one or
     two bytes, depending on the maximum length of the column.  If all
     columns in the index are 'NOT NULL' and have a fixed length, the
     record header has no variable-length part.

   * For each non-'NULL' variable-length field, the record header
     contains the length of the column in one or two bytes.  Two bytes
     are only needed if part of the column is stored externally in
     overflow pages or the maximum length exceeds 255 bytes and the
     actual length exceeds 127 bytes.  For an externally stored column,
     the 2-byte length indicates the length of the internally stored
     part plus the 20-byte pointer to the externally stored part.  The
     internal part is 768 bytes, so the length is 768+20.  The 20-byte
     pointer stores the true length of the column.

   * The record header is followed by the data contents of non-'NULL'
     columns.

   * Records in the clustered index contain fields for all user-defined
     columns.  In addition, there is a 6-byte transaction ID field and a
     7-byte roll pointer field.

   * If no primary key is defined for a table, each clustered index
     record also contains a 6-byte row ID field.

   * Each secondary index record contains all the primary key columns
     defined for the clustered index key that are not in the secondary
     index.  If any of the primary key columns are variable length, the
     record header for each secondary index has a variable-length part
     to record their lengths, even if the secondary index is defined on
     fixed-length columns.

   * Internally, for nonvariable-length character sets, fixed-length
     character columns such as *note 'CHAR(10)': char. are stored in a
     fixed-length format.

     Trailing spaces are not truncated from *note 'VARCHAR': char.
     columns.

   * Internally, for variable-length character sets such as 'utf8mb3'
     and 'utf8mb4', 'InnoDB' attempts to store *note 'CHAR(N)': char. in
     N bytes by trimming trailing spaces.  If the byte length of a *note
     'CHAR(N)': char. column value exceeds N bytes, trailing spaces are
     trimmed to a minimum of the column value byte length.  The maximum
     length of a *note 'CHAR(N)': char. column is the maximum character
     byte length x N.

     A minimum of N bytes is reserved for *note 'CHAR(N)': char.
     Reserving the minimum space N in many cases enables column updates
     to be done in place without causing index page fragmentation.  By
     comparison, *note 'CHAR(N)': char. columns occupy the maximum
     character byte length x N when using the 'REDUNDANT' row format.

     Fixed-length columns greater than or equal to 768 bytes are encoded
     as variable-length fields, which can be stored off-page.  For
     example, a 'CHAR(255)' column can exceed 768 bytes if the maximum
     byte length of the character set is greater than 3, as it is with
     'utf8mb4'.

*DYNAMIC Row Format*

The 'DYNAMIC' row format offers the same storage characteristics as the
'COMPACT' row format but adds enhanced storage capabilities for long
variable-length columns and supports large index key prefixes.

The 'DYNAMIC' row format is supported by the 'Barracuda' file format.
For more information, see *note innodb-file-format::.  To create tables
that use the 'DYNAMIC' row format, the 'innodb_file_format' variable
must be set to 'Barracuda', and the 'innodb_file_per_table' variable
must be enabled.

When a table is created with 'ROW_FORMAT=DYNAMIC', 'InnoDB' can store
long variable-length column values (for *note 'VARCHAR': char, *note
'VARBINARY': binary-varbinary, and *note 'BLOB': blob. and *note 'TEXT':
blob. types) fully off-page, with the clustered index record containing
only a 20-byte pointer to the overflow page.  Fixed-length fields
greater than or equal to 768 bytes are encoded as variable-length
fields.  For example, a 'CHAR(255)' column can exceed 768 bytes if the
maximum byte length of the character set is greater than 3, as it is
with 'utf8mb4'.

Whether columns are stored off-page depends on the page size and the
total size of the row.  When a row is too long, the longest columns are
chosen for off-page storage until the clustered index record fits on the
B-tree page.  *note 'TEXT': blob. and *note 'BLOB': blob. columns that
are less than or equal to 40 bytes are stored in line.

The 'DYNAMIC' row format maintains the efficiency of storing the entire
row in the index node if it fits (as do the 'COMPACT' and 'REDUNDANT'
formats), but the 'DYNAMIC' row format avoids the problem of filling
B-tree nodes with a large number of data bytes of long columns.  The
'DYNAMIC' row format is based on the idea that if a portion of a long
data value is stored off-page, it is usually most efficient to store the
entire value off-page.  With 'DYNAMIC' format, shorter columns are
likely to remain in the B-tree node, minimizing the number of overflow
pages required for a given row.

The 'DYNAMIC' row format supports index key prefixes up to 3072 bytes.
This feature is controlled by the 'innodb_large_prefix' variable, which
is disabled by default.  See the 'innodb_large_prefix' variable
description for more information.

*DYNAMIC Row Format Storage Characteristics*

The 'DYNAMIC' row format is a variation of the 'COMPACT' row format.
For storage characteristics, see *note
innodb-compact-row-format-characteristics::.

*COMPRESSED Row Format*

The 'COMPRESSED' row format offers the same the storage characteristics
and capabilities as the 'DYNAMIC' row format but adds support for table
and index data compression.

The 'COMPRESSED' row format is supported by the 'Barracuda' file format.
For more information, see *note innodb-file-format::.  To create tables
that use the 'COMPRESSED' row format, the 'innodb_file_format' variable
must be set to 'Barracuda', and the 'innodb_file_per_table' variable
must be enabled.

The 'COMPRESSED' row format uses similar internal details for off-page
storage as the 'DYNAMIC' row format, with additional storage and
performance considerations from the table and index data being
compressed and using smaller page sizes.  With the 'COMPRESSED' row
format, the 'KEY_BLOCK_SIZE' option controls how much column data is
stored in the clustered index, and how much is placed on overflow pages.
For more information about the 'COMPRESSED' row format, see InnoDB Table
Compression
(https://dev.mysql.com/doc/refman/5.7/en/innodb-table-compression.html).

The 'COMPRESSED' row format supports index key prefixes up to 3072
bytes.  This feature is controlled by the 'innodb_large_prefix'
variable, which is disabled by default.  See the 'innodb_large_prefix'
variable description for more information.

*Compressed Row Format Storage Characteristics*

The 'COMPRESSED' row format is a variation of the 'COMPACT' row format.
For storage characteristics, see *note
innodb-compact-row-format-characteristics::.

*Defining the Row Format of a Table*

The default row format for 'InnoDB' tables is 'COMPACT'.

The row format of a table can be defined explicitly using the
'ROW_FORMAT' table option in a *note 'CREATE TABLE': create-table. or
*note 'ALTER TABLE': alter-table. statement.  For example:

     CREATE TABLE t1 (c1 INT) ROW_FORMAT=COMPACT;

'ROW_FORMAT' options include 'REDUNDANT', 'COMPACT', 'DYNAMIC', and
'COMPRESSED'.

To create tables that use the 'DYNAMIC' or 'COMPRESSED' row format, the
'innodb_file_format' variable must be set to 'Barracuda', and the
'innodb_file_per_table' variable must be enabled.  Otherwise, if
'innodb_strict_mode' is not enabled, 'InnoDB' tables are created with
the default 'COMPACT' row format.

Oracle recommends enabling 'innodb_strict_mode' when using the
'ROW_FORMAT' table option in *note 'CREATE TABLE': create-table. and
*note 'ALTER TABLE': alter-table. statements.

*Determining the Row Format of a Table*

To determine the row format of a table, use *note 'SHOW TABLE STATUS':
show-table-status.:

     mysql> SHOW TABLE STATUS IN test1\G
     *************************** 1. row ***************************
                Name: t1
              Engine: InnoDB
             Version: 10
          Row_format: Compact
                Rows: 0
      Avg_row_length: 0
         Data_length: 16384
     Max_data_length: 0
        Index_length: 16384
           Data_free: 0
      Auto_increment: 1
         Create_time: 2014-10-31 16:02:01
         Update_time: NULL
          Check_time: NULL
           Collation: latin1_swedish_ci
            Checksum: NULL
      Create_options:
             Comment:


File: manual.info.tmp,  Node: innodb-disk-management,  Next: innodb-create-index,  Prev: innodb-row-format,  Up: innodb-storage-engine

14.15 InnoDB Disk I/O and File Space Management
===============================================

* Menu:

* innodb-disk-io::               InnoDB Disk I/O
* innodb-file-space::            File Space Management
* innodb-checkpoints::           InnoDB Checkpoints
* innodb-file-defragmenting::    Defragmenting a Table
* innodb-truncate-table-reclaim-space::  Reclaiming Disk Space with TRUNCATE TABLE

As a DBA, you must manage disk I/O to keep the I/O subsystem from
becoming saturated, and manage disk space to avoid filling up storage
devices.  The ACID design model requires a certain amount of I/O that
might seem redundant, but helps to ensure data reliability.  Within
these constraints, 'InnoDB' tries to optimize the database work and the
organization of disk files to minimize the amount of disk I/O.
Sometimes, I/O is postponed until the database is not busy, or until
everything needs to be brought to a consistent state, such as during a
database restart after a fast shutdown.

This section discusses the main considerations for I/O and disk space
with the default kind of MySQL tables (also known as 'InnoDB' tables):

   * Controlling the amount of background I/O used to improve query
     performance.

   * Enabling or disabling features that provide extra durability at the
     expense of additional I/O.

   * Organizing tables into many small files, a few larger files, or a
     combination of both.

   * Balancing the size of redo log files against the I/O activity that
     occurs when the log files become full.

   * How to reorganize a table for optimal query performance.


File: manual.info.tmp,  Node: innodb-disk-io,  Next: innodb-file-space,  Prev: innodb-disk-management,  Up: innodb-disk-management

14.15.1 InnoDB Disk I/O
-----------------------

'InnoDB' uses asynchronous disk I/O where possible, by creating a number
of threads to handle I/O operations, while permitting other database
operations to proceed while the I/O is still in progress.  On Linux and
Windows platforms, 'InnoDB' uses the available OS and library functions
to perform 'native' asynchronous I/O. On other platforms, 'InnoDB' still
uses I/O threads, but the threads may actually wait for I/O requests to
complete; this technique is known as 'simulated' asynchronous I/O.

*Read-Ahead*

If 'InnoDB' can determine there is a high probability that data might be
needed soon, it performs read-ahead operations to bring that data into
the buffer pool so that it is available in memory.  Making a few large
read requests for contiguous data can be more efficient than making
several small, spread-out requests.  There are two read-ahead heuristics
in 'InnoDB':

   * In sequential read-ahead, if 'InnoDB' notices that the access
     pattern to a segment in the tablespace is sequential, it posts in
     advance a batch of reads of database pages to the I/O system.

   * In random read-ahead, if 'InnoDB' notices that some area in a
     tablespace seems to be in the process of being fully read into the
     buffer pool, it posts the remaining reads to the I/O system.

For information about configuring read-ahead heuristics, see *note
innodb-performance-read_ahead::.

*Doublewrite Buffer*

'InnoDB' uses a novel file flush technique involving a structure called
the doublewrite buffer, which is enabled by default
('innodb_doublewrite=ON').  It adds safety to recovery following a crash
or power outage, and improves performance on most varieties of Unix by
reducing the need for 'fsync()' operations.

Before writing pages to a data file, 'InnoDB' first writes them to a
contiguous tablespace area called the doublewrite buffer.  Only after
the write and the flush to the doublewrite buffer has completed does
'InnoDB' write the pages to their proper positions in the data file.  If
there is an operating system, storage subsystem, or *note 'mysqld':
mysqld. process crash in the middle of a page write (causing a torn page
condition), 'InnoDB' can later find a good copy of the page from the
doublewrite buffer during recovery.


File: manual.info.tmp,  Node: innodb-file-space,  Next: innodb-checkpoints,  Prev: innodb-disk-io,  Up: innodb-disk-management

14.15.2 File Space Management
-----------------------------

The data files that you define in the configuration file using the
'innodb_data_file_path' configuration option form the 'InnoDB' system
tablespace.  The files are logically concatenated to form the system
tablespace.  There is no striping in use.  You cannot define where
within the system tablespace your tables are allocated.  In a newly
created system tablespace, 'InnoDB' allocates space starting from the
first data file.

To avoid the issues that come with storing all tables and indexes inside
the system tablespace, you can enable the 'innodb_file_per_table'
configuration option, which stores each newly created table in a
separate tablespace file (with extension '.ibd').  For tables stored
this way, there is less fragmentation within the disk file, and when the
table is truncated, the space is returned to the operating system rather
than still being reserved by InnoDB within the system tablespace.

*Pages, Extents, Segments, and Tablespaces*

Each tablespace consists of database pages with a default size of 16KB.
The pages are grouped into extents of size 1MB (64 consecutive pages).
The 'files' inside a tablespace are called segments in 'InnoDB'.  (These
segments are different from the rollback segment, which actually
contains many tablespace segments.)

When a segment grows inside the tablespace, 'InnoDB' allocates the first
32 pages to it one at a time.  After that, 'InnoDB' starts to allocate
whole extents to the segment.  'InnoDB' can add up to 4 extents at a
time to a large segment to ensure good sequentiality of data.

Two segments are allocated for each index in 'InnoDB'.  One is for
nonleaf nodes of the B-tree, the other is for the leaf nodes.  Keeping
the leaf nodes contiguous on disk enables better sequential I/O
operations, because these leaf nodes contain the actual table data.

Some pages in the tablespace contain bitmaps of other pages, and
therefore a few extents in an 'InnoDB' tablespace cannot be allocated to
segments as a whole, but only as individual pages.

When you ask for available free space in the tablespace by issuing a
*note 'SHOW TABLE STATUS': show-table-status. statement, 'InnoDB'
reports the extents that are definitely free in the tablespace.
'InnoDB' always reserves some extents for cleanup and other internal
purposes; these reserved extents are not included in the free space.

When you delete data from a table, 'InnoDB' contracts the corresponding
B-tree indexes.  Whether the freed space becomes available for other
users depends on whether the pattern of deletes frees individual pages
or extents to the tablespace.  Dropping a table or deleting all rows
from it is guaranteed to release the space to other users, but remember
that deleted rows are physically removed only by the purge operation,
which happens automatically some time after they are no longer needed
for transaction rollbacks or consistent reads.  (See *note
innodb-multi-versioning::.)

To see information about the tablespace, use the Tablespace Monitor.
See *note innodb-monitors::.

*How Pages Relate to Table Rows*

The maximum row length is slightly less than half a database page.  For
example, the maximum row length is slightly less than 8KB for the 16KB
'InnoDB' page size.

If a row does not exceed the half page limit, all of it is stored
locally within the page.  If a row exceeds the half page limit,
variable-length columns are chosen for external off-page storage until
the row fits within half a page.  External off-page storage for
variable-length columns differs by row format:

   * _COMPACT and REDUNDANT Row Formats_

     When a variable-length column is chosen for external off-page
     storage, 'InnoDB' stores the first 768 bytes locally in the row,
     and the rest externally into overflow pages.  Each such column has
     its own list of overflow pages.  The 768-byte prefix is accompanied
     by a 20-byte value that stores the true length of the column and
     points into the overflow list where the rest of the value is
     stored.  See *note innodb-row-format::.

   * _DYNAMIC and COMPRESSED Row Formats_

     When a variable-length column is chosen for external off-page
     storage, 'InnoDB' stores a 20-byte pointer locally in the row, and
     the rest externally into overflow pages.  See *note
     innodb-row-format::.

*note 'LONGBLOB': blob. and *note 'LONGTEXT': blob. columns must be less
than 4GB, and the total row length, including *note 'BLOB': blob. and
*note 'TEXT': blob. columns, must be less than 4GB.


File: manual.info.tmp,  Node: innodb-checkpoints,  Next: innodb-file-defragmenting,  Prev: innodb-file-space,  Up: innodb-disk-management

14.15.3 InnoDB Checkpoints
--------------------------

Making your log files very large may reduce disk I/O during
checkpointing.  It often makes sense to set the total size of the log
files as large as the buffer pool or even larger.  Although in the past
large log files could make crash recovery take excessive time, starting
with MySQL 5.5, performance enhancements to crash recovery make it
possible to use large log files with fast startup after a crash.
(Strictly speaking, this performance improvement is available for MySQL
5.1 with the InnoDB Plugin 1.0.7 and higher.  It is with MySQL 5.5 that
this improvement is available in the default InnoDB storage engine.)

*How Checkpoint Processing Works*

'InnoDB' implements a checkpoint mechanism known as fuzzy checkpointing.
'InnoDB' flushes modified database pages from the buffer pool in small
batches.  There is no need to flush the buffer pool in one single batch,
which would disrupt processing of user SQL statements during the
checkpointing process.

During crash recovery, 'InnoDB' looks for a checkpoint label written to
the log files.  It knows that all modifications to the database before
the label are present in the disk image of the database.  Then 'InnoDB'
scans the log files forward from the checkpoint, applying the logged
modifications to the database.

'InnoDB' writes to its log files on a rotating basis.  It also writes
checkpoint information to the first log file at each checkpoint.  All
committed modifications that make the database pages in the buffer pool
different from the images on disk must be available in the log files in
case 'InnoDB' has to do a recovery.  This means that when 'InnoDB'
starts to reuse a log file, it has to make sure that the database page
images on disk contain the modifications logged in the log file that
'InnoDB' is going to reuse.  In other words, 'InnoDB' must create a
checkpoint and this often involves flushing of modified database pages
to disk.


File: manual.info.tmp,  Node: innodb-file-defragmenting,  Next: innodb-truncate-table-reclaim-space,  Prev: innodb-checkpoints,  Up: innodb-disk-management

14.15.4 Defragmenting a Table
-----------------------------

Random insertions into or deletions from a secondary index can cause the
index to become fragmented.  Fragmentation means that the physical
ordering of the index pages on the disk is not close to the index
ordering of the records on the pages, or that there are many unused
pages in the 64-page blocks that were allocated to the index.

One symptom of fragmentation is that a table takes more space than it
'should' take.  How much that is exactly, is difficult to determine.
All 'InnoDB' data and indexes are stored in B-trees, and their fill
factor may vary from 50% to 100%.  Another symptom of fragmentation is
that a table scan such as this takes more time than it 'should' take:

     SELECT COUNT(*) FROM t WHERE NON_INDEXED_COLUMN <> 12345;

The preceding query requires MySQL to perform a full table scan, the
slowest type of query for a large table.

To speed up index scans, you can periodically perform a 'null' *note
'ALTER TABLE': alter-table. operation, which causes MySQL to rebuild the
table:

     ALTER TABLE TBL_NAME ENGINE=INNODB

Another way to perform a defragmentation operation is to use *note
'mysqldump': mysqldump. to dump the table to a text file, drop the
table, and reload it from the dump file.

If the insertions into an index are always ascending and records are
deleted only from the end, the 'InnoDB' filespace management algorithm
guarantees that fragmentation in the index does not occur.


File: manual.info.tmp,  Node: innodb-truncate-table-reclaim-space,  Prev: innodb-file-defragmenting,  Up: innodb-disk-management

14.15.5 Reclaiming Disk Space with TRUNCATE TABLE
-------------------------------------------------

To reclaim operating system disk space when truncating an 'InnoDB'
table, the table must be stored in its own .ibd file.  For a table to be
stored in its own .ibd file, 'innodb_file_per_table' must enabled when
the table is created.  Additionally, there cannot be a foreign key
constraint between the table being truncated and other tables, otherwise
the 'TRUNCATE TABLE' operation fails.  This is a change from previous
behavior, which would transform the 'TRUNCATE' operation to a *note
'DELETE': delete. operation that removes all rows and triggers 'ON
DELETE' operations on child tables.  A foreign key constraint between
two columns in the same table, however, is permitted.

When a table is truncated, it is dropped and re-created in a new '.ibd'
file (previous versions of 'InnoDB' would keep the existing '.idb'
file), and the freed space is returned to the operating system.  This is
in contrast to truncating 'InnoDB' tables that are stored within the
'InnoDB' system tablespace (tables created when
'innodb_file_per_table=OFF'), where only 'InnoDB' can use the freed
space after the table is truncated.

The ability to truncate tables and return disk space to the operating
system also means that physical backups can be smaller.  Truncating
tables that are stored in the system tablespace (tables created when
'innodb_file_per_table=OFF') leaves blocks of unused space in the system
tablespace.


File: manual.info.tmp,  Node: innodb-create-index,  Next: innodb-parameters,  Prev: innodb-disk-management,  Up: innodb-storage-engine

14.16 InnoDB Fast Index Creation
================================

* Menu:

* innodb-create-index-overview::  Overview of Fast Index Creation
* innodb-create-index-examples::  Examples of Fast Index Creation
* innodb-create-index-implementation::  Implementation Details of Fast Index Creation
* innodb-create-index-concurrency::  Concurrency Considerations for Fast Index Creation
* innodb-create-index-recovery::  How Crash Recovery Works with Fast Index Creation
* innodb-create-index-limitations::  Limitations of Fast Index Creation

In MySQL 5.5 and higher, or in MySQL 5.1 with the InnoDB Plugin,
creating and dropping secondary indexes does not copy the contents of
the entire table, making this operation much more efficient than with
prior releases.


File: manual.info.tmp,  Node: innodb-create-index-overview,  Next: innodb-create-index-examples,  Prev: innodb-create-index,  Up: innodb-create-index

14.16.1 Overview of Fast Index Creation
---------------------------------------

With MySQL 5.5 and higher, creating and dropping secondary indexes for
InnoDB tables is much faster than before.  Historically, adding or
dropping an index on a table with existing data could be very slow.  The
*note 'CREATE INDEX': create-index. and *note 'DROP INDEX': drop-index.
statements worked by creating a new, empty table defined with the
requested set of indexes, then copying the existing rows to the new
table one-by-one, updating the indexes as the rows are inserted.  After
all rows from the original table were copied, the old table was dropped
and the copy was renamed with the name of the original table.

The performance speedup for fast index creation applies to secondary
indexes, not to the primary key index.  The rows of an InnoDB table are
stored in a clustered index organized based on the primary key, forming
what some database systems call an 'index-organized table'.  Because the
table structure is so closely tied to the primary key, redefining the
primary key still requires copying the data.

This new mechanism also means that you can generally speed the overall
process of creating and loading an indexed table by creating the table
with only the clustered index, and adding the secondary indexes after
the data is loaded.

Although no syntax changes are required in the *note 'CREATE INDEX':
create-index. or *note 'DROP INDEX': drop-index. commands, some factors
affect the performance, space usage, and semantics of this operation
(see *note innodb-create-index-limitations::).


File: manual.info.tmp,  Node: innodb-create-index-examples,  Next: innodb-create-index-implementation,  Prev: innodb-create-index-overview,  Up: innodb-create-index

14.16.2 Examples of Fast Index Creation
---------------------------------------

It is possible to create multiple indexes on a table with one *note
'ALTER TABLE': alter-table. statement.  This is relatively efficient,
because the clustered index of the table needs to be scanned only once
(although the data is sorted separately for each new index).  For
example:

     CREATE TABLE T1(A INT PRIMARY KEY, B INT, C CHAR(1)) ENGINE=InnoDB;
     INSERT INTO T1 VALUES (1,2,'a'), (2,3,'b'), (3,2,'c'), (4,3,'d'), (5,2,'e');
     COMMIT;
     ALTER TABLE T1 ADD INDEX (B), ADD UNIQUE INDEX (C);

The above statements create table 'T1' with the clustered index (primary
key) on column 'A', insert several rows, and then build two new indexes
on columns 'B' and 'C'.  If there were many rows inserted into 'T1'
before the *note 'ALTER TABLE': alter-table. statement, this approach is
much more efficient than creating all the secondary indexes before
loading the data.

You can also create the indexes one at a time, but then the clustered
index of the table is scanned (as well as sorted) once for each *note
'CREATE INDEX': create-index. statement.  Thus, the following statements
are not as efficient as the *note 'ALTER TABLE': alter-table. statement
above, even though neither requires recreating the clustered index for
table 'T1'.

     CREATE INDEX B ON T1 (B);
     CREATE UNIQUE INDEX C ON T1 (C);

Dropping InnoDB secondary indexes also does not require any copying of
table data.  You can equally quickly drop multiple indexes with a single
*note 'ALTER TABLE': alter-table. statement or multiple *note 'DROP
INDEX': drop-index. statements:

     ALTER TABLE T1 DROP INDEX B, DROP INDEX C;

or:

     DROP INDEX B ON T1;
     DROP INDEX C ON T1;

Restructuring the clustered index in InnoDB always requires copying the
data in the table.  For example, if you create a table without a primary
key, InnoDB chooses one for you, which may be the first 'UNIQUE' key
defined on 'NOT NULL' columns, or a system-generated key.  Defining a
'PRIMARY KEY' later causes the data to be copied, as in the following
example:

     CREATE TABLE T2 (A INT, B INT) ENGINE=InnoDB;
     INSERT INTO T2 VALUES (NULL, 1);
     ALTER TABLE T2 ADD PRIMARY KEY (B);

When you create a 'UNIQUE' or 'PRIMARY KEY' index, InnoDB must do some
extra work.  For 'UNIQUE' indexes, InnoDB checks that the table contains
no duplicate values for the key.  For a 'PRIMARY KEY' index, InnoDB also
checks that none of the 'PRIMARY KEY' columns contains a 'NULL'.  It is
best to define the primary key when you create a table, so you need not
rebuild the table later.


File: manual.info.tmp,  Node: innodb-create-index-implementation,  Next: innodb-create-index-concurrency,  Prev: innodb-create-index-examples,  Up: innodb-create-index

14.16.3 Implementation Details of Fast Index Creation
-----------------------------------------------------

InnoDB has two types of indexes: the clustered index and secondary
indexes.  Since the clustered index contains the data values in its
B-tree nodes, adding or dropping a clustered index does involve copying
the data, and creating a new copy of the table.  A secondary index,
however, contains only the index key and the value of the primary key.
This type of index can be created or dropped without copying the data in
the clustered index.  Because each secondary index contains copies of
the primary key values (used to access the clustered index when needed),
when you change the definition of the primary key, all secondary indexes
are recreated as well.

Dropping a secondary index is simple.  Only the internal InnoDB system
tables and the MySQL data dictionary tables are updated to reflect the
fact that the index no longer exists.  InnoDB returns the storage used
for the index to the tablespace that contained it, so that new indexes
or additional table rows can use the space.

To add a secondary index to an existing table, InnoDB scans the table,
and sorts the rows using memory buffers and temporary files in order by
the values of the secondary index key columns.  The B-tree is then built
in key-value order, which is more efficient than inserting rows into an
index in random order.  Because the B-tree nodes are split when they
fill, building the index in this way results in a higher fill-factor for
the index, making it more efficient for subsequent access.


File: manual.info.tmp,  Node: innodb-create-index-concurrency,  Next: innodb-create-index-recovery,  Prev: innodb-create-index-implementation,  Up: innodb-create-index

14.16.4 Concurrency Considerations for Fast Index Creation
----------------------------------------------------------

While an InnoDB secondary index is being created or dropped, the table
is locked in shared mode.  Any writes to the table are blocked, but the
data in the table can be read.  When you alter the clustered index of a
table, the table is locked in exclusive mode, because the data must be
copied.  Thus, during the creation of a new clustered index, all
operations on the table are blocked.

A *note 'CREATE INDEX': create-index. or *note 'ALTER TABLE':
alter-table. statement for an InnoDB table always waits for currently
executing transactions that are accessing the table to commit or roll
back.  *note 'ALTER TABLE': alter-table. statements that redefine an
InnoDB primary key wait for all 'SELECT' statements that access the
table to complete, or their containing transactions to commit.  No
transactions whose execution spans the creation of the index can be
accessing the table, because the original table is dropped when the
clustered index is restructured.

Once a *note 'CREATE INDEX': create-index. or *note 'ALTER TABLE':
alter-table. statement that creates an InnoDB secondary index begins
executing, queries can access the table for read access, but cannot
update the table.  If an *note 'ALTER TABLE': alter-table. statement is
changing the clustered index for an InnoDB table, all queries wait until
the operation completes.

A newly-created InnoDB secondary index contains only the committed data
in the table at the time the *note 'CREATE INDEX': create-index. or
*note 'ALTER TABLE': alter-table. statement begins to execute.  It does
not contain any uncommitted values, old versions of values, or values
marked for deletion but not yet removed from the old index.

Because a newly-created index contains only information about data
current at the time the index was created, queries that need to see data
that was deleted or changed before the index was created cannot use the
index.  The only queries that could be affected by this limitation are
those executing in transactions that began before the creation of the
index was begun.  For such queries, unpredictable results could occur.
Newer queries can use the index.


File: manual.info.tmp,  Node: innodb-create-index-recovery,  Next: innodb-create-index-limitations,  Prev: innodb-create-index-concurrency,  Up: innodb-create-index

14.16.5 How Crash Recovery Works with Fast Index Creation
---------------------------------------------------------

Although no data is lost if the server crashes while an *note 'ALTER
TABLE': alter-table. statement is executing, the crash recovery process
is different for clustered indexes and secondary indexes.

If the server crashes while creating an InnoDB secondary index, upon
recovery, MySQL drops any partially created indexes.  You must re-run
the *note 'ALTER TABLE': alter-table. or *note 'CREATE INDEX':
create-index. statement.

When a crash occurs during the creation of an InnoDB clustered index,
recovery is more complicated, because the data in the table must be
copied to an entirely new clustered index.  Remember that all InnoDB
tables are stored as clustered indexes.  In the following discussion, we
use the word table and clustered index interchangeably.

MySQL creates the new clustered index by copying the existing data from
the original InnoDB table to a temporary table that has the desired
index structure.  Once the data is completely copied to this temporary
table, the original table is renamed with a different temporary table
name.  The temporary table comprising the new clustered index is renamed
with the name of the original table, and the original table is dropped
from the database.

If a system crash occurs while creating a new clustered index, no data
is lost, but you must complete the recovery process using the temporary
tables that exist during the process.  Since it is rare to re-create a
clustered index or re-define primary keys on large tables, or to
encounter a system crash during this operation, this manual does not
provide information on recovering from this scenario.  Contact MySQL
support.


File: manual.info.tmp,  Node: innodb-create-index-limitations,  Prev: innodb-create-index-recovery,  Up: innodb-create-index

14.16.6 Limitations of Fast Index Creation
------------------------------------------

Take the following considerations into account when creating or dropping
InnoDB indexes:

   * During index creation, files are written to the temporary directory
     ('$TMPDIR' on Unix, '%TEMP%' on Windows, or the value of the
     '--tmpdir' configuration variable).  Each temporary file is large
     enough to hold one column that makes up the new index, and each one
     is removed as soon as it is merged into the final index.

   * An *note 'ALTER TABLE': alter-table. statement that contains 'DROP
     INDEX' and 'ADD INDEX' clauses that both name the same index uses a
     table copy, not Fast Index Creation.

   * The table is copied, rather than using Fast Index Creation when you
     create an index on a 'TEMPORARY TABLE'.  This has been reported as
     MySQL Bug #39833.

   * To avoid consistency issues between the InnoDB data dictionary and
     the MySQL data dictionary, the table is copied rather than using
     Fast Index Creation when renaming a column using *note 'ALTER TABLE
     ... CHANGE': alter-table. syntax.

   * The statement 'ALTER IGNORE TABLE T ADD UNIQUE INDEX' does not
     delete duplicate rows.  This has been reported as MySQL Bug #40344.
     The 'IGNORE' keyword is ignored.  If any duplicate rows exist, the
     operation fails with the following error message:

          ERROR 23000: Duplicate entry '347' for key 'PL'

   * As noted above, a newly-created index contains only information
     about data current at the time the index was created.  Therefore,
     you should not run queries in a transaction that might use a
     secondary index that did not exist at the beginning of the
     transaction.  There is no way for InnoDB to access 'old' data that
     is consistent with the rest of the data read by the transaction.
     See the discussion of locking in *note
     innodb-create-index-concurrency::.

     Prior to InnoDB storage engine 1.0.4, unexpected results could
     occur if a query attempts to use an index created after the start
     of the transaction containing the query.  If an old transaction
     attempts to access a 'too new' index, InnoDB storage engine 1.0.4
     and later reports an error:

          ERROR HY000: Table definition has changed, please retry transaction

     As the error message suggests, committing (or rolling back) the
     transaction, and restarting it, cures the problem.

   * InnoDB storage engine 1.0.2 introduces some improvements in error
     handling when users attempt to drop indexes.  See *note
     server-error-reference:: for information related to errors '1025',
     '1553', and '1173'.

   * MySQL 5.5 does not support efficient creation or dropping of
     'FOREIGN KEY' constraints.  Therefore, if you use *note 'ALTER
     TABLE': alter-table. to add or remove a 'REFERENCES' constraint,
     the child table is copied, rather than using Fast Index Creation.

   * *note 'OPTIMIZE TABLE': optimize-table. for an 'InnoDB' table is
     mapped to an *note 'ALTER TABLE': alter-table. operation to rebuild
     the table and update index statistics and free unused space in the
     clustered index.  This operation does not use fast index creation.
     Secondary indexes are not created as efficiently because keys are
     inserted in the order they appeared in the primary key.


File: manual.info.tmp,  Node: innodb-parameters,  Next: innodb-information-schema,  Prev: innodb-create-index,  Up: innodb-storage-engine

14.17 InnoDB Startup Options and System Variables
=================================================

This section describes the 'InnoDB'-related command options and system
variables.

   * System variables that are true or false can be enabled at server
     startup by naming them, or disabled by using a '--skip-' prefix.
     For example, to enable or disable 'InnoDB' checksums, you can use
     '--innodb-checksums' or '--skip-innodb-checksums' on the command
     line, or 'innodb_checksums' or 'skip_innodb_checksums' in an option
     file.

   * System variables that take a numeric value can be specified as
     '--VAR_NAME=VALUE' on the command line or as 'VAR_NAME=VALUE' in
     option files.

   * Many system variables can be changed at runtime (see *note
     dynamic-system-variables::).

   * For information about 'GLOBAL' and 'SESSION' variable scope
     modifiers, refer to the *note 'SET': set-variable. statement
     documentation.

   * Certain options control the locations and layout of the 'InnoDB'
     data files.  *note innodb-init-startup-configuration:: explains how
     to use these options.

   * Some options, which you might not use initially, help tune 'InnoDB'
     performance characteristics based on machine capacity and your
     database workload.

   * For more information on specifying options and system variables,
     see *note program-options::.

*InnoDB Option and Variable Reference*

Name           Cmd-Line    Option      System      Status      Var Scope   Dynamic
                           File        Var         Var                     
                                                   
foreign_key_checks                     Yes                     Varies      Yes
                                                                           
have_innodb                            Yes                     Global      No
                                                                           
ignore_builtin_innodbYes   Yes         Yes                     Global      No
                                                                           
innodb         Yes         Yes                                             
                           
innodb_adaptive_flushingYesYes         Yes                     Global      Yes
                                                                           
innodb_adaptive_hash_indexYesYes       Yes                     Global      Yes
                                                                           
innodb_additional_mem_pool_sizeYesYes  Yes                     Global      No
                                                                           
innodb_autoextend_incrementYesYes      Yes                     Global      Yes
                                                                           
innodb_autoinc_lock_modeYesYes         Yes                     Global      No
                                                                           
Innodb_buffer_pool_bytes_data                      Yes         Global      No
                                                                           
Innodb_buffer_pool_bytes_dirty                     Yes         Global      No
                                                                           
innodb_buffer_pool_instancesYesYes     Yes                     Global      No
                                                                           
Innodb_buffer_pool_pages_data                      Yes         Global      No
                                                                           
Innodb_buffer_pool_pages_dirty                     Yes         Global      No
                                                                           
Innodb_buffer_pool_pages_flushed                   Yes         Global      No
                                                                           
Innodb_buffer_pool_pages_free                      Yes         Global      No
                                                                           
Innodb_buffer_pool_pages_latched                   Yes         Global      No
                                                                           
Innodb_buffer_pool_pages_misc                      Yes         Global      No
                                                                           
Innodb_buffer_pool_pages_total                     Yes         Global      No
                                                                           
Innodb_buffer_pool_read_ahead                      Yes         Global      No
                                                                           
Innodb_buffer_pool_read_ahead_evicted              Yes         Global      No
                                                                           
Innodb_buffer_pool_read_ahead_rnd                  Yes         Global      No
                                                                           
Innodb_buffer_pool_read_requests                   Yes         Global      No
                                                                           
Innodb_buffer_pool_reads                           Yes         Global      No
                                                                           
innodb_buffer_pool_sizeYes Yes         Yes                     Global      No
                                                                           
Innodb_buffer_pool_wait_free                       Yes         Global      No
                                                                           
Innodb_buffer_pool_write_requests                  Yes         Global      No
                                                                           
innodb_change_bufferingYes Yes         Yes                     Global      Yes
                                                                           
innodb_change_buffering_debugYesYes    Yes                     Global      Yes
                                                                           
innodb_checksumsYes        Yes         Yes                     Global      No
                                                                           
innodb_commit_concurrencyYesYes        Yes                     Global      Yes
                                                                           
innodb_concurrency_ticketsYesYes       Yes                     Global      Yes
                                                                           
innodb_data_file_pathYes   Yes         Yes                     Global      No
                                                                           
Innodb_data_fsyncs                                 Yes         Global      No
                                                                           
innodb_data_home_dirYes    Yes         Yes                     Global      No
                                                                           
Innodb_data_pending_fsyncs                         Yes         Global      No
                                                                           
Innodb_data_pending_reads                          Yes         Global      No
                                                                           
Innodb_data_pending_writes                         Yes         Global      No
                                                                           
Innodb_data_read                                   Yes         Global      No
                                                                           
Innodb_data_reads                                  Yes         Global      No
                                                                           
Innodb_data_writes                                 Yes         Global      No
                                                                           
Innodb_data_written                                Yes         Global      No
                                                                           
Innodb_dblwr_pages_written                         Yes         Global      No
                                                                           
Innodb_dblwr_writes                                Yes         Global      No
                                                                           
innodb_doublewriteYes      Yes         Yes                     Global      No
                                                                           
innodb_fast_shutdownYes    Yes         Yes                     Global      Yes
                                                                           
innodb_file_formatYes      Yes         Yes                     Global      Yes
                                                                           
innodb_file_format_checkYesYes         Yes                     Global      Varies
                                                                           
innodb_file_format_maxYes  Yes         Yes                     Global      Yes
                                                                           
innodb_file_per_tableYes   Yes         Yes                     Global      Yes
                                                                           
innodb_flush_log_at_trx_commitYesYes   Yes                     Global      Yes
                                                                           
innodb_flush_methodYes     Yes         Yes                     Global      No
                                                                           
innodb_force_load_corruptedYesYes      Yes                     Global      No
                                                                           
innodb_force_recoveryYes   Yes         Yes                     Global      No
                                                                           
Innodb_have_atomic_builtins                        Yes         Global      No
                                                                           
innodb_io_capacityYes      Yes         Yes                     Global      Yes
                                                                           
innodb_large_prefixYes     Yes         Yes                     Global      Yes
                                                                           
innodb_limit_optimistic_insert_debugYesYesYes                  Global      Yes
                                                                           
innodb_lock_wait_timeoutYesYes         Yes                     Both        Yes
                                                                           
innodb_locks_unsafe_for_binlogYesYes   Yes                     Global      No
                                                                           
innodb_log_buffer_sizeYes  Yes         Yes                     Global      No
                                                                           
innodb_log_file_sizeYes    Yes         Yes                     Global      No
                                                                           
innodb_log_files_in_groupYesYes        Yes                     Global      No
                                                                           
innodb_log_group_home_dirYesYes        Yes                     Global      No
                                                                           
Innodb_log_waits                                   Yes         Global      No
                                                                           
Innodb_log_write_requests                          Yes         Global      No
                                                                           
Innodb_log_writes                                  Yes         Global      No
                                                                           
innodb_max_dirty_pages_pctYesYes       Yes                     Global      Yes
                                                                           
innodb_max_purge_lagYes    Yes         Yes                     Global      Yes
                                                                           
innodb_mirrored_log_groupsYesYes       Yes                     Global      No
                                                                           
innodb_old_blocks_pctYes   Yes         Yes                     Global      Yes
                                                                           
innodb_old_blocks_timeYes  Yes         Yes                     Global      Yes
                                                                           
innodb_open_filesYes       Yes         Yes                     Global      No
                                                                           
Innodb_os_log_fsyncs                               Yes         Global      No
                                                                           
Innodb_os_log_pending_fsyncs                       Yes         Global      No
                                                                           
Innodb_os_log_pending_writes                       Yes         Global      No
                                                                           
Innodb_os_log_written                              Yes         Global      No
                                                                           
Innodb_page_size                                   Yes         Global      No
                                                                           
Innodb_pages_created                               Yes         Global      No
                                                                           
Innodb_pages_read                                  Yes         Global      No
                                                                           
Innodb_pages_written                               Yes         Global      No
                                                                           
innodb_print_all_deadlocksYesYes       Yes                     Global      Yes
                                                                           
innodb_purge_batch_sizeYes Yes         Yes                     Global      Yes
                                                                           
innodb_purge_threadsYes    Yes         Yes                     Global      No
                                                                           
innodb_random_read_aheadYesYes         Yes                     Global      Yes
                                                                           
innodb_read_ahead_thresholdYesYes      Yes                     Global      Yes
                                                                           
innodb_read_io_threadsYes  Yes         Yes                     Global      No
                                                                           
innodb_replication_delayYesYes         Yes                     Global      Yes
                                                                           
innodb_rollback_on_timeoutYesYes       Yes                     Global      No
                                                                           
innodb_rollback_segmentsYesYes         Yes                     Global      Yes
                                                                           
Innodb_row_lock_current_waits                      Yes         Global      No
                                                                           
Innodb_row_lock_time                               Yes         Global      No
                                                                           
Innodb_row_lock_time_avg                           Yes         Global      No
                                                                           
Innodb_row_lock_time_max                           Yes         Global      No
                                                                           
Innodb_row_lock_waits                              Yes         Global      No
                                                                           
Innodb_rows_deleted                                Yes         Global      No
                                                                           
Innodb_rows_inserted                               Yes         Global      No
                                                                           
Innodb_rows_read                                   Yes         Global      No
                                                                           
Innodb_rows_updated                                Yes         Global      No
                                                                           
innodb_spin_wait_delayYes  Yes         Yes                     Global      Yes
                                                                           
innodb_stats_methodYes     Yes         Yes                     Global      Yes
                                                                           
innodb_stats_on_metadataYesYes         Yes                     Global      Yes
                                                                           
innodb_stats_sample_pagesYesYes        Yes                     Global      Yes
                                                                           
innodb-status-fileYes      Yes                                             
                           
innodb_strict_modeYes      Yes         Yes                     Both        Yes
                                                                           
innodb_support_xaYes       Yes         Yes                     Both        Yes
                                                                           
innodb_sync_spin_loopsYes  Yes         Yes                     Global      Yes
                                                                           
innodb_table_locksYes      Yes         Yes                     Both        Yes
                                                                           
innodb_thread_concurrencyYesYes        Yes                     Global      Yes
                                                                           
innodb_thread_sleep_delayYesYes        Yes                     Global      Yes
                                                                           
Innodb_truncated_status_writes                     Yes         Global      No
                                                                           
innodb_trx_purge_view_update_only_debugYesYesYes               Global      Yes
                                                                           
innodb_trx_rseg_n_slots_debugYesYes    Yes                     Global      Yes
                                                                           
innodb_use_native_aioYes   Yes         Yes                     Global      No
                                                                           
innodb_use_sys_mallocYes   Yes         Yes                     Global      No
                                                                           
innodb_version                         Yes                     Global      No
                                                                           
innodb_write_io_threadsYes Yes         Yes                     Global      No
                                                                           
timed_mutexes  Yes         Yes         Yes                     Global      Yes
                                                                           
unique_checks                          Yes                     Varies      Yes
                                                               

*InnoDB Command Options*

   * 
     '--innodb[=VALUE]'

     Property               Value
                            
     *Command-Line          '--innodb[=value]'
     Format*                

     *Type*                 Enumeration
                            
     *Default Value*        'ON'
                            
     *Valid Values*         'OFF' 'ON' 'FORCE'

     Controls loading of the 'InnoDB' storage engine, if the server was
     compiled with 'InnoDB' support.  This option has a tristate format,
     with possible values of 'OFF', 'ON', or 'FORCE'.  See *note
     plugin-loading::.

     To disable 'InnoDB', use '--innodb=OFF' or '--skip-innodb'.  In
     this case, because the default storage engine is *note 'InnoDB':
     innodb-storage-engine, the server will not start unless you also
     change the 'default_storage_engine' system variable to set the
     default to some other available engine.

   * 
     '--innodb-status-file'

     Property               Value
                            
     *Command-Line          '--innodb-status-file[={OFF|ON}]'
     Format*                

     *Type*                 Boolean
                            
     *Default Value*        'OFF'

     The '--innodb-status-file' startup option controls whether 'InnoDB'
     creates a file named 'innodb_status.PID' in the data directory and
     writes *note 'SHOW ENGINE INNODB STATUS': show-engine. output to it
     every 15 seconds, approximately.

     The 'innodb_status.PID' file is not created by default.  To create
     it, start *note 'mysqld': mysqld. with the '--innodb-status-file'
     option.  'InnoDB' removes the file when the server is shut down
     normally.  If an abnormal shutdown occurs, the status file may have
     to be removed manually.

     The '--innodb-status-file' option is intended for temporary use, as
     *note 'SHOW ENGINE INNODB STATUS': show-engine. output generation
     can affect performance, and the 'innodb_status.PID' file can become
     quite large over time.

     For related information, see *note innodb-enabling-monitors::.

   * 
     '--skip-innodb'

     Disable the 'InnoDB' storage engine.  See the description of
     '--innodb'.

*InnoDB System Variables*

   * 
     'ignore_builtin_innodb'

     Property               Value
                            
     *Command-Line          '--ignore-builtin-innodb[={OFF|ON}]'
     Format*                

     *Deprecated*           5.5.22
                            
     *System Variable*      'ignore_builtin_innodb'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Boolean

     In MySQL 5.1, enabling this variable caused the server to behave as
     if the built-in 'InnoDB' were not present, which enabled the
     'InnoDB Plugin' to be used instead.  In MySQL 5.5, 'InnoDB' is the
     default storage engine and 'InnoDB Plugin' is not used, so this
     variable has no effect.  As of MySQL 5.5.22, it is deprecated and
     its use results in a warning.

   * 
     'innodb_adaptive_flushing'

     Property               Value
                            
     *Command-Line          '--innodb-adaptive-flushing[={OFF|ON}]'
     Format*                

     *System Variable*      'innodb_adaptive_flushing'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Boolean
                            
     *Default Value*        'ON'

     Specifies whether to dynamically adjust the rate of flushing dirty
     pages in the 'InnoDB' buffer pool based on the workload.  Adjusting
     the flush rate dynamically is intended to avoid bursts of I/O
     activity.  This setting is enabled by default.  See *note
     innodb-buffer-pool-flushing:: for more information.  For general
     I/O tuning advice, see *note optimizing-innodb-diskio::.

   * 
     'innodb_adaptive_hash_index'

     Property               Value
                            
     *Command-Line          '--innodb-adaptive-hash-index[={OFF|ON}]'
     Format*                

     *System Variable*      'innodb_adaptive_hash_index'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Boolean
                            
     *Default Value*        'ON'

     Whether the 'InnoDB' adaptive hash index is enabled or disabled.
     It may be desirable, depending on your workload, to dynamically
     enable or disable adaptive hash indexing to improve query
     performance.  Because the adaptive hash index may not be useful for
     all workloads, conduct benchmarks with it both enabled and
     disabled, using realistic workloads.  See *note
     innodb-adaptive-hash:: for details.

     This variable is enabled by default.  As of MySQL 5.5, You can
     modify this parameter using the 'SET GLOBAL' statement, without
     restarting the server.  Changing the setting at runtime requires
     privileges sufficient to set global system variables.  See *note
     system-variable-privileges::.  You can also use
     '--skip-innodb-adaptive-hash-index' at server startup to disable
     it.

     Disabling the adaptive hash index empties the hash table
     immediately.  Normal operations can continue while the hash table
     is emptied, and executing queries that were using the hash table
     access the index B-trees directly instead.  When the adaptive hash
     index is re-enabled, the hash table is populated again during
     normal operation.

   * 
     'innodb_additional_mem_pool_size'

     Property               Value
                            
     *Command-Line          '--innodb-additional-mem-pool-size=#'
     Format*                

     *System Variable*      'innodb_additional_mem_pool_size'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Integer
                            
     *Default Value*        '8388608'
                            
     *Minimum Value*        '2097152'
                            
     *Maximum Value*        '4294967295'

     The size in bytes of a memory pool 'InnoDB' uses to store data
     dictionary information and other internal data structures.  The
     more tables you have in your application, the more memory you need
     to allocate here.  If 'InnoDB' runs out of memory in this pool, it
     starts to allocate memory from the operating system and writes
     warning messages to the MySQL error log.  The default value is 8MB.

     This variable relates to the 'InnoDB' internal memory allocator,
     which is unused if 'innodb_use_sys_malloc' is enabled.  For more
     information, see *note innodb-performance-use_sys_malloc::.

   * 
     'innodb_autoextend_increment'

     Property               Value
                            
     *Command-Line          '--innodb-autoextend-increment=#'
     Format*                

     *System Variable*      'innodb_autoextend_increment'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '8'
                            
     *Minimum Value*        '1'
                            
     *Maximum Value*        '1000'

     The increment size (in megabytes) for extending the size of an
     auto-extending system tablespace file when it becomes full.  The
     default value is 8.  For related information, see *note
     innodb-startup-data-file-configuration::, and *note
     innodb-resize-system-tablespace::.

     The 'innodb_autoextend_increment' setting does not affect
     file-per-table tablespace files.  These files are auto-extending
     regardless of the 'innodb_autoextend_increment' setting.  The
     initial extensions are by small amounts, after which extensions
     occur in increments of 4MB.

   * 
     'innodb_autoinc_lock_mode'

     Property               Value
                            
     *Command-Line          '--innodb-autoinc-lock-mode=#'
     Format*                

     *System Variable*      'innodb_autoinc_lock_mode'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Integer
                            
     *Default Value*        '1'
                            
     *Valid Values*         '0' '1' '2'

     The lock mode to use for generating auto-increment values.
     Permissible values are 0, 1, or 2, for traditional, consecutive, or
     interleaved, respectively.  The default setting is 1 (consecutive).
     For the characteristics of each lock mode, see *note
     innodb-auto-increment-lock-modes::.

   * 
     'innodb_buffer_pool_instances'

     Property               Value
                            
     *Command-Line          '--innodb-buffer-pool-instances=#'
     Format*                

     *Introduced*           5.5.4
                            
     *System Variable*      'innodb_buffer_pool_instances'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Integer
                            
     *Default Value*        '1'
                            
     *Minimum Value*        '1'
                            
     *Maximum Value*        '64'

     The number of regions that the *note 'InnoDB':
     innodb-storage-engine. buffer pool is divided into.  For systems
     with buffer pools in the multi-gigabyte range, dividing the buffer
     pool into separate instances can improve concurrency, by reducing
     contention as different threads read and write to cached pages.
     Each page that is stored in or read from the buffer pool is
     assigned to one of the buffer pool instances randomly, using a
     hashing function.  Each buffer pool manages its own free lists,
     flush lists, LRUs, and all other data structures connected to a
     buffer pool, and is protected by its own buffer pool mutex.

     This option only takes effect when setting
     'innodb_buffer_pool_size' to a size of 1GB or more.  The total size
     you specify is divided among all the buffer pools.  For best
     efficiency, specify a combination of 'innodb_buffer_pool_instances'
     and 'innodb_buffer_pool_size' so that each buffer pool instance is
     at least 1GB.

   * 
     'innodb_buffer_pool_size'

     Property               Value
                            
     *Command-Line          '--innodb-buffer-pool-size=#'
     Format*                

     *System Variable*      'innodb_buffer_pool_size'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Integer
                            
     *Default Value*        '134217728'
                            
     *Minimum Value*        '5242880'
                            
     *Maximum Value*        '2**64-1'
     (64-bit platforms)     

     *Maximum Value*        '2**32-1'
     (32-bit platforms)

     The size in bytes of the buffer pool, the memory area where
     'InnoDB' caches table and index data.  The default value is
     134217728 bytes (128MB). The maximum value depends on the CPU
     architecture; the maximum is 4294967295 (2^32-1) on 32-bit systems
     and 18446744073709551615 (2^64-1) on 64-bit systems.  On 32-bit
     systems, the CPU architecture and operating system may impose a
     lower practical maximum size than the stated maximum.  When the
     size of the buffer pool is greater than 1GB, setting
     'innodb_buffer_pool_instances' to a value greater than 1 can
     improve the scalability on a busy server.

     A larger buffer pool requires less disk I/O to access the same
     table data more than once.  On a dedicated database server, you
     might set the buffer pool size to 80% of the machine's physical
     memory size.  Be aware of the following potential issues when
     configuring buffer pool size, and be prepared to scale back the
     size of the buffer pool if necessary.

        * Competition for physical memory can cause paging in the
          operating system.

        * 'InnoDB' reserves additional memory for buffers and control
          structures, so that the total allocated space is approximately
          10% greater than the specified buffer pool size.

        * Address space for the buffer pool must be contiguous, which
          can be an issue on Windows systems with DLLs that load at
          specific addresses.

        * The time to initialize the buffer pool is roughly proportional
          to its size.  On instances with large buffer pools,
          initialization time might be significant.

   * 
     'innodb_change_buffering'

     Property               Value
                            
     *Command-Line          '--innodb-change-buffering=value'
     Format*                

     *System Variable*      'innodb_change_buffering'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Enumeration
                            
     *Default Value* (>=    'all'
     5.5.4)                 

     *Default Value* (<=    'inserts'
     5.5.3)                 

     *Valid Values* (>=     'none' 'inserts' 'deletes' 'changes' 'purges'
     5.5.4)                 'all'
                            
     *Valid Values* (<=     'inserts' 'none'
     5.5.3)

     Whether 'InnoDB' performs change buffering, an optimization that
     delays write operations to secondary indexes so that the I/O
     operations can be performed sequentially.  Permitted values are
     described in the following table.

     *Permitted Values for innodb_change_buffering*

     Value          Description
                    
     'none'         Do not buffer any operations.
                    
     'inserts'      Buffer insert operations.
                    
     'deletes'      Buffer delete marking operations;
                    strictly speaking, the writes that mark
                    index records for later deletion during a
                    purge operation.
                    
     'changes'      Buffer inserts and delete-marking
                    operations.
                    
     'purges'       Buffer the physical deletion operations
                    that happen in the background.
                    
     'all'          The default.  Buffer inserts,
                    delete-marking operations, and purges.

     For more information, see *note innodb-change-buffer::.  For
     general I/O tuning advice, see *note optimizing-innodb-diskio::.

   * 
     'innodb_change_buffering_debug'

     Property               Value
                            
     *Command-Line          '--innodb-change-buffering-debug=#'
     Format*                

     *System Variable*      'innodb_change_buffering_debug'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '0'
                            
     *Maximum Value*        '2'

     Sets a debug flag for 'InnoDB' change buffering.  A value of 1
     forces all changes to the change buffer.  A value of 2 causes a
     crash at merge.  A default value of 0 indicates that the change
     buffering debug flag is not set.  This option is only available
     when debugging support is compiled in using the 'WITH_DEBUG'
     'CMake' option.

   * 
     'innodb_checksums'

     Property               Value
                            
     *Command-Line          '--innodb-checksums[={OFF|ON}]'
     Format*                

     *System Variable*      'innodb_checksums'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Boolean
                            
     *Default Value*        'ON'

     'InnoDB' can use checksum validation on all pages read from disk to
     ensure extra fault tolerance against broken hardware or data files.
     This validation is enabled by default.  Under specialized
     circumstances (such as when running benchmarks) this safety feature
     can be disabled with '--skip-innodb-checksums'.

   * 
     'innodb_commit_concurrency'

     Property               Value
                            
     *Command-Line          '--innodb-commit-concurrency=#'
     Format*                

     *System Variable*      'innodb_commit_concurrency'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '0'
                            
     *Minimum Value*        '0'
                            
     *Maximum Value*        '1000'

     The number of threads that can commit at the same time.  A value of
     0 (the default) permits any number of transactions to commit
     simultaneously.

     The value of 'innodb_commit_concurrency' cannot be changed at
     runtime from zero to nonzero or vice versa.  The value can be
     changed from one nonzero value to another.

   * 
     'innodb_concurrency_tickets'

     Property               Value
                            
     *Command-Line          '--innodb-concurrency-tickets=#'
     Format*                

     *System Variable*      'innodb_concurrency_tickets'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '500'
                            
     *Minimum Value*        '1'
                            
     *Maximum Value*        '4294967295'

     Determines the number of threads that can enter 'InnoDB'
     concurrently.  A thread is placed in a queue when it tries to enter
     'InnoDB' if the number of threads has already reached the
     concurrency limit.  When a thread is permitted to enter 'InnoDB',
     it is given a number of ' tickets' equal to the value of
     'innodb_concurrency_tickets', and the thread can enter and leave
     'InnoDB' freely until it has used up its tickets.  After that
     point, the thread again becomes subject to the concurrency check
     (and possible queuing) the next time it tries to enter 'InnoDB'.
     The default value is 500.

     With a small 'innodb_concurrency_tickets' value, small transactions
     that only need to process a few rows compete fairly with larger
     transactions that process many rows.  The disadvantage of a small
     'innodb_concurrency_tickets' value is that large transactions must
     loop through the queue many times before they can complete, which
     extends the amount of time required to complete their task.

     With a large 'innodb_concurrency_tickets' value, large transactions
     spend less time waiting for a position at the end of the queue
     (controlled by 'innodb_thread_concurrency') and more time
     retrieving rows.  Large transactions also require fewer trips
     through the queue to complete their task.  The disadvantage of a
     large 'innodb_concurrency_tickets' value is that too many large
     transactions running at the same time can starve smaller
     transactions by making them wait a longer time before executing.

     With a nonzero 'innodb_thread_concurrency' value, you may need to
     adjust the 'innodb_concurrency_tickets' value up or down to find
     the optimal balance between larger and smaller transactions.  The
     'SHOW ENGINE INNODB STATUS' report shows the number of tickets
     remaining for an executing transaction in its current pass through
     the queue.  This data may also be obtained from the
     'TRX_CONCURRENCY_TICKETS' column of the *note
     'INFORMATION_SCHEMA.INNODB_TRX': innodb-trx-table. table.

     For more information, see *note
     innodb-performance-thread_concurrency::.

   * 
     'innodb_data_file_path'

     Property               Value
                            
     *Command-Line          '--innodb-data-file-path=file_name'
     Format*                

     *System Variable*      'innodb_data_file_path'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 String
                            
     *Default Value*        'ibdata1:10M:autoextend'

     Defines the name, size, and attributes of 'InnoDB' system
     tablespace data files.  If you do not specify a value for
     'innodb_data_file_path', the default behavior is to create a single
     auto-extending data file, slightly larger than 10MB, named
     'ibdata1'.

     The full syntax for a data file specification includes the file
     name, file size, 'autoextend' attribute, and 'max' attribute:

          FILE_NAME:FILE_SIZE[:autoextend[:max:MAX_FILE_SIZE]]

     File sizes are specified in kilobytes, megabytes, or gigabytes by
     appending 'K', 'M' or 'G' to the size value.  If specifying the
     data file size in kilobytes, do so in multiples of 1024.
     Otherwise, KB values are rounded to nearest megabyte (MB) boundary.
     The sum of file sizes must be, at a minimum, slightly larger than
     12MB.

     For additional configuration information, see *note
     innodb-startup-data-file-configuration::.  For resizing
     instructions, see *note innodb-resize-system-tablespace::.

   * 
     'innodb_data_home_dir'

     Property               Value
                            
     *Command-Line          '--innodb-data-home-dir=dir_name'
     Format*                

     *System Variable*      'innodb_data_home_dir'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Directory name

     The common part of the directory path for 'InnoDB' system
     tablespace data files.  This setting does not affect the location
     of file-per-table tablespaces when 'innodb_file_per_table' is
     enabled.  The default value is the MySQL 'data' directory.  If you
     specify the value as an empty string, you can specify an absolute
     file paths for 'innodb_data_file_path'.

     A trailing slash is required when specifying a value for
     'innodb_data_home_dir'.  For example:

          [mysqld]
          innodb_data_home_dir = /path/to/myibdata/

     For related information, see *note
     innodb-init-startup-configuration::.

   * 
     'innodb_doublewrite'

     Property               Value
                            
     *Command-Line          '--innodb-doublewrite[={OFF|ON}]'
     Format*                

     *System Variable*      'innodb_doublewrite'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Boolean
                            
     *Default Value*        'ON'

     When enabled (the default), 'InnoDB' stores all data twice, first
     to the doublewrite buffer, and then to the actual data files.  This
     variable can be turned off with '--skip-innodb-doublewrite' for
     benchmarks or cases when top performance is needed rather than
     concern for data integrity or possible failures.

     For related information, see *note innodb-doublewrite-buffer::.

   * 
     'innodb_fast_shutdown'

     Property               Value
                            
     *Command-Line          '--innodb-fast-shutdown=#'
     Format*                

     *System Variable*      'innodb_fast_shutdown'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '1'
                            
     *Valid Values*         '0' '1' '2'

     The 'InnoDB' shutdown mode.  If the value is 0, 'InnoDB' does a
     slow shutdown, a full purge and a change buffer merge before
     shutting down.  If the value is 1 (the default), 'InnoDB' skips
     these operations at shutdown, a process known as a fast shutdown.
     If the value is 2, 'InnoDB' flushes its logs and shuts down cold,
     as if MySQL had crashed; no committed transactions are lost, but
     the crash recovery operation makes the next startup take longer.

     The slow shutdown can take minutes, or even hours in extreme cases
     where substantial amounts of data are still buffered.  Use the slow
     shutdown technique before upgrading or downgrading between MySQL
     major releases, so that all data files are fully prepared in case
     the upgrade process updates the file format.

     Use 'innodb_fast_shutdown=2' in emergency or troubleshooting
     situations, to get the absolute fastest shutdown if data is at risk
     of corruption.

   * 
     'innodb_file_format'

     Property               Value
                            
     *Command-Line          '--innodb-file-format=value'
     Format*                

     *System Variable*      'innodb_file_format'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 String
                            
     *Default Value* (>=    'Antelope'
     5.5.7)                 

     *Default Value* (<=    'Barracuda'
     5.5.6)                 

     *Valid Values*         'Antelope' 'Barracuda'

     Enables an 'InnoDB' file format for file-per-table tablespaces.
     Supported file formats are 'Antelope' and 'Barracuda'.  'Antelope'
     is the original 'InnoDB' file format, which supports 'REDUNDANT'
     and 'COMPACT' row formats for 'InnoDB' tables.  'Barracuda' is the
     newer file format, which supports 'COMPRESSED' and 'DYNAMIC' row
     formats.

     'COMPRESSED' and 'DYNAMIC' row formats enable important storage
     features for 'InnoDB' tables.  See *note innodb-row-format::.

     To create tables that use 'COMPRESSED' or 'DYNAMIC' row format, the
     'Barracuda' file format and 'innodb_file_per_table' must be
     enabled.

     Changing the 'innodb_file_format' setting does not affect the file
     format of existing 'InnoDB' tablespace files.

     For more information, see *note innodb-file-format::.

   * 
     'innodb_file_format_check'

     Property               Value
                            
     *Command-Line          '--innodb-file-format-check=value' (<= 5.5.4)
     Format*                '--innodb-file-format-check[={OFF|ON}]' (>=
                            5.5.5)
                            
     *System Variable*      'innodb_file_format_check'
                            
     *Scope*                Global
                            
     *Dynamic* (>= 5.5.5)   No
                            
     *Dynamic* (<= 5.5.4)   Yes
                            
     *Type* (>= 5.5.5)      Boolean
                            
     *Type* (<= 5.5.4)      String
                            
     *Default Value* (>=    'ON'
     5.5.5)                 

     *Default Value* (>=    'Barracuda'
     5.5.1, <= 5.5.4)       

     *Default Value*        'Antelope'
     (5.5.0)

     As of MySQL 5.5.5, this variable can be set to 1 or 0 at server
     startup to enable or disable whether 'InnoDB' checks the file
     format tag in the system tablespace (for example, 'Antelope' or
     'Barracuda').  If the tag is checked and is higher than that
     supported by the current version of 'InnoDB', an error occurs and
     'InnoDB' does not start.  If the tag is not higher, 'InnoDB' sets
     the value of 'innodb_file_format_max' to the file format tag.

     Before MySQL 5.5.5, this variable can be set to 1 or 0 at server
     startup to enable or disable whether 'InnoDB' checks the file
     format tag in the shared tablespace.  If the tag is checked and is
     higher than that supported by the current version of 'InnoDB', an
     error occurs and 'InnoDB' does not start.  If the tag is not
     higher, 'InnoDB' sets the value of 'innodb_file_format_check' to
     the file format tag, which is the value seen at runtime.

     *Note*:

     Despite the default value sometimes being displayed as 'ON' or
     'OFF', always use the numeric values 1 or 0 to turn this option on
     or off in your configuration file or command line string.

     For more information, see *note
     innodb-file-format-compatibility-checking::.

   * 
     'innodb_file_format_max'

     Property               Value
                            
     *Command-Line          '--innodb-file-format-max=value'
     Format*                

     *Introduced*           5.5.5
                            
     *System Variable*      'innodb_file_format_max'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 String
                            
     *Default Value*        'Antelope'
                            
     *Valid Values*         'Antelope' 'Barracuda'

     At server startup, 'InnoDB' sets the value of this variable to the
     file format tag in the system tablespace (for example, 'Antelope'
     or 'Barracuda').  If the server creates or opens a table with a
     'higher' file format, it sets the value of 'innodb_file_format_max'
     to that format.

     For related information, see *note innodb-file-format::.

   * 
     'innodb_file_per_table'

     Property               Value
                            
     *Command-Line          '--innodb-file-per-table[={OFF|ON}]'
     Format*                

     *System Variable*      'innodb_file_per_table'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Boolean
                            
     *Default Value* (>=    'OFF'
     5.5.7)                 

     *Default Value* (<=    'ON'
     5.5.6)

     When 'innodb_file_per_table' is enabled, tables are created in
     file-per-table tablespaces.  When disabled, tables are created in
     the system tablespace.  For information about file-per-table
     tablespaces, see *note innodb-file-per-table-tablespaces::.  For
     information about the 'InnoDB' system tablespace, see *note
     innodb-system-tablespace::.

     The 'innodb_file_per_table' variable can be configured at runtime
     using a *note 'SET GLOBAL': set-variable. statement, specified on
     the command line at startup, or specified in an option file.
     Configuration at runtime requires privileges sufficient to set
     global system variables (see *note system-variable-privileges::)
     and immediately affects the operation of all connections.

     When a table that resides in a file-per-table tablespace is
     truncated or dropped, the freed space is returned to the operating
     system.  Truncating or dropping a table that resides in the system
     tablespace only frees space in the system tablespace.  Freed space
     in the system tablespace can be used again for 'InnoDB' data but is
     not returned to the operating system, as system tablespace data
     files never shrink.

     When 'innodb_file_per_table' is enabled, a table-copying *note
     'ALTER TABLE': alter-table. operation on a table that resides in
     the system tablespace implicitly re-creates the table in a
     file-per-table tablespace.  To prevent this from occurring, disable
     'innodb_file_per_table' before executing table-copying *note 'ALTER
     TABLE': alter-table. operations on tables that reside in the system
     tablespace.

   * 
     'innodb_flush_log_at_trx_commit'

     Property               Value
                            
     *Command-Line          '--innodb-flush-log-at-trx-commit=#'
     Format*                

     *System Variable*      'innodb_flush_log_at_trx_commit'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Enumeration
                            
     *Default Value*        '1'
                            
     *Valid Values*         '0' '1' '2'

     Controls the balance between strict ACID compliance for commit
     operations and higher performance that is possible when
     commit-related I/O operations are rearranged and done in batches.
     You can achieve better performance by changing the default value
     but then you can lose transactions in a crash.

        * The default setting of 1 is required for full ACID compliance.
          Logs are written and flushed to disk at each transaction
          commit.

        * With a setting of 0, logs are written and flushed to disk once
          per second.  Transactions for which logs have not been flushed
          can be lost in a crash.

        * With a setting of 2, logs are written after each transaction
          commit and flushed to disk once per second.  Transactions for
          which logs have not been flushed can be lost in a crash.

        * For settings 0 and 2, once-per-second flushing is not 100%
          guaranteed.  Flushing may occur more frequently due to DDL
          changes and other internal 'InnoDB' activities that cause logs
          to be flushed independently of the
          'innodb_flush_log_at_trx_commit' setting, and sometimes less
          frequently due to scheduling issues.  If logs are flushed once
          per second, up to one second of transactions can be lost in a
          crash.  If logs are flushed more or less frequently than once
          per second, the amount of transactions that can be lost varies
          accordingly.

        * DDL changes and other internal 'InnoDB' activities flush the
          log independently of the 'innodb_flush_log_at_trx_commit'
          setting.

        * 'InnoDB' crash recovery works regardless of the
          'innodb_flush_log_at_trx_commit' setting.  Transactions are
          either applied entirely or erased entirely.

     For the greatest possible durability and consistency in a
     replication setup using 'InnoDB' with transactions, use
     'innodb_flush_log_at_trx_commit=1' and 'sync_binlog=1' in your
     master server 'my.cnf' file.

     *Caution*:

     Many operating systems and some disk hardware fool the
     flush-to-disk operation.  They may tell *note 'mysqld': mysqld.
     that the flush has taken place, even though it has not.  In this
     case, the durability of transactions is not guaranteed even with
     the recommended settings, and in the worst case, a power outage can
     corrupt 'InnoDB' data.  Using a battery-backed disk cache in the
     SCSI disk controller or in the disk itself speeds up file flushes,
     and makes the operation safer.  You can also try to disable the
     caching of disk writes in hardware caches.

   * 
     'innodb_flush_method'

     Property               Value
                            
     *Command-Line          '--innodb-flush-method=value'
     Format*                

     *System Variable*      'innodb_flush_method'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 String
                            
     *Default Value*        'NULL'
                            
     *Valid Values*         'async_unbuffered' 'normal' 'unbuffered'
     (Windows)              

     *Valid Values*         'fsync' 'littlesync' 'nosync' 'O_DSYNC'
     (Unix)                 'O_DIRECT'

     Defines the method used to flush data to 'InnoDB' data files and
     log files, which can affect I/O throughput.

     If 'innodb_flush_method' is set to 'NULL' on a Unix-like system,
     the 'fsync' option is used by default.  If 'innodb_flush_method' is
     set to 'NULL' on Windows, the 'async_unbuffered' option is used by
     default.

     The 'innodb_flush_method' options for Unix-like systems include:

        * 'fsync': 'InnoDB' uses the 'fsync()' system call to flush both
          the data and log files.  'fsync' is the default setting.

        * 'O_DSYNC': 'InnoDB' uses 'O_SYNC' to open and flush the log
          files, and 'fsync()' to flush the data files.  'InnoDB' does
          not use 'O_DSYNC' directly because there have been problems
          with it on many varieties of Unix.

        * 'littlesync': This option is used for internal performance
          testing and is currently unsupported.  Use at your own risk.

        * 'nosync': This option is used for internal performance testing
          and is currently unsupported.  Use at your own risk.

        * 'O_DIRECT': 'InnoDB' uses 'O_DIRECT' (or 'directio()' on
          Solaris) to open the data files, and uses 'fsync()' to flush
          both the data and log files.  This option is available on some
          GNU/Linux versions, FreeBSD, and Solaris.

     The 'innodb_flush_method' options for Windows systems include:

        * 'async_unbuffered': 'InnoDB' uses Windows asynchronous I/O and
          non-buffered I/O. 'async_unbuffered' is the default setting on
          Windows systems.

          Running MySQL server on a 4K sector hard drive on Windows is
          not supported with 'async_unbuffered'.  The workaround is to
          use 'innodb_flush_method=normal'.

        * 'normal': 'InnoDB' uses simulated asynchronous I/O and
          buffered I/O. This option is used for internal performance
          testing and is currently unsupported.  Use at your own risk.

        * 'unbuffered': 'InnoDB' uses simulated asynchronous I/O and
          non-buffered I/O. This option is used for internal performance
          testing and is currently unsupported.  Use at your own risk.

     How each setting affects performance depends on hardware
     configuration and workload.  Benchmark your particular
     configuration to decide which setting to use, or whether to keep
     the default setting.  Examine the 'Innodb_data_fsyncs' status
     variable to see the overall number of 'fsync()' calls for each
     setting.  The mix of read and write operations in your workload can
     affect how a setting performs.  For example, on a system with a
     hardware RAID controller and battery-backed write cache, 'O_DIRECT'
     can help to avoid double buffering between the 'InnoDB' buffer pool
     and the operating system file system cache.  On some systems where
     'InnoDB' data and log files are located on a SAN, the default value
     or 'O_DSYNC' might be faster for a read-heavy workload with mostly
     'SELECT' statements.  Always test this parameter with hardware and
     workload that reflect your production environment.  For general I/O
     tuning advice, see *note optimizing-innodb-diskio::.

     Prior to MySQL 5.1.24, the default 'innodb_flush_method' option was
     named 'fdatasync'.  When 'fdatasync' was specified, 'InnoDB' used
     the 'fsync()' system call to flush both the data and log files.  To
     avoid confusing the 'fdatasync' option name with the 'fdatasync()'
     system call, the option name was changed to 'fsync' in MySQL
     5.1.24.

   * 
     'innodb_force_load_corrupted'

     Property               Value
                            
     *Command-Line          '--innodb-force-load-corrupted[={OFF|ON}]'
     Format*                

     *Introduced*           5.5.18
                            
     *System Variable*      'innodb_force_load_corrupted'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Boolean
                            
     *Default Value*        'OFF'

     Permits 'InnoDB' to load tables at startup that are marked as
     corrupted.  Use only during troubleshooting, to recover data that
     is otherwise inaccessible.  When troubleshooting is complete,
     disable this setting and restart the server.

   * 
     'innodb_force_recovery'

     Property               Value
                            
     *Command-Line          '--innodb-force-recovery=#'
     Format*                

     *System Variable*      'innodb_force_recovery'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Integer
                            
     *Default Value*        '0'
                            
     *Minimum Value*        '0'
                            
     *Maximum Value*        '6'

     The crash recovery mode, typically only changed in serious
     troubleshooting situations.  Possible values are from 0 to 6.  For
     the meanings of these values and important information about
     'innodb_force_recovery', see *note forcing-innodb-recovery::.

     *Warning*:

     Only set this variable to a value greater than 0 in an emergency
     situation so that you can start 'InnoDB' and dump your tables.  As
     a safety measure, 'InnoDB' prevents *note 'INSERT': insert, *note
     'UPDATE': update, or *note 'DELETE': delete. operations when
     'innodb_force_recovery' is greater than 0.

   * 
     'innodb_io_capacity'

     Property               Value
                            
     *Command-Line          '--innodb-io-capacity=#'
     Format*                

     *System Variable*      'innodb_io_capacity'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '200'
                            
     *Minimum Value*        '100'
                            
     *Maximum Value*        '2**64-1'
     (64-bit platforms)     

     *Maximum Value*        '2**32-1'
     (32-bit platforms)

     The 'innodb_io_capacity' variable defines the number of I/O
     operations per second (IOPS) available to 'InnoDB' background
     tasks, such as flushing pages from the buffer pool and merging data
     from the change buffer.

     For information about configuring the 'innodb_io_capacity'
     variable, see *note innodb-configuring-io-capacity::.

   * 
     'innodb_large_prefix'

     Property               Value
                            
     *Command-Line          '--innodb-large-prefix[={OFF|ON}]'
     Format*                

     *Introduced*           5.5.14
                            
     *System Variable*      'innodb_large_prefix'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Boolean
                            
     *Default Value*        'OFF'

     Enable this option to allow index key prefixes longer than 767
     bytes (up to 3072 bytes), for 'InnoDB' tables that use 'DYNAMIC' or
     'COMPRESSED' row format.  (Creating such tables also requires the
     option values 'innodb_file_format=barracuda' and
     'innodb_file_per_table=true'.)  See *note innodb-limits:: for
     maximums associated with index key prefixes under various settings.

     For tables that use 'REDUNDANT' or 'COMPACT' row format, this
     option does not affect the permitted index key prefix length.  When
     this setting is enabled, attempting to create an index prefix with
     a key length greater than 3072 for a 'REDUNDANT' or 'COMPACT' table
     causes an 'ER_INDEX_COLUMN_TOO_LONG' error.

   * 
     'innodb_limit_optimistic_insert_debug'

     Property               Value
                            
     *Command-Line          '--innodb-limit-optimistic-insert-debug=#'
     Format*                

     *System Variable*      'innodb_limit_optimistic_insert_debug'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '0'
                            
     *Minimum Value*        '0'
                            
     *Maximum Value*        '2**32-1'

     Limits the number of records per B-tree page.  A default value of 0
     means that no limit is imposed.  This option is only available if
     debugging support is compiled in using the 'WITH_DEBUG' 'CMake'
     option.

   * 
     'innodb_lock_wait_timeout'

     Property               Value
                            
     *Command-Line          '--innodb-lock-wait-timeout=#'
     Format*                

     *System Variable*      'innodb_lock_wait_timeout'
                            
     *Scope*                Global, Session
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '50'
                            
     *Minimum Value*        '1'
                            
     *Maximum Value*        '1073741824'

     The length of time in seconds an 'InnoDB' transaction waits for a
     row lock before giving up.  The default value is 50 seconds.  A
     transaction that tries to access a row that is locked by another
     'InnoDB' transaction waits at most this many seconds for write
     access to the row before issuing the following error:

          ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction

     When a lock wait timeout occurs, the current statement is rolled
     back (not the entire transaction).  To have the entire transaction
     roll back, start the server with the '--innodb-rollback-on-timeout'
     option.  See also *note innodb-error-handling::.

     You might decrease this value for highly interactive applications
     or OLTP systems, to display user feedback quickly or put the update
     into a queue for processing later.  You might increase this value
     for long-running back-end operations, such as a transform step in a
     data warehouse that waits for other large insert or update
     operations to finish.

     'innodb_lock_wait_timeout' applies to 'InnoDB' row locks only.  A
     MySQL table lock does not happen inside 'InnoDB' and this timeout
     does not apply to waits for table locks.

     The lock wait timeout value does not apply to deadlocks, because
     'InnoDB' detects them immediately and rolls back one of the
     deadlocked transactions.  See *note innodb-deadlock-detection::.

     'innodb_lock_wait_timeout' can be set at runtime with the 'SET
     GLOBAL' or 'SET SESSION' statement.  Changing the 'GLOBAL' setting
     requires privileges sufficient to set global system variables (see
     *note system-variable-privileges::) and affects the operation of
     all clients that subsequently connect.  Any client can change the
     'SESSION' setting for 'innodb_lock_wait_timeout', which affects
     only that client.

   * 
     'innodb_locks_unsafe_for_binlog'

     Property               Value
                            
     *Command-Line          '--innodb-locks-unsafe-for-binlog[={OFF|ON}]'
     Format*                

     *System Variable*      'innodb_locks_unsafe_for_binlog'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Boolean
                            
     *Default Value*        'OFF'

     This variable affects how 'InnoDB' uses gap locking for searches
     and index scans.  Normally, 'InnoDB' uses an algorithm called
     _next-key locking_ that combines index-row locking with gap
     locking.  'InnoDB' performs row-level locking in such a way that
     when it searches or scans a table index, it sets shared or
     exclusive locks on the index records it encounters.  Thus,
     row-level locks are actually index-record locks.  In addition, a
     next-key lock on an index record also affects the gap before the
     index record.  That is, a next-key lock is an index-record lock
     plus a gap lock on the gap preceding the index record.  If one
     session has a shared or exclusive lock on record 'R' in an index,
     another session cannot insert a new index record in the gap
     immediately before 'R' in the index order.  See *note
     innodb-locking::.

     By default, the value of 'innodb_locks_unsafe_for_binlog' is 0
     (disabled), which means that gap locking is enabled: 'InnoDB' uses
     next-key locks for searches and index scans.  To enable the
     variable, set it to 1.  This causes gap locking to be disabled:
     'InnoDB' uses only index-record locks for searches and index scans.

     Enabling 'innodb_locks_unsafe_for_binlog' does not disable the use
     of gap locking for foreign-key constraint checking or duplicate-key
     checking.

     The effects of enabling 'innodb_locks_unsafe_for_binlog' are the
     same as setting the transaction isolation level to 'READ
     COMMITTED', with these exceptions:

        * Enabling 'innodb_locks_unsafe_for_binlog' is a global setting
          and affects all sessions, whereas the isolation level can be
          set globally for all sessions, or individually per session.

        * 'innodb_locks_unsafe_for_binlog' can be set only at server
          startup, whereas the isolation level can be set at startup or
          changed at runtime.

     'READ COMMITTED' therefore offers finer and more flexible control
     than 'innodb_locks_unsafe_for_binlog'.  For more information about
     the effect of isolation level on gap locking, see *note
     innodb-transaction-isolation-levels::.

     Enabling 'innodb_locks_unsafe_for_binlog' may cause phantom
     problems because other sessions can insert new rows into the gaps
     when gap locking is disabled.  Suppose that there is an index on
     the 'id' column of the 'child' table and that you want to read and
     lock all rows from the table having an identifier value larger than
     100, with the intention of updating some column in the selected
     rows later:

          SELECT * FROM child WHERE id > 100 FOR UPDATE;

     The query scans the index starting from the first record where the
     'id' is greater than 100.  If the locks set on the index records in
     that range do not lock out inserts made in the gaps, another
     session can insert a new row into the table.  Consequently, if you
     were to execute the same *note 'SELECT': select. again within the
     same transaction, you would see a new row in the result set
     returned by the query.  This also means that if new items are added
     to the database, 'InnoDB' does not guarantee serializability.
     Therefore, if 'innodb_locks_unsafe_for_binlog' is enabled, 'InnoDB'
     guarantees at most an isolation level of 'READ COMMITTED'.
     (Conflict serializability is still guaranteed.)  For more
     information about phantoms, see *note innodb-next-key-locking::.

     Enabling 'innodb_locks_unsafe_for_binlog' has additional effects:

        * For *note 'UPDATE': update. or *note 'DELETE': delete.
          statements, 'InnoDB' holds locks only for rows that it updates
          or deletes.  Record locks for nonmatching rows are released
          after MySQL has evaluated the 'WHERE' condition.  This greatly
          reduces the probability of deadlocks, but they can still
          happen.

        * For *note 'UPDATE': update. statements, if a row is already
          locked, 'InnoDB' performs a 'semi-consistent' read, returning
          the latest committed version to MySQL so that MySQL can
          determine whether the row matches the 'WHERE' condition of the
          *note 'UPDATE': update.  If the row matches (must be updated),
          MySQL reads the row again and this time 'InnoDB' either locks
          it or waits for a lock on it.

     Consider the following example, beginning with this table:

          CREATE TABLE t (a INT NOT NULL, b INT) ENGINE = InnoDB;
          INSERT INTO t VALUES (1,2),(2,3),(3,2),(4,3),(5,2);
          COMMIT;

     In this case, table has no indexes, so searches and index scans use
     the hidden clustered index for record locking (see *note
     innodb-index-types::).

     Suppose that one client performs an *note 'UPDATE': update. using
     these statements:

          SET autocommit = 0;
          UPDATE t SET b = 5 WHERE b = 3;

     Suppose also that a second client performs an *note 'UPDATE':
     update. by executing these statements following those of the first
     client:

          SET autocommit = 0;
          UPDATE t SET b = 4 WHERE b = 2;

     As *note 'InnoDB': innodb-storage-engine. executes each *note
     'UPDATE': update, it first acquires an exclusive lock for each row,
     and then determines whether to modify it.  If *note 'InnoDB':
     innodb-storage-engine. does not modify the row and
     'innodb_locks_unsafe_for_binlog' is enabled, it releases the lock.
     Otherwise, *note 'InnoDB': innodb-storage-engine. retains the lock
     until the end of the transaction.  This affects transaction
     processing as follows.

     If 'innodb_locks_unsafe_for_binlog' is disabled, the first *note
     'UPDATE': update. acquires x-locks and does not release any of
     them:

          x-lock(1,2); retain x-lock
          x-lock(2,3); update(2,3) to (2,5); retain x-lock
          x-lock(3,2); retain x-lock
          x-lock(4,3); update(4,3) to (4,5); retain x-lock
          x-lock(5,2); retain x-lock

     The second *note 'UPDATE': update. blocks as soon as it tries to
     acquire any locks (because the first update has retained locks on
     all rows), and does not proceed until the first *note 'UPDATE':
     update. commits or rolls back:

          x-lock(1,2); block and wait for first UPDATE to commit or roll back

     If 'innodb_locks_unsafe_for_binlog' is enabled, the first *note
     'UPDATE': update. acquires x-locks and releases those for rows that
     it does not modify:

          x-lock(1,2); unlock(1,2)
          x-lock(2,3); update(2,3) to (2,5); retain x-lock
          x-lock(3,2); unlock(3,2)
          x-lock(4,3); update(4,3) to (4,5); retain x-lock
          x-lock(5,2); unlock(5,2)

     For the second 'UPDATE', 'InnoDB' does a 'semi-consistent' read,
     returning the latest committed version of each row to MySQL so that
     MySQL can determine whether the row matches the 'WHERE' condition
     of the *note 'UPDATE': update.:

          x-lock(1,2); update(1,2) to (1,4); retain x-lock
          x-lock(2,3); unlock(2,3)
          x-lock(3,2); update(3,2) to (3,4); retain x-lock
          x-lock(4,3); unlock(4,3)
          x-lock(5,2); update(5,2) to (5,4); retain x-lock

   * 
     'innodb_log_buffer_size'

     Property               Value
                            
     *Command-Line          '--innodb-log-buffer-size=#'
     Format*                

     *System Variable*      'innodb_log_buffer_size'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Integer
                            
     *Default Value*        '8388608'
                            
     *Minimum Value*        '262144'
                            
     *Maximum Value*        '4294967295'

     The size in bytes of the buffer that 'InnoDB' uses to write to the
     log files on disk.  The default value is 8MB. A large log buffer
     enables large transactions to run without the need to write the log
     to disk before the transactions commit.  Thus, if you have
     transactions that update, insert, or delete many rows, making the
     log buffer larger saves disk I/O. For related information, see
     *note innodb-startup-memory-configuration::, and *note
     optimizing-innodb-logging::.  For general I/O tuning advice, see
     *note optimizing-innodb-diskio::.

   * 
     'innodb_log_file_size'

     Property               Value
                            
     *Command-Line          '--innodb-log-file-size=#'
     Format*                

     *System Variable*      'innodb_log_file_size'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Integer
                            
     *Default Value*        '5242880'
                            
     *Minimum Value*        '1048576'
                            
     *Maximum Value*        '4GB / innodb_log_files_in_group'

     The size in bytes of each log file in a log group.  The combined
     size of log files ('innodb_log_file_size' *
     'innodb_log_files_in_group') cannot exceed a maximum value that is
     slightly less than 4GB. A pair of 2047 MB log files, for example,
     approaches the limit but does not exceed it.  The default value is
     5MB.

     Generally, the combined size of the log files should be large
     enough that the server can smooth out peaks and troughs in workload
     activity, which often means that there is enough redo log space to
     handle more than an hour of write activity.  The larger the value,
     the less checkpoint flush activity is required in the buffer pool,
     saving disk I/O. Larger log files also make crash recovery slower,
     although improvements to recovery performance in MySQL 5.5 and
     higher make the log file size less of a consideration.

     For related information, see *note
     innodb-startup-log-file-configuration::.  For general I/O tuning
     advice, see *note optimizing-innodb-diskio::.

   * 
     'innodb_log_files_in_group'

     Property               Value
                            
     *Command-Line          '--innodb-log-files-in-group=#'
     Format*                

     *System Variable*      'innodb_log_files_in_group'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Integer
                            
     *Default Value*        '2'
                            
     *Minimum Value*        '2'
                            
     *Maximum Value*        '100'

     The number of log files in the log group.  'InnoDB' writes to the
     files in a circular fashion.  The default (and recommended) value
     is 2.  The location of the files is specified by
     'innodb_log_group_home_dir'.

     For related information, see *note
     innodb-startup-log-file-configuration::.

   * 
     'innodb_log_group_home_dir'

     Property               Value
                            
     *Command-Line          '--innodb-log-group-home-dir=dir_name'
     Format*                

     *System Variable*      'innodb_log_group_home_dir'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Directory name

     The directory path to the 'InnoDB' redo log files, whose number is
     specified by 'innodb_log_files_in_group'.  If you do not specify
     any 'InnoDB' log variables, the default is to create two files
     named 'ib_logfile0' and 'ib_logfile1' in the MySQL data directory.
     Log file size is given by the 'innodb_log_file_size' system
     variable.

     For related information, see *note
     innodb-startup-log-file-configuration::.

   * 
     'innodb_max_dirty_pages_pct'

     Property               Value
                            
     *Command-Line          '--innodb-max-dirty-pages-pct=#'
     Format*                

     *System Variable*      'innodb_max_dirty_pages_pct'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Numeric
                            
     *Default Value*        '75'
                            
     *Minimum Value*        '0'
                            
     *Maximum Value*        '99'

     'InnoDB' tries to flush data from the buffer pool so that the
     percentage of dirty pages does not exceed this value.  Specify an
     integer in the range from 0 to 99.  The default value is 75.

     For additional information about this variable, see *note
     innodb-buffer-pool-flushing::.  For general I/O tuning advice, see
     *note optimizing-innodb-diskio::.

   * 
     'innodb_max_purge_lag'

     Property               Value
                            
     *Command-Line          '--innodb-max-purge-lag=#'
     Format*                

     *System Variable*      'innodb_max_purge_lag'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '0'
                            
     *Minimum Value*        '0'
                            
     *Maximum Value*        '4294967295'

     Defines the desired maximum purge lag.  If this value is exceeded,
     a delay is imposed on *note 'INSERT': insert, *note 'UPDATE':
     update, and *note 'DELETE': delete. operations to allow time for
     purge to catch up.  The default value is 0, which means there is no
     maximum purge lag and no delay.

     For more information, see *note innodb-purge-configuration::.

   * 
     'innodb_mirrored_log_groups'

     Has no effect.

   * 
     'innodb_old_blocks_pct'

     Property               Value
                            
     *Command-Line          '--innodb-old-blocks-pct=#'
     Format*                

     *System Variable*      'innodb_old_blocks_pct'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '37'
                            
     *Minimum Value*        '5'
                            
     *Maximum Value*        '95'

     Specifies the approximate percentage of the 'InnoDB' buffer pool
     used for the old block sublist.  The range of values is 5 to 95.
     The default value is 37 (that is, 3/8 of the pool).

     For more information, see *note
     innodb-performance-midpoint_insertion::.  For information about
     buffer pool management, the LRU algorithm, and eviction policies,
     see *note innodb-buffer-pool::.

   * 
     'innodb_old_blocks_time'

     Property               Value
                            
     *Command-Line          '--innodb-old-blocks-time=#'
     Format*                

     *System Variable*      'innodb_old_blocks_time'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '0'
                            
     *Minimum Value*        '0'
                            
     *Maximum Value*        '2**32-1'

     Non-zero values protect against the buffer pool being filled by
     data that is referenced only for a brief period, such as during a
     full table scan.  Increasing this value offers more protection
     against full table scans interfering with data cached in the buffer
     pool.

     Specifies how long in milliseconds a block inserted into the old
     sublist must stay there after its first access before it can be
     moved to the new sublist.  If the value is 0, a block inserted into
     the old sublist moves immediately to the new sublist the first time
     it is accessed, no matter how soon after insertion the access
     occurs.  If the value is greater than 0, blocks remain in the old
     sublist until an access occurs at least that many milliseconds
     after the first access.  For example, a value of 1000 causes blocks
     to stay in the old sublist for 1 second after the first access
     before they become eligible to move to the new sublist.

     This variable is often used in combination with
     'innodb_old_blocks_pct'.  For more information, see *note
     innodb-performance-midpoint_insertion::.  For information about
     buffer pool management, the LRU algorithm, and eviction policies,
     see *note innodb-buffer-pool::.

   * 
     'innodb_open_files'

     Property               Value
                            
     *Command-Line          '--innodb-open-files=#'
     Format*                

     *System Variable*      'innodb_open_files'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Integer
                            
     *Default Value*        '300'
                            
     *Minimum Value*        '10'
                            
     *Maximum Value*        '4294967295'

     This variable is only relevant if you use multiple 'InnoDB'
     tablespaces.  It specifies the maximum number of '.ibd' files that
     MySQL can keep open at one time.  The minimum value is 10.  The
     default value is 300.

     The file descriptors used for '.ibd' files are for 'InnoDB' tables
     only.  They are independent of those specified by the
     'open_files_limit' system variable, and do not affect the operation
     of the table cache.

   * 
     'innodb_print_all_deadlocks'

     Property               Value
                            
     *Command-Line          '--innodb-print-all-deadlocks[={OFF|ON}]'
     Format*                

     *Introduced*           5.5.30
                            
     *System Variable*      'innodb_print_all_deadlocks'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Boolean
                            
     *Default Value*        'OFF'

     When this option is enabled, information about all deadlocks in
     'InnoDB' user transactions is recorded in the 'mysqld' *note error
     log: error-log.  Otherwise, you see information about only the last
     deadlock, using the 'SHOW ENGINE INNODB STATUS' command.  An
     occasional 'InnoDB' deadlock is not necessarily an issue, because
     'InnoDB' detects the condition immediately and rolls back one of
     the transactions automatically.  You might use this option to
     troubleshoot why deadlocks are occurring if an application does not
     have appropriate error-handling logic to detect the rollback and
     retry its operation.  A large number of deadlocks might indicate
     the need to restructure transactions that issue DML or 'SELECT ...
     FOR UPDATE' statements for multiple tables, so that each
     transaction accesses the tables in the same order, thus avoiding
     the deadlock condition.

     For related information, see *note innodb-deadlocks::.

   * 
     'innodb_purge_batch_size'

     Property               Value
                            
     *Command-Line          '--innodb-purge-batch-size=#'
     Format*                

     *Introduced*           5.5.4
                            
     *System Variable*      'innodb_purge_batch_size'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '20'
                            
     *Minimum Value*        '1'
                            
     *Maximum Value*        '5000'

     Defines the number of undo log pages that purge parses and
     processes in one batch from the history list.  It also defines the
     number of undo log pages that purge frees after every 128
     iterations through the undo logs.

     The 'innodb_purge_batch_size' option is intended for advanced
     performance tuning in combination with the 'innodb_purge_threads'
     setting.  Most users need not change 'innodb_purge_batch_size' from
     its default value.

     For related information, see *note innodb-purge-configuration::.

   * 
     'innodb_purge_threads'

     Property               Value
                            
     *Command-Line          '--innodb-purge-threads=#'
     Format*                

     *Introduced*           5.5.4
                            
     *System Variable*      'innodb_purge_threads'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Integer
                            
     *Default Value*        '0'
                            
     *Minimum Value*        '0'
                            
     *Maximum Value*        '1'

     The number of background threads devoted to the 'InnoDB' purge
     operation.  Currently, the value can only be 0 (the default) or 1.
     The default value of 0 means that the purge operation is performed
     by the 'InnoDB' master thread.

     For related information, see *note innodb-purge-configuration::.

   * 
     'innodb_random_read_ahead'

     Property               Value
                            
     *Command-Line          '--innodb-random-read-ahead[={OFF|ON}]'
     Format*                

     *Introduced*           5.5.16
                            
     *System Variable*      'innodb_random_read_ahead'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Boolean
                            
     *Default Value*        'OFF'

     Enables the random read-ahead technique for optimizing 'InnoDB'
     I/O. Random read-ahead functionality was removed from the 'InnoDB
     Plugin' (version 1.0.4) and was therefore not included in MySQL
     5.5.0 when 'InnoDB Plugin' became the 'built-in' version of
     'InnoDB'.  Random read-ahead was reintroduced in MySQL 5.1.59 and
     5.5.16 and higher along with the 'innodb_random_read_ahead'
     configuration option, which is disabled by default.

     For details about performance considerations for different types of
     read-ahead requests, see *note innodb-performance-read_ahead::.
     For general I/O tuning advice, see *note
     optimizing-innodb-diskio::.

   * 
     'innodb_read_ahead_threshold'

     Property               Value
                            
     *Command-Line          '--innodb-read-ahead-threshold=#'
     Format*                

     *System Variable*      'innodb_read_ahead_threshold'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '56'
                            
     *Minimum Value*        '0'
                            
     *Maximum Value*        '64'

     Controls the sensitivity of linear read-ahead that 'InnoDB' uses to
     prefetch pages into the buffer pool.  If 'InnoDB' reads at least
     'innodb_read_ahead_threshold' pages sequentially from an extent (64
     pages), it initiates an asynchronous read for the entire following
     extent.  The permissible range of values is 0 to 64.  The default
     is 56: 'InnoDB' must read at least 56 pages sequentially from an
     extent to initiate an asynchronous read for the following extent.

     Knowing how many pages are read through the read-ahead mechanism,
     and how many of these pages are evicted from the buffer pool
     without ever being accessed, can be useful when fine-tuning the
     'innodb_read_ahead_threshold' setting.  As of MySQL 5.5, *note
     'SHOW ENGINE INNODB STATUS': show-engine. output displays counter
     information from the 'Innodb_buffer_pool_read_ahead' and
     'Innodb_buffer_pool_read_ahead_evicted' global status variables,
     which report the number of pages brought into the buffer pool by
     read-ahead requests, and the number of such pages evicted from the
     buffer pool without ever being accessed, respectively.  The status
     variables report global values since the last server restart.

     *note 'SHOW ENGINE INNODB STATUS': show-engine. also shows the rate
     at which the read-ahead pages are read and the rate at which such
     pages are evicted without being accessed.  The per-second averages
     are based on the statistics collected since the last invocation of
     'SHOW ENGINE INNODB STATUS' and are displayed in the 'BUFFER POOL
     AND MEMORY' section of the *note 'SHOW ENGINE INNODB STATUS':
     show-engine. output.

     For more information, see *note innodb-performance-read_ahead::.
     For general I/O tuning advice, see *note
     optimizing-innodb-diskio::.

   * 
     'innodb_read_io_threads'

     Property               Value
                            
     *Command-Line          '--innodb-read-io-threads=#'
     Format*                

     *System Variable*      'innodb_read_io_threads'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Integer
                            
     *Default Value*        '4'
                            
     *Minimum Value*        '1'
                            
     *Maximum Value*        '64'

     The number of I/O threads for read operations in 'InnoDB'.  Its
     counterpart for write threads is 'innodb_write_io_threads'.  For
     more information, see *note
     innodb-performance-multiple_io_threads::.  For general I/O tuning
     advice, see *note optimizing-innodb-diskio::.

     *Note*:

     On Linux systems, running multiple MySQL servers (typically more
     than 12) with default settings for 'innodb_read_io_threads',
     'innodb_write_io_threads', and the Linux 'aio-max-nr' setting can
     exceed system limits.  Ideally, increase the 'aio-max-nr' setting;
     as a workaround, you might reduce the settings for one or both of
     the MySQL variables.

   * 
     'innodb_replication_delay'

     Property               Value
                            
     *Command-Line          '--innodb-replication-delay=#'
     Format*                

     *System Variable*      'innodb_replication_delay'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '0'
                            
     *Minimum Value*        '0'
                            
     *Maximum Value*        '4294967295'

     The replication thread delay in milliseconds on a slave server if
     'innodb_thread_concurrency' is reached.

   * 
     'innodb_rollback_on_timeout'

     Property               Value
                            
     *Command-Line          '--innodb-rollback-on-timeout[={OFF|ON}]'
     Format*                

     *System Variable*      'innodb_rollback_on_timeout'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Boolean
                            
     *Default Value*        'OFF'

     'InnoDB' rolls back only the last statement on a transaction
     timeout by default.  If '--innodb-rollback-on-timeout' is
     specified, a transaction timeout causes 'InnoDB' to abort and roll
     back the entire transaction (the same behavior as in MySQL 4.1).

     *Note*:

     If the start-transaction statement was *note 'START TRANSACTION':
     commit. or *note 'BEGIN': commit. statement, rollback does not
     cancel that statement.  Further SQL statements become part of the
     transaction until the occurrence of *note 'COMMIT': commit, *note
     'ROLLBACK': commit, or some SQL statement that causes an implicit
     commit.

     For more information, see *note innodb-error-handling::.

   * 
     'innodb_rollback_segments'

     Property               Value
                            
     *Command-Line          '--innodb-rollback-segments=#'
     Format*                

     *Introduced*           5.5.11
                            
     *System Variable*      'innodb_rollback_segments'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '128'
                            
     *Minimum Value*        '1'
                            
     *Maximum Value*        '128'

     Defines the number of rollback segments used by 'InnoDB' for
     transactions that generate undo records.  Each rollback segment can
     support up to 1024 transactions, depending on the number of undo
     logs assigned to each transaction.  For more information, see *note
     innodb-undo-logs::.

     This setting is appropriate for tuning performance if you observe
     mutex contention related to the undo logs.

     Although you can increase or decrease the number of rollback
     segments used by 'InnoDB', the number of rollback segments
     physically present in the system never decreases.  Thus, you might
     start with a low value and gradually increase it to avoid
     allocating rollback segments that are not required.  The
     'innodb_rollback_segments' default and maximum value is 128.

     For related information, see *note innodb-multi-versioning::.

   * 
     'innodb_spin_wait_delay'

     Property               Value
                            
     *Command-Line          '--innodb-spin-wait-delay=#'
     Format*                

     *System Variable*      'innodb_spin_wait_delay'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '6'
                            
     *Minimum Value*        '0'
                            
     *Maximum Value*        '2**64-1'
     (64-bit platforms)     

     *Maximum Value*        '2**32-1'
     (32-bit platforms)

     The maximum delay between polls for a spin lock.  The low-level
     implementation of this mechanism varies depending on the
     combination of hardware and operating system, so the delay does not
     correspond to a fixed time interval.  For more information, see
     *note innodb-performance-spin_lock_polling::.

   * 
     'innodb_stats_method'

     Property               Value
                            
     *Command-Line          '--innodb-stats-method=value'
     Format*                

     *Introduced*           5.5.10
                            
     *System Variable*      'innodb_stats_method'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Enumeration
                            
     *Default Value*        'nulls_equal'
                            
     *Valid Values*         'nulls_equal' 'nulls_unequal' 'nulls_ignored'

     How the server treats 'NULL' values when collecting statistics
     about the distribution of index values for 'InnoDB' tables.
     Permitted values are 'nulls_equal', 'nulls_unequal', and
     'nulls_ignored'.  For 'nulls_equal', all 'NULL' index values are
     considered equal and form a single value group with a size equal to
     the number of 'NULL' values.  For 'nulls_unequal', 'NULL' values
     are considered unequal, and each 'NULL' forms a distinct value
     group of size 1.  For 'nulls_ignored', 'NULL' values are ignored.

     The method used to generate table statistics influences how the
     optimizer chooses indexes for query execution, as described in
     *note index-statistics::.

   * 
     'innodb_stats_on_metadata'

     Property               Value
                            
     *Command-Line          '--innodb-stats-on-metadata[={OFF|ON}]'
     Format*                

     *Introduced*           5.5.4
                            
     *System Variable*      'innodb_stats_on_metadata'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Boolean
                            
     *Default Value*        'ON'

     When this option is enabled (the default), 'InnoDB' updates
     statistics when metadata statements such as *note 'SHOW TABLE
     STATUS': show-table-status. or *note 'SHOW INDEX': show-index. are
     run, or when accessing the *note 'INFORMATION_SCHEMA.TABLES':
     tables-table. or *note 'INFORMATION_SCHEMA.STATISTICS':
     statistics-table. tables.  (These updates are similar to what
     happens for *note 'ANALYZE TABLE': analyze-table.)  When disabled,
     'InnoDB' does not update statistics during these operations.
     Disabling this variable can improve access speed for schemas that
     have a large number of tables or indexes.  It can also improve the
     stability of execution plans for queries that involve 'InnoDB'
     tables.

     To change the setting, issue the statement 'SET GLOBAL
     innodb_stats_on_metadata=MODE', where 'MODE' is either 'ON' or
     'OFF' (or '1' or '0').  Changing the setting requires privileges
     sufficient to set global system variables (see *note
     system-variable-privileges::) and immediately affects the operation
     of all connections.

   * 
     'innodb_stats_sample_pages'

     Property               Value
                            
     *Command-Line          '--innodb-stats-sample-pages=#'
     Format*                

     *System Variable*      'innodb_stats_sample_pages'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '8'
                            
     *Minimum Value*        '1'
                            
     *Maximum Value*        '2**64-1'

     The number of index pages to sample for index distribution
     statistics such as are calculated by *note 'ANALYZE TABLE':
     analyze-table.  The default value is 8.  For more information, see
     *note innodb-statistics-estimation::.

     Setting a high value for 'innodb_stats_sample_pages' could result
     in lengthy *note 'ANALYZE TABLE': analyze-table. execution time.
     To estimate the number of database pages accessed by *note 'ANALYZE
     TABLE': analyze-table, see *note innodb-analyze-table-complexity::.

   * 
     'innodb_strict_mode'

     Property               Value
                            
     *Command-Line          '--innodb-strict-mode[={OFF|ON}]'
     Format*                

     *System Variable*      'innodb_strict_mode'
                            
     *Scope*                Global, Session
                            
     *Dynamic*              Yes
                            
     *Type*                 Boolean
                            
     *Default Value*        'OFF'

     When 'innodb_strict_mode' is enabled, 'InnoDB' returns errors
     rather than warnings for certain conditions.

     Strict mode helps guard against ignored typos and syntax errors in
     SQL, or other unintended consequences of various combinations of
     operational modes and SQL statements.  When 'innodb_strict_mode' is
     enabled, 'InnoDB' raises error conditions in certain cases, rather
     than issuing a warning and processing the specified statement
     (perhaps with unintended behavior).  This is analogous to *note
     'sql_mode': sql-mode. in MySQL, which controls what SQL syntax
     MySQL accepts, and determines whether it silently ignores errors,
     or validates input syntax and data values.

     The 'innodb_strict_mode' setting affects the handling of syntax
     errors for *note 'CREATE TABLE': create-table, *note 'ALTER TABLE':
     alter-table. and *note 'CREATE INDEX': create-index. statements.
     'innodb_strict_mode' also enables a record size check, so that an
     'INSERT' or 'UPDATE' never fails due to the record being too large
     for the selected page size.

     Oracle recommends enabling 'innodb_strict_mode' when using
     'ROW_FORMAT' and 'KEY_BLOCK_SIZE' clauses in *note 'CREATE TABLE':
     create-table, *note 'ALTER TABLE': alter-table, and *note 'CREATE
     INDEX': create-index. statements.  When 'innodb_strict_mode' is
     disabled, 'InnoDB' ignores conflicting clauses and creates the
     table or index with only a warning in the message log.  The
     resulting table might have different characteristics than intended,
     such as lack of compression support when attempting to create a
     compressed table.  When 'innodb_strict_mode' is enabled, such
     problems generate an immediate error and the table or index is not
     created.

     You can enable or disable 'innodb_strict_mode' on the command line
     when starting 'mysqld', or in a MySQL configuration file.  You can
     also enable or disable 'innodb_strict_mode' at runtime with the
     statement 'SET [GLOBAL|SESSION] innodb_strict_mode=MODE', where
     'MODE' is either 'ON' or 'OFF'.  Changing the 'GLOBAL' setting
     requires privileges sufficient to set global system variables (see
     *note system-variable-privileges::) and affects the operation of
     all clients that subsequently connect.  Any client can change the
     'SESSION' setting for 'innodb_strict_mode', and the setting affects
     only that client.

   * 
     'innodb_support_xa'

     Property               Value
                            
     *Command-Line          '--innodb-support-xa[={OFF|ON}]'
     Format*                

     *System Variable*      'innodb_support_xa'
                            
     *Scope*                Global, Session
                            
     *Dynamic*              Yes
                            
     *Type*                 Boolean
                            
     *Default Value*        'ON'

     Enables 'InnoDB' support for two-phase commit in XA transactions,
     causing an extra disk flush for transaction preparation.  The XA
     mechanism is used internally and is essential for any server that
     has its binary log turned on and is accepting changes to its data
     from more than one thread.  If you disable 'innodb_support_xa',
     transactions can be written to the binary log in a different order
     than the live database is committing them, which can produce
     different data when the binary log is replayed in disaster recovery
     or on a replication slave.  Do not disable 'innodb_support_xa' on a
     replication master server unless you have an unusual setup where
     only one thread is able to change data.

     For a server that is accepting data changes from only one thread,
     it is safe and recommended to disable this option to improve
     performance for *note 'InnoDB': innodb-storage-engine. tables.  For
     example, you can turn it off on replication slaves where only the
     replication SQL thread is changing data.

     You can also disable this option if you do not need it for safe
     binary logging or replication, and you also do not use an external
     XA transaction manager.

   * 
     'innodb_sync_spin_loops'

     Property               Value
                            
     *Command-Line          '--innodb-sync-spin-loops=#'
     Format*                

     *System Variable*      'innodb_sync_spin_loops'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '30'
                            
     *Minimum Value*        '0'
                            
     *Maximum Value*        '4294967295'

     The number of times a thread waits for an 'InnoDB' mutex to be
     freed before the thread is suspended.

   * 
     'innodb_table_locks'

     Property               Value
                            
     *Command-Line          '--innodb-table-locks[={OFF|ON}]'
     Format*                

     *System Variable*      'innodb_table_locks'
                            
     *Scope*                Global, Session
                            
     *Dynamic*              Yes
                            
     *Type*                 Boolean
                            
     *Default Value*        'ON'

     If 'autocommit = 0', 'InnoDB' honors *note 'LOCK TABLES':
     lock-tables.; MySQL does not return from 'LOCK TABLES ... WRITE'
     until all other threads have released all their locks to the table.
     The default value of 'innodb_table_locks' is 1, which means that
     *note 'LOCK TABLES': lock-tables. causes 'InnoDB' to lock a table
     internally if 'autocommit = 0'.

     As of MySQL 5.5.3, 'innodb_table_locks = 0' has no effect for
     tables locked explicitly with *note 'LOCK TABLES ... WRITE':
     lock-tables.  It still has an effect for tables locked for read or
     write by *note 'LOCK TABLES ... WRITE': lock-tables. implicitly
     (for example, through triggers) or by *note 'LOCK TABLES ... READ':
     lock-tables.

     For related information, see *note
     innodb-locking-transaction-model::.

   * 
     'innodb_thread_concurrency'

     Property               Value
                            
     *Command-Line          '--innodb-thread-concurrency=#'
     Format*                

     *System Variable*      'innodb_thread_concurrency'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '0'
                            
     *Minimum Value*        '0'
                            
     *Maximum Value*        '1000'

     'InnoDB' tries to keep the number of operating system threads
     concurrently inside 'InnoDB' less than or equal to the limit given
     by this variable ('InnoDB' uses operating system threads to process
     user transactions).  Once the number of threads reaches this limit,
     additional threads are placed into a wait state within a 'First In,
     First Out' (FIFO) queue for execution.  Threads waiting for locks
     are not counted in the number of concurrently executing threads.

     The range of this variable is 0 to 1000.  A value of 0 (the
     default) is interpreted as infinite concurrency (no concurrency
     checking).  Disabling thread concurrency checking enables 'InnoDB'
     to create as many threads as it needs.  A value of 0 also disables
     the 'queries inside InnoDB' and 'queries in queue counters' in the
     'ROW OPERATIONS' section of 'SHOW ENGINE INNODB STATUS' output.

     Consider setting this variable if your MySQL instance shares CPU
     resources with other applications, or if your workload or number of
     concurrent users is growing.  The correct setting depends on
     workload, computing environment, and the version of MySQL that you
     are running.  You will need to test a range of values to determine
     the setting that provides the best performance.
     'innodb_thread_concurrency' is a dynamic variable, which allows you
     to experiment with different settings on a live test system.  If a
     particular setting performs poorly, you can quickly set
     'innodb_thread_concurrency' back to 0.

     Use the following guidelines to help find and maintain an
     appropriate setting:

        * If the number of concurrent user threads for a workload is
          less than 64, set 'innodb_thread_concurrency=0'.

        * If your workload is consistently heavy or occasionally spikes,
          start by setting 'innodb_thread_concurrency=128' and then
          lowering the value to 96, 80, 64, and so on, until you find
          the number of threads that provides the best performance.
          Suppose that your system typically has 40 to 50 users, but
          periodically the number increases to 60, 70, or even 200.  You
          find that performance is stable at 80 concurrent users but
          starts to show a regression above this number.  In this case,
          you would set 'innodb_thread_concurrency=80' to avoid
          impacting performance.

        * If you do not want 'InnoDB' to use more than a certain number
          of virtual CPUs for user threads (20 virtual CPUs, for
          example), set 'innodb_thread_concurrency' to this number (or
          possibly lower, depending on performance results).  If your
          goal is to isolate MySQL from other applications, you may
          consider binding the 'mysqld' process exclusively to the
          virtual CPUs.  Be aware, however, that exclusive binding could
          result in non-optimal hardware usage if the 'mysqld' process
          is not consistently busy.  In this case, you might bind the
          'mysqld' process to the virtual CPUs but also allow other
          applications to use some or all of the virtual CPUs.

          *Note*:

          From an operating system perspective, using a resource
          management solution to manage how CPU time is shared among
          applications may be preferable to binding the 'mysqld'
          process.  For example, you could assign 90% of virtual CPU
          time to a given application while other critical processes
          _are not_ running, and scale that value back to 40% when other
          critical processes _are_ running.

        * 'innodb_thread_concurrency' values that are too high can cause
          performance regression due to increased contention on system
          internals and resources.

        * In some cases, the optimal 'innodb_thread_concurrency' setting
          can be smaller than the number of virtual CPUs.

        * Monitor and analyze your system regularly.  Changes to
          workload, number of users, or computing environment may
          require that you adjust the 'innodb_thread_concurrency'
          setting.

     For related information, see *note
     innodb-performance-thread_concurrency::.

   * 
     'innodb_thread_sleep_delay'

     Property               Value
                            
     *Command-Line          '--innodb-thread-sleep-delay=#'
     Format*                

     *System Variable*      'innodb_thread_sleep_delay'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '10000'
                            
     *Minimum Value*        '0'
                            
     *Maximum Value*        '18446744073709551615'
     (64-bit platforms,     
     <= 5.5.36)

     *Maximum Value*        '4294967295'
     (32-bit platforms,     
     <= 5.5.36)

     *Maximum Value* (>=    '1000000'
     5.5.37)

     Defines how long 'InnoDB' threads sleep before joining the 'InnoDB'
     queue, in microseconds.  The default value is 10000.  A value of 0
     disables sleep.

     For more information, see *note
     innodb-performance-thread_concurrency::.

   * 
     'innodb_use_native_aio'

     Property               Value
                            
     *Command-Line          '--innodb-use-native-aio[={OFF|ON}]'
     Format*                

     *Introduced*           5.5.4
                            
     *System Variable*      'innodb_use_native_aio'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Boolean
                            
     *Default Value*        'ON'

     Specifies whether to use the Linux asynchronous I/O subsystem.
     This variable applies to Linux systems only, and cannot be changed
     while the server is running.  Normally, you do not need to
     configure this option, because it is enabled by default.

     As of MySQL 5.5, the asynchronous I/O capability that 'InnoDB' has
     on Windows systems is available on Linux systems.  (Other Unix-like
     systems continue to use synchronous I/O calls.)  This feature
     improves the scalability of heavily I/O-bound systems, which
     typically show many pending reads/writes in 'SHOW ENGINE INNODB
     STATUS\G' output.

     Running with a large number of 'InnoDB' I/O threads, and especially
     running multiple such instances on the same server machine, can
     exceed capacity limits on Linux systems.  In this case, you may
     receive the following error:

          EAGAIN: The specified maxevents exceeds the user's limit of available events.

     You can typically address this error by writing a higher limit to
     '/proc/sys/fs/aio-max-nr'.

     However, if a problem with the asynchronous I/O subsystem in the OS
     prevents 'InnoDB' from starting, you can start the server with
     'innodb_use_native_aio=0'.  This option may also be disabled
     automatically during startup if 'InnoDB' detects a potential
     problem such as a combination of 'tmpdir' location, 'tmpfs' file
     system, and Linux kernel that does not support AIO on 'tmpfs'.

     For more information, see *note innodb-linux-native-aio::.

   * 
     'innodb_trx_purge_view_update_only_debug'

     Property               Value
                            
     *Command-Line          '--innodb-trx-purge-view-update-only-debug[={OFF|ON}]'
     Format*                

     *System Variable*      'innodb_trx_purge_view_update_only_debug'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Boolean
                            
     *Default Value*        'OFF'

     Pauses purging of delete-marked records while allowing the purge
     view to be updated.  This option artificially creates a situation
     in which the purge view is updated but purges have not yet been
     performed.  This option is only available if debugging support is
     compiled in using the 'WITH_DEBUG' 'CMake' option.

   * 
     'innodb_trx_rseg_n_slots_debug'

     Property               Value
                            
     *Command-Line          '--innodb-trx-rseg-n-slots-debug=#'
     Format*                

     *System Variable*      'innodb_trx_rseg_n_slots_debug'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '0'
                            
     *Maximum Value*        '1024'

     Sets a debug flag that limits 'TRX_RSEG_N_SLOTS' to a given value
     for the 'trx_rsegf_undo_find_free' function that looks for free
     slots for undo log segments.  This option is only available if
     debugging support is compiled in using the 'WITH_DEBUG' 'CMake'
     option.

   * 
     'innodb_use_sys_malloc'

     Property               Value
                            
     *Command-Line          '--innodb-use-sys-malloc[={OFF|ON}]'
     Format*                

     *System Variable*      'innodb_use_sys_malloc'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Boolean
                            
     *Default Value*        'ON'

     Enables the operating system memory allocator.  If disabled,
     'InnoDB' uses its own allocator.  The default value is 'ON'.  For
     more information, see *note innodb-performance-use_sys_malloc::.

   * 
     'innodb_version'

     The 'InnoDB' version number.  Starting in MySQL 5.5.30, separate
     version numbering for 'InnoDB' is discontinued and this value is
     the same the 'version' number of the server.

   * 
     'innodb_write_io_threads'

     Property               Value
                            
     *Command-Line          '--innodb-write-io-threads=#'
     Format*                

     *System Variable*      'innodb_write_io_threads'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Integer
                            
     *Default Value*        '4'
                            
     *Minimum Value*        '1'
                            
     *Maximum Value*        '64'

     The number of I/O threads for write operations in 'InnoDB'.  The
     default value is 4.  Its counterpart for read threads is
     'innodb_read_io_threads'.  For more information, see *note
     innodb-performance-multiple_io_threads::.  For general I/O tuning
     advice, see *note optimizing-innodb-diskio::.

     *Note*:

     On Linux systems, running multiple MySQL servers (typically more
     than 12) with default settings for 'innodb_read_io_threads',
     'innodb_write_io_threads', and the Linux 'aio-max-nr' setting can
     exceed system limits.  Ideally, increase the 'aio-max-nr' setting;
     as a workaround, you might reduce the settings for one or both of
     the MySQL variables.

Also take into consideration the value of 'sync_binlog', which controls
synchronization of the binary log to disk.

For general I/O tuning advice, see *note optimizing-innodb-diskio::.


File: manual.info.tmp,  Node: innodb-information-schema,  Next: innodb-performance-schema,  Prev: innodb-parameters,  Up: innodb-storage-engine

14.18 InnoDB INFORMATION_SCHEMA Tables
======================================

* Menu:

* innodb-information-schema-compression-tables::  InnoDB INFORMATION_SCHEMA Tables about Compression
* innodb-information-schema-transactions::  InnoDB INFORMATION_SCHEMA Transaction and Locking Information
* innodb-information-schema-buffer-pool-tables::  InnoDB INFORMATION_SCHEMA Buffer Pool Tables

This section provides information and usage examples for 'InnoDB' *note
'INFORMATION_SCHEMA': information-schema. tables.

'InnoDB' 'INFORMATION_SCHEMA' tables provide metadata, status
information, and statistics about various aspects of the 'InnoDB'
storage engine.  You can view a list of 'InnoDB' 'INFORMATION_SCHEMA'
tables by issuing a *note 'SHOW TABLES': show-tables. statement on the
'INFORMATION_SCHEMA' database:

     mysql> SHOW TABLES FROM INFORMATION_SCHEMA LIKE 'INNODB%';

For table definitions, see *note innodb-i_s-tables::.  For general
information regarding the 'MySQL' 'INFORMATION_SCHEMA' database, see
*note information-schema::.

The 'InnoDB' 'INFORMATION_SCHEMA' tables are themselves plugins to the
MySQL server.  To see what plugins are installed, use the *note 'SHOW
PLUGINS': show-plugins. statement or query the *note
'INFORMATION_SCHEMA.PLUGINS': plugins-table. table.  Use *note 'INSTALL
PLUGIN': install-plugin. syntax to install an 'INFORMATION_SCHEMA' table
plugin.  If 'INFORMATION_SCHEMA' table plugins are installed, but the
'InnoDB' storage engine plugin is not installed, the tables appear
empty.


File: manual.info.tmp,  Node: innodb-information-schema-compression-tables,  Next: innodb-information-schema-transactions,  Prev: innodb-information-schema,  Up: innodb-information-schema

14.18.1 InnoDB INFORMATION_SCHEMA Tables about Compression
----------------------------------------------------------

* Menu:

* innodb-information-schema-innodb_cmp::  INNODB_CMP and INNODB_CMP_RESET
* innodb-information-schema-innodb_cmpmem::  INNODB_CMPMEM and INNODB_CMPMEM_RESET
* innodb-information-schema-examples-compression-sect::  Using the Compression Information Schema Tables

There are two pairs of 'InnoDB' 'INFORMATION_SCHEMA' tables about
compression that can provide insight into how well compression is
working overall:

   * *note 'INNODB_CMP': innodb-cmp-table. and *note 'INNODB_CMP_RESET':
     innodb-cmp-table. provide information about the number of
     compression operations and the amount of time spent performing
     compression.

   * *note 'INNODB_CMPMEM': innodb-cmpmem-table. and *note
     'INNODB_CMPMEM_RESET': innodb-cmpmem-table. provide information
     about the way memory is allocated for compression.


File: manual.info.tmp,  Node: innodb-information-schema-innodb_cmp,  Next: innodb-information-schema-innodb_cmpmem,  Prev: innodb-information-schema-compression-tables,  Up: innodb-information-schema-compression-tables

14.18.1.1 INNODB_CMP and INNODB_CMP_RESET
.........................................

The *note 'INNODB_CMP': innodb-cmp-table. and *note 'INNODB_CMP_RESET':
innodb-cmp-table. tables provide status information about operations
related to compressed tables, which are described in *note
innodb-compression::.  The 'PAGE_SIZE' column reports the compressed
page size.

These two tables have identical contents, but reading from *note
'INNODB_CMP_RESET': innodb-cmp-table. resets the statistics on
compression and uncompression operations.  For example, if you archive
the output of *note 'INNODB_CMP_RESET': innodb-cmp-table. every 60
minutes, you see the statistics for each hourly period.  If you monitor
the output of *note 'INNODB_CMP': innodb-cmp-table. (making sure never
to read *note 'INNODB_CMP_RESET': innodb-cmp-table.), you see the
cumulative statistics since 'InnoDB' was started.

For the table definition, see *note innodb-cmp-table::.


File: manual.info.tmp,  Node: innodb-information-schema-innodb_cmpmem,  Next: innodb-information-schema-examples-compression-sect,  Prev: innodb-information-schema-innodb_cmp,  Up: innodb-information-schema-compression-tables

14.18.1.2 INNODB_CMPMEM and INNODB_CMPMEM_RESET
...............................................

The *note 'INNODB_CMPMEM': innodb-cmpmem-table. and *note
'INNODB_CMPMEM_RESET': innodb-cmpmem-table. tables provide status
information about compressed pages that reside in the buffer pool.
Please consult *note innodb-compression:: for further information on
compressed tables and the use of the buffer pool.  The *note
'INNODB_CMP': innodb-cmp-table. and *note 'INNODB_CMP_RESET':
innodb-cmp-table. tables should provide more useful statistics on
compression.

*Internal Details*

'InnoDB' uses a buddy allocator system to manage memory allocated to
pages of various sizes, from 1KB to 16KB. Each row of the two tables
described here corresponds to a single page size.

The *note 'INNODB_CMPMEM': innodb-cmpmem-table. and *note
'INNODB_CMPMEM_RESET': innodb-cmpmem-table. tables have identical
contents, but reading from *note 'INNODB_CMPMEM_RESET':
innodb-cmpmem-table. resets the statistics on relocation operations.
For example, if every 60 minutes you archived the output of *note
'INNODB_CMPMEM_RESET': innodb-cmpmem-table, it would show the hourly
statistics.  If you never read *note 'INNODB_CMPMEM_RESET':
innodb-cmpmem-table. and monitored the output of *note 'INNODB_CMPMEM':
innodb-cmpmem-table. instead, it would show the cumulative statistics
since 'InnoDB' was started.

For the table definition, see *note innodb-cmpmem-table::.


File: manual.info.tmp,  Node: innodb-information-schema-examples-compression-sect,  Prev: innodb-information-schema-innodb_cmpmem,  Up: innodb-information-schema-compression-tables

14.18.1.3 Using the Compression Information Schema Tables
.........................................................

The following is sample output from a database that contains compressed
tables (see *note innodb-compression::, *note 'INNODB_CMP':
innodb-cmp-table, and *note 'INNODB_CMPMEM': innodb-cmpmem-table.).

The following table shows the contents of *note
'INFORMATION_SCHEMA.INNODB_CMP': innodb-cmp-table. under a light
workload.  The only compressed page size that the buffer pool contains
is 8K. Compressing or uncompressing pages has consumed less than a
second since the time the statistics were reset, because the columns
'COMPRESS_TIME' and 'UNCOMPRESS_TIME' are zero.

page size   compress    compress    compress    uncompress     uncompress
            ops         ops ok      time        ops            time
                                                               
1024        0           0           0           0              0
                                                               
2048        0           0           0           0              0
                                                               
4096        0           0           0           0              0
                                                               
8192        1048        921         0           61             0
                                                               
16384       0           0           0           0              0
                                                

According to *note 'INNODB_CMPMEM': innodb-cmpmem-table, there are 6169
compressed 8KB pages in the buffer pool.

The following table shows the contents of *note
'INFORMATION_SCHEMA.INNODB_CMPMEM': innodb-cmpmem-table. under a light
workload.  Some memory is unusable due to fragmentation of the 'InnoDB'
memory allocator for compressed pages: 'SUM(PAGE_SIZE*PAGES_FREE)=6784'.
This is because small memory allocation requests are fulfilled by
splitting bigger blocks, starting from the 16K blocks that are allocated
from the main buffer pool, using the buddy allocation system.  The
fragmentation is this low because some allocated blocks have been
relocated (copied) to form bigger adjacent free blocks.  This copying of
'SUM(PAGE_SIZE*RELOCATION_OPS)' bytes has consumed less than a second
'(SUM(RELOCATION_TIME)=0)'.

page size   pages       pages free     relocation     relocation
            used                       ops            time
                                                      
1024        0           0              0              0
                                                      
2048        0           1              0              0
                                                      
4096        0           1              0              0
                                                      
8192        6169        0              5              0
                                                      
16384       0           0              0              0
                                       


File: manual.info.tmp,  Node: innodb-information-schema-transactions,  Next: innodb-information-schema-buffer-pool-tables,  Prev: innodb-information-schema-compression-tables,  Up: innodb-information-schema

14.18.2 InnoDB INFORMATION_SCHEMA Transaction and Locking Information
---------------------------------------------------------------------

* Menu:

* innodb-information-schema-examples::  Using InnoDB Transaction and Locking Information
* innodb-information-schema-understanding-innodb-locking::  InnoDB Lock and Lock-Wait Information
* innodb-information-schema-internal-data::  Persistence and Consistency of InnoDB Transaction and Locking Information

Three 'InnoDB' 'INFORMATION_SCHEMA' tables enable you to monitor
transactions and diagnose potential locking problems:

   * *note 'INNODB_TRX': innodb-trx-table.: Provides information about
     every transaction currently executing inside 'InnoDB', including
     the transaction state (for example, whether it is running or
     waiting for a lock), when the transaction started, and the
     particular SQL statement the transaction is executing.

   * *note 'INNODB_LOCKS': innodb-locks-table.: Each transaction in
     InnoDB that is waiting for another transaction to release a lock
     ('INNODB_TRX.TRX_STATE' is 'LOCK WAIT') is blocked by exactly one
     blocking lock request.  That blocking lock request is for a row or
     table lock held by another transaction in an incompatible mode.  A
     lock that blocks a transaction is always held in a mode
     incompatible with the mode of requested lock (read vs.  write,
     shared vs.  exclusive).  The blocked transaction cannot proceed
     until the other transaction commits or rolls back, thereby
     releasing the requested lock.  For every blocked transaction, *note
     'INNODB_LOCKS': innodb-locks-table. contains one row that describes
     each lock the transaction has requested, and for which it is
     waiting.  *note 'INNODB_LOCKS': innodb-locks-table. also contains
     one row for each lock that is blocking another transaction,
     whatever the state of the transaction that holds the lock
     ('INNODB_TRX.TRX_STATE' is 'RUNNING', 'LOCK WAIT', 'ROLLING BACK'
     or 'COMMITTING').

   * *note 'INNODB_LOCK_WAITS': innodb-lock-waits-table.: This table
     indicates which transactions are waiting for a given lock, or for
     which lock a given transaction is waiting.  This table contains one
     or more rows for each blocked transaction, indicating the lock it
     has requested and any locks that are blocking that request.  The
     'REQUESTED_LOCK_ID' value refers to the lock requested by a
     transaction, and the 'BLOCKING_LOCK_ID' value refers to the lock
     (held by another transaction) that prevents the first transaction
     from proceeding.  For any given blocked transaction, all rows in
     *note 'INNODB_LOCK_WAITS': innodb-lock-waits-table. have the same
     value for 'REQUESTED_LOCK_ID' and different values for
     'BLOCKING_LOCK_ID'.

For more information about the preceding tables, see *note
innodb-trx-table::, *note innodb-locks-table::, and *note
innodb-lock-waits-table::.


File: manual.info.tmp,  Node: innodb-information-schema-examples,  Next: innodb-information-schema-understanding-innodb-locking,  Prev: innodb-information-schema-transactions,  Up: innodb-information-schema-transactions

14.18.2.1 Using InnoDB Transaction and Locking Information
..........................................................

*Identifying Blocking Transactions*

It is sometimes helpful to identify which transaction blocks another.
The tables that contain information about 'InnoDB' transactions and data
locks enable you to determine which transaction is waiting for another,
and which resource is being requested.  (For descriptions of these
tables, see *note innodb-information-schema-transactions::.)

Suppose that three sessions are running concurrently.  Each session
corresponds to a MySQL thread, and executes one transaction after
another.  Consider the state of the system when these sessions have
issued the following statements, but none has yet committed its
transaction:

   * Session A:

          BEGIN;
          SELECT a FROM t FOR UPDATE;
          SELECT SLEEP(100);

   * Session B:

          SELECT b FROM t FOR UPDATE;

   * Session C:

          SELECT c FROM t FOR UPDATE;

In this scenario, use the following query to see which transactions are
waiting and which transactions are blocking them:

     SELECT
       r.trx_id waiting_trx_id,
       r.trx_mysql_thread_id waiting_thread,
       r.trx_query waiting_query,
       b.trx_id blocking_trx_id,
       b.trx_mysql_thread_id blocking_thread,
       b.trx_query blocking_query
     FROM       information_schema.innodb_lock_waits w
     INNER JOIN information_schema.innodb_trx b
       ON b.trx_id = w.blocking_trx_id
     INNER JOIN information_schema.innodb_trx r
       ON r.trx_id = w.requesting_trx_id;

waiting trx id                                                    waiting thread                                                    waiting query            blockingblockingblocking query
                                                                                                                                                             trx     thread  
                                                                                                                                                             id      
                                                                                                                                                             
'A4'                                                              '6'                                                               'SELECT b FROM t FOR     'A3'    '5'     'SELECT SLEEP(100)'
                                                                                                                                    UPDATE'                                  
                                                                                                                                    
'A5'                                                              '7'                                                               'SELECT c FROM t FOR     'A3'    '5'     'SELECT SLEEP(100)'
                                                                                                                                    UPDATE'                                  
                                                                                                                                    
'A5'                                                              '7'                                                               'SELECT c FROM t FOR     'A4'    '6'     'SELECT b FROM t FOR
                                                                                                                                    UPDATE'                                  UPDATE'
                                                                                                                                    

In the preceding table, you can identify sessions by the 'waiting query'
or 'blocking query' columns.  As you can see:

   * Session B (trx id 'A4', thread '6') and Session C (trx id 'A5',
     thread '7') are both waiting for Session A (trx id 'A3', thread
     '5').

   * Session C is waiting for Session B as well as Session A.

You can see the underlying data in the tables 'INNODB_TRX',
'INNODB_LOCKS', and 'INNODB_LOCK_WAITS'.

The following table shows some sample contents of *note
'INFORMATION_SCHEMA.INNODB_TRX': innodb-trx-table.

trx     trx       trx started                trx requested lock     trx wait started           trx weight     trx mysql         trx query
id      state                                id                                                               thread id         
                                                                                                              
'A3'    'RUNNING''2008-01-15 16:44:54'     'NULL'                 'NULL'                     '2'            '5'               'SELECT SLEEP(100)'
                                                                                                                                
'A4'    'LOCK     '2008-01-15 16:45:09'      'A4:1:3:2'             '2008-01-15 16:45:09'      '2'            '6'               'SELECT b FROM t FOR UPDATE'
        WAIT'                                                                                                                   
        
'A5'    'LOCK     '2008-01-15 16:45:14'      'A5:1:3:2'             '2008-01-15 16:45:14'      '2'            '7'               'SELECT c FROM t FOR UPDATE'
        WAIT'                                                                                                 
        

The following table shows some sample contents of *note
'INFORMATION_SCHEMA.INNODB_LOCKS': innodb-locks-table.

lock id             lock      lock       lock type       lock table             lock index            lock data
                    trx id    mode                                                                    
                              
'A3:1:3:2'          'A3'      'X'        'RECORD'        'test.t'               'PRIMARY'             '0x0200'
                                                                                                      
'A4:1:3:2'          'A4'      'X'        'RECORD'        'test.t'               'PRIMARY'             '0x0200'
                                                                                                      
'A5:1:3:2'          'A5'      'X'        'RECORD'        'test.t'               'PRIMARY'             '0x0200'
                                                                                

The following table shows some sample contents of *note
'INFORMATION_SCHEMA.INNODB_LOCK_WAITS': innodb-lock-waits-table.

requestingrequested blockingblocking
trx     lock id     trx     lock id
id                  id      
                    
'A4'    'A4:1:3:2'  'A3'    'A3:1:3:2'
                            
'A5'    'A5:1:3:2'  'A3'    'A3:1:3:2'
                            
'A5'    'A5:1:3:2'  'A4'    'A4:1:3:2'
                    

*Correlating InnoDB Transactions with MySQL Sessions*

Sometimes it is useful to correlate internal 'InnoDB' locking
information with the session-level information maintained by MySQL. For
example, you might like to know, for a given 'InnoDB' transaction ID,
the corresponding MySQL session ID and name of the session that may be
holding a lock, and thus blocking other transactions.

The following output from the 'INFORMATION_SCHEMA' tables is taken from
a somewhat loaded system.  As can be seen, there are several
transactions running.

The following 'INNODB_LOCKS' and 'INNODB_LOCK_WAITS' tables show that:

   * Transaction '77F' (executing an *note 'INSERT': insert.) is waiting
     for transactions '77E', '77D', and '77B' to commit.

   * Transaction '77E' (executing an *note 'INSERT': insert.) is waiting
     for transactions '77D' and '77B' to commit.

   * Transaction '77D' (executing an *note 'INSERT': insert.) is waiting
     for transaction '77B' to commit.

   * Transaction '77B' (executing an *note 'INSERT': insert.) is waiting
     for transaction '77A' to commit.

   * Transaction '77A' is running, currently executing *note 'SELECT':
     select.

   * Transaction 'E56' (executing an *note 'INSERT': insert.) is waiting
     for transaction 'E55' to commit.

   * Transaction 'E55' (executing an *note 'INSERT': insert.) is waiting
     for transaction '19C' to commit.

   * Transaction '19C' is running, currently executing an *note
     'INSERT': insert.

*Note*:

There may be inconsistencies between queries shown in the
'INFORMATION_SCHEMA' *note 'PROCESSLIST': processlist-table. and *note
'INNODB_TRX': innodb-trx-table. tables.  For an explanation, see *note
innodb-information-schema-internal-data::.

The following table shows the contents of *note
'INFORMATION_SCHEMA.PROCESSLIST': processlist-table. for a system
running a heavy workload.

ID                                                         USER     HOST            DB      COMMAND        TIME    STATE          INFO
                                                                                                                                  
'384'                                                      'root'   'localhost'     'test'  'Query'        '10'    'update'       'INSERT INTO t2
                                                                                                                                  VALUES ...'
                                                                                                                                  
'257'                                                      'root'   'localhost'     'test'  'Query'        '3'     'update'       'INSERT INTO t2
                                                                                                                                  VALUES ...'
                                                                                                                                  
'130'                                                      'root'   'localhost'     'test'  'Query'        '0'     'update'       'INSERT INTO t2
                                                                                                                                  VALUES ...'
                                                                                                                                  
'61'                                                       'root'   'localhost'     'test'  'Query'        '1'     'update'       'INSERT INTO t2
                                                                                                                                  VALUES ...'
                                                                                                                                  
'8'                                                        'root'   'localhost'     'test'  'Query'        '1'     'update'       'INSERT INTO t2
                                                                                                                                  VALUES ...'
                                                                                                                                  
'4'                                                        'root'   'localhost'     'test'  'Query'        '0'     'preparing'    'SELECT * FROM
                                                                                                                                  PROCESSLIST'
                                                                                                                                  
'2'                                                        'root'   'localhost'     'test'  'Sleep'        '566'   ''             'NULL'
                                                                                                                   

The following table shows the contents of *note
'INFORMATION_SCHEMA.INNODB_TRX': innodb-trx-table. for a system running
a heavy workload.

trx id                                                     trx     trx started    trx requested   trx wait       trx     trx     trx query
                                                           state                  lock id         started        weight  mysql   
                                                                                                                         thread
                                                                                                                         id
                                                                                                                         
'77F'                                                      'LOCK   '2008-01-15    '77F'           '2008-01-15    '1'     '876'   'INSERT INTO t09 (D,
                                                           WAIT'   13:10:16'                      13:10:16'                      B, C) VALUES ...'
                                                                                                                                 
'77E'                                                      'LOCK   '2008-01-15    '77E'           '2008-01-15    '1'     '875'   'INSERT INTO t09 (D,
                                                           WAIT'   13:10:16'                      13:10:16'                      B, C) VALUES ...'
                                                                                                                                 
'77D'                                                      'LOCK   '2008-01-15    '77D'           '2008-01-15    '1'     '874'   'INSERT INTO t09 (D,
                                                           WAIT'   13:10:16'                      13:10:16'                      B, C) VALUES ...'
                                                                                                                                 
'77B'                                                      'LOCK   '2008-01-15    '77B:733:12:1'  '2008-01-15    '4'     '873'   'INSERT INTO t09 (D,
                                                           WAIT'   13:10:16'                      13:10:16'                      B, C) VALUES ...'
                                                                                                                                 
'77A'                                                      'RUNNING''2008-01-15 'NULL'          'NULL'         '4'     '872'   'SELECT b, c FROM
                                                                   13:10:16'                                                     t09 WHERE ...'
                                                                                                                                 
'E56'                                                      'LOCK   '2008-01-15    'E56:743:6:2'   '2008-01-15    '5'     '384'   'INSERT INTO t2
                                                           WAIT'   13:10:06'                      13:10:06'                      VALUES ...'
                                                                                                                                 
'E55'                                                      'LOCK   '2008-01-15    'E55:743:38:2'  '2008-01-15    '965'   '257'   'INSERT INTO t2
                                                           WAIT'   13:10:06'                      13:10:13'                      VALUES ...'
                                                                                                                                 
'19C'                                                      'RUNNING''2008-01-15 'NULL'          'NULL'         '2900'  '130'   'INSERT INTO t2
                                                                   13:09:10'                                                     VALUES ...'
                                                                                                                                 
'E15'                                                      'RUNNING''2008-01-15 'NULL'          'NULL'         '5395'  '61'    'INSERT INTO t2
                                                                   13:08:59'                                                     VALUES ...'
                                                                                                                                 
'51D'                                                      'RUNNING''2008-01-15 'NULL'          'NULL'         '9807'  '8'     'INSERT INTO t2
                                                                   13:08:47'                                                     VALUES ...'
                                                                   

The following table shows the contents of *note
'INFORMATION_SCHEMA.INNODB_LOCK_WAITS': innodb-lock-waits-table. for a
system running a heavy workload.

requesting trx     requested lock     blocking trx id    blocking lock id
id                 id                                    
                   
'77F'              '77F:806'          '77E'              '77E:806'
                                                         
'77F'              '77F:806'          '77D'              '77D:806'
                                                         
'77F'              '77F:806'          '77B'              '77B:806'
                                                         
'77E'              '77E:806'          '77D'              '77D:806'
                                                         
'77E'              '77E:806'          '77B'              '77B:806'
                                                         
'77D'              '77D:806'          '77B'              '77B:806'
                                                         
'77B'              '77B:733:12:1'     '77A'              '77A:733:12:1'
                                                         
'E56'              'E56:743:6:2'      'E55'              'E55:743:6:2'
                                                         
'E55'              'E55:743:38:2'     '19C'              '19C:743:38:2'
                                      

The following table shows the contents of *note
'INFORMATION_SCHEMA.INNODB_LOCKS': innodb-locks-table. for a system
running a heavy workload.

lock id       lock trx id                                                       lock      lock      lock       lock index   lock data
                                                                                mode      type      table                   
                                                                                                    
'77F:806'     '77F'                                                             'AUTO_INC''TABLE'   'test.t09' 'NULL'       'NULL'
                                                                                                                            
'77E:806'     '77E'                                                             'AUTO_INC''TABLE'   'test.t09' 'NULL'       'NULL'
                                                                                                                            
'77D:806'     '77D'                                                             'AUTO_INC''TABLE'   'test.t09' 'NULL'       'NULL'
                                                                                                                            
'77B:806'     '77B'                                                             'AUTO_INC''TABLE'   'test.t09' 'NULL'       'NULL'
                                                                                                                            
'77B:733:12:1''77B'                                                             'X'       'RECORD'  'test.t09' 'PRIMARY'    'supremum
                                                                                                                            pseudo-record'
                                                                                                                            
'77A:733:12:1''77A'                                                             'X'       'RECORD'  'test.t09' 'PRIMARY'    'supremum
                                                                                                                            pseudo-record'
                                                                                                                            
'E56:743:6:2' 'E56'                                                             'S'       'RECORD'  'test.t2'  'PRIMARY'    '0, 0'
                                                                                                                            
'E55:743:6:2' 'E55'                                                             'X'       'RECORD'  'test.t2'  'PRIMARY'    '0, 0'
                                                                                                                            
'E55:743:38:2''E55'                                                             'S'       'RECORD'  'test.t2'  'PRIMARY'    '1922,
                                                                                                                            1922'
                                                                                                                            
'19C:743:38:2''19C'                                                             'X'       'RECORD'  'test.t2'  'PRIMARY'    '1922,
                                                                                                                            1922'


File: manual.info.tmp,  Node: innodb-information-schema-understanding-innodb-locking,  Next: innodb-information-schema-internal-data,  Prev: innodb-information-schema-examples,  Up: innodb-information-schema-transactions

14.18.2.2 InnoDB Lock and Lock-Wait Information
...............................................

When a transaction updates a row in a table, or locks it with 'SELECT
FOR UPDATE', 'InnoDB' establishes a list or queue of locks on that row.
Similarly, 'InnoDB' maintains a list of locks on a table for table-level
locks.  If a second transaction wants to update a row or lock a table
already locked by a prior transaction in an incompatible mode, 'InnoDB'
adds a lock request for the row to the corresponding queue.  For a lock
to be acquired by a transaction, all incompatible lock requests
previously entered into the lock queue for that row or table must be
removed (which occurs when the transactions holding or requesting those
locks either commit or roll back).

A transaction may have any number of lock requests for different rows or
tables.  At any given time, a transaction may request a lock that is
held by another transaction, in which case it is blocked by that other
transaction.  The requesting transaction must wait for the transaction
that holds the blocking lock to commit or roll back.  If a transaction
is not waiting for a lock, it is in a 'RUNNING' state.  If a transaction
is waiting for a lock, it is in a 'LOCK WAIT' state.  (The
'INFORMATION_SCHEMA' *note 'INNODB_TRX': innodb-trx-table. table
indicates transaction state values.)

The *note 'INNODB_LOCKS': innodb-locks-table. table holds one or more
rows for each 'LOCK WAIT' transaction, indicating any lock requests that
prevent its progress.  This table also contains one row describing each
lock in a queue of locks pending for a given row or table.  The *note
'INNODB_LOCK_WAITS': innodb-lock-waits-table. table shows which locks
already held by a transaction are blocking locks requested by other
transactions.


File: manual.info.tmp,  Node: innodb-information-schema-internal-data,  Prev: innodb-information-schema-understanding-innodb-locking,  Up: innodb-information-schema-transactions

14.18.2.3 Persistence and Consistency of InnoDB Transaction and Locking Information
...................................................................................

The data exposed by the transaction and locking tables (*note
'INNODB_TRX': innodb-trx-table, *note 'INNODB_LOCKS':
innodb-locks-table, and *note 'INNODB_LOCK_WAITS':
innodb-lock-waits-table.) represents a glimpse into fast-changing data.
This is not like user tables, where the data changes only when
application-initiated updates occur.  The underlying data is internal
system-managed data, and can change very quickly.

For performance reasons, and to minimize the chance of misleading joins
between the transaction and locking tables, 'InnoDB' collects the
required transaction and locking information into an intermediate buffer
whenever a 'SELECT' on any of the tables is issued.  This buffer is
refreshed only if more than 0.1 seconds has elapsed since the last time
the buffer was read.  The data needed to fill the three tables is
fetched atomically and consistently and is saved in this global internal
buffer, forming a point-in-time 'snapshot'.  If multiple table accesses
occur within 0.1 seconds (as they almost certainly do when MySQL
processes a join among these tables), then the same snapshot is used to
satisfy the query.

A correct result is returned when you join any of these tables together
in a single query, because the data for the three tables comes from the
same snapshot.  Because the buffer is not refreshed with every query of
any of these tables, if you issue separate queries against these tables
within a tenth of a second, the results are the same from query to
query.  On the other hand, two separate queries of the same or different
tables issued more than a tenth of a second apart may see different
results, since the data come from different snapshots.

Because 'InnoDB' must temporarily stall while the transaction and
locking data is collected, too frequent queries of these tables can
negatively impact performance as seen by other users.

As these tables contain sensitive information (at least
'INNODB_LOCKS.LOCK_DATA' and 'INNODB_TRX.TRX_QUERY'), for security
reasons, only the users with the 'PROCESS' privilege are allowed to
'SELECT' from them.

As described previously, the data that fills the transaction and locking
tables (*note 'INNODB_TRX': innodb-trx-table, *note 'INNODB_LOCKS':
innodb-locks-table. and *note 'INNODB_LOCK_WAITS':
innodb-lock-waits-table.) is fetched automatically and saved to an
intermediate buffer that provides a 'point-in-time' snapshot.  The data
across all three tables is consistent when queried from the same
snapshot.  However, the underlying data changes so fast that similar
glimpses at other, similarly fast-changing data, may not be in
synchrony.  Thus, you should be careful when comparing data in the
'InnoDB' transaction and locking tables with data in the *note
'PROCESSLIST': processlist-table. table.  The data from the *note
'PROCESSLIST': processlist-table. table does not come from the same
snapshot as the data about locking and transactions.  Even if you issue
a single 'SELECT' (joining *note 'INNODB_TRX': innodb-trx-table. and
*note 'PROCESSLIST': processlist-table, for example), the content of
those tables is generally not consistent.  *note 'INNODB_TRX':
innodb-trx-table. may reference rows that are not present in *note
'PROCESSLIST': processlist-table. or the currently executing SQL query
of a transaction shown in 'INNODB_TRX.TRX_QUERY' may differ from the one
in 'PROCESSLIST.INFO'.


File: manual.info.tmp,  Node: innodb-information-schema-buffer-pool-tables,  Prev: innodb-information-schema-transactions,  Up: innodb-information-schema

14.18.3 InnoDB INFORMATION_SCHEMA Buffer Pool Tables
----------------------------------------------------

The 'InnoDB' 'INFORMATION_SCHEMA' buffer pool tables provide buffer pool
status information and metadata about the pages within the 'InnoDB'
buffer pool.

The 'InnoDB' 'INFORMATION_SCHEMA' buffer pool tables include those
listed below:

     mysql> SHOW TABLES FROM INFORMATION_SCHEMA LIKE 'INNODB_BUFFER%';
     +-----------------------------------------------+
     | Tables_in_INFORMATION_SCHEMA (INNODB_BUFFER%) |
     +-----------------------------------------------+
     | INNODB_BUFFER_PAGE_LRU                        |
     | INNODB_BUFFER_PAGE                            |
     | INNODB_BUFFER_POOL_STATS                      |
     +-----------------------------------------------+

*Table Overview*

   * *note 'INNODB_BUFFER_PAGE': innodb-buffer-page-table.: Holds
     information about each page in the 'InnoDB' buffer pool.

   * *note 'INNODB_BUFFER_PAGE_LRU': innodb-buffer-page-lru-table.:
     Holds information about the pages in the 'InnoDB' buffer pool, in
     particular how they are ordered in the LRU list that determines
     which pages to evict from the buffer pool when it becomes full.
     The *note 'INNODB_BUFFER_PAGE_LRU': innodb-buffer-page-lru-table.
     table has the same columns as the *note 'INNODB_BUFFER_PAGE':
     innodb-buffer-page-table. table, except that the *note
     'INNODB_BUFFER_PAGE_LRU': innodb-buffer-page-lru-table. table has
     an 'LRU_POSITION' column instead of a 'BLOCK_ID' column.

   * *note 'INNODB_BUFFER_POOL_STATS': innodb-buffer-pool-stats-table.:
     Provides buffer pool status information.  Much of the same
     information is provided by *note 'SHOW ENGINE INNODB STATUS':
     show-engine. output, or may be obtained using 'InnoDB' buffer pool
     server status variables.

*Warning*:

Querying the *note 'INNODB_BUFFER_PAGE': innodb-buffer-page-table. or
*note 'INNODB_BUFFER_PAGE_LRU': innodb-buffer-page-lru-table. table can
affect performance.  Do not query these tables on a production system
unless you are aware of the performance impact and have determined it to
be acceptable.  To avoid impacting performance on a production system,
reproduce the issue you want to investigate and query buffer pool
statistics on a test instance.

This query provides an approximate count of pages that contain system
data by excluding pages where the 'TABLE_NAME' value is either 'NULL' or
includes a slash '/' or period '.' in the table name, which indicates a
user-defined table.

     mysql> SELECT COUNT(*) FROM INFORMATION_SCHEMA.INNODB_BUFFER_PAGE
            WHERE TABLE_NAME IS NULL OR (INSTR(TABLE_NAME, '/') = 0 AND INSTR(TABLE_NAME, '.') = 0);
     +----------+
     | COUNT(*) |
     +----------+
     |      381 |
     +----------+

This query returns the approximate number of pages that contain system
data, the total number of buffer pool pages, and an approximate
percentage of pages that contain system data.

     mysql> SELECT
            (SELECT COUNT(*) FROM INFORMATION_SCHEMA.INNODB_BUFFER_PAGE
            WHERE TABLE_NAME IS NULL OR (INSTR(TABLE_NAME, '/') = 0 AND INSTR(TABLE_NAME, '.') = 0)
            ) AS system_pages,
            (
            SELECT COUNT(*)
            FROM INFORMATION_SCHEMA.INNODB_BUFFER_PAGE
            ) AS total_pages,
            (
            SELECT ROUND((system_pages/total_pages) * 100)
            ) AS system_page_percentage;
     +--------------+-------------+------------------------+
     | system_pages | total_pages | system_page_percentage |
     +--------------+-------------+------------------------+
     |          381 |        8192 |                      5 |
     +--------------+-------------+------------------------+

The type of system data in the buffer pool can be determined by querying
the 'PAGE_TYPE' value.  For example, the following query returns eight
distinct 'PAGE_TYPE' values among the pages that contain system data:

     mysql> SELECT DISTINCT PAGE_TYPE FROM INFORMATION_SCHEMA.INNODB_BUFFER_PAGE
            WHERE TABLE_NAME IS NULL OR (INSTR(TABLE_NAME, '/') = 0 AND INSTR(TABLE_NAME, '.') = 0);
     +-------------------+
     | PAGE_TYPE         |
     +-------------------+
     | IBUF_BITMAP       |
     | SYSTEM            |
     | INDEX             |
     | UNDO_LOG          |
     | FILE_SPACE_HEADER |
     | UNKNOWN           |
     | INODE             |
     | EXTENT_DESCRIPTOR |
     +-------------------+

This query provides an approximate count of pages containing user data
by counting pages where the 'TABLE_NAME' value is 'NOT NULL'.

     mysql> SELECT COUNT(*) FROM INFORMATION_SCHEMA.INNODB_BUFFER_PAGE
            WHERE TABLE_NAME IS NOT NULL;
     +----------+
     | COUNT(*) |
     +----------+
     |     7811 |
     +----------+

This query returns the approximate number of pages that contain user
data, the total number of buffer pool pages, and an approximate
percentage of pages that contain user data.

     mysql> SELECT
            (SELECT COUNT(*) FROM INFORMATION_SCHEMA.INNODB_BUFFER_PAGE
            WHERE TABLE_NAME IS NOT NULL AND (INSTR(TABLE_NAME, '/') > 0 OR INSTR(TABLE_NAME, '.') > 0)
            ) AS user_pages,
            (
            SELECT COUNT(*)
            FROM INFORMATION_SCHEMA.INNODB_BUFFER_PAGE
            ) AS total_pages,
            (
            SELECT ROUND((user_pages/total_pages) * 100)
            ) AS user_page_percentage;
     +------------+-------------+----------------------+
     | user_pages | total_pages | user_page_percentage |
     +------------+-------------+----------------------+
     |       7811 |        8192 |                   95 |
     +------------+-------------+----------------------+

This query identifies user-defined tables with pages in the buffer pool:

     mysql> SELECT DISTINCT TABLE_NAME
            FROM INFORMATION_SCHEMA.INNODB_BUFFER_PAGE
            WHERE TABLE_NAME IS NOT NULL
            AND (INSTR(TABLE_NAME, '/') > 0 OR INSTR(TABLE_NAME, '.') > 0);
     +---------------------+
     | TABLE_NAME          |
     +---------------------+
     | employees/salaries  |
     | employees/employees |
     +---------------------+

For information about index pages, query the 'INDEX_NAME' column using
the name of the index.  For example, the following query returns the
number of pages and total data size of pages for the 'emp_no' index that
is defined on the 'employees.salaries' table:

     mysql> SELECT INDEX_NAME, COUNT(*) AS Pages,
            ROUND(SUM(IF(COMPRESSED_SIZE = 0, 16384, COMPRESSED_SIZE))/1024/1024)
            AS 'Total Data (MB)'  FROM INFORMATION_SCHEMA.INNODB_BUFFER_PAGE
            WHERE INDEX_NAME='emp_no' AND TABLE_NAME = 'employees/salaries';
     +------------+-------+-----------------+
     | INDEX_NAME | Pages | Total Data (MB) |
     +------------+-------+-----------------+
     | emp_no     |  1244 |              19 |
     +------------+-------+-----------------+

This query returns the number of pages and total data size of pages for
all indexes defined on the 'employees.salaries' table:

     mysql> SELECT INDEX_NAME, COUNT(*) AS Pages,
            ROUND(SUM(IF(COMPRESSED_SIZE = 0, 16384, COMPRESSED_SIZE))/1024/1024)
            AS 'Total Data (MB)'
            FROM INFORMATION_SCHEMA.INNODB_BUFFER_PAGE
            WHERE TABLE_NAME = 'employees/salaries'
            GROUP BY INDEX_NAME;
     +------------+-------+-----------------+
     | INDEX_NAME | Pages | Total Data (MB) |
     +------------+-------+-----------------+
     | emp_no     |  1244 |              19 |
     | PRIMARY    |  6086 |              95 |
     +------------+-------+-----------------+

The *note 'INNODB_BUFFER_PAGE_LRU': innodb-buffer-page-lru-table. table
holds information about the pages in the 'InnoDB' buffer pool, in
particular how they are ordered that determines which pages to evict
from the buffer pool when it becomes full.  The definition for this page
is the same as for *note 'INNODB_BUFFER_PAGE': innodb-buffer-page-table,
except this table has an 'LRU_POSITION' column instead of a 'BLOCK_ID'
column.

This query counts the number of positions at a specific location in the
LRU list occupied by pages of the 'employees.employees' table.

     mysql> SELECT COUNT(LRU_POSITION) FROM INFORMATION_SCHEMA.INNODB_BUFFER_PAGE_LRU
            WHERE TABLE_NAME='employees/employees' AND LRU_POSITION < 3072;
     +---------------------+
     | COUNT(LRU_POSITION) |
     +---------------------+
     |                 481 |
     +---------------------+

The *note 'INNODB_BUFFER_POOL_STATS': innodb-buffer-pool-stats-table.
table provides information similar to *note 'SHOW ENGINE INNODB STATUS':
show-engine. and 'InnoDB' buffer pool status variables.

     mysql> SELECT * FROM INFORMATION_SCHEMA.INNODB_BUFFER_POOL_STATS \G
     *************************** 1. row ***************************
                              POOL_ID: 0
                            POOL_SIZE: 8192
                         FREE_BUFFERS: 1
                       DATABASE_PAGES: 7942
                   OLD_DATABASE_PAGES: 2911
              MODIFIED_DATABASE_PAGES: 0
                   PENDING_DECOMPRESS: 0
                        PENDING_READS: 0
                    PENDING_FLUSH_LRU: 0
                   PENDING_FLUSH_LIST: 0
                     PAGES_MADE_YOUNG: 8358
                 PAGES_NOT_MADE_YOUNG: 0
                PAGES_MADE_YOUNG_RATE: 0
            PAGES_MADE_NOT_YOUNG_RATE: 0
                    NUMBER_PAGES_READ: 7045
                 NUMBER_PAGES_CREATED: 12382
                 NUMBER_PAGES_WRITTEN: 15790
                      PAGES_READ_RATE: 0
                    PAGES_CREATE_RATE: 0
                   PAGES_WRITTEN_RATE: 0
                     NUMBER_PAGES_GET: 28731589
                             HIT_RATE: 0
         YOUNG_MAKE_PER_THOUSAND_GETS: 0
     NOT_YOUNG_MAKE_PER_THOUSAND_GETS: 0
              NUMBER_PAGES_READ_AHEAD: 2934
            NUMBER_READ_AHEAD_EVICTED: 23
                      READ_AHEAD_RATE: 0
              READ_AHEAD_EVICTED_RATE: 0
                         LRU_IO_TOTAL: 0
                       LRU_IO_CURRENT: 0
                     UNCOMPRESS_TOTAL: 0
                   UNCOMPRESS_CURRENT: 0

For comparison, *note 'SHOW ENGINE INNODB STATUS': show-engine. output
and 'InnoDB' buffer pool status variable output is shown below, based on
the same data set.

For more information about *note 'SHOW ENGINE INNODB STATUS':
show-engine. output, see *note innodb-standard-monitor::.

     mysql> SHOW ENGINE INNODB STATUS \G
     ...
     ----------------------
     BUFFER POOL AND MEMORY
     ----------------------
     Total memory allocated 137363456; in additional pool allocated 0
     Dictionary memory allocated 71426
     Buffer pool size   8192
     Free buffers       1
     Database pages     7942
     Old database pages 2911
     Modified db pages  0
     Pending reads 0
     Pending writes: LRU 0, flush list 0, single page 0
     Pages made young 8358, not young 0
     0.00 youngs/s, 0.00 non-youngs/s
     Pages read 7045, created 12382, written 15790
     0.00 reads/s, 0.00 creates/s, 0.00 writes/s
     No buffer pool page gets since the last printout
     Pages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead 0.00/s
     LRU len: 7942, unzip_LRU len: 0
     I/O sum[0]:cur[0], unzip sum[0]:cur[0]
     ...

For status variable descriptions, see *note server-status-variables::.

     mysql> SHOW STATUS LIKE 'Innodb_buffer%';
     +---------------------------------------+-----------+
     | Variable_name                         | Value     |
     +---------------------------------------+-----------+
     | Innodb_buffer_pool_pages_data         | 7942      |
     | Innodb_buffer_pool_bytes_data         | 130121728 |
     | Innodb_buffer_pool_pages_dirty        | 0         |
     | Innodb_buffer_pool_bytes_dirty        | 0         |
     | Innodb_buffer_pool_pages_flushed      | 15790     |
     | Innodb_buffer_pool_pages_free         | 1         |
     | Innodb_buffer_pool_pages_misc         | 249       |
     | Innodb_buffer_pool_pages_total        | 8192      |
     | Innodb_buffer_pool_read_ahead_rnd     | 0         |
     | Innodb_buffer_pool_read_ahead         | 2934      |
     | Innodb_buffer_pool_read_ahead_evicted | 23        |
     | Innodb_buffer_pool_read_requests      | 28731589  |
     | Innodb_buffer_pool_reads              | 4112      |
     | Innodb_buffer_pool_wait_free          | 0         |
     | Innodb_buffer_pool_write_requests     | 11965146  |
     +---------------------------------------+-----------+


File: manual.info.tmp,  Node: innodb-performance-schema,  Next: innodb-monitors,  Prev: innodb-information-schema,  Up: innodb-storage-engine

14.19 InnoDB Integration with MySQL Performance Schema
======================================================

* Menu:

* monitor-innodb-mutex-waits-performance-schema::  Monitoring InnoDB Mutex Waits Using Performance Schema

This section provides a brief introduction to 'InnoDB' integration with
Performance Schema.  For comprehensive Performance Schema documentation,
see *note performance-schema::.

Starting with InnoDB 1.1 with MySQL 5.5, you can profile certain
internal 'InnoDB' operations using the MySQL *note Performance Schema
feature: performance-schema.  This type of tuning is primarily for
expert users who evaluate optimization strategies to overcome
performance bottlenecks.  DBAs can also use this feature for capacity
planning, to see whether their typical workload encounters any
performance bottlenecks with a particular combination of CPU, RAM, and
disk storage; and if so, to judge whether performance can be improved by
increasing the capacity of some part of the system.

To use this feature to examine 'InnoDB' performance:

   * You must be running MySQL 5.5 or higher with the Performance Schema
     feature available and enabled, as described in *note
     performance-schema-build-configuration::, and *note
     performance-schema-startup-configuration::.  Since the Performance
     Schema feature introduces some performance overhead, you should use
     it on a test or development system rather than on a production
     system.

   * You must be running InnoDB 1.1 or higher.

   * You must be generally familiar with how to use the *note
     Performance Schema feature: performance-schema.  For example, you
     should know how enable instruments and consumers, and how to query
     'performance_schema' tables to retrieve data.  For an introductory
     overview, see *note performance-schema-quick-start::.

   * You should be familiar with Performance Schema instruments that are
     available for 'InnoDB'.  To view 'InnoDB'-related instruments, you
     can query the *note 'setup_instruments': setup-instruments-table.
     table for instrument names that contain ''innodb''.

          mysql> SELECT *
                 FROM performance_schema.setup_instruments
                 WHERE NAME LIKE '%innodb%';
          +-------------------------------------------------------+---------+-------+
          | NAME                                                  | ENABLED | TIMED |
          +-------------------------------------------------------+---------+-------+
          | wait/synch/mutex/innodb/commit_cond_mutex             | YES     | YES   |
          | wait/synch/mutex/innodb/innobase_share_mutex          | YES     | YES   |
          | wait/synch/mutex/innodb/prepare_commit_mutex          | YES     | YES   |
          | wait/synch/mutex/innodb/autoinc_mutex                 | YES     | YES   |
          | wait/synch/mutex/innodb/btr_search_enabled_mutex      | YES     | YES   |
          | wait/synch/mutex/innodb/buf_pool_mutex                | YES     | YES   |
          | wait/synch/mutex/innodb/buf_pool_zip_mutex            | YES     | YES   |
          | wait/synch/mutex/innodb/cache_last_read_mutex         | YES     | YES   |
          | wait/synch/mutex/innodb/dict_foreign_err_mutex        | YES     | YES   |
          | wait/synch/mutex/innodb/dict_sys_mutex                | YES     | YES   |
          | wait/synch/mutex/innodb/file_format_max_mutex         | YES     | YES   |
          ...
          | wait/synch/rwlock/innodb/btr_search_latch             | YES     | YES   |
          | wait/synch/rwlock/innodb/dict_operation_lock          | YES     | YES   |
          | wait/synch/rwlock/innodb/fil_space_latch              | YES     | YES   |
          | wait/synch/rwlock/innodb/checkpoint_lock              | YES     | YES   |
          | wait/synch/rwlock/innodb/trx_i_s_cache_lock           | YES     | YES   |
          | wait/synch/rwlock/innodb/trx_purge_latch              | YES     | YES   |
          | wait/synch/rwlock/innodb/index_tree_rw_lock           | YES     | YES   |
          | wait/synch/rwlock/innodb/dict_table_stats             | YES     | YES   |
          | wait/synch/cond/innodb/commit_cond                    | YES     | YES   |
          | wait/io/file/innodb/innodb_data_file                  | YES     | YES   |
          | wait/io/file/innodb/innodb_log_file                   | YES     | YES   |
          | wait/io/file/innodb/innodb_temp_file                  | YES     | YES   |
          +-------------------------------------------------------+---------+-------+
          46 rows in set (0.00 sec)

     For additional information about the instrumented 'InnoDB' objects,
     you can query Performance Schema *note instances tables:
     performance-schema-instance-tables, which provide additional
     information about instrumented objects.  Instance tables relevant
     to 'InnoDB' include:

        * The *note 'mutex_instances': mutex-instances-table. table

        * The *note 'rwlock_instances': rwlock-instances-table. table

        * The *note 'cond_instances': cond-instances-table. table

        * The *note 'file_instances': file-instances-table. table

     *Note*:

     Mutexes and RW-locks related to the 'InnoDB' buffer pool are not
     included in this coverage; the same applies to the output of the
     'SHOW ENGINE INNODB MUTEX' command.

     For example, to view information about instrumented 'InnoDB' file
     objects seen by the Performance Schema when executing file I/O
     instrumentation, you might issue the following query:

          mysql> SELECT *
                 FROM performance_schema.file_instances
                 WHERE EVENT_NAME LIKE '%innodb%'\G
          *************************** 1. row ***************************
           FILE_NAME: /path/to/mysql-5.5/data/ibdata1
          EVENT_NAME: wait/io/file/innodb/innodb_data_file
          OPEN_COUNT: 1
          *************************** 2. row ***************************
           FILE_NAME: /path/to/mysql-5.5/data/ib_logfile0
          EVENT_NAME: wait/io/file/innodb/innodb_log_file
          OPEN_COUNT: 1
          *************************** 3. row ***************************
           FILE_NAME: /path/to/mysql-5.5/data/ib_logfile1
          EVENT_NAME: wait/io/file/innodb/innodb_log_file
          OPEN_COUNT: 1

   * You should be familiar with 'performance_schema' tables that store
     'InnoDB' event data.  Tables relevant to 'InnoDB'-related events
     include:

        * The *note Wait Event: performance-schema-wait-tables. tables,
          which store wait events.

        * The *note Summary: performance-schema-summary-tables. tables,
          which provide aggregated information for terminated events
          over time.  Summary tables include *note file I/O summary
          tables: file-summary-tables, which aggregate information about
          I/O operations.

     If you are only interested in 'InnoDB'-related objects, use the
     clause 'WHERE EVENT_NAME LIKE '%innodb%'' or 'WHERE NAME LIKE
     '%innodb%'' (as required) when querying these tables.


File: manual.info.tmp,  Node: monitor-innodb-mutex-waits-performance-schema,  Prev: innodb-performance-schema,  Up: innodb-performance-schema

14.19.1 Monitoring InnoDB Mutex Waits Using Performance Schema
--------------------------------------------------------------

A mutex is a synchronization mechanism used in the code to enforce that
only one thread at a given time can have access to a common resource.
When two or more threads executing in the server need to access the same
resource, the threads compete against each other.  The first thread to
obtain a lock on the mutex causes the other threads to wait until the
lock is released.

For 'InnoDB' mutexes that are instrumented, mutex waits can be monitored
using *note Performance Schema: performance-schema.  Wait event data
collected in Performance Schema tables can help identify mutexes with
the most waits or the greatest total wait time, for example.

The following example demonstrates how to view 'InnoDB' mutex wait
instruments, how to verify that associated consumers are enabled, and
how to query wait event data.  It is assumed that Performance Schema was
enabled at server startup.  For information about enabling Performance
Schema, see *note performance-schema-quick-start::.

  1. To view available 'InnoDB' mutex wait instruments, query the
     Performance Schema *note 'setup_instruments':
     setup-instruments-table. table, as shown below.  Instruments are
     enabled by default.

          mysql> SELECT *
                 FROM performance_schema.setup_instruments
                 WHERE NAME LIKE '%wait/synch/mutex/innodb%';
          +-------------------------------------------------------+---------+-------+
          | NAME                                                  | ENABLED | TIMED |
          +-------------------------------------------------------+---------+-------+
          | wait/synch/mutex/innodb/commit_cond_mutex             | YES     | YES   |
          | wait/synch/mutex/innodb/innobase_share_mutex          | YES     | YES   |
          | wait/synch/mutex/innodb/prepare_commit_mutex          | YES     | YES   |
          | wait/synch/mutex/innodb/autoinc_mutex                 | YES     | YES   |
          | wait/synch/mutex/innodb/btr_search_enabled_mutex      | YES     | YES   |
          | wait/synch/mutex/innodb/buf_pool_mutex                | YES     | YES   |
          | wait/synch/mutex/innodb/buf_pool_zip_mutex            | YES     | YES   |
          | wait/synch/mutex/innodb/cache_last_read_mutex         | YES     | YES   |
          | wait/synch/mutex/innodb/dict_foreign_err_mutex        | YES     | YES   |
          | wait/synch/mutex/innodb/dict_sys_mutex                | YES     | YES   |
          | wait/synch/mutex/innodb/file_format_max_mutex         | YES     | YES   |
          | wait/synch/mutex/innodb/fil_system_mutex              | YES     | YES   |
          | wait/synch/mutex/innodb/flush_list_mutex              | YES     | YES   |
          | wait/synch/mutex/innodb/log_flush_order_mutex         | YES     | YES   |
          | wait/synch/mutex/innodb/hash_table_mutex              | YES     | YES   |
          | wait/synch/mutex/innodb/ibuf_bitmap_mutex             | YES     | YES   |
          | wait/synch/mutex/innodb/ibuf_mutex                    | YES     | YES   |
          | wait/synch/mutex/innodb/ibuf_pessimistic_insert_mutex | YES     | YES   |
          | wait/synch/mutex/innodb/kernel_mutex                  | YES     | YES   |
          | wait/synch/mutex/innodb/log_sys_mutex                 | YES     | YES   |
          | wait/synch/mutex/innodb/mem_pool_mutex                | YES     | YES   |
          | wait/synch/mutex/innodb/mutex_list_mutex              | YES     | YES   |
          | wait/synch/mutex/innodb/purge_sys_bh_mutex            | YES     | YES   |
          | wait/synch/mutex/innodb/recv_sys_mutex                | YES     | YES   |
          | wait/synch/mutex/innodb/rseg_mutex                    | YES     | YES   |
          | wait/synch/mutex/innodb/rw_lock_list_mutex            | YES     | YES   |
          | wait/synch/mutex/innodb/rw_lock_mutex                 | YES     | YES   |
          | wait/synch/mutex/innodb/srv_dict_tmpfile_mutex        | YES     | YES   |
          | wait/synch/mutex/innodb/srv_innodb_monitor_mutex      | YES     | YES   |
          | wait/synch/mutex/innodb/srv_misc_tmpfile_mutex        | YES     | YES   |
          | wait/synch/mutex/innodb/srv_monitor_file_mutex        | YES     | YES   |
          | wait/synch/mutex/innodb/syn_arr_mutex                 | YES     | YES   |
          | wait/synch/mutex/innodb/trx_doublewrite_mutex         | YES     | YES   |
          | wait/synch/mutex/innodb/trx_undo_mutex                | YES     | YES   |
          +-------------------------------------------------------+---------+-------+
          34 rows in set (0.00 sec)

  2. Verify that wait event consumers are enabled by querying the *note
     'setup_consumers': setup-consumers-table. table.  The *note
     'events_waits_current': events-waits-current-table, *note
     'events_waits_history': events-waits-history-table, and *note
     'events_waits_history_long': events-waits-history-long-table.
     consumers should be enabled by default.

          mysql> SELECT * FROM performance_schema.setup_consumers;
          +----------------------------------------------+---------+
          | NAME                                         | ENABLED |
          +----------------------------------------------+---------+
          | events_waits_current                         | YES     |
          | events_waits_history                         | YES     |
          | events_waits_history_long                    | YES     |
          | events_waits_summary_by_thread_by_event_name | YES     |
          | events_waits_summary_by_event_name           | YES     |
          | events_waits_summary_by_instance             | YES     |
          | file_summary_by_event_name                   | YES     |
          | file_summary_by_instance                     | YES     |
          +----------------------------------------------+---------+
          8 rows in set (0.00 sec)

  3. Run the workload that you want to monitor.  In this example, the
     *note 'mysqlslap': mysqlslap. load emulation client is used to
     simulate a workload.

          shell> ./mysqlslap --auto-generate-sql --concurrency=100 --iterations=10
                 --number-of-queries=1000 --number-char-cols=6 --number-int-cols=6;

  4. Query the wait event data.  In this example, wait event data is
     queried from the *note 'events_waits_summary_global_by_event_name':
     wait-summary-tables. table which aggregates data found in the *note
     'events_waits_current': events-waits-current-table, *note
     'events_waits_history': events-waits-history-table, and *note
     'events_waits_history_long': events-waits-history-long-table.
     tables.  Data is summarized by event name ('EVENT_NAME'), which is
     the name of the instrument that produced the event.  Summarized
     data includes:

        * 'COUNT_STAR'

          The number of summarized wait events.

        * 'SUM_TIMER_WAIT'

          The total wait time of the summarized timed wait events.

        * 'MIN_TIMER_WAIT'

          The minimum wait time of the summarized timed wait events.

        * 'AVG_TIMER_WAIT'

          The average wait time of the summarized timed wait events.

        * 'MAX_TIMER_WAIT'

          The maximum wait time of the summarized timed wait events.

     The following query returns the instrument name ('EVENT_NAME'), the
     number of wait events ('COUNT_STAR'), and the total wait time for
     the events for that instrument ('SUM_TIMER_WAIT').  Because waits
     are timed in picoseconds (trillionths of a second) by default, wait
     times are divided by 1000000000 to show wait times in milliseconds.
     Data is presented in descending order, by the number of summarized
     wait events ('COUNT_STAR').  You can adjust the 'ORDER BY' clause
     to order the data by total wait time.

          mysql> SELECT EVENT_NAME, COUNT_STAR, SUM_TIMER_WAIT/1000000000 SUM_TIMER_WAIT_MS
                 FROM performance_schema.events_waits_summary_global_by_event_name
                 WHERE SUM_TIMER_WAIT > 0 AND EVENT_NAME LIKE 'wait/synch/mutex/innodb/%'
                 ORDER BY COUNT_STAR DESC;
          +-------------------------------------------------------+------------+-------------------+
          | EVENT_NAME                                            | COUNT_STAR | SUM_TIMER_WAIT_MS |
          +-------------------------------------------------------+------------+-------------------+
          | wait/synch/mutex/innodb/buf_pool_mutex                |     154477 |         6258.6407 |
          | wait/synch/mutex/innodb/kernel_mutex                  |      54294 |         1747.1980 |
          | wait/synch/mutex/innodb/log_sys_mutex                 |      40578 |         3167.6126 |
          | wait/synch/mutex/innodb/dict_sys_mutex                |      34261 |           26.4183 |
          | wait/synch/mutex/innodb/log_flush_order_mutex         |      24463 |            0.5867 |
          | wait/synch/mutex/innodb/rseg_mutex                    |      18204 |            0.4750 |
          | wait/synch/mutex/innodb/flush_list_mutex              |      15949 |            0.7182 |
          | wait/synch/mutex/innodb/mutex_list_mutex              |      10439 |            0.2299 |
          | wait/synch/mutex/innodb/fil_system_mutex              |       9815 |            0.5027 |
          | wait/synch/mutex/innodb/rw_lock_list_mutex            |       8292 |            0.1763 |
          | wait/synch/mutex/innodb/trx_undo_mutex                |       6070 |            0.2339 |
          | wait/synch/mutex/innodb/innobase_share_mutex          |       1994 |            0.0761 |
          | wait/synch/mutex/innodb/file_format_max_mutex         |       1007 |            0.0245 |
          | wait/synch/mutex/innodb/trx_doublewrite_mutex         |        387 |            0.0214 |
          | wait/synch/mutex/innodb/recv_sys_mutex                |        186 |            0.0047 |
          | wait/synch/mutex/innodb/ibuf_mutex                    |        121 |            0.0030 |
          | wait/synch/mutex/innodb/purge_sys_bh_mutex            |         99 |            0.0033 |
          | wait/synch/mutex/innodb/ibuf_pessimistic_insert_mutex |         40 |            0.0011 |
          | wait/synch/mutex/innodb/srv_innodb_monitor_mutex      |          3 |            0.0003 |
          +-------------------------------------------------------+------------+-------------------+
          19 rows in set (0.00 sec)

     *Note*:

     The preceding result set includes wait event data produced during
     the startup process.  To exclude this data, you can truncate the
     *note 'events_waits_summary_global_by_event_name':
     wait-summary-tables. table immediately after startup and before
     running your workload.  However, the truncate operation itself may
     produce a negligible amount wait event data.

          mysql> TRUNCATE performance_schema.events_waits_summary_global_by_event_name;


File: manual.info.tmp,  Node: innodb-monitors,  Next: innodb-backup-recovery,  Prev: innodb-performance-schema,  Up: innodb-storage-engine

14.20 InnoDB Monitors
=====================

* Menu:

* innodb-monitor-types::         InnoDB Monitor Types
* innodb-enabling-monitors::     Enabling InnoDB Monitors
* innodb-standard-monitor::      InnoDB Standard Monitor and Lock Monitor Output
* innodb-tablespace-monitor::    InnoDB Tablespace Monitor Output
* innodb-table-monitor::         InnoDB Table Monitor Output

'InnoDB' monitors provide information about the 'InnoDB' internal state.
This information is useful for performance tuning.


File: manual.info.tmp,  Node: innodb-monitor-types,  Next: innodb-enabling-monitors,  Prev: innodb-monitors,  Up: innodb-monitors

14.20.1 InnoDB Monitor Types
----------------------------

There are four types of 'InnoDB' monitors:

   * The standard 'InnoDB' Monitor displays the following types of
     information:

        * Work done by the main background thread

        * Semaphore waits

        * Data about the most recent foreign key and deadlock errors

        * Lock waits for transactions

        * Table and record locks held by active transactions

        * Pending I/O operations and related statistics

        * Insert buffer and adaptive hash index statistics

        * Redo log data

        * Buffer pool statistics

        * Row operation data

   * The 'InnoDB' Lock Monitor prints additional lock information as
     part of the standard 'InnoDB' Monitor output.

   * The 'InnoDB' Tablespace Monitor prints a list of file segments in
     the shared tablespace and validates the tablespace allocation data
     structures.

   * The 'InnoDB' Table Monitor prints the contents of the 'InnoDB'
     internal data dictionary.

For additional information about 'InnoDB' table and tablespace monitors,
see Mark Leith: InnoDB Table and Tablespace Monitors
(http://www.markleith.co.uk/?p=25).


File: manual.info.tmp,  Node: innodb-enabling-monitors,  Next: innodb-standard-monitor,  Prev: innodb-monitor-types,  Up: innodb-monitors

14.20.2 Enabling InnoDB Monitors
--------------------------------

When 'InnoDB' monitors are enabled for periodic output, 'InnoDB' writes
the output to *note 'mysqld': mysqld. server standard error output
('stderr') every 15 seconds, approximately.

'InnoDB' sends the monitor output to 'stderr' rather than to 'stdout' or
fixed-size memory buffers to avoid potential buffer overflows.

On Windows, 'stderr' is directed to the default log file unless
configured otherwise.  If you want to direct the output to the console
window rather than to the error log, start the server from a command
prompt in a console window with the '--console' option.  For more
information, see *note error-log-windows::.

On Unix and Unix-like systems, 'stderr' is typically directed to the
terminal unless configured otherwise.  For more information, see *note
error-log-unix::.

'InnoDB' monitors should only be enabled when you actually want to see
monitor information because output generation causes some performance
decrement.  Also, if monitor output is directed to the error log, the
log may become quite large if you forget to disable the monitor later by
dropping the monitor table.

*Note*:

To assist with troubleshooting, 'InnoDB' temporarily enables standard
'InnoDB' Monitor output under certain conditions.  For more information,
see *note innodb-troubleshooting::.

Each monitor begins with a header containing a timestamp and the monitor
name.  For example:

     =====================================
     141016 15:41:44 INNODB MONITOR OUTPUT
     =====================================

The header for the standard 'InnoDB' Monitor ('INNODB MONITOR OUTPUT')
is also used for the Lock Monitor because the latter produces the same
output with the addition of extra lock information.

Enabling an 'InnoDB' monitor for periodic output involves using a
'CREATE TABLE' statement to create a specially named 'InnoDB' table that
is associated with the monitor.  For example, to enable the standard
'InnoDB' Monitor, you would create an 'InnoDB' table named
'innodb_monitor'.

Using *note 'CREATE TABLE': create-table. syntax is just a way to pass a
command to the 'InnoDB' engine through MySQL's SQL parser.  The only
things that matter are the table name and that it be an 'InnoDB' table.
The structure of the table and the database where the table is created
are not relevant.  If you shut down the server, the monitor does not
restart automatically when you restart the server.  Drop the monitor
table and issue a new *note 'CREATE TABLE': create-table. statement to
start the monitor.

The 'PROCESS' privilege is required to enable or disable 'InnoDB'
Monitors.

*Enabling the Standard InnoDB Monitor*

To enable the standard 'InnoDB' Monitor for periodic output, create the
'innodb_monitor' table:

     CREATE TABLE innodb_monitor (a INT) ENGINE=INNODB;

To disable the standard 'InnoDB' Monitor, drop the table:

     DROP TABLE innodb_monitor;

*Enabling the InnoDB Lock Monitor*

To enable the 'InnoDB' Lock Monitor for periodic output, create the
'innodb_lock_monitor' table:

     CREATE TABLE innodb_lock_monitor (a INT) ENGINE=INNODB;

To disable the 'InnoDB' Lock Monitor, drop the table:

     DROP TABLE innodb_lock_monitor;

*Obtaining Standard InnoDB Monitor Output On Demand*

As an alternative to enabling the standard 'InnoDB' Monitor for periodic
output, you can obtain standard 'InnoDB' Monitor output on demand using
the *note 'SHOW ENGINE INNODB STATUS': show-engine. SQL statement, which
fetches the output to your client program.  If you are using the *note
'mysql': mysql. interactive client, the output is more readable if you
replace the usual semicolon statement terminator with '\G':

     mysql> SHOW ENGINE INNODB STATUS\G

*note 'SHOW ENGINE INNODB STATUS': show-engine. output also includes
'InnoDB' Lock Monitor data if the 'InnoDB' Lock Monitor is enabled.

*Directing Standard InnoDB Monitor Output to a Status File*

Standard 'InnoDB' Monitor output can be enabled and directed to a status
file by specifying the '--innodb-status-file' option at startup.  When
this option is used, 'InnoDB' creates a file named 'innodb_status.PID'
in the data directory and writes output to it every 15 seconds,
approximately.

'InnoDB' removes the status file when the server is shut down normally.
If an abnormal shutdown occurs, the status file may have to be removed
manually.

The '--innodb-status-file' option is intended for temporary use, as
output generation can affect performance, and the 'innodb_status.PID'
file can become quite large over time.

*Enabling the InnoDB Tablespace Monitor*

To enable the 'InnoDB' Tablespace Monitor for periodic output, create
the 'innodb_tablespace_monitor' table:

     CREATE TABLE innodb_tablespace_monitor (a INT) ENGINE=INNODB;

To disable the standard 'InnoDB' Tablespace Monitor, drop the table:

     DROP TABLE innodb_tablespace_monitor;

*Enabling the InnoDB Table Monitor*

To enable the 'InnoDB' Table Monitor for periodic output, create the
'innodb_table_monitor' table:

     CREATE TABLE innodb_table_monitor (a INT) ENGINE=INNODB;

To disable the 'InnoDB' Table Monitor, drop the table:

     DROP TABLE innodb_table_monitor;


File: manual.info.tmp,  Node: innodb-standard-monitor,  Next: innodb-tablespace-monitor,  Prev: innodb-enabling-monitors,  Up: innodb-monitors

14.20.3 InnoDB Standard Monitor and Lock Monitor Output
-------------------------------------------------------

The Lock Monitor is the same as the Standard Monitor except that it
includes additional lock information.  Enabling either monitor for
periodic output turns on the same output stream, but the stream includes
extra information if the Lock Monitor is enabled.  For example, if you
enable the Standard Monitor and Lock Monitor, that turns on a single
output stream.  The stream includes extra lock information until you
disable the Lock Monitor.

Standard Monitor output is limited to 1MB when produced using the *note
'SHOW ENGINE INNODB STATUS': show-engine. statement.  This limit does
not apply to output written to server standard error output ('stderr').

Example Standard Monitor output:

     mysql> SHOW ENGINE INNODB STATUS\G
     *************************** 1. row ***************************
       Type: InnoDB
       Name:
     Status:
     =====================================
     141016 15:41:44 INNODB MONITOR OUTPUT
     =====================================
     Per second averages calculated from the last 6 seconds
     -----------------
     BACKGROUND THREAD
     -----------------
     srv_master_thread loops: 49 1_second, 48 sleeps, 3 10_second, 18 background,
     18 flush
     srv_master_thread log flush and writes: 48
     ----------
     SEMAPHORES
     ----------
     OS WAIT ARRAY INFO: reservation count 46, signal count 45
     Mutex spin waits 30, rounds 900, OS waits 27
     RW-shared spins 14, rounds 420, OS waits 14
     RW-excl spins 0, rounds 150, OS waits 5
     Spin rounds per wait: 30.00 mutex, 30.00 RW-shared, 150.00 RW-excl
     ------------------------
     LATEST FOREIGN KEY ERROR
     ------------------------
     141016 15:37:30 Transaction:
     TRANSACTION 3D005, ACTIVE 0 sec inserting
     mysql tables in use 1, locked 1
     4 lock struct(s), heap size 1248, 3 row lock(s), undo log entries 3
     MySQL thread id 1, OS thread handle 0x7f0ee440e700, query id 70 localhost root
     update
     INSERT INTO child VALUES
         (NULL, 1)
         , (NULL, 2)
         , (NULL, 3)
         , (NULL, 4)
         , (NULL, 5)
         , (NULL, 6)
     Foreign key constraint fails for table `mysql`.`child`:
     ,
       CONSTRAINT `child_ibfk_1` FOREIGN KEY (`parent_id`) REFERENCES `parent` (`id`)
       ON DELETE CASCADE ON UPDATE CASCADE
     Trying to add in child table, in index `par_ind` tuple:
     DATA TUPLE: 2 fields;
      0: len 4; hex 80000003; asc     ;;
      1: len 4; hex 80000003; asc     ;;

     But in parent table `mysql`.`parent`, in index `PRIMARY`,
     the closest match we can find is record:
     PHYSICAL RECORD: n_fields 3; compact format; info bits 0
      0: len 4; hex 80000004; asc     ;;
      1: len 6; hex 00000003d002; asc       ;;
      2: len 7; hex 8300001d480137; asc     H 7;;

     ------------------------
     LATEST DETECTED DEADLOCK
     ------------------------
     141016 15:39:58
     *** (1) TRANSACTION:
     TRANSACTION 3D009, ACTIVE 19 sec starting index read
     mysql tables in use 1, locked 1
     LOCK WAIT 2 lock struct(s), heap size 376, 1 row lock(s)
     MySQL thread id 2, OS thread handle 0x7f0ee43cd700, query id 78 localhost root
     updating
     DELETE FROM t WHERE i = 1
     *** (1) WAITING FOR THIS LOCK TO BE GRANTED:
     RECORD LOCKS space id 0 page no 2428 n bits 72 index `GEN_CLUST_INDEX` of table
     `mysql`.`t` trx id 3D009 lock_mode X waiting
     Record lock, heap no 2 PHYSICAL RECORD: n_fields 4; compact format; info bits 0
      0: len 6; hex 000000000700; asc       ;;
      1: len 6; hex 00000003d007; asc       ;;
      2: len 7; hex 87000009560110; asc     V  ;;
      3: len 4; hex 80000001; asc     ;;

     *** (2) TRANSACTION:
     TRANSACTION 3D008, ACTIVE 69 sec starting index read
     mysql tables in use 1, locked 1
     4 lock struct(s), heap size 1248, 3 row lock(s)
     MySQL thread id 1, OS thread handle 0x7f0ee440e700, query id 79 localhost root
     updating
     DELETE FROM t WHERE i = 1
     *** (2) HOLDS THE LOCK(S):
     RECORD LOCKS space id 0 page no 2428 n bits 72 index `GEN_CLUST_INDEX` of table
     `mysql`.`t` trx id 3D008 lock mode S
     Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0
      0: len 8; hex 73757072656d756d; asc supremum;;

     Record lock, heap no 2 PHYSICAL RECORD: n_fields 4; compact format; info bits 0
      0: len 6; hex 000000000700; asc       ;;
      1: len 6; hex 00000003d007; asc       ;;
      2: len 7; hex 87000009560110; asc     V  ;;
      3: len 4; hex 80000001; asc     ;;

     *** (2) WAITING FOR THIS LOCK TO BE GRANTED:
     RECORD LOCKS space id 0 page no 2428 n bits 72 index `GEN_CLUST_INDEX` of table
     `mysql`.`t` trx id 3D008 lock_mode X waiting
     Record lock, heap no 2 PHYSICAL RECORD: n_fields 4; compact format; info bits 0
      0: len 6; hex 000000000700; asc       ;;
      1: len 6; hex 00000003d007; asc       ;;
      2: len 7; hex 87000009560110; asc     V  ;;
      3: len 4; hex 80000001; asc     ;;

     *** WE ROLL BACK TRANSACTION (1)
     ------------
     TRANSACTIONS
     ------------
     Trx id counter 3D038
     Purge done for trx's n:o < 3D02A undo n:o < 0
     History list length 1047
     Total number of lock structs in row lock hash table 0
     LIST OF TRANSACTIONS FOR EACH SESSION:
     ---TRANSACTION 3D009, not started
     MySQL thread id 2, OS thread handle 0x7f0ee43cd700, query id 78 localhost root
     ---TRANSACTION 3D008, not started
     MySQL thread id 1, OS thread handle 0x7f0ee440e700, query id 113 localhost root
     SHOW ENGINE INNODB STATUS
     ---TRANSACTION 3D037, ACTIVE 1 sec inserting
     mysql tables in use 1, locked 1
     1 lock struct(s), heap size 376, 0 row lock(s), undo log entries 11940
     MySQL thread id 3, OS thread handle 0x7f0ee438c700, query id 112 localhost root
     update
     INSERT INTO `employees` VALUES (413215,'1962-07-08','Ronghao','Molberg','F',
     '1985-06-20'),(413216,'1954-05-25','Masaru','Lieberherr','M','1992-04-08'),
     (413217,'1953-03-17','Phule','Waschkowski','F','1988-07-28'),(413218,'1964-10-07',
     'Vitaly','Negoita','M','1986-01-13'),(413219,'1957-03-31','Danil','Kalafatis','F',
     '1985-04-12'),(413220,'1958-07-25','Jianwen','Radwan','M','1986-09-03'),(413221,
     '1964-04-08','Paloma','Bach','M','1986-05-03'),(413222,'1955-06-10','Stafford',
     'Muhlberg','M','1989-03-22'),(413223,'1963-10-27','Aiichiro','Benzmuller','M',
     '1987-12-02'),(413224,'1955-10-02','Giordano','N
     TABLE LOCK table `employees`.`employees` trx id 3D037 lock mode IX
     --------
     FILE I/O
     --------
     I/O thread 0 state: waiting for completed aio requests (insert buffer thread)
     I/O thread 1 state: waiting for completed aio requests (log thread)
     I/O thread 2 state: waiting for completed aio requests (read thread)
     I/O thread 3 state: waiting for completed aio requests (read thread)
     I/O thread 4 state: waiting for completed aio requests (read thread)
     I/O thread 5 state: waiting for completed aio requests (read thread)
     I/O thread 6 state: waiting for completed aio requests (write thread)
     I/O thread 7 state: waiting for completed aio requests (write thread)
     I/O thread 8 state: waiting for completed aio requests (write thread)
     I/O thread 9 state: waiting for completed aio requests (write thread)
     Pending normal aio reads: 0 [0, 0, 0, 0] , aio writes: 0 [0, 0, 0, 0] ,
      ibuf aio reads: 0, log i/o's: 0, sync i/o's: 0
     Pending flushes (fsync) log: 0; buffer pool: 0
     439 OS file reads, 917 OS file writes, 199 OS fsyncs
     0.00 reads/s, 0 avg bytes/read, 56.32 writes/s, 7.67 fsyncs/s
     -------------------------------------
     INSERT BUFFER AND ADAPTIVE HASH INDEX
     -------------------------------------
     Ibuf: size 1, free list len 0, seg size 2, 0 merges
     merged operations:
      insert 0, delete mark 0, delete 0
     discarded operations:
      insert 0, delete mark 0, delete 0
     Hash table size 4425293, used cells 32, node heap has 1 buffer(s)
     13577.57 hash searches/s, 202.47 non-hash searches/s
     ---
     LOG
     ---
     Log sequence number 794838329
     Log flushed up to   793815740
     Last checkpoint at  788417971
     0 pending log writes, 0 pending chkp writes
     96 log i/o's done, 3.50 log i/o's/second
     ----------------------
     BUFFER POOL AND MEMORY
     ----------------------
     Total memory allocated 2217738240; in additional pool allocated 0
     Dictionary memory allocated 121719
     Buffer pool size   131072
     Free buffers       129937
     Database pages     1134
     Old database pages 211
     Modified db pages  187
     Pending reads 0
     Pending writes: LRU 0, flush list 0, single page 0
     Pages made young 0, not young 0
     0.00 youngs/s, 0.00 non-youngs/s
     Pages read 426, created 708, written 768
     0.00 reads/s, 40.99 creates/s, 50.49 writes/s
     Buffer pool hit rate 1000 / 1000, young-making rate 0 / 1000 not 0 / 1000
     Pages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead
     0.00/s
     LRU len: 1134, unzip_LRU len: 0
     I/O sum[0]:cur[0], unzip sum[0]:cur[0]
     ----------------------
     INDIVIDUAL BUFFER POOL INFO
     ----------------------
     ---BUFFER POOL 0
     Buffer pool size   65536
     Free buffers       65029
     Database pages     506
     Old database pages 0
     Modified db pages  95
     Pending reads 0
     Pending writes: LRU 0, flush list 0, single page 0
     Pages made young 0, not young 0
     0.00 youngs/s, 0.00 non-youngs/s
     Pages read 137, created 369, written 412
     0.00 reads/s, 20.16 creates/s, 18.00 writes/s
     Buffer pool hit rate 1000 / 1000, young-making rate 0 / 1000 not 0 / 1000
     Pages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead
     0.00/s
     LRU len: 506, unzip_LRU len: 0
     I/O sum[0]:cur[0], unzip sum[0]:cur[0]
     ---BUFFER POOL 1
     Buffer pool size   65536
     Free buffers       64908
     Database pages     628
     Old database pages 211
     Modified db pages  92
     Pending reads 0
     Pending writes: LRU 0, flush list 0, single page 0
     Pages made young 0, not young 0
     0.00 youngs/s, 0.00 non-youngs/s
     Pages read 289, created 339, written 356
     0.00 reads/s, 20.83 creates/s, 32.49 writes/s
     Buffer pool hit rate 1000 / 1000, young-making rate 0 / 1000 not 0 / 1000
     Pages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead
     0.00/s
     LRU len: 628, unzip_LRU len: 0
     I/O sum[0]:cur[0], unzip sum[0]:cur[0]
     --------------
     ROW OPERATIONS
     --------------
     0 queries inside InnoDB, 0 queries in queue
     1 read views open inside InnoDB
     Main thread process no. 30091, id 139699544078080, state: sleeping
     Number of rows inserted 225354, updated 0, deleted 3, read 4
     13690.55 inserts/s, 0.00 updates/s, 0.00 deletes/s, 0.00 reads/s
     ----------------------------
     END OF INNODB MONITOR OUTPUT
     ============================

*Standard Monitor Output Sections*

For a description of each metric reported by the Standard Monitor, refer
to the Metrics
(http://dev.mysql.com/doc/mysql-em-plugin/en/myoem-metrics.html) chapter
in the Oracle Enterprise Manager for MySQL Database User's Guide
(http://dev.mysql.com/doc/mysql-em-plugin/en/).

   * 'Status'

     This section shows the timestamp, the monitor name, and the number
     of seconds that per-second averages are based on.  The number of
     seconds is the elapsed time between the current time and the last
     time 'InnoDB' Monitor output was printed.

   * 'BACKGROUND THREAD'

     The 'srv_master_thread' lines shows work done by the main
     background thread.

   * 'SEMAPHORES'

     This section reports threads waiting for a semaphore and statistics
     on how many times threads have needed a spin or a wait on a mutex
     or a rw-lock semaphore.  A large number of threads waiting for
     semaphores may be a result of disk I/O, or contention problems
     inside 'InnoDB'.  Contention can be due to heavy parallelism of
     queries or problems in operating system thread scheduling.  Setting
     the 'innodb_thread_concurrency' system variable smaller than the
     default value might help in such situations.  The 'Spin rounds per
     wait' line shows the number of spinlock rounds per OS wait for a
     mutex.

   * 'LATEST FOREIGN KEY ERROR'

     This section provides information about the most recent foreign key
     constraint error.  It is not present if no such error has occurred.
     The contents include the statement that failed as well as
     information about the constraint that failed and the referenced and
     referencing tables.

   * 'LATEST DETECTED DEADLOCK'

     This section provides information about the most recent deadlock.
     It is not present if no deadlock has occurred.  The contents show
     which transactions are involved, the statement each was attempting
     to execute, the locks they have and need, and which transaction
     'InnoDB' decided to roll back to break the deadlock.  The lock
     modes reported in this section are explained in *note
     innodb-locking::.

   * 'TRANSACTIONS'

     If this section reports lock waits, your applications might have
     lock contention.  The output can also help to trace the reasons for
     transaction deadlocks.

   * 'FILE I/O'

     This section provides information about threads that 'InnoDB' uses
     to perform various types of I/O. The first few of these are
     dedicated to general 'InnoDB' processing.  The contents also
     display information for pending I/O operations and statistics for
     I/O performance.

     The number of these threads are controlled by the
     'innodb_read_io_threads' and 'innodb_write_io_threads' parameters.
     See *note innodb-parameters::.

   * 'INSERT BUFFER AND ADAPTIVE HASH INDEX'

     This section shows the status of the 'InnoDB' insert buffer (also
     referred to as the change buffer) and the adaptive hash index.

     For related information, see *note innodb-change-buffer::, and
     *note innodb-adaptive-hash::.

   * 'LOG'

     This section displays information about the 'InnoDB' log.  The
     contents include the current log sequence number, how far the log
     has been flushed to disk, and the position at which 'InnoDB' last
     took a checkpoint.  (See *note innodb-checkpoints::.)  The section
     also displays information about pending writes and write
     performance statistics.

   * 'BUFFER POOL AND MEMORY'

     This section gives you statistics on pages read and written.  You
     can calculate from these numbers how many data file I/O operations
     your queries currently are doing.

     For buffer pool statistics descriptions, see *note
     innodb-buffer-pool-monitoring::.  For additional information about
     the operation of the buffer pool, see *note innodb-buffer-pool::.

   * 'ROW OPERATIONS'

     This section shows what the main thread is doing, including the
     number and performance rate for each type of row operation.

     In MySQL 5.5, output from the standard 'InnoDB' Monitor includes
     additional sections compared to the output for previous versions.
     For details, see *note monitoring-improvements::.


File: manual.info.tmp,  Node: innodb-tablespace-monitor,  Next: innodb-table-monitor,  Prev: innodb-standard-monitor,  Up: innodb-monitors

14.20.4 InnoDB Tablespace Monitor Output
----------------------------------------

The 'InnoDB' Tablespace Monitor prints information about the file
segments in the shared tablespace and validates the tablespace
allocation data structures.  The Tablespace Monitor does not describe
file-per-table tablespaces created with the 'innodb_file_per_table'
option.

Example 'InnoDB' Tablespace Monitor output:

     ================================================
     090408 21:28:09 INNODB TABLESPACE MONITOR OUTPUT
     ================================================
     FILE SPACE INFO: id 0
     size 13440, free limit 3136, free extents 28
     not full frag extents 2: used pages 78, full frag extents 3
     first seg id not used 0 23845
     SEGMENT id 0 1 space 0; page 2; res 96 used 46; full ext 0
     fragm pages 32; free extents 0; not full extents 1: pages 14
     SEGMENT id 0 2 space 0; page 2; res 1 used 1; full ext 0
     fragm pages 1; free extents 0; not full extents 0: pages 0
     SEGMENT id 0 3 space 0; page 2; res 1 used 1; full ext 0
     fragm pages 1; free extents 0; not full extents 0: pages 0
     ...
     SEGMENT id 0 15 space 0; page 2; res 160 used 160; full ext 2
     fragm pages 32; free extents 0; not full extents 0: pages 0
     SEGMENT id 0 488 space 0; page 2; res 1 used 1; full ext 0
     fragm pages 1; free extents 0; not full extents 0: pages 0
     SEGMENT id 0 17 space 0; page 2; res 1 used 1; full ext 0
     fragm pages 1; free extents 0; not full extents 0: pages 0
     ...
     SEGMENT id 0 171 space 0; page 2; res 592 used 481; full ext 7
     fragm pages 16; free extents 0; not full extents 2: pages 17
     SEGMENT id 0 172 space 0; page 2; res 1 used 1; full ext 0
     fragm pages 1; free extents 0; not full extents 0: pages 0
     SEGMENT id 0 173 space 0; page 2; res 96 used 44; full ext 0
     fragm pages 32; free extents 0; not full extents 1: pages 12
     ...
     SEGMENT id 0 601 space 0; page 2; res 1 used 1; full ext 0
     fragm pages 1; free extents 0; not full extents 0: pages 0
     NUMBER of file segments: 73
     Validating tablespace
     Validation ok
     ---------------------------------------
     END OF INNODB TABLESPACE MONITOR OUTPUT
     =======================================

The Tablespace Monitor output includes information about the shared
tablespace as a whole, followed by a list containing a breakdown for
each segment within the tablespace.

In this example using the default page size, the tablespace consists of
database pages that are 16KB each.  The pages are grouped into extents
of size 1MB (64 consecutive pages).

The initial part of the output that displays overall tablespace
information has this format:

     FILE SPACE INFO: id 0
     size 13440, free limit 3136, free extents 28
     not full frag extents 2: used pages 78, full frag extents 3
     first seg id not used 0 23845

Overall tablespace information includes these values:

   * 'id'

     The tablespace ID. A value of 0 refers to the shared tablespace.

   * 'size'

     The current tablespace size in pages.

   * 'free limit'

     The minimum page number for which the free list has not been
     initialized.  Pages at or above this limit are free.

   * 'free extents'

     The number of free extents.

   * 'not full frag extents', 'used pages'

     The number of fragment extents that are not completely filled, and
     the number of pages in those extents that have been allocated.

   * 'full frag extents'

     The number of completely full fragment extents.

   * 'first seg id not used'

     The first unused segment ID.

Individual segment information has this format:

     SEGMENT id 0 15 space 0; page 2; res 160 used 160; full ext 2
     fragm pages 32; free extents 0; not full extents 0: pages 0

Segment information includes these values:

   * 'id'

     The segment ID.

   * 'space', 'page'

     The tablespace number and page within the tablespace where the
     segment 'inode' is located.  A tablespace number of 0 indicates the
     shared tablespace.  'InnoDB' uses inodes to keep track of segments
     in the tablespace.  The other fields displayed for a segment ('id',
     'res', and so forth) are derived from information in the inode.

   * 'res'

     The number of pages allocated (reserved) for the segment.

   * 'used'

     The number of allocated pages in use by the segment.

   * 'full ext'

     The number of extents allocated for the segment that are completely
     used.

   * 'fragm pages'

     The number of initial pages that have been allocated to the
     segment.

   * 'free extents'

     The number of extents allocated for the segment that are completely
     unused.

   * 'not full extents'

     The number of extents allocated for the segment that are partially
     used.

   * 'pages'

     The number of pages used within the not-full extents.

When a segment grows, it starts as a single page, and 'InnoDB' allocates
the first pages for it one at a time, up to 32 pages (this is the 'fragm
pages' value).  After that, 'InnoDB' allocates complete extents.
'InnoDB' can add up to 4 extents at a time to a large segment to ensure
good sequentiality of data.

For the example segment shown earlier, it has 32 fragment pages, plus 2
full extents (64 pages each), for a total of 160 pages used out of 160
pages allocated.  The following segment has 32 fragment pages and one
partially full extent using 14 pages for a total of 46 pages used out of
96 pages allocated:

     SEGMENT id 0 1 space 0; page 2; res 96 used 46; full ext 0
     fragm pages 32; free extents 0; not full extents 1: pages 14

It is possible for a segment that has extents allocated to it to have a
'fragm pages' value less than 32 if some of the individual pages have
been deallocated subsequent to extent allocation.


File: manual.info.tmp,  Node: innodb-table-monitor,  Prev: innodb-tablespace-monitor,  Up: innodb-monitors

14.20.5 InnoDB Table Monitor Output
-----------------------------------

The 'InnoDB' Table Monitor prints the contents of the 'InnoDB' internal
data dictionary.

The output contains one section per table.  The 'SYS_FOREIGN' and
'SYS_FOREIGN_COLS' sections are for internal data dictionary tables that
maintain information about foreign keys.  There are also sections for
the Table Monitor table and each user-created 'InnoDB' table.  Suppose
that the following two tables have been created in the 'test' database:

     CREATE TABLE parent
     (
       par_id    INT NOT NULL,
       fname      CHAR(20),
       lname      CHAR(20),
       PRIMARY KEY (par_id),
       UNIQUE INDEX (lname, fname)
     ) ENGINE = INNODB;

     CREATE TABLE child
     (
       par_id      INT NOT NULL,
       child_id    INT NOT NULL,
       name        VARCHAR(40),
       birth       DATE,
       weight      DECIMAL(10,2),
       misc_info   VARCHAR(255),
       last_update TIMESTAMP,
       PRIMARY KEY (par_id, child_id),
       INDEX (name),
       FOREIGN KEY (par_id) REFERENCES parent (par_id)
         ON DELETE CASCADE
         ON UPDATE CASCADE
     ) ENGINE = INNODB;

Then the Table Monitor output will look something like this (reformatted
slightly):

     ===========================================
     090420 12:09:32 INNODB TABLE MONITOR OUTPUT
     ===========================================
     --------------------------------------
     TABLE: name SYS_FOREIGN, id 0 11, columns 7, indexes 3, appr.rows 1
       COLUMNS: ID: DATA_VARCHAR DATA_ENGLISH len 0;
                FOR_NAME: DATA_VARCHAR DATA_ENGLISH len 0;
                REF_NAME: DATA_VARCHAR DATA_ENGLISH len 0;
                N_COLS: DATA_INT len 4;
                DB_ROW_ID: DATA_SYS prtype 256 len 6;
                DB_TRX_ID: DATA_SYS prtype 257 len 6;
       INDEX: name ID_IND, id 0 11, fields 1/6, uniq 1, type 3
        root page 46, appr.key vals 1, leaf pages 1, size pages 1
        FIELDS:  ID DB_TRX_ID DB_ROLL_PTR FOR_NAME REF_NAME N_COLS
       INDEX: name FOR_IND, id 0 12, fields 1/2, uniq 2, type 0
        root page 47, appr.key vals 1, leaf pages 1, size pages 1
        FIELDS:  FOR_NAME ID
       INDEX: name REF_IND, id 0 13, fields 1/2, uniq 2, type 0
        root page 48, appr.key vals 1, leaf pages 1, size pages 1
        FIELDS:  REF_NAME ID
     --------------------------------------
     TABLE: name SYS_FOREIGN_COLS, id 0 12, columns 7, indexes 1, appr.rows 1
       COLUMNS: ID: DATA_VARCHAR DATA_ENGLISH len 0;
                POS: DATA_INT len 4;
                FOR_COL_NAME: DATA_VARCHAR DATA_ENGLISH len 0;
                REF_COL_NAME: DATA_VARCHAR DATA_ENGLISH len 0;
                DB_ROW_ID: DATA_SYS prtype 256 len 6;
                DB_TRX_ID: DATA_SYS prtype 257 len 6;
       INDEX: name ID_IND, id 0 14, fields 2/6, uniq 2, type 3
        root page 49, appr.key vals 1, leaf pages 1, size pages 1
        FIELDS:  ID POS DB_TRX_ID DB_ROLL_PTR FOR_COL_NAME REF_COL_NAME
     --------------------------------------
     TABLE: name test/child, id 0 14, columns 10, indexes 2, appr.rows 201
       COLUMNS: par_id: DATA_INT DATA_BINARY_TYPE DATA_NOT_NULL len 4;
                child_id: DATA_INT DATA_BINARY_TYPE DATA_NOT_NULL len 4;
                name: DATA_VARCHAR prtype 524303 len 40;
                birth: DATA_INT DATA_BINARY_TYPE len 3;
                weight: DATA_FIXBINARY DATA_BINARY_TYPE len 5;
                misc_info: DATA_VARCHAR prtype 524303 len 255;
                last_update: DATA_INT DATA_UNSIGNED DATA_BINARY_TYPE DATA_NOT_NULL len 4;
                DB_ROW_ID: DATA_SYS prtype 256 len 6;
                DB_TRX_ID: DATA_SYS prtype 257 len 6;
       INDEX: name PRIMARY, id 0 17, fields 2/9, uniq 2, type 3
        root page 52, appr.key vals 201, leaf pages 5, size pages 6
        FIELDS:  par_id child_id DB_TRX_ID DB_ROLL_PTR name birth weight misc_info last_update
       INDEX: name name, id 0 18, fields 1/3, uniq 3, type 0
        root page 53, appr.key vals 210, leaf pages 1, size pages 1
        FIELDS:  name par_id child_id
       FOREIGN KEY CONSTRAINT test/child_ibfk_1: test/child ( par_id )
                  REFERENCES test/parent ( par_id )
     --------------------------------------
     TABLE: name test/innodb_table_monitor, id 0 15, columns 4, indexes 1, appr.rows 0
       COLUMNS: i: DATA_INT DATA_BINARY_TYPE len 4;
                DB_ROW_ID: DATA_SYS prtype 256 len 6;
                DB_TRX_ID: DATA_SYS prtype 257 len 6;
       INDEX: name GEN_CLUST_INDEX, id 0 19, fields 0/4, uniq 1, type 1
        root page 193, appr.key vals 0, leaf pages 1, size pages 1
        FIELDS:  DB_ROW_ID DB_TRX_ID DB_ROLL_PTR i
     --------------------------------------
     TABLE: name test/parent, id 0 13, columns 6, indexes 2, appr.rows 299
       COLUMNS: par_id: DATA_INT DATA_BINARY_TYPE DATA_NOT_NULL len 4;
                fname: DATA_CHAR prtype 524542 len 20;
                lname: DATA_CHAR prtype 524542 len 20;
                DB_ROW_ID: DATA_SYS prtype 256 len 6;
                DB_TRX_ID: DATA_SYS prtype 257 len 6;
       INDEX: name PRIMARY, id 0 15, fields 1/5, uniq 1, type 3
        root page 50, appr.key vals 299, leaf pages 2, size pages 3
        FIELDS:  par_id DB_TRX_ID DB_ROLL_PTR fname lname
       INDEX: name lname, id 0 16, fields 2/3, uniq 2, type 2
        root page 51, appr.key vals 300, leaf pages 1, size pages 1
        FIELDS:  lname fname par_id
       FOREIGN KEY CONSTRAINT test/child_ibfk_1: test/child ( par_id )
                  REFERENCES test/parent ( par_id )
     -----------------------------------
     END OF INNODB TABLE MONITOR OUTPUT
     ==================================

For each table, Table Monitor output contains a section that displays
general information about the table and specific information about its
columns, indexes, and foreign keys.

The general information for each table includes the table name (in
'DB_NAME/TBL_NAME' format except for internal tables), its ID, the
number of columns and indexes, and an approximate row count.

The 'COLUMNS' part of a table section lists each column in the table.
Information for each column indicates its name and data type
characteristics.  Some internal columns are added by 'InnoDB', such as
'DB_ROW_ID' (row ID), 'DB_TRX_ID' (transaction ID), and 'DB_ROLL_PTR' (a
pointer to the rollback/undo data).

   * 'DATA_XXX'

     These symbols indicate the data type.  There may be multiple
     'DATA_XXX' symbols for a given column.

   * 'prtype'

     The column's 'precise' type.  This field includes information such
     as the column data type, character set code, nullability,
     signedness, and whether it is a binary string.  This field is
     described in the 'innobase/include/data0type.h' source file.

   * 'len'

     The column length in bytes.

Each 'INDEX' part of the table section provides the name and
characteristics of one table index:

   * 'name'

     The index name.  If the name is 'PRIMARY', the index is a primary
     key.  If the name is 'GEN_CLUST_INDEX', the index is the clustered
     index that is created automatically if the table definition doesn't
     include a primary key or non-'NULL' unique index.  See *note
     innodb-index-types::.

   * 'id'

     The index ID.

   * 'fields'

     The number of fields in the index, as a value in 'M/N' format:

        * M is the number of user-defined columns; that is, the number
          of columns you would see in the index definition in a 'CREATE
          TABLE' statement.

        * N is the total number of index columns, including those added
          internally.  For the clustered index, the total includes the
          other columns in the table definition, plus any columns added
          internally.  For a secondary index, the total includes the
          columns from the primary key that are not part of the
          secondary index.

   * 'uniq'

     The number of leading fields that are enough to determine index
     values uniquely.

   * 'type'

     The index type.  This is a bit field.  For example, 1 indicates a
     clustered index and 2 indicates a unique index, so a clustered
     index (which always contains unique values), will have a 'type'
     value of 3.  An index with a 'type' value of 0 is neither clustered
     nor unique.  The flag values are defined in the
     'innobase/include/dict0mem.h' source file.

   * 'root page'

     The index root page number.

   * 'appr. key vals'

     The approximate index cardinality.

   * 'leaf pages'

     The approximate number of leaf pages in the index.

   * 'size pages'

     The approximate total number of pages in the index.

   * 'FIELDS'

     The names of the fields in the index.  For a clustered index that
     was generated automatically, the field list begins with the
     internal 'DB_ROW_ID' (row ID) field.  'DB_TRX_ID' and 'DB_ROLL_PTR'
     are always added internally to the clustered index, following the
     fields that comprise the primary key.  For a secondary index, the
     final fields are those from the primary key that are not part of
     the secondary index.

The end of the table section lists the 'FOREIGN KEY' definitions that
apply to the table.  This information appears whether the table is a
referencing or referenced table.


File: manual.info.tmp,  Node: innodb-backup-recovery,  Next: innodb-and-mysql-replication,  Prev: innodb-monitors,  Up: innodb-storage-engine

14.21 InnoDB Backup and Recovery
================================

* Menu:

* innodb-backup::                InnoDB Backup
* innodb-recovery::              InnoDB Recovery

This section covers topics related to 'InnoDB' backup and recovery.

   * For information about backup techniques applicable to 'InnoDB', see
     *note innodb-backup::.

   * For information about point-in-time recovery, recovery from disk
     failure or corruption, and how 'InnoDB' performs crash recovery,
     see *note innodb-recovery::.


File: manual.info.tmp,  Node: innodb-backup,  Next: innodb-recovery,  Prev: innodb-backup-recovery,  Up: innodb-backup-recovery

14.21.1 InnoDB Backup
---------------------

The key to safe database management is making regular backups.
Depending on your data volume, number of MySQL servers, and database
workload, you can use these backup techniques, alone or in combination:
hot backup with MySQL Enterprise Backup; cold backup by copying files
while the MySQL server is shut down; logical backup with *note
'mysqldump': mysqldump. for smaller data volumes or to record the
structure of schema objects.  Hot and cold backups are physical backups
that copy actual data files, which can be used directly by the *note
'mysqld': mysqld. server for faster restore.

Using _MySQL Enterprise Backup_ is the recommended method for backing up
'InnoDB' data.

*Note*:

'InnoDB' does not support databases that are restored using third-party
backup tools.

*Hot Backups*

The 'mysqlbackup' command, part of the MySQL Enterprise Backup
component, lets you back up a running MySQL instance, including 'InnoDB'
tables, with minimal disruption to operations while producing a
consistent snapshot of the database.  When 'mysqlbackup' is copying
'InnoDB' tables, reads and writes to 'InnoDB' can continue.  MySQL
Enterprise Backup can also create compressed backup files, and back up
subsets of tables and databases.  In conjunction with the MySQL binary
log, users can perform point-in-time recovery.  MySQL Enterprise Backup
is part of the MySQL Enterprise subscription.  For more details, see
*note mysql-enterprise-backup::.

*Cold Backups*

If you can shut down the MySQL server, you can make a physical backup
that consists of all files used by 'InnoDB' to manage its tables.  Use
the following procedure:

  1. Perform a slow shutdown of the MySQL server and make sure that it
     stops without errors.

  2. Copy all 'InnoDB' data files ('ibdata' files and '.ibd' files) into
     a safe place.

  3. Copy all the '.frm' files for 'InnoDB' tables to a safe place.

  4. Copy all 'InnoDB' log files ('ib_logfile' files) to a safe place.

  5. Copy your 'my.cnf' configuration file or files to a safe place.

*Logical Backups Using mysqldump*

In addition to physical backups, it is recommended that you regularly
create logical backups by dumping your tables using *note 'mysqldump':
mysqldump.  A binary file might be corrupted without you noticing it.
Dumped tables are stored into text files that are human-readable, so
spotting table corruption becomes easier.  Also, because the format is
simpler, the chance for serious data corruption is smaller.  *note
'mysqldump': mysqldump. also has a '--single-transaction' option for
making a consistent snapshot without locking out other clients.  See
*note backup-policy::.

Replication works with *note 'InnoDB': innodb-storage-engine. tables, so
you can use MySQL replication capabilities to keep a copy of your
database at database sites requiring high availability.  See *note
innodb-and-mysql-replication::.


File: manual.info.tmp,  Node: innodb-recovery,  Prev: innodb-backup,  Up: innodb-backup-recovery

14.21.2 InnoDB Recovery
-----------------------

This section describes 'InnoDB' recovery.  Topics include:

   * *note innodb-recovery-point-in-time::

   * *note innodb-corruption-disk-failure-recovery::

   * *note innodb-crash-recovery::

*Point-in-Time Recovery*

To recover an 'InnoDB' database to the present from the time at which
the physical backup was made, you must run MySQL server with binary
logging enabled, even before taking the backup.  To achieve
point-in-time recovery after restoring a backup, you can apply changes
from the binary log that occurred after the backup was made.  See *note
point-in-time-recovery::.

*Recovery from Data Corruption or Disk Failure*

If your database becomes corrupted or disk failure occurs, you must
perform the recovery using a backup.  In the case of corruption, first
find a backup that is not corrupted.  After restoring the base backup,
do a point-in-time recovery from the binary log files using *note
'mysqlbinlog': mysqlbinlog. and *note 'mysql': mysql. to restore the
changes that occurred after the backup was made.

In some cases of database corruption, it is enough to dump, drop, and
re-create one or a few corrupt tables.  You can use the *note 'CHECK
TABLE': check-table. statement to check whether a table is corrupt,
although *note 'CHECK TABLE': check-table. naturally cannot detect every
possible kind of corruption.  You can use the Tablespace Monitor to
check the integrity of the file space management inside the tablespace
files.

In some cases, apparent database page corruption is actually due to the
operating system corrupting its own file cache, and the data on disk may
be okay.  It is best to try restarting the computer first.  Doing so may
eliminate errors that appeared to be database page corruption.  If MySQL
still has trouble starting because of 'InnoDB' consistency problems, see
*note forcing-innodb-recovery:: for steps to start the instance in
recovery mode, which permits you to dump the data.

*InnoDB Crash Recovery*

To recover from a MySQL server crash, the only requirement is to restart
the MySQL server.  'InnoDB' automatically checks the logs and performs a
roll-forward of the database to the present.  'InnoDB' automatically
rolls back uncommitted transactions that were present at the time of the
crash.  During recovery, *note 'mysqld': mysqld. displays output similar
to this:

     InnoDB: Log scan progressed past the checkpoint lsn 452854464
     InnoDB: Database was not shut down normally!
     InnoDB: Starting crash recovery.
     InnoDB: Reading tablespace information from the .ibd files...
     InnoDB: Restoring possible half-written data pages from the doublewrite
     InnoDB: buffer...
     InnoDB: Doing recovery: scanned up to log sequence number 457028695
     InnoDB: 1 transaction(s) which must be rolled back or cleaned up
     InnoDB: in total 990682 row operations to undo
     InnoDB: Trx id counter is 500
     InnoDB: Starting an apply batch of log records to the database...
     InnoDB: Progress in percents: 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
     22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45
     46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69
     70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93
     94 95 96 97 98 99
     InnoDB: Apply batch completed
     InnoDB: Waiting for the background threads to start
     InnoDB: Starting in background the rollback of uncommitted transactions
     InnoDB: Rolling back trx with id 3B1, 990682 rows to undo
     ...
     InnoDB: 5.5.55 started; log sequence number 457028695
     ...
     ./mysqld: ready for connections.

The 'InnoDB' crash recovery process consists of several steps:

   * Redo log application

     Redo log application is the first step and is performed during
     initialization, before accepting any connections.  If all changes
     are flushed from the buffer pool to the tablespaces ('ibdata*' and
     '*.ibd' files) at the time of the shutdown or crash, redo log
     application is skipped.  'InnoDB' also skips redo log application
     if redo log files are missing at startup.

     Removing redo logs to speed up recovery is not recommended, even if
     some data loss is acceptable.  Removing redo logs should only be
     considered after a clean shutdown, with 'innodb_fast_shutdown' set
     to '0' or '1'.

   * Roll back of incomplete transactions

     Incomplete transactions are any transactions that were active at
     the time of crash or fast shutdown.  The time it takes to roll back
     an incomplete transaction can be three or four times the amount of
     time a transaction is active before it is interrupted, depending on
     server load.

     You cannot cancel transactions that are being rolled back.  In
     extreme cases, when rolling back transactions is expected to take
     an exceptionally long time, it may be faster to start 'InnoDB' with
     an 'innodb_force_recovery' setting of '3' or greater.  See *note
     forcing-innodb-recovery::.

   * Change buffer merge

     Applying changes from the change buffer (part of the system
     tablespace) to leaf pages of secondary indexes, as the index pages
     are read to the buffer pool.

   * Purge

     Deleting delete-marked records that are no longer visible to active
     transactions.

The steps that follow redo log application do not depend on the redo log
(other than for logging the writes) and are performed in parallel with
normal processing.  Of these, only rollback of incomplete transactions
is special to crash recovery.  The insert buffer merge and the purge are
performed during normal processing.

After redo log application, 'InnoDB' attempts to accept connections as
early as possible, to reduce downtime.  As part of crash recovery,
'InnoDB' rolls back transactions that were not committed or in 'XA
PREPARE' state when the server crashed.  The rollback is performed by a
background thread, executed in parallel with transactions from new
connections.  Until the rollback operation is completed, new connections
may encounter locking conflicts with recovered transactions.

In most situations, even if the MySQL server was killed unexpectedly in
the middle of heavy activity, the recovery process happens automatically
and no action is required of the DBA. If a hardware failure or severe
system error corrupted 'InnoDB' data, MySQL might refuse to start.  In
this case, see *note forcing-innodb-recovery::.

For information about the binary log and 'InnoDB' crash recovery, see
*note binary-log::.


File: manual.info.tmp,  Node: innodb-and-mysql-replication,  Next: innodb-troubleshooting,  Prev: innodb-backup-recovery,  Up: innodb-storage-engine

14.22 InnoDB and MySQL Replication
==================================

MySQL replication works for 'InnoDB' tables as it does for 'MyISAM'
tables.  It is also possible to use replication in a way where the
storage engine on the slave is not the same as the original storage
engine on the master.  For example, you can replicate modifications to
an 'InnoDB' table on the master to a 'MyISAM' table on the slave.  For
more information see, *note replication-solutions-diffengines::.

For information about setting up a new slave for a master, see *note
replication-howto-existingdata::.  To make a new slave without taking
down the master or an existing slave, use the *note MySQL Enterprise
Backup: mysql-enterprise-backup. product.

Transactions that fail on the master do not affect replication at all.
MySQL replication is based on the binary log where MySQL writes SQL
statements that modify data.  A transaction that fails (for example,
because of a foreign key violation, or because it is rolled back) is not
written to the binary log, so it is not sent to slaves.  See *note
commit::.

Replication and CASCADE

Cascading actions for 'InnoDB' tables on the master are replicated on
the slave _only_ if the tables sharing the foreign key relation use
'InnoDB' on both the master and slave.  This is true whether you are
using statement-based or row-based replication.  Suppose that you have
started replication, and then create two tables on the master using the
following *note 'CREATE TABLE': create-table. statements:

     CREATE TABLE fc1 (
         i INT PRIMARY KEY,
         j INT
     ) ENGINE = InnoDB;

     CREATE TABLE fc2 (
         m INT PRIMARY KEY,
         n INT,
         FOREIGN KEY ni (n) REFERENCES fc1 (i)
             ON DELETE CASCADE
     ) ENGINE = InnoDB;

Suppose that the slave does not have 'InnoDB' support enabled.  If this
is the case, then the tables on the slave are created, but they use the
'MyISAM' storage engine, and the 'FOREIGN KEY' option is ignored.  Now
we insert some rows into the tables on the master:

     master> INSERT INTO fc1 VALUES (1, 1), (2, 2);
     Query OK, 2 rows affected (0.09 sec)
     Records: 2  Duplicates: 0  Warnings: 0

     master> INSERT INTO fc2 VALUES (1, 1), (2, 2), (3, 1);
     Query OK, 3 rows affected (0.19 sec)
     Records: 3  Duplicates: 0  Warnings: 0

At this point, on both the master and the slave, table 'fc1' contains 2
rows, and table 'fc2' contains 3 rows, as shown here:

     master> SELECT * FROM fc1;
     +---+------+
     | i | j    |
     +---+------+
     | 1 |    1 |
     | 2 |    2 |
     +---+------+
     2 rows in set (0.00 sec)

     master> SELECT * FROM fc2;
     +---+------+
     | m | n    |
     +---+------+
     | 1 |    1 |
     | 2 |    2 |
     | 3 |    1 |
     +---+------+
     3 rows in set (0.00 sec)

     slave> SELECT * FROM fc1;
     +---+------+
     | i | j    |
     +---+------+
     | 1 |    1 |
     | 2 |    2 |
     +---+------+
     2 rows in set (0.00 sec)

     slave> SELECT * FROM fc2;
     +---+------+
     | m | n    |
     +---+------+
     | 1 |    1 |
     | 2 |    2 |
     | 3 |    1 |
     +---+------+
     3 rows in set (0.00 sec)

Now suppose that you perform the following *note 'DELETE': delete.
statement on the master:

     master> DELETE FROM fc1 WHERE i=1;
     Query OK, 1 row affected (0.09 sec)

Due to the cascade, table 'fc2' on the master now contains only 1 row:

     master> SELECT * FROM fc2;
     +---+---+
     | m | n |
     +---+---+
     | 2 | 2 |
     +---+---+
     1 row in set (0.00 sec)

However, the cascade does not propagate on the slave because on the
slave the *note 'DELETE': delete. for 'fc1' deletes no rows from 'fc2'.
The slave's copy of 'fc2' still contains all of the rows that were
originally inserted:

     slave> SELECT * FROM fc2;
     +---+---+
     | m | n |
     +---+---+
     | 1 | 1 |
     | 3 | 1 |
     | 2 | 2 |
     +---+---+
     3 rows in set (0.00 sec)

This difference is due to the fact that the cascading deletes are
handled internally by the 'InnoDB' storage engine, which means that none
of the changes are logged.


File: manual.info.tmp,  Node: innodb-troubleshooting,  Next: innodb-limits,  Prev: innodb-and-mysql-replication,  Up: innodb-storage-engine

14.23 InnoDB Troubleshooting
============================

* Menu:

* error-creating-innodb::        Troubleshooting InnoDB I/O Problems
* forcing-innodb-recovery::      Forcing InnoDB Recovery
* innodb-troubleshooting-datadict::  Troubleshooting InnoDB Data Dictionary Operations
* innodb-error-handling::        InnoDB Error Handling

The following general guidelines apply to troubleshooting 'InnoDB'
problems:

   * When an operation fails or you suspect a bug, look at the MySQL
     server error log (see *note error-log::).  *note
     server-error-reference:: provides troubleshooting information for
     some of the common 'InnoDB'-specific errors that you may encounter.

   * If the failure is related to a deadlock, run with the
     'innodb_print_all_deadlocks' option enabled so that details about
     each deadlock are printed to the MySQL server error log.  For
     information about deadlocks, see *note innodb-deadlocks::.

   * Issues relating to the 'InnoDB' data dictionary include failed
     *note 'CREATE TABLE': create-table. statements (orphan table
     files), inability to open 'InnoDB' files, and 'system cannot find
     the path specified' errors.  For information about these sorts of
     problems and errors, see *note innodb-troubleshooting-datadict::.

   * When troubleshooting, it is usually best to run the MySQL server
     from the command prompt, rather than through *note 'mysqld_safe':
     mysqld-safe. or as a Windows service.  You can then see what *note
     'mysqld': mysqld. prints to the console, and so have a better grasp
     of what is going on.  On Windows, start *note 'mysqld': mysqld.
     with the '--console' option to direct the output to the console
     window.

   * 
     Enable the 'InnoDB' Monitors to obtain information about a problem
     (see *note innodb-monitors::).  If the problem is
     performance-related, or your server appears to be hung, you should
     enable the standard Monitor to print information about the internal
     state of 'InnoDB'.  If the problem is with locks, enable the Lock
     Monitor.  If the problem is in creation of tables or other data
     dictionary operations, enable the Table Monitor to print the
     contents of the 'InnoDB' internal data dictionary.  To see
     tablespace information enable the Tablespace Monitor.

     'InnoDB' temporarily enables standard 'InnoDB' Monitor output under
     the following conditions:

        * A long semaphore wait

        * 'InnoDB' cannot find free blocks in the buffer pool

        * Over 67% of the buffer pool is occupied by lock heaps or the
          adaptive hash index

   * If you suspect that a table is corrupt, run *note 'CHECK TABLE':
     check-table. on that table.


File: manual.info.tmp,  Node: error-creating-innodb,  Next: forcing-innodb-recovery,  Prev: innodb-troubleshooting,  Up: innodb-troubleshooting

14.23.1 Troubleshooting InnoDB I/O Problems
-------------------------------------------

The troubleshooting steps for 'InnoDB' I/O problems depend on when the
problem occurs: during startup of the MySQL server, or during normal
operations when a DML or DDL statement fails due to problems at the file
system level.

*Initialization Problems*

If something goes wrong when 'InnoDB' attempts to initialize its
tablespace or its log files, delete all files created by 'InnoDB': all
'ibdata' files and all 'ib_logfile' files.  If you already created some
'InnoDB' tables, also delete the corresponding '.frm' files for these
tables, and any '.ibd' files if you are using multiple tablespaces, from
the MySQL database directories.  Then try the 'InnoDB' database creation
again.  For easiest troubleshooting, start the MySQL server from a
command prompt so that you see what is happening.

*Runtime Problems*

If 'InnoDB' prints an operating system error during a file operation,
usually the problem has one of the following solutions:

   * Make sure the 'InnoDB' data file directory and the 'InnoDB' log
     directory exist.

   * Make sure *note 'mysqld': mysqld. has access rights to create files
     in those directories.

   * Make sure *note 'mysqld': mysqld. can read the proper 'my.cnf' or
     'my.ini' option file, so that it starts with the options that you
     specified.

   * Make sure the disk is not full and you are not exceeding any disk
     quota.

   * Make sure that the names you specify for subdirectories and data
     files do not clash.

   * Doublecheck the syntax of the 'innodb_data_home_dir' and
     'innodb_data_file_path' values.  In particular, any 'MAX' value in
     the 'innodb_data_file_path' option is a hard limit, and exceeding
     that limit causes a fatal error.


File: manual.info.tmp,  Node: forcing-innodb-recovery,  Next: innodb-troubleshooting-datadict,  Prev: error-creating-innodb,  Up: innodb-troubleshooting

14.23.2 Forcing InnoDB Recovery
-------------------------------

To investigate database page corruption, you might dump your tables from
the database with *note 'SELECT ... INTO OUTFILE': select-into.
Usually, most of the data obtained in this way is intact.  Serious
corruption might cause 'SELECT * FROM TBL_NAME' statements or 'InnoDB'
background operations to crash or assert, or even cause 'InnoDB'
roll-forward recovery to crash.  In such cases, you can use the
'innodb_force_recovery' option to force the 'InnoDB' storage engine to
start up while preventing background operations from running, so that
you can dump your tables.  For example, you can add the following line
to the '[mysqld]' section of your option file before restarting the
server:

     [mysqld]
     innodb_force_recovery = 1

For information about using option files, see *note option-files::.

*Warning*:

Only set 'innodb_force_recovery' to a value greater than 0 in an
emergency situation, so that you can start 'InnoDB' and dump your
tables.  Before doing so, ensure that you have a backup copy of your
database in case you need to recreate it.  Values of 4 or greater can
permanently corrupt data files.  Only use an 'innodb_force_recovery'
setting of 4 or greater on a production server instance after you have
successfully tested the setting on a separate physical copy of your
database.  When forcing 'InnoDB' recovery, you should always start with
'innodb_force_recovery=1' and only increase the value incrementally, as
necessary.

'innodb_force_recovery' is 0 by default (normal startup without forced
recovery).  The permissible nonzero values for 'innodb_force_recovery'
are 1 to 6.  A larger value includes the functionality of lesser values.
For example, a value of 3 includes all of the functionality of values 1
and 2.

If you are able to dump your tables with an 'innodb_force_recovery'
value of 3 or less, then you are relatively safe that only some data on
corrupt individual pages is lost.  A value of 4 or greater is considered
dangerous because data files can be permanently corrupted.  A value of 6
is considered drastic because database pages are left in an obsolete
state, which in turn may introduce more corruption into B-trees and
other database structures.

As a safety measure, 'InnoDB' prevents *note 'INSERT': insert, *note
'UPDATE': update, or *note 'DELETE': delete. operations when
'innodb_force_recovery' is greater than 0.

   * '1' ('SRV_FORCE_IGNORE_CORRUPT')

     Lets the server run even if it detects a corrupt page.  Tries to
     make 'SELECT * FROM TBL_NAME' jump over corrupt index records and
     pages, which helps in dumping tables.

   * '2' ('SRV_FORCE_NO_BACKGROUND')

     Prevents the master thread and any purge threads from running.  If
     a crash would occur during the purge operation, this recovery value
     prevents it.

   * '3' ('SRV_FORCE_NO_TRX_UNDO')

     Does not run transaction rollbacks after crash recovery.

   * '4' ('SRV_FORCE_NO_IBUF_MERGE')

     Prevents insert buffer merge operations.  If they would cause a
     crash, does not do them.  Does not calculate table statistics.
     This value can permanently corrupt data files.  After using this
     value, be prepared to drop and recreate all secondary indexes.

   * '5' ('SRV_FORCE_NO_UNDO_LOG_SCAN')

     Does not look at undo logs when starting the database: 'InnoDB'
     treats even incomplete transactions as committed.  This value can
     permanently corrupt data files.

   * '6' ('SRV_FORCE_NO_LOG_REDO')

     Does not do the redo log roll-forward in connection with recovery.
     This value can permanently corrupt data files.  Leaves database
     pages in an obsolete state, which in turn may introduce more
     corruption into B-trees and other database structures.

You can *note 'SELECT': select. from tables to dump them, or 'DROP' or
'CREATE' tables even if forced recovery is used.  If you know that a
given table is causing a crash on rollback, you can drop it.  You can
also use this to stop a runaway rollback caused by a failing mass import
or *note 'ALTER TABLE': alter-table.: kill the *note 'mysqld': mysqld.
process and set 'innodb_force_recovery' to '3' to bring the database up
without the rollback, then 'DROP' the table that is causing the runaway
rollback.

If corruption within the table data prevents you from dumping the entire
table contents, a query with an 'ORDER BY PRIMARY_KEY DESC' clause might
be able to dump the portion of the table after the corrupted part.

If a high 'innodb_force_recovery' value is required to start 'InnoDB',
there may be corrupted data structures that could cause complex queries
(queries containing 'WHERE', 'ORDER BY', or other clauses) to fail.  In
this case, you may only be able to run basic 'SELECT * FROM t' queries.


File: manual.info.tmp,  Node: innodb-troubleshooting-datadict,  Next: innodb-error-handling,  Prev: forcing-innodb-recovery,  Up: innodb-troubleshooting

14.23.3 Troubleshooting InnoDB Data Dictionary Operations
---------------------------------------------------------

Information about table definitions is stored both in the '.frm' files,
and in the InnoDB data dictionary.  If you move '.frm' files around, or
if the server crashes in the middle of a data dictionary operation,
these sources of information can become inconsistent.

If a data dictionary corruption or consistency issue prevents you from
starting 'InnoDB', see *note forcing-innodb-recovery:: for information
about manual recovery.

*CREATE TABLE Failure Due to Orphan Table*

A symptom of an out-of-sync data dictionary is that a *note 'CREATE
TABLE': create-table. statement fails.  If this occurs, look in the
server's error log.  If the log says that the table already exists
inside the 'InnoDB' internal data dictionary, you have an orphan table
inside the 'InnoDB' tablespace files that has no corresponding '.frm'
file.  The error message looks like this:

     InnoDB: Error: table test/parent already exists in InnoDB internal
     InnoDB: data dictionary. Have you deleted the .frm file
     InnoDB: and not used DROP TABLE? Have you used DROP DATABASE
     InnoDB: for InnoDB tables in MySQL version <= 3.23.43?
     InnoDB: See the Restrictions section of the InnoDB manual.
     InnoDB: You can drop the orphaned table inside InnoDB by
     InnoDB: creating an InnoDB table with the same name in another
     InnoDB: database and moving the .frm file to the current database.
     InnoDB: Then MySQL thinks the table exists, and DROP TABLE will
     InnoDB: succeed.

You can drop the orphan table by following the instructions given in the
error message.  If you are still unable to use *note 'DROP TABLE':
drop-table. successfully, the problem may be due to name completion in
the *note 'mysql': mysql. client.  To work around this problem, start
the *note 'mysql': mysql. client with the '--skip-auto-rehash' option
and try *note 'DROP TABLE': drop-table. again.  (With name completion
on, *note 'mysql': mysql. tries to construct a list of table names,
which fails when a problem such as just described exists.)

*Cannot Open File Error*

Another symptom of an out-of-sync data dictionary is that MySQL prints
an error that it cannot open an 'InnoDB' file:

     ERROR 1016: Can't open file: 'child2.ibd'. (errno: 1)

In the error log you can find a message like this:

     InnoDB: Cannot find table test/child2 from the internal data dictionary
     InnoDB: of InnoDB though the .frm file for the table exists. Maybe you
     InnoDB: have deleted and recreated InnoDB data files but have forgotten
     InnoDB: to delete the corresponding .frm files of InnoDB tables?

This means that there is an orphan '.frm' file without a corresponding
table inside 'InnoDB'.  You can drop the orphan '.frm' file by deleting
it manually.

*Orphan Temporary Tables*

If MySQL exits in the middle of an *note 'ALTER TABLE': alter-table.
operation, you may be left with an orphan temporary table that takes up
space on your system.  This section describes how to identify and remove
orphan temporary tables.

Orphan temporary table names begin with an '#sql-' prefix (e.g.,
'#sql-540_3').  The accompanying '.frm' file has the same base name as
the orphan temporary table.

*Note*:

If there is no '.frm' file, you can recreate it.  The '.frm' file must
have the same table schema as the orphan temporary table (it must have
the same columns and indexes) and must be placed in the database
directory of the orphan temporary table.

To identify orphan temporary tables on your system, you can view *note
Table Monitor: innodb-monitors. output.  Look for table names that begin
with '#sql'.  If the original table resides in a file-per-table
tablespace, the tablespace file (the '#sql-*.ibd' file) for the orphan
temporary table should be visible in the database directory.

To remove an orphan temporary table, drop the table by issuing a *note
'DROP TABLE': drop-table. statement, prefixing the name of the table
with '#mysql50#' and enclosing the table name in backticks.  For
example:

     mysql> DROP TABLE `#mysql50##sql-540_3`;

The '#mysql50#' prefix tells MySQL to ignore 'file name safe encoding'
introduced in MySQL 5.1.  Enclosing the table name in backticks is
required to perform SQL statements on table names with special
characters such as '#'.

*Tablespace Does Not Exist*

With 'innodb_file_per_table' enabled, the following message might occur
if the '.frm' or '.ibd' files (or both) are missing:

     InnoDB: in InnoDB data dictionary has tablespace id N,
     InnoDB: but tablespace with that id or name does not exist. Have
     InnoDB: you deleted or moved .ibd files?
     InnoDB: This may also be a table created with CREATE TEMPORARY TABLE
     InnoDB: whose .ibd and .frm files MySQL automatically removed, but the
     InnoDB: table still exists in the InnoDB internal data dictionary.

If this occurs, try the following procedure to resolve the problem:

  1. Create a matching '.frm' file in some other database directory and
     copy it to the database directory where the orphan table is
     located.

  2. Issue *note 'DROP TABLE': drop-table. for the original table.  That
     should successfully drop the table and 'InnoDB' should print a
     warning to the error log that the '.ibd' file was missing.


File: manual.info.tmp,  Node: innodb-error-handling,  Prev: innodb-troubleshooting-datadict,  Up: innodb-troubleshooting

14.23.4 InnoDB Error Handling
-----------------------------

The following items describe how 'InnoDB' performs error handling.
'InnoDB' sometimes rolls back only the statement that failed, other
times it rolls back the entire transaction.

   * If you run out of file space in a tablespace, a MySQL 'Table is
     full' error occurs and 'InnoDB' rolls back the SQL statement.

   * A transaction deadlock causes 'InnoDB' to roll back the entire
     transaction.  Retry the whole transaction when this happens.

     A lock wait timeout causes 'InnoDB' to roll back only the single
     statement that was waiting for the lock and encountered the
     timeout.  (To have the entire transaction roll back, start the
     server with the '--innodb-rollback-on-timeout' option.)  Retry the
     statement if using the current behavior, or the entire transaction
     if using '--innodb-rollback-on-timeout'.

     Both deadlocks and lock wait timeouts are normal on busy servers
     and it is necessary for applications to be aware that they may
     happen and handle them by retrying.  You can make them less likely
     by doing as little work as possible between the first change to
     data during a transaction and the commit, so the locks are held for
     the shortest possible time and for the smallest possible number of
     rows.  Sometimes splitting work between different transactions may
     be practical and helpful.

     When a transaction rollback occurs due to a deadlock or lock wait
     timeout, it cancels the effect of the statements within the
     transaction.  But if the start-transaction statement was *note
     'START TRANSACTION': commit. or *note 'BEGIN': commit. statement,
     rollback does not cancel that statement.  Further SQL statements
     become part of the transaction until the occurrence of *note
     'COMMIT': commit, *note 'ROLLBACK': commit, or some SQL statement
     that causes an implicit commit.

   * A duplicate-key error rolls back the SQL statement, if you have not
     specified the 'IGNORE' option in your statement.

   * A 'row too long error' rolls back the SQL statement.

   * Other errors are mostly detected by the MySQL layer of code (above
     the 'InnoDB' storage engine level), and they roll back the
     corresponding SQL statement.  Locks are not released in a rollback
     of a single SQL statement.

During implicit rollbacks, as well as during the execution of an
explicit *note 'ROLLBACK': commit. SQL statement, *note 'SHOW
PROCESSLIST': show-processlist. displays 'Rolling back' in the 'State'
column for the relevant connection.


File: manual.info.tmp,  Node: innodb-limits,  Next: innodb-restrictions-limitations,  Prev: innodb-troubleshooting,  Up: innodb-storage-engine

14.24 InnoDB Limits
===================

This section describes limits for 'InnoDB' tables, indexes, tablespaces,
and other aspects of the 'InnoDB' storage engine.

   * A table can contain a maximum of 1000 columns.

   * A table can contain a maximum of 64 secondary indexes.

   * By default, the index key prefix length limit is 767 bytes.  See
     *note create-index::.  For example, you might hit this limit with a
     column prefix index of more than 255 characters on a 'TEXT' or
     'VARCHAR' column, assuming a 'utf8mb3' character set and the
     maximum of 3 bytes for each character.  When the
     'innodb_large_prefix' configuration option is enabled, the index
     key prefix length limit is raised to 3072 bytes for 'InnoDB' tables
     that use the 'DYNAMIC' or 'COMPRESSED' row format.

     If you specify an index key prefix length that exceeds the limit,
     the length is silently reduced to the maximum length.

     When 'innodb_large_prefix' is enabled, attempting to create an
     index key prefix with a length greater than 3072 bytes for a
     'DYNAMIC' or 'COMPRESSED' table causes an
     'ER_INDEX_COLUMN_TOO_LONG' error.

     The limits that apply to index key prefixes also apply to
     full-column index keys.

   * A maximum of 16 columns is permitted for multicolumn indexes.
     Exceeding the limit returns an error.

          ERROR 1070 (42000): Too many key parts specified; max 16 parts allowed

   * The maximum row size, excluding any variable-length columns that
     are stored off-page, is slightly less than half of a page.  That
     is, the maximum row size is about 8000 bytes.  *note 'LONGBLOB':
     blob. and *note 'LONGTEXT': blob. columns must be less than 4GB,
     and the total row size, including *note 'BLOB': blob. and *note
     'TEXT': blob. columns must be less than 4GB.

     If a row is less than half a page long, all of it is stored locally
     within the page.  If it exceeds half a page, variable-length
     columns are chosen for off-page storage until the row fits within
     half a page, as described in *note innodb-file-space::.

     The row size for *note 'BLOB': blob. columns that are chosen for
     off-page storage should not exceed 10% of the combined redo log
     file size.  If the row size exceeds 10% of the combined redo log
     file size, 'InnoDB' could overwrite the most recent checkpoint
     which may result in lost data during crash recovery.  (Bug#69477).

   * Although 'InnoDB' supports row sizes larger than 65,535 bytes
     internally, MySQL itself imposes a row-size limit of 65,535 for the
     combined size of all columns.  See *note column-count-limit::.

   * On some older operating systems, files must be less than 2GB. This
     is not an 'InnoDB' limitation.  If you require a large system
     tablespace, configure it using several smaller data files rather
     than one large data file, or distribute table data across
     file-per-table data files.

   * The combined maximum size for 'InnoDB' log files is 4GB.

   * The minimum tablespace size is slightly larger than 10MB. The
     maximum tablespace size is 64TB, which is also the maximum size for
     a table.

   * Tablespace files cannot exceed 4GB on Windows 32-bit systems (Bug
     #80149).

   * The path of a tablespace file, including the file name, cannot
     exceed the 'MAX_PATH' limit on Windows.  Prior to Windows 10, the
     'MAX_PATH' limit is 260 characters.  As of Windows 10, version
     1607, 'MAX_PATH' limitations are removed from common Win32 file and
     directory functions, but you must enable the new behavior.

   * For limits associated with concurrent read-write transactions, see
     *note innodb-undo-logs::.


File: manual.info.tmp,  Node: innodb-restrictions-limitations,  Prev: innodb-limits,  Up: innodb-storage-engine

14.25 InnoDB Restrictions and Limitations
=========================================

This section describes restrictions and limitations of the 'InnoDB'
storage engine.

   * You cannot create a table with a column name that matches the name
     of an internal 'InnoDB' column (including 'DB_ROW_ID', 'DB_TRX_ID',
     and 'DB_ROLL_PTR'.  This restriction applies to use of the names in
     any lettercase.

          mysql> CREATE TABLE t1 (c1 INT, db_row_id INT) ENGINE=INNODB;
          ERROR 1166 (42000): Incorrect column name 'db_row_id'

   * *note 'SHOW TABLE STATUS': show-table-status. does not provide
     accurate statistics for 'InnoDB' tables except for the physical
     size reserved by the table.  The row count is only a rough estimate
     used in SQL optimization.

   * 'InnoDB' does not keep an internal count of rows in a table because
     concurrent transactions might 'see' different numbers of rows at
     the same time.  Consequently, 'SELECT COUNT(*)' statements only
     count rows visible to the current transaction.

     For information about how 'InnoDB' processes 'SELECT COUNT(*)'
     statements, refer to the 'COUNT()' description in *note
     group-by-functions::.

   * Changing the page size is not a supported operation and there is no
     guarantee that *note 'InnoDB': innodb-storage-engine. functions
     normally with a page size other than 16KB. Problems compiling or
     running 'InnoDB' may occur.  In particular, 'ROW_FORMAT=COMPRESSED'
     in the Barracuda file format assumes that the page size is at most
     16KB and uses 14-bit pointers.

   * A version of *note 'InnoDB': innodb-storage-engine. built for one
     page size cannot use data files or log files from a version built
     for a different page size.  This limitation could affect restore or
     downgrade operations using data from MySQL 5.6, which does support
     page sizes other than 16KB.

   * 'InnoDB' does not support 'FULLTEXT' indexes.

   * 'InnoDB' does not support indexes on spatial data type columns.

   * For limitations associated with fast index creation, see *note
     innodb-create-index-limitations::.


File: manual.info.tmp,  Node: storage-engines,  Next: ha-overview,  Prev: innodb-storage-engine,  Up: Top

15 Alternative Storage Engines
******************************

* Menu:

* storage-engine-setting::       Setting the Storage Engine
* pluggable-storage-overview::   Overview of MySQL Storage Engine Architecture
* myisam-storage-engine::        The MyISAM Storage Engine
* memory-storage-engine::        The MEMORY Storage Engine
* csv-storage-engine::           The CSV Storage Engine
* archive-storage-engine::       The ARCHIVE Storage Engine
* blackhole-storage-engine::     The BLACKHOLE Storage Engine
* merge-storage-engine::         The MERGE Storage Engine
* federated-storage-engine::     The FEDERATED Storage Engine
* example-storage-engine::       The EXAMPLE Storage Engine
* storage-engines-other::        Other Storage Engines

Storage engines are MySQL components that handle the SQL operations for
different table types.  MySQL storage engines include both those that
handle transaction-safe tables and those that handle nontransaction-safe
tables.  *note 'InnoDB': innodb-storage-engine. is the default storage
engine as of MySQL 5.5.5 (The *note 'CREATE TABLE': create-table.
statement in MySQL 5.5 creates 'InnoDB' tables by default.)

MySQL uses a pluggable storage engine architecture that enables storage
engines to be loaded into and unloaded from a running MySQL server.

To determine which storage engines your server supports, use the *note
'SHOW ENGINES': show-engines. statement.  The value in the 'Support'
column indicates whether an engine can be used.  A value of 'YES', 'NO',
or 'DEFAULT' indicates that an engine is available, not available, or
available and currently set as the default storage engine.

     mysql> SHOW ENGINES\G
     *************************** 1. row ***************************
           Engine: PERFORMANCE_SCHEMA
          Support: YES
          Comment: Performance Schema
     Transactions: NO
               XA: NO
       Savepoints: NO
     *************************** 2. row ***************************
           Engine: InnoDB
          Support: DEFAULT
          Comment: Supports transactions, row-level locking, and foreign keys
     Transactions: YES
               XA: YES
       Savepoints: YES
     *************************** 3. row ***************************
           Engine: MRG_MYISAM
          Support: YES
          Comment: Collection of identical MyISAM tables
     Transactions: NO
               XA: NO
       Savepoints: NO
     *************************** 4. row ***************************
           Engine: BLACKHOLE
          Support: YES
          Comment: /dev/null storage engine (anything you write to it disappears)
     Transactions: NO
               XA: NO
       Savepoints: NO
     *************************** 5. row ***************************
           Engine: MyISAM
          Support: YES
          Comment: MyISAM storage engine
     Transactions: NO
               XA: NO
       Savepoints: NO
     ...

This chapter covers use cases for special-purpose MySQL storage engines.
It does not cover the default *note 'InnoDB': innodb-storage-engine.
storage engine or the *note 'NDB': mysql-cluster. storage engine which
are covered in *note innodb-storage-engine:: and *note mysql-cluster::.
For advanced users, this chapter also contains a description of the
pluggable storage engine architecture (see *note
pluggable-storage-overview::).

For information about features offered in commercial MySQL Server
binaries, see 'MySQL Editions' (https://www.mysql.com/products/), on the
MySQL website.  The storage engines available might depend on which
edition of MySQL you are using.

For answers to some commonly asked questions about MySQL storage
engines, see *note faqs-storage-engines::.

*MySQL 5.5 Supported Storage Engines*

   * *note 'InnoDB': innodb-storage-engine.: The default storage engine
     as of MySQL 5.5.5.  'InnoDB' is a transaction-safe (ACID compliant)
     storage engine for MySQL that has commit, rollback, and
     crash-recovery capabilities to protect user data.  'InnoDB'
     row-level locking (without escalation to coarser granularity locks)
     and Oracle-style consistent nonlocking reads increase multi-user
     concurrency and performance.  'InnoDB' stores user data in
     clustered indexes to reduce I/O for common queries based on primary
     keys.  To maintain data integrity, 'InnoDB' also supports 'FOREIGN
     KEY' referential-integrity constraints.  For more information about
     'InnoDB', see *note innodb-storage-engine::.

   * *note 'MyISAM': myisam-storage-engine.: The MySQL storage engine
     that is used the most in Web, data warehousing, and other
     application environments.  'MyISAM' is supported in all MySQL
     configurations, and is the default storage engine prior to MySQL
     5.5.5.

   * *note 'Memory': memory-storage-engine.: Stores all data in RAM for
     extremely fast access in environments that require quick lookups of
     reference and other like data.  This engine was formerly known as
     the 'HEAP' engine.

   * *note 'Merge': merge-storage-engine.: Enables a MySQL DBA or
     developer to logically group a series of identical 'MyISAM' tables
     and reference them as one object.  Good for VLDB environments such
     as data warehousing.

   * *note 'Archive': archive-storage-engine.: Provides the perfect
     solution for storing and retrieving large amounts of
     seldom-referenced historical, archived, or security audit
     information.

   * *note 'Federated': federated-storage-engine.: Offers the ability to
     link separate MySQL servers to create one logical database from
     many physical servers.  Very good for distributed or data mart
     environments.

   * *note 'NDB': mysql-cluster. (also known as *note 'NDBCLUSTER':
     mysql-cluster.)--This clustered database engine is particularly
     suited for applications that require the highest possible degree of
     uptime and availability.

     *Note*:

     The *note 'NDB': mysql-cluster. storage engine is not supported in
     standard MySQL 5.5 releases.  Currently supported NDB Cluster
     releases include MySQL NDB Cluster 7.0 and MySQL NDB Cluster 7.1,
     which are based on MySQL 5.1, and MySQL NDB Cluster 7.2, which is
     based on MySQL 5.5.  While based on MySQL Server, these releases
     also contain support for *note 'NDB': mysql-cluster.

   * *note 'CSV': csv-storage-engine.: The CSV storage engine stores
     data in text files using comma-separated values format.  You can
     use the CSV engine to easily exchange data between other software
     and applications that can import and export in CSV format.

   * *note 'Blackhole': blackhole-storage-engine.: The Blackhole storage
     engine accepts but does not store data and retrievals always return
     an empty set.  The functionality can be used in distributed
     database design where data is automatically replicated, but not
     stored locally.

   * *note 'Example': example-storage-engine.: The Example storage
     engine is 'stub' engine that does nothing.  You can create tables
     with this engine, but no data can be stored in them or retrieved
     from them.  The purpose of this engine is to serve as an example in
     the MySQL source code that illustrates how to begin writing new
     storage engines.  As such, it is primarily of interest to
     developers.

It is important to remember that you are not restricted to using the
same storage engine for an entire server or schema: you can use a
different storage engine for each table in your schema.

*Choosing a Storage Engine*

The various storage engines provided with MySQL are designed with
different use cases in mind.  The following table provides an overview
of some storage engines provided with MySQL, with clarifying notes
following the table.

*Storage Engines Feature Summary*

Feature MyISAM       Memory       InnoDB       Archive      NDB
                                                            
*B-tree Yes          Yes          Yes          No           No
indexes*                                                    

*Backup/point-in-timeYesYes       Yes          Yes          Yes
recovery*                                                   
(note
1)

*ClusterNo           No           No           No           Yes
database                                                    
support*

*ClusteredNo         No           Yes          No           No
indexes*                                                    

*CompressedYes (note No           Yes          Yes          No
data*   2)                                                  
        
*Data   No           N/A          Yes          No           Yes
caches*                                                     

*EncryptedYes (note  Yes (note    Yes (note    Yes (note    Yes (note
data*   3)           3)           4)           3)           3)
                                                            
*ForeignNo           No           Yes          No           Yes (note
key                                                         5)
support*                                                    

*Full-textYes        No           Yes (note    No           No
search                            6)                        
indexes*                          

*GeospatialYes       No           Yes          Yes          Yes
data                                                        
type
support*

*GeospatialYes       No           Yes (note    No           No
indexing                          7)                        
support*                          

*Hash   No           Yes          No (note     No           Yes
indexes*                          8)                        
                                  
*Index  Yes          N/A          Yes          No           Yes
caches*                                                     

*LockingTable        Table        Row          Row          Row
granularity*                                                

*MVCC*  No           No           Yes          No           No
                                                            
*ReplicationYes      Limited      Yes          Yes          Yes
support*             (note 9)                               
(note                
1)

*Storage256TB        RAM          64TB         None         384EB
limits*                                                     

*T-tree No           No           No           No           Yes
indexes*                                                    

*Transactions*No     No           Yes          No           Yes
                                                            
*Update Yes          Yes          Yes          Yes          Yes
statistics                                     
for
data
dictionary*

*Notes:* 1.  Implemented in the server, rather than in the storage
engine.  2.  Compressed MyISAM tables are supported only when using the
compressed row format.  Tables using the compressed row format with
MyISAM are read only.  3.  Implemented in the server via encryption
functions.  4.  Implemented in the server via encryption functions; In
MySQL 5.7 and later, data-at-rest tablespace encryption is supported.
5.  Support for foreign keys is available in MySQL Cluster NDB 7.3 and
later.  6.  InnoDB support for FULLTEXT indexes is available in MySQL
5.6 and later.  7.  InnoDB support for geospatial indexing is available
in MySQL 5.7 and later.  8.  InnoDB utilizes hash indexes internally for
its Adaptive Hash Index feature.  9.  See the discussion later in this
section.


File: manual.info.tmp,  Node: storage-engine-setting,  Next: pluggable-storage-overview,  Prev: storage-engines,  Up: storage-engines

15.1 Setting the Storage Engine
===============================

When you create a new table, you can specify which storage engine to use
by adding an 'ENGINE' table option to the *note 'CREATE TABLE':
create-table. statement:

     -- ENGINE=INNODB not needed as of 5.5.5 unless you have set a
     -- different default storage engine.
     CREATE TABLE t1 (i INT) ENGINE = INNODB;
     -- Simple table definitions can be switched from one to another.
     CREATE TABLE t2 (i INT) ENGINE = CSV;
     CREATE TABLE t3 (i INT) ENGINE = MEMORY;

If you omit the 'ENGINE' option, the default storage engine is used.
The default engine is *note 'InnoDB': innodb-storage-engine. as of MySQL
5.5.5 (*note 'MyISAM': myisam-storage-engine. before 5.5.5).  You can
specify the default engine by using the '--default-storage-engine'
server startup option, or by setting the 'default-storage-engine' option
in the 'my.cnf' configuration file.

At runtime, you can set the default storage engine to be used during the
current session by setting the 'default_storage_engine' system variable:

     SET default_storage_engine=MYISAM;

When MySQL is installed on Windows using the MySQL Configuration Wizard,
the 'InnoDB' or 'MyISAM' storage engine can be selected as the default.
See *note mysql-config-wizard-database-usage::.

To convert a table from one storage engine to another, use an *note
'ALTER TABLE': alter-table. statement that indicates the new engine:

     ALTER TABLE t ENGINE = MYISAM;

See *note create-table::, and *note alter-table::.

If you try to use a storage engine that is not compiled in or that is
compiled in but deactivated, MySQL instead creates a table using the
default storage engine.  This behavior is convenient when you want to
copy tables between MySQL servers that support different storage
engines.  (For example, in a replication setup, perhaps your master
server supports transactional storage engines for increased safety, but
the slave servers use only nontransactional storage engines for greater
speed.)

This automatic substitution of the default storage engine for
unavailable engines can be confusing for new MySQL users.  A warning is
generated whenever a storage engine is automatically changed.  To
prevent this from happening if the desired engine is unavailable, enable
the 'NO_ENGINE_SUBSTITUTION' SQL mode.  In this case, an error occurs
instead of a warning and the table is not created or altered if the
desired engine is unavailable.  See *note sql-mode::.

For new tables, MySQL always creates an '.frm' file to hold the table
and column definitions.  The table's index and data may be stored in one
or more other files, depending on the storage engine.  The server
creates the '.frm' file above the storage engine level.  Individual
storage engines create any additional files required for the tables that
they manage.  If a table name contains special characters, the names for
the table files contain encoded versions of those characters as
described in *note identifier-mapping::.

A database may contain tables of different types.  That is, tables need
not all be created with the same storage engine.


File: manual.info.tmp,  Node: pluggable-storage-overview,  Next: myisam-storage-engine,  Prev: storage-engine-setting,  Up: storage-engines

15.2 Overview of MySQL Storage Engine Architecture
==================================================

* Menu:

* pluggable-storage::            Pluggable Storage Engine Architecture
* pluggable-storage-common-layer::  The Common Database Server Layer

The MySQL pluggable storage engine architecture enables a database
professional to select a specialized storage engine for a particular
application need while being completely shielded from the need to manage
any specific application coding requirements.  The MySQL server
architecture isolates the application programmer and DBA from all of the
low-level implementation details at the storage level, providing a
consistent and easy application model and API. Thus, although there are
different capabilities across different storage engines, the application
is shielded from these differences.

The pluggable storage engine architecture provides a standard set of
management and support services that are common among all underlying
storage engines.  The storage engines themselves are the components of
the database server that actually perform actions on the underlying data
that is maintained at the physical server level.

This efficient and modular architecture provides huge benefits for those
wishing to specifically target a particular application need--such as
data warehousing, transaction processing, or high availability
situations--while enjoying the advantage of utilizing a set of
interfaces and services that are independent of any one storage engine.

The application programmer and DBA interact with the MySQL database
through Connector APIs and service layers that are above the storage
engines.  If application changes bring about requirements that demand
the underlying storage engine change, or that one or more storage
engines be added to support new needs, no significant coding or process
changes are required to make things work.  The MySQL server architecture
shields the application from the underlying complexity of the storage
engine by presenting a consistent and easy-to-use API that applies
across storage engines.


File: manual.info.tmp,  Node: pluggable-storage,  Next: pluggable-storage-common-layer,  Prev: pluggable-storage-overview,  Up: pluggable-storage-overview

15.2.1 Pluggable Storage Engine Architecture
--------------------------------------------

MySQL Server uses a pluggable storage engine architecture that enables
storage engines to be loaded into and unloaded from a running MySQL
server.

*Plugging in a Storage Engine*

Before a storage engine can be used, the storage engine plugin shared
library must be loaded into MySQL using the *note 'INSTALL PLUGIN':
install-plugin. statement.  For example, if the 'EXAMPLE' engine plugin
is named 'example' and the shared library is named 'ha_example.so', you
load it with the following statement:

     INSTALL PLUGIN example SONAME 'ha_example.so';

To install a pluggable storage engine, the plugin file must be located
in the MySQL plugin directory, and the user issuing the *note 'INSTALL
PLUGIN': install-plugin. statement must have 'INSERT' privilege for the
'mysql.plugin' table.

The shared library must be located in the MySQL server plugin directory,
the location of which is given by the 'plugin_dir' system variable.

*Unplugging a Storage Engine*

To unplug a storage engine, use the *note 'UNINSTALL PLUGIN':
uninstall-plugin. statement:

     UNINSTALL PLUGIN example;

If you unplug a storage engine that is needed by existing tables, those
tables become inaccessible, but will still be present on disk (where
applicable).  Ensure that there are no tables using a storage engine
before you unplug the storage engine.


File: manual.info.tmp,  Node: pluggable-storage-common-layer,  Prev: pluggable-storage,  Up: pluggable-storage-overview

15.2.2 The Common Database Server Layer
---------------------------------------

A MySQL pluggable storage engine is the component in the MySQL database
server that is responsible for performing the actual data I/O operations
for a database as well as enabling and enforcing certain feature sets
that target a specific application need.  A major benefit of using
specific storage engines is that you are only delivered the features
needed for a particular application, and therefore you have less system
overhead in the database, with the end result being more efficient and
higher database performance.  This is one of the reasons that MySQL has
always been known to have such high performance, matching or beating
proprietary monolithic databases in industry standard benchmarks.

From a technical perspective, what are some of the unique supporting
infrastructure components that are in a storage engine?  Some of the key
feature differentiations include:

   * _Concurrency_: Some applications have more granular lock
     requirements (such as row-level locks) than others.  Choosing the
     right locking strategy can reduce overhead and therefore improve
     overall performance.  This area also includes support for
     capabilities such as multi-version concurrency control or
     'snapshot' read.

   * _Transaction Support_: Not every application needs transactions,
     but for those that do, there are very well defined requirements
     such as ACID compliance and more.

   * _Referential Integrity_: The need to have the server enforce
     relational database referential integrity through DDL defined
     foreign keys.

   * _Physical Storage_: This involves everything from the overall page
     size for tables and indexes as well as the format used for storing
     data to physical disk.

   * _Index Support_: Different application scenarios tend to benefit
     from different index strategies.  Each storage engine generally has
     its own indexing methods, although some (such as B-tree indexes)
     are common to nearly all engines.

   * _Memory Caches_: Different applications respond better to some
     memory caching strategies than others, so although some memory
     caches are common to all storage engines (such as those used for
     user connections or MySQL's high-speed Query Cache), others are
     uniquely defined only when a particular storage engine is put in
     play.

   * _Performance Aids_: This includes multiple I/O threads for parallel
     operations, thread concurrency, database checkpointing, bulk insert
     handling, and more.

   * _Miscellaneous Target Features_: This may include support for
     geospatial operations, security restrictions for certain data
     manipulation operations, and other similar features.

Each set of the pluggable storage engine infrastructure components are
designed to offer a selective set of benefits for a particular
application.  Conversely, avoiding a set of component features helps
reduce unnecessary overhead.  It stands to reason that understanding a
particular application's set of requirements and selecting the proper
MySQL storage engine can have a dramatic impact on overall system
efficiency and performance.


File: manual.info.tmp,  Node: myisam-storage-engine,  Next: memory-storage-engine,  Prev: pluggable-storage-overview,  Up: storage-engines

15.3 The MyISAM Storage Engine
==============================

* Menu:

* myisam-start::                 MyISAM Startup Options
* key-space::                    Space Needed for Keys
* myisam-table-formats::         MyISAM Table Storage Formats
* myisam-table-problems::        MyISAM Table Problems

Before MySQL 5.5.5, 'MyISAM' is the default storage engine.  (The
default was changed to 'InnoDB' in MySQL 5.5.5.)  'MyISAM' is based on
the older (and no longer available) 'ISAM' storage engine but has many
useful extensions.

*MyISAM Storage Engine Features*

Feature                                     Support
                                            
*B-tree indexes*                            Yes
                                            
*Backup/point-in-time recovery*             Yes
(Implemented in the server, rather than     
in the storage engine.)

*Cluster database support*                  No
                                            
*Clustered indexes*                         No
                                            
*Compressed data*                           Yes (Compressed MyISAM
                                            tables are supported only
                                            when using the compressed
                                            row format.  Tables using
                                            the compressed row format
                                            with MyISAM are read only.)
                                            
*Data caches*                               No
                                            
*Encrypted data*                            Yes (Implemented in the
                                            server via encryption
                                            functions.)
                                            
*Foreign key support*                       No
                                            
*Full-text search indexes*                  Yes
                                            
*Geospatial data type support*              Yes
                                            
*Geospatial indexing support*               Yes
                                            
*Hash indexes*                              No
                                            
*Index caches*                              Yes
                                            
*Locking granularity*                       Table
                                            
*MVCC*                                      No
                                            
*Replication support* (Implemented in the   Yes
server, rather than in the storage          
engine.)

*Storage limits*                            256TB
                                            
*T-tree indexes*                            No
                                            
*Transactions*                              No
                                            
*Update statistics for data dictionary*     Yes

Each 'MyISAM' table is stored on disk in three files.  The files have
names that begin with the table name and have an extension to indicate
the file type.  An '.frm' file stores the table format.  The data file
has an '.MYD' ('MYData') extension.  The index file has an '.MYI'
('MYIndex') extension.

To specify explicitly that you want a 'MyISAM' table, indicate that with
an 'ENGINE' table option:

     CREATE TABLE t (i INT) ENGINE = MYISAM;

As of MySQL 5.5.5, it is normally necessary to use 'ENGINE' to specify
the 'MyISAM' storage engine because 'InnoDB' is the default engine.
Before 5.5.5, this is unnecessary because 'MyISAM' is the default engine
unless the default has been changed.  To ensure that 'MyISAM' is used in
situations where the default might have been changed, include the
'ENGINE' option explicitly.

You can check or repair 'MyISAM' tables with the *note 'mysqlcheck':
mysqlcheck. client or *note 'myisamchk': myisamchk. utility.  You can
also compress 'MyISAM' tables with *note 'myisampack': myisampack. to
take up much less space.  See *note mysqlcheck::, *note myisamchk::, and
*note myisampack::.

'MyISAM' tables have the following characteristics:

   * All data values are stored with the low byte first.  This makes the
     data machine and operating system independent.  The only
     requirements for binary portability are that the machine uses
     two's-complement signed integers and IEEE floating-point format.
     These requirements are widely used among mainstream machines.
     Binary compatibility might not be applicable to embedded systems,
     which sometimes have peculiar processors.

     There is no significant speed penalty for storing data low byte
     first; the bytes in a table row normally are unaligned and it takes
     little more processing to read an unaligned byte in order than in
     reverse order.  Also, the code in the server that fetches column
     values is not time critical compared to other code.

   * All numeric key values are stored with the high byte first to
     permit better index compression.

   * Large files (up to 63-bit file length) are supported on file
     systems and operating systems that support large files.

   * There is a limit of (2^32)^2 (1.844E+19) rows in a 'MyISAM' table.

   * The maximum number of indexes per 'MyISAM' table is 64.

     The maximum number of columns per index is 16.

   * The maximum key length is 1000 bytes.  This can also be changed by
     changing the source and recompiling.  For the case of a key longer
     than 250 bytes, a larger key block size than the default of 1024
     bytes is used.

   * When rows are inserted in sorted order (as when you are using an
     'AUTO_INCREMENT' column), the index tree is split so that the high
     node only contains one key.  This improves space utilization in the
     index tree.

   * Internal handling of one 'AUTO_INCREMENT' column per table is
     supported.  'MyISAM' automatically updates this column for *note
     'INSERT': insert. and *note 'UPDATE': update. operations.  This
     makes 'AUTO_INCREMENT' columns faster (at least 10%).  Values at
     the top of the sequence are not reused after being deleted.  (When
     an 'AUTO_INCREMENT' column is defined as the last column of a
     multiple-column index, reuse of values deleted from the top of a
     sequence does occur.)  The 'AUTO_INCREMENT' value can be reset with
     *note 'ALTER TABLE': alter-table. or *note 'myisamchk': myisamchk.

   * Dynamic-sized rows are much less fragmented when mixing deletes
     with updates and inserts.  This is done by automatically combining
     adjacent deleted blocks and by extending blocks if the next block
     is deleted.

   * 'MyISAM' supports concurrent inserts: If a table has no free blocks
     in the middle of the data file, you can *note 'INSERT': insert. new
     rows into it at the same time that other threads are reading from
     the table.  A free block can occur as a result of deleting rows or
     an update of a dynamic length row with more data than its current
     contents.  When all free blocks are used up (filled in), future
     inserts become concurrent again.  See *note concurrent-inserts::.

   * You can put the data file and index file in different directories
     on different physical devices to get more speed with the 'DATA
     DIRECTORY' and 'INDEX DIRECTORY' table options to *note 'CREATE
     TABLE': create-table.  See *note create-table::.

   * *note 'BLOB': blob. and *note 'TEXT': blob. columns can be indexed.

   * 'NULL' values are permitted in indexed columns.  This takes 0 to 1
     bytes per key.

   * Each character column can have a different character set.  See
     *note charset::.

   * There is a flag in the 'MyISAM' index file that indicates whether
     the table was closed correctly.  If *note 'mysqld': mysqld. is
     started with the 'myisam_recover_options' system variable set,
     'MyISAM' tables are automatically checked when opened, and are
     repaired if the table wasn't closed properly.

   * *note 'myisamchk': myisamchk. marks tables as checked if you run it
     with the '--update-state' option.  *note 'myisamchk --fast':
     myisamchk. checks only those tables that don't have this mark.

   * *note 'myisamchk --analyze': myisamchk. stores statistics for
     portions of keys, as well as for entire keys.

   * *note 'myisampack': myisampack. can pack *note 'BLOB': blob. and
     *note 'VARCHAR': char. columns.

'MyISAM' also supports the following features:

   * Support for a true *note 'VARCHAR': char. type; a *note 'VARCHAR':
     char. column starts with a length stored in one or two bytes.

   * Tables with *note 'VARCHAR': char. columns may have fixed or
     dynamic row length.

   * The sum of the lengths of the *note 'VARCHAR': char. and *note
     'CHAR': char. columns in a table may be up to 64KB.

   * Arbitrary length 'UNIQUE' constraints.

*Additional Resources*

   * A forum dedicated to the 'MyISAM' storage engine is available at
     <https://forums.mysql.com/list.php?21>.


File: manual.info.tmp,  Node: myisam-start,  Next: key-space,  Prev: myisam-storage-engine,  Up: myisam-storage-engine

15.3.1 MyISAM Startup Options
-----------------------------

The following options to *note 'mysqld': mysqld. can be used to change
the behavior of 'MyISAM' tables.  For additional information, see *note
server-options::.

*MyISAM Option and Variable Reference*

Name           Cmd-Line    Option      System      Status      Var Scope   Dynamic
                           File        Var         Var                     
                                                   
bulk_insert_buffer_sizeYes Yes         Yes                     Both        Yes
                                                                           
concurrent_insertYes       Yes         Yes                     Global      Yes
                                                                           
delay_key_writeYes         Yes         Yes                     Global      Yes
                                                                           
delay-key-write-for-all-tablesYes                                          
               
have_rtree_keys                        Yes                     Global      No
                                                                           
key_buffer_sizeYes         Yes         Yes                     Global      Yes
                                                                           
log-isam       Yes         Yes                                             
                           
myisam-block-sizeYes       Yes                                             
                           
myisam_data_pointer_sizeYesYes         Yes                     Global      Yes
                                                                           
myisam_max_sort_file_sizeYesYes        Yes                     Global      Yes
                                                                           
myisam_mmap_sizeYes        Yes         Yes                     Global      No
                                                                           
myisam-recover Yes         Yes                                             
                           
-                                                                          
_Variable_:
myisam_recover_options

myisam_recover_optionsYes  Yes         Yes                     Global      No
                                                                           
myisam_repair_threadsYes   Yes         Yes                     Both        Yes
                                                                           
myisam_sort_buffer_sizeYes Yes         Yes                     Both        Yes
                                                                           
myisam_stats_methodYes     Yes         Yes                     Both        Yes
                                                                           
myisam_use_mmapYes         Yes         Yes                     Global      Yes
                                                                           
tmp_table_size Yes         Yes         Yes                     Both        Yes
                                                               

The following system variables affect the behavior of 'MyISAM' tables.
For additional information, see *note server-system-variables::.

   * 'bulk_insert_buffer_size'

     The size of the tree cache used in bulk insert optimization.

     *Note*:

     This is a limit _per thread_!

   * 'delay_key_write=ALL'

     Don't flush key buffers between writes for any 'MyISAM' table.

     *Note*:

     If you do this, you should not access 'MyISAM' tables from another
     program (such as from another MySQL server or with *note
     'myisamchk': myisamchk.) when the tables are in use.  Doing so
     risks index corruption.  Using '--external-locking' does not
     eliminate this risk.

   * 'myisam_max_sort_file_size'

     The maximum size of the temporary file that MySQL is permitted to
     use while re-creating a 'MyISAM' index (during *note 'REPAIR
     TABLE': repair-table, *note 'ALTER TABLE': alter-table, or *note
     'LOAD DATA': load-data.).  If the file size would be larger than
     this value, the index is created using the key cache instead, which
     is slower.  The value is given in bytes.

   * 'myisam_recover_options=MODE'

     Set the mode for automatic recovery of crashed 'MyISAM' tables.

   * 'myisam_sort_buffer_size'

     Set the size of the buffer used when recovering tables.

Automatic recovery is activated if you start *note 'mysqld': mysqld.
with the 'myisam_recover_options' system variable set.  In this case,
when the server opens a 'MyISAM' table, it checks whether the table is
marked as crashed or whether the open count variable for the table is
not 0 and you are running the server with external locking disabled.  If
either of these conditions is true, the following happens:

   * The server checks the table for errors.

   * If the server finds an error, it tries to do a fast table repair
     (with sorting and without re-creating the data file).

   * If the repair fails because of an error in the data file (for
     example, a duplicate-key error), the server tries again, this time
     re-creating the data file.

   * If the repair still fails, the server tries once more with the old
     repair option method (write row by row without sorting).  This
     method should be able to repair any type of error and has low disk
     space requirements.

If the recovery wouldn't be able to recover all rows from previously
completed statements and you didn't specify 'FORCE' in the value of the
'myisam_recover_options' system variable, automatic repair aborts with
an error message in the error log:

     Error: Couldn't repair table: test.g00pages

If you specify 'FORCE', a warning like this is written instead:

     Warning: Found 344 of 354 rows when repairing ./test/g00pages

If the automatic recovery value includes 'BACKUP', the recovery process
creates files with names of the form 'TBL_NAME-DATETIME.BAK'.  You
should have a 'cron' script that automatically moves these files from
the database directories to backup media.


File: manual.info.tmp,  Node: key-space,  Next: myisam-table-formats,  Prev: myisam-start,  Up: myisam-storage-engine

15.3.2 Space Needed for Keys
----------------------------

'MyISAM' tables use B-tree indexes.  You can roughly calculate the size
for the index file as '(key_length+4)/0.67', summed over all keys.  This
is for the worst case when all keys are inserted in sorted order and the
table doesn't have any compressed keys.

String indexes are space compressed.  If the first index part is a
string, it is also prefix compressed.  Space compression makes the index
file smaller than the worst-case figure if a string column has a lot of
trailing space or is a *note 'VARCHAR': char. column that is not always
used to the full length.  Prefix compression is used on keys that start
with a string.  Prefix compression helps if there are many strings with
an identical prefix.

In 'MyISAM' tables, you can also prefix compress numbers by specifying
the 'PACK_KEYS=1' table option when you create the table.  Numbers are
stored with the high byte first, so this helps when you have many
integer keys that have an identical prefix.


File: manual.info.tmp,  Node: myisam-table-formats,  Next: myisam-table-problems,  Prev: key-space,  Up: myisam-storage-engine

15.3.3 MyISAM Table Storage Formats
-----------------------------------

* Menu:

* static-format::                Static (Fixed-Length) Table Characteristics
* dynamic-format::               Dynamic Table Characteristics
* compressed-format::            Compressed Table Characteristics

'MyISAM' supports three different storage formats.  Two of them, fixed
and dynamic format, are chosen automatically depending on the type of
columns you are using.  The third, compressed format, can be created
only with the *note 'myisampack': myisampack. utility (see *note
myisampack::).

When you use *note 'CREATE TABLE': create-table. or *note 'ALTER TABLE':
alter-table. for a table that has no *note 'BLOB': blob. or *note
'TEXT': blob. columns, you can force the table format to 'FIXED' or
'DYNAMIC' with the 'ROW_FORMAT' table option.

See *note create-table::, for information about 'ROW_FORMAT'.

You can decompress (unpack) compressed 'MyISAM' tables using *note
'myisamchk': myisamchk. '--unpack'; see *note myisamchk::, for more
information.


File: manual.info.tmp,  Node: static-format,  Next: dynamic-format,  Prev: myisam-table-formats,  Up: myisam-table-formats

15.3.3.1 Static (Fixed-Length) Table Characteristics
....................................................

Static format is the default for 'MyISAM' tables.  It is used when the
table contains no variable-length columns (*note 'VARCHAR': char, *note
'VARBINARY': binary-varbinary, *note 'BLOB': blob, or *note 'TEXT':
blob.).  Each row is stored using a fixed number of bytes.

Of the three 'MyISAM' storage formats, static format is the simplest and
most secure (least subject to corruption).  It is also the fastest of
the on-disk formats due to the ease with which rows in the data file can
be found on disk: To look up a row based on a row number in the index,
multiply the row number by the row length to calculate the row position.
Also, when scanning a table, it is very easy to read a constant number
of rows with each disk read operation.

The security is evidenced if your computer crashes while the MySQL
server is writing to a fixed-format 'MyISAM' file.  In this case, *note
'myisamchk': myisamchk. can easily determine where each row starts and
ends, so it can usually reclaim all rows except the partially written
one.  'MyISAM' table indexes can always be reconstructed based on the
data rows.

*Note*:

Fixed-length row format is only available for tables without *note
'BLOB': blob. or *note 'TEXT': blob. columns.  Creating a table with
these columns with an explicit 'ROW_FORMAT' clause will not raise an
error or warning; the format specification will be ignored.

Static-format tables have these characteristics:

   * *note 'CHAR': char. and *note 'VARCHAR': char. columns are
     space-padded to the specified column width, although the column
     type is not altered.  *note 'BINARY': binary-varbinary. and *note
     'VARBINARY': binary-varbinary. columns are padded with '0x00' bytes
     to the column width.

   * 'NULL' columns require additional space in the row to record
     whether their values are 'NULL'.  Each 'NULL' column takes one bit
     extra, rounded up to the nearest byte.

   * Very quick.

   * Easy to cache.

   * Easy to reconstruct after a crash, because rows are located in
     fixed positions.

   * Reorganization is unnecessary unless you delete a huge number of
     rows and want to return free disk space to the operating system.
     To do this, use *note 'OPTIMIZE TABLE': optimize-table. or *note
     'myisamchk -r': myisamchk.

   * Usually require more disk space than dynamic-format tables.

   * The expected row length in bytes for static-sized rows is
     calculated using the following expression:

          row length = 1
                       + (SUM OF COLUMN LENGTHS)
                       + (NUMBER OF NULL COLUMNS + DELETE_FLAG + 7)/8
                       + (NUMBER OF VARIABLE-LENGTH COLUMNS)

     DELETE_FLAG is 1 for tables with static row format.  Static tables
     use a bit in the row record for a flag that indicates whether the
     row has been deleted.  DELETE_FLAG is 0 for dynamic tables because
     the flag is stored in the dynamic row header.


File: manual.info.tmp,  Node: dynamic-format,  Next: compressed-format,  Prev: static-format,  Up: myisam-table-formats

15.3.3.2 Dynamic Table Characteristics
......................................

Dynamic storage format is used if a 'MyISAM' table contains any
variable-length columns (*note 'VARCHAR': char, *note 'VARBINARY':
binary-varbinary, *note 'BLOB': blob, or *note 'TEXT': blob.), or if the
table was created with the 'ROW_FORMAT=DYNAMIC' table option.

Dynamic format is a little more complex than static format because each
row has a header that indicates how long it is.  A row can become
fragmented (stored in noncontiguous pieces) when it is made longer as a
result of an update.

You can use *note 'OPTIMIZE TABLE': optimize-table. or *note 'myisamchk
-r': myisamchk. to defragment a table.  If you have fixed-length columns
that you access or change frequently in a table that also contains some
variable-length columns, it might be a good idea to move the
variable-length columns to other tables just to avoid fragmentation.

Dynamic-format tables have these characteristics:

   * All string columns are dynamic except those with a length less than
     four.

   * Each row is preceded by a bitmap that indicates which columns
     contain the empty string (for string columns) or zero (for numeric
     columns).  This does not include columns that contain 'NULL'
     values.  If a string column has a length of zero after trailing
     space removal, or a numeric column has a value of zero, it is
     marked in the bitmap and not saved to disk.  Nonempty strings are
     saved as a length byte plus the string contents.

   * 'NULL' columns require additional space in the row to record
     whether their values are 'NULL'.  Each 'NULL' column takes one bit
     extra, rounded up to the nearest byte.

   * Much less disk space usually is required than for fixed-length
     tables.

   * Each row uses only as much space as is required.  However, if a row
     becomes larger, it is split into as many pieces as are required,
     resulting in row fragmentation.  For example, if you update a row
     with information that extends the row length, the row becomes
     fragmented.  In this case, you may have to run *note 'OPTIMIZE
     TABLE': optimize-table. or *note 'myisamchk -r': myisamchk. from
     time to time to improve performance.  Use *note 'myisamchk -ei':
     myisamchk. to obtain table statistics.

   * More difficult than static-format tables to reconstruct after a
     crash, because rows may be fragmented into many pieces and links
     (fragments) may be missing.

   * The expected row length for dynamic-sized rows is calculated using
     the following expression:

          3
          + (NUMBER OF COLUMNS + 7) / 8
          + (NUMBER OF CHAR COLUMNS)
          + (PACKED SIZE OF NUMERIC COLUMNS)
          + (LENGTH OF STRINGS)
          + (NUMBER OF NULL COLUMNS + 7) / 8

     There is a penalty of 6 bytes for each link.  A dynamic row is
     linked whenever an update causes an enlargement of the row.  Each
     new link is at least 20 bytes, so the next enlargement probably
     goes in the same link.  If not, another link is created.  You can
     find the number of links using *note 'myisamchk -ed': myisamchk.
     All links may be removed with *note 'OPTIMIZE TABLE':
     optimize-table. or *note 'myisamchk -r': myisamchk.


File: manual.info.tmp,  Node: compressed-format,  Prev: dynamic-format,  Up: myisam-table-formats

15.3.3.3 Compressed Table Characteristics
.........................................

Compressed storage format is a read-only format that is generated with
the *note 'myisampack': myisampack. tool.  Compressed tables can be
uncompressed with *note 'myisamchk': myisamchk.

Compressed tables have the following characteristics:

   * Compressed tables take very little disk space.  This minimizes disk
     usage, which is helpful when using slow disks (such as CD-ROMs).

   * Each row is compressed separately, so there is very little access
     overhead.  The header for a row takes up one to three bytes
     depending on the biggest row in the table.  Each column is
     compressed differently.  There is usually a different Huffman tree
     for each column.  Some of the compression types are:

        * Suffix space compression.

        * Prefix space compression.

        * Numbers with a value of zero are stored using one bit.

        * If values in an integer column have a small range, the column
          is stored using the smallest possible type.  For example, a
          *note 'BIGINT': integer-types. column (eight bytes) can be
          stored as a *note 'TINYINT': integer-types. column (one byte)
          if all its values are in the range from '-128' to '127'.

        * If a column has only a small set of possible values, the data
          type is converted to *note 'ENUM': enum.

        * A column may use any combination of the preceding compression
          types.

   * Can be used for fixed-length or dynamic-length rows.

*Note*:

While a compressed table is read only, and you cannot therefore update
or add rows in the table, DDL (Data Definition Language) operations are
still valid.  For example, you may still use 'DROP' to drop the table,
and *note 'TRUNCATE TABLE': truncate-table. to empty the table.


File: manual.info.tmp,  Node: myisam-table-problems,  Prev: myisam-table-formats,  Up: myisam-storage-engine

15.3.4 MyISAM Table Problems
----------------------------

* Menu:

* corrupted-myisam-tables::      Corrupted MyISAM Tables
* myisam-table-close::           Problems from Tables Not Being Closed Properly

The file format that MySQL uses to store data has been extensively
tested, but there are always circumstances that may cause database
tables to become corrupted.  The following discussion describes how this
can happen and how to handle it.


File: manual.info.tmp,  Node: corrupted-myisam-tables,  Next: myisam-table-close,  Prev: myisam-table-problems,  Up: myisam-table-problems

15.3.4.1 Corrupted MyISAM Tables
................................

Even though the 'MyISAM' table format is very reliable (all changes to a
table made by an SQL statement are written before the statement
returns), you can still get corrupted tables if any of the following
events occur:

   * The *note 'mysqld': mysqld. process is killed in the middle of a
     write.

   * An unexpected computer shutdown occurs (for example, the computer
     is turned off).

   * Hardware failures.

   * You are using an external program (such as *note 'myisamchk':
     myisamchk.) to modify a table that is being modified by the server
     at the same time.

   * A software bug in the MySQL or 'MyISAM' code.

Typical symptoms of a corrupt table are:

   * You get the following error while selecting data from the table:

          Incorrect key file for table: '...'. Try to repair it

   * Queries don't find rows in the table or return incomplete results.

You can check the health of a 'MyISAM' table using the *note 'CHECK
TABLE': check-table. statement, and repair a corrupted 'MyISAM' table
with *note 'REPAIR TABLE': repair-table.  When *note 'mysqld': mysqld.
is not running, you can also check or repair a table with the *note
'myisamchk': myisamchk. command.  See *note check-table::, *note
repair-table::, and *note myisamchk::.

If your tables become corrupted frequently, you should try to determine
why this is happening.  The most important thing to know is whether the
table became corrupted as a result of a server crash.  You can verify
this easily by looking for a recent 'restarted mysqld' message in the
error log.  If there is such a message, it is likely that table
corruption is a result of the server dying.  Otherwise, corruption may
have occurred during normal operation.  This is a bug.  You should try
to create a reproducible test case that demonstrates the problem.  See
*note crashing::, and *note porting::.


File: manual.info.tmp,  Node: myisam-table-close,  Prev: corrupted-myisam-tables,  Up: myisam-table-problems

15.3.4.2 Problems from Tables Not Being Closed Properly
.......................................................

Each 'MyISAM' index file ('.MYI' file) has a counter in the header that
can be used to check whether a table has been closed properly.  If you
get the following warning from *note 'CHECK TABLE': check-table. or
*note 'myisamchk': myisamchk, it means that this counter has gone out of
sync:

     clients are using or haven't closed the table properly

This warning doesn't necessarily mean that the table is corrupted, but
you should at least check the table.

The counter works as follows:

   * The first time a table is updated in MySQL, a counter in the header
     of the index files is incremented.

   * The counter is not changed during further updates.

   * When the last instance of a table is closed (because a 'FLUSH
     TABLES' operation was performed or because there is no room in the
     table cache), the counter is decremented if the table has been
     updated at any point.

   * When you repair the table or check the table and it is found to be
     okay, the counter is reset to zero.

   * To avoid problems with interaction with other processes that might
     check the table, the counter is not decremented on close if it was
     zero.

In other words, the counter can become incorrect only under these
conditions:

   * A 'MyISAM' table is copied without first issuing *note 'LOCK
     TABLES': lock-tables. and 'FLUSH TABLES'.

   * MySQL has crashed between an update and the final close.  (The
     table may still be okay because MySQL always issues writes for
     everything between each statement.)

   * A table was modified by *note 'myisamchk --recover': myisamchk. or
     *note 'myisamchk --update-state': myisamchk. at the same time that
     it was in use by *note 'mysqld': mysqld.

   * Multiple *note 'mysqld': mysqld. servers are using the table and
     one server performed a *note 'REPAIR TABLE': repair-table. or *note
     'CHECK TABLE': check-table. on the table while it was in use by
     another server.  In this setup, it is safe to use *note 'CHECK
     TABLE': check-table, although you might get the warning from other
     servers.  However, *note 'REPAIR TABLE': repair-table. should be
     avoided because when one server replaces the data file with a new
     one, this is not known to the other servers.

     In general, it is a bad idea to share a data directory among
     multiple servers.  See *note multiple-servers::, for additional
     discussion.


File: manual.info.tmp,  Node: memory-storage-engine,  Next: csv-storage-engine,  Prev: myisam-storage-engine,  Up: storage-engines

15.4 The MEMORY Storage Engine
==============================

The 'MEMORY' storage engine (formerly known as 'HEAP') creates
special-purpose tables with contents that are stored in memory.  Because
the data is vulnerable to crashes, hardware issues, or power outages,
only use these tables as temporary work areas or read-only caches for
data pulled from other tables.

*MEMORY Storage Engine Features*

Feature                                     Support
                                            
*B-tree indexes*                            Yes
                                            
*Backup/point-in-time recovery*             Yes
(Implemented in the server, rather than     
in the storage engine.)

*Cluster database support*                  No
                                            
*Clustered indexes*                         No
                                            
*Compressed data*                           No
                                            
*Data caches*                               N/A
                                            
*Encrypted data*                            Yes (Implemented in the
                                            server via encryption
                                            functions.)
                                            
*Foreign key support*                       No
                                            
*Full-text search indexes*                  No
                                            
*Geospatial data type support*              No
                                            
*Geospatial indexing support*               No
                                            
*Hash indexes*                              Yes
                                            
*Index caches*                              N/A
                                            
*Locking granularity*                       Table
                                            
*MVCC*                                      No
                                            
*Replication support* (Implemented in the   Limited (See the discussion
server, rather than in the storage          later in this section.)
engine.)                                    

*Storage limits*                            RAM
                                            
*T-tree indexes*                            No
                                            
*Transactions*                              No
                                            
*Update statistics for data dictionary*     Yes

   * *note memory-storage-engine-compared-cluster::

   * *note memory-storage-engine-performance-characteristics::

   * *note memory-storage-engine-characteristics-of-memory-tables::

   * *note memory-storage-engine-ddl-operations-for-memory-tables::

   * *note memory-storage-engine-indexes::

   * *note memory-storage-engine-user-created-and-temporary-tables::

   * *note memory-storage-engine-loading-data::

   * *note memory-tables-replication::

   * *note memory-storage-engine-managing-memory-use::

   * *note memory-storage-engine-additional-resources::

*When to Use MEMORY or NDB Cluster*

Developers looking to deploy applications that use the 'MEMORY' storage
engine for important, highly available, or frequently updated data
should consider whether NDB Cluster is a better choice.  A typical use
case for the 'MEMORY' engine involves these characteristics:

   * Operations involving transient, non-critical data such as session
     management or caching.  When the MySQL server halts or restarts,
     the data in 'MEMORY' tables is lost.

   * In-memory storage for fast access and low latency.  Data volume can
     fit entirely in memory without causing the operating system to swap
     out virtual memory pages.

   * A read-only or read-mostly data access pattern (limited updates).

NDB Cluster offers the same features as the 'MEMORY' engine with higher
performance levels, and provides additional features not available with
'MEMORY':

   * Row-level locking and multiple-thread operation for low contention
     between clients.

   * Scalability even with statement mixes that include writes.

   * Optional disk-backed operation for data durability.

   * Shared-nothing architecture and multiple-host operation with no
     single point of failure, enabling 99.999% availability.

   * Automatic data distribution across nodes; application developers
     need not craft custom sharding or partitioning solutions.

   * Support for variable-length data types (including *note 'BLOB':
     blob. and *note 'TEXT': blob.) not supported by 'MEMORY'.

*Performance Characteristics*

'MEMORY' performance is constrained by contention resulting from
single-thread execution and table lock overhead when processing updates.
This limits scalability when load increases, particularly for statement
mixes that include writes.

Despite the in-memory processing for 'MEMORY' tables, they are not
necessarily faster than *note 'InnoDB': innodb-storage-engine. tables on
a busy server, for general-purpose queries, or under a read/write
workload.  In particular, the table locking involved with performing
updates can slow down concurrent usage of 'MEMORY' tables from multiple
sessions.

Depending on the kinds of queries performed on a 'MEMORY' table, you
might create indexes as either the default hash data structure (for
looking up single values based on a unique key), or a general-purpose
B-tree data structure (for all kinds of queries involving equality,
inequality, or range operators such as less than or greater than).  The
following sections illustrate the syntax for creating both kinds of
indexes.  A common performance issue is using the default hash indexes
in workloads where B-tree indexes are more efficient.

*Characteristics of MEMORY Tables*

The 'MEMORY' storage engine associates each table with one disk file,
which stores the table definition (not the data).  The file name begins
with the table name and has an extension of '.frm'.

'MEMORY' tables have the following characteristics:

   * Space for 'MEMORY' tables is allocated in small blocks.  Tables use
     100% dynamic hashing for inserts.  No overflow area or extra key
     space is needed.  No extra space is needed for free lists.  Deleted
     rows are put in a linked list and are reused when you insert new
     data into the table.  'MEMORY' tables also have none of the
     problems commonly associated with deletes plus inserts in hashed
     tables.

   * 'MEMORY' tables use a fixed-length row-storage format.
     Variable-length types such as *note 'VARCHAR': char. are stored
     using a fixed length.

   * 'MEMORY' tables cannot contain *note 'BLOB': blob. or *note 'TEXT':
     blob. columns.

   * 'MEMORY' includes support for 'AUTO_INCREMENT' columns.

   * Non-'TEMPORARY' 'MEMORY' tables are shared among all clients, just
     like any other non-'TEMPORARY' table.

*DDL Operations for MEMORY Tables*

To create a 'MEMORY' table, specify the clause 'ENGINE=MEMORY' on the
*note 'CREATE TABLE': create-table. statement.

     CREATE TABLE t (i INT) ENGINE = MEMORY;

As indicated by the engine name, 'MEMORY' tables are stored in memory.
They use hash indexes by default, which makes them very fast for
single-value lookups, and very useful for creating temporary tables.
However, when the server shuts down, all rows stored in 'MEMORY' tables
are lost.  The tables themselves continue to exist because their
definitions are stored in '.frm' files on disk, but they are empty when
the server restarts.

This example shows how you might create, use, and remove a 'MEMORY'
table:

     mysql> CREATE TABLE test ENGINE=MEMORY
                SELECT ip,SUM(downloads) AS down
                FROM log_table GROUP BY ip;
     mysql> SELECT COUNT(ip),AVG(down) FROM test;
     mysql> DROP TABLE test;

The maximum size of 'MEMORY' tables is limited by the
'max_heap_table_size' system variable, which has a default value of
16MB. To enforce different size limits for 'MEMORY' tables, change the
value of this variable.  The value in effect for *note 'CREATE TABLE':
create-table, or a subsequent *note 'ALTER TABLE': alter-table. or *note
'TRUNCATE TABLE': truncate-table, is the value used for the life of the
table.  A server restart also sets the maximum size of existing 'MEMORY'
tables to the global 'max_heap_table_size' value.  You can set the size
for individual tables as described later in this section.

*Indexes*

The 'MEMORY' storage engine supports both 'HASH' and 'BTREE' indexes.
You can specify one or the other for a given index by adding a 'USING'
clause as shown here:

     CREATE TABLE lookup
         (id INT, INDEX USING HASH (id))
         ENGINE = MEMORY;
     CREATE TABLE lookup
         (id INT, INDEX USING BTREE (id))
         ENGINE = MEMORY;

For general characteristics of B-tree and hash indexes, see *note
mysql-indexes::.

'MEMORY' tables can have up to 64 indexes per table, 16 columns per
index and a maximum key length of 3072 bytes.

If a 'MEMORY' table hash index has a high degree of key duplication
(many index entries containing the same value), updates to the table
that affect key values and all deletes are significantly slower.  The
degree of this slowdown is proportional to the degree of duplication
(or, inversely proportional to the index cardinality).  You can use a
'BTREE' index to avoid this problem.

'MEMORY' tables can have nonunique keys.  (This is an uncommon feature
for implementations of hash indexes.)

Columns that are indexed can contain 'NULL' values.

*User-Created and Temporary Tables*

'MEMORY' table contents are stored in memory, which is a property that
'MEMORY' tables share with internal temporary tables that the server
creates on the fly while processing queries.  However, the two types of
tables differ in that 'MEMORY' tables are not subject to storage
conversion, whereas internal temporary tables are:

   * If an internal temporary table becomes too large, the server
     automatically converts it to on-disk storage, as described in *note
     internal-temporary-tables::.

   * User-created 'MEMORY' tables are never converted to disk tables.

*Loading Data*

To populate a 'MEMORY' table when the MySQL server starts, you can use
the 'init_file' system variable.  For example, you can put statements
such as *note 'INSERT INTO ... SELECT': insert-select. or *note 'LOAD
DATA': load-data. into a file to load the table from a persistent data
source, and use 'init_file' to name the file.  See *note
server-system-variables::, and *note load-data::.

For loading data into 'MEMORY' tables accessed by other sessions
concurrently, 'MEMORY' supports *note 'INSERT DELAYED': insert-delayed.
See *note insert-delayed::.

*MEMORY Tables and Replication*

A server's 'MEMORY' tables become empty when it is shut down and
restarted.  If the server is a replication master, its slaves are not
aware that these tables have become empty, so you see out-of-date
content if you select data from the tables on the slaves.  To
synchronize master and slave 'MEMORY' tables, when a 'MEMORY' table is
used on a master for the first time since it was started, a *note
'DELETE': delete. statement is written to the master's binary log, to
empty the table on the slaves also.  The slave still has outdated data
in the table during the interval between the master's restart and its
first use of the table.  To avoid this interval when a direct query to
the slave could return stale data, use the 'init_file' system variable
set to name a file containing statements that populate the 'MEMORY'
table on the master at startup.

*Managing Memory Use*

The server needs sufficient memory to maintain all 'MEMORY' tables that
are in use at the same time.

Memory is not reclaimed if you delete individual rows from a 'MEMORY'
table.  Memory is reclaimed only when the entire table is deleted.
Memory that was previously used for deleted rows is re-used for new rows
within the same table.  To free all the memory used by a 'MEMORY' table
when you no longer require its contents, execute *note 'DELETE': delete.
or *note 'TRUNCATE TABLE': truncate-table. to remove all rows, or remove
the table altogether using *note 'DROP TABLE': drop-table.  To free up
the memory used by deleted rows, use 'ALTER TABLE ENGINE=MEMORY' to
force a table rebuild.

The memory needed for one row in a 'MEMORY' table is calculated using
the following expression:

     SUM_OVER_ALL_BTREE_KEYS(MAX_LENGTH_OF_KEY + sizeof(char*) * 4)
     + SUM_OVER_ALL_HASH_KEYS(sizeof(char*) * 2)
     + ALIGN(LENGTH_OF_ROW+1, sizeof(char*))

'ALIGN()' represents a round-up factor to cause the row length to be an
exact multiple of the 'char' pointer size.  'sizeof(char*)' is 4 on
32-bit machines and 8 on 64-bit machines.

As mentioned earlier, the 'max_heap_table_size' system variable sets the
limit on the maximum size of 'MEMORY' tables.  To control the maximum
size for individual tables, set the session value of this variable
before creating each table.  (Do not change the global
'max_heap_table_size' value unless you intend the value to be used for
'MEMORY' tables created by all clients.)  The following example creates
two 'MEMORY' tables, with a maximum size of 1MB and 2MB, respectively:

     mysql> SET max_heap_table_size = 1024*1024;
     Query OK, 0 rows affected (0.00 sec)

     mysql> CREATE TABLE t1 (id INT, UNIQUE(id)) ENGINE = MEMORY;
     Query OK, 0 rows affected (0.01 sec)

     mysql> SET max_heap_table_size = 1024*1024*2;
     Query OK, 0 rows affected (0.00 sec)

     mysql> CREATE TABLE t2 (id INT, UNIQUE(id)) ENGINE = MEMORY;
     Query OK, 0 rows affected (0.00 sec)

Both tables revert to the server's global 'max_heap_table_size' value if
the server restarts.

You can also specify a 'MAX_ROWS' table option in *note 'CREATE TABLE':
create-table. statements for 'MEMORY' tables to provide a hint about the
number of rows you plan to store in them.  This does not enable the
table to grow beyond the 'max_heap_table_size' value, which still acts
as a constraint on maximum table size.  For maximum flexibility in being
able to use 'MAX_ROWS', set 'max_heap_table_size' at least as high as
the value to which you want each 'MEMORY' table to be able to grow.

*Additional Resources*

A forum dedicated to the 'MEMORY' storage engine is available at
<https://forums.mysql.com/list.php?92>.


File: manual.info.tmp,  Node: csv-storage-engine,  Next: archive-storage-engine,  Prev: memory-storage-engine,  Up: storage-engines

15.5 The CSV Storage Engine
===========================

* Menu:

* se-csv-repair::                Repairing and Checking CSV Tables
* se-csv-limitations::           CSV Limitations

The 'CSV' storage engine stores data in text files using comma-separated
values format.

The 'CSV' storage engine is always compiled into the MySQL server.

To examine the source for the 'CSV' engine, look in the 'storage/csv'
directory of a MySQL source distribution.

When you create a 'CSV' table, the server creates a table format file in
the database directory.  The file begins with the table name and has an
'.frm' extension.  The storage engine also creates plain text data file
having a name that begins with the table name and has a '.CSV'
extension.  When you store data into the table, the storage engine saves
it into the data file in comma-separated values format.

     mysql> CREATE TABLE test (i INT NOT NULL, c CHAR(10) NOT NULL)
            ENGINE = CSV;
     Query OK, 0 rows affected (0.06 sec)

     mysql> INSERT INTO test VALUES(1,'record one'),(2,'record two');
     Query OK, 2 rows affected (0.05 sec)
     Records: 2  Duplicates: 0  Warnings: 0

     mysql> SELECT * FROM test;
     +---+------------+
     | i | c          |
     +---+------------+
     | 1 | record one |
     | 2 | record two |
     +---+------------+
     2 rows in set (0.00 sec)

Creating a 'CSV' table also creates a corresponding metafile that stores
the state of the table and the number of rows that exist in the table.
The name of this file is the same as the name of the table with the
extension 'CSM'.

If you examine the 'test.CSV' file in the database directory created by
executing the preceding statements, its contents should look like this:

     "1","record one"
     "2","record two"

This format can be read, and even written, by spreadsheet applications
such as Microsoft Excel or StarOffice Calc.


File: manual.info.tmp,  Node: se-csv-repair,  Next: se-csv-limitations,  Prev: csv-storage-engine,  Up: csv-storage-engine

15.5.1 Repairing and Checking CSV Tables
----------------------------------------

The 'CSV' storage engine supports the *note 'CHECK TABLE': check-table.
and *note 'REPAIR TABLE': repair-table. statements to verify and, if
possible, repair a damaged 'CSV' table.

When running the *note 'CHECK TABLE': check-table. statement, the 'CSV'
file will be checked for validity by looking for the correct field
separators, escaped fields (matching or missing quotation marks), the
correct number of fields compared to the table definition and the
existence of a corresponding 'CSV' metafile.  The first invalid row
discovered will report an error.  Checking a valid table produces output
like that shown below:

     mysql> CHECK TABLE csvtest;
     +--------------+-------+----------+----------+
     | Table        | Op    | Msg_type | Msg_text |
     +--------------+-------+----------+----------+
     | test.csvtest | check | status   | OK       |
     +--------------+-------+----------+----------+

A check on a corrupted table returns a fault:

     mysql> CHECK TABLE csvtest;
     +--------------+-------+----------+----------+
     | Table        | Op    | Msg_type | Msg_text |
     +--------------+-------+----------+----------+
     | test.csvtest | check | error    | Corrupt  |
     +--------------+-------+----------+----------+

If the check fails, the table is marked as crashed (corrupt).  Once a
table has been marked as corrupt, it is automatically repaired when you
next run *note 'CHECK TABLE': check-table. or execute a *note 'SELECT':
select. statement.  The corresponding corrupt status and new status will
be displayed when running *note 'CHECK TABLE': check-table.:

     mysql> CHECK TABLE csvtest;
     +--------------+-------+----------+----------------------------+
     | Table        | Op    | Msg_type | Msg_text                   |
     +--------------+-------+----------+----------------------------+
     | test.csvtest | check | warning  | Table is marked as crashed |
     | test.csvtest | check | status   | OK                         |
     +--------------+-------+----------+----------------------------+

To repair a table, use *note 'REPAIR TABLE': repair-table, which copies
as many valid rows from the existing 'CSV' data as possible, and then
replaces the existing 'CSV' file with the recovered rows.  Any rows
beyond the corrupted data are lost.

     mysql> REPAIR TABLE csvtest;
     +--------------+--------+----------+----------+
     | Table        | Op     | Msg_type | Msg_text |
     +--------------+--------+----------+----------+
     | test.csvtest | repair | status   | OK       |
     +--------------+--------+----------+----------+

*Warning*:

During repair, only the rows from the 'CSV' file up to the first damaged
row are copied to the new table.  All other rows from the first damaged
row to the end of the table are removed, even valid rows.


File: manual.info.tmp,  Node: se-csv-limitations,  Prev: se-csv-repair,  Up: csv-storage-engine

15.5.2 CSV Limitations
----------------------

The 'CSV' storage engine does not support indexing.

Partitioning is not supported for tables using the 'CSV' storage engine.

All tables that you create using the 'CSV' storage engine must have the
'NOT NULL' attribute on all columns.


File: manual.info.tmp,  Node: archive-storage-engine,  Next: blackhole-storage-engine,  Prev: csv-storage-engine,  Up: storage-engines

15.6 The ARCHIVE Storage Engine
===============================

The 'ARCHIVE' storage engine is used for storing large amounts of data
without indexes in a very small footprint.

*ARCHIVE Storage Engine Features*

Feature                                     Support
                                            
*B-tree indexes*                            No
                                            
*Backup/point-in-time recovery*             Yes
(Implemented in the server, rather than     
in the storage engine.)

*Cluster database support*                  No
                                            
*Clustered indexes*                         No
                                            
*Compressed data*                           Yes
                                            
*Data caches*                               No
                                            
*Encrypted data*                            Yes (Implemented in the
                                            server via encryption
                                            functions.)
                                            
*Foreign key support*                       No
                                            
*Full-text search indexes*                  No
                                            
*Geospatial data type support*              Yes
                                            
*Geospatial indexing support*               No
                                            
*Hash indexes*                              No
                                            
*Index caches*                              No
                                            
*Locking granularity*                       Row
                                            
*MVCC*                                      No
                                            
*Replication support* (Implemented in the   Yes
server, rather than in the storage          
engine.)

*Storage limits*                            None
                                            
*T-tree indexes*                            No
                                            
*Transactions*                              No
                                            
*Update statistics for data dictionary*     Yes

The 'ARCHIVE' storage engine is included in MySQL binary distributions.
To enable this storage engine if you build MySQL from source, invoke
'CMake' with the '-DWITH_ARCHIVE_STORAGE_ENGINE' option.

To examine the source for the 'ARCHIVE' engine, look in the
'storage/archive' directory of a MySQL source distribution.

You can check whether the 'ARCHIVE' storage engine is available with the
*note 'SHOW ENGINES': show-engines. statement.

When you create an 'ARCHIVE' table, the server creates a table format
file in the database directory.  The file begins with the table name and
has an '.frm' extension.  The storage engine creates other files, all
having names beginning with the table name.  The data file has an
extension of '.ARZ'.  An '.ARN' file may appear during optimization
operations.

The 'ARCHIVE' engine supports *note 'INSERT': insert, *note 'REPLACE':
replace, and *note 'SELECT': select, but not *note 'DELETE': delete. or
*note 'UPDATE': update.  It does support 'ORDER BY' operations, *note
'BLOB': blob. columns, and basically all data types including spatial
data types (see *note spatial-type-overview::).  Geographic spatial
reference systems are not supported.  The 'ARCHIVE' engine uses
row-level locking.

The 'ARCHIVE' engine supports the 'AUTO_INCREMENT' column attribute.
The 'AUTO_INCREMENT' column can have either a unique or nonunique index.
Attempting to create an index on any other column results in an error.
The 'ARCHIVE' engine also supports the 'AUTO_INCREMENT' table option in
*note 'CREATE TABLE': create-table. statements to specify the initial
sequence value for a new table or reset the sequence value for an
existing table, respectively.

'ARCHIVE' does not support inserting a value into an 'AUTO_INCREMENT'
column less than the current maximum column value.  Attempts to do so
result in an 'ER_DUP_KEY' error.

The 'ARCHIVE' engine ignores *note 'BLOB': blob. columns if they are not
requested and scans past them while reading.

*Storage:* Rows are compressed as they are inserted.  The 'ARCHIVE'
engine uses 'zlib' lossless data compression (see
<http://www.zlib.net/>).  You can use *note 'OPTIMIZE TABLE':
optimize-table. to analyze the table and pack it into a smaller format
(for a reason to use *note 'OPTIMIZE TABLE': optimize-table, see later
in this section).  The engine also supports *note 'CHECK TABLE':
check-table.  There are several types of insertions that are used:

   * An *note 'INSERT': insert. statement just pushes rows into a
     compression buffer, and that buffer flushes as necessary.  The
     insertion into the buffer is protected by a lock.  A *note
     'SELECT': select. forces a flush to occur, unless the only
     insertions that have come in were *note 'INSERT DELAYED':
     insert-delayed. (those flush as necessary).  See *note
     insert-delayed::.

   * A bulk insert is visible only after it completes, unless other
     inserts occur at the same time, in which case it can be seen
     partially.  A *note 'SELECT': select. never causes a flush of a
     bulk insert unless a normal insert occurs while it is loading.

*Retrieval*: On retrieval, rows are uncompressed on demand; there is no
row cache.  A *note 'SELECT': select. operation performs a complete
table scan: When a *note 'SELECT': select. occurs, it finds out how many
rows are currently available and reads that number of rows.  *note
'SELECT': select. is performed as a consistent read.  Note that lots of
*note 'SELECT': select. statements during insertion can deteriorate the
compression, unless only bulk or delayed inserts are used.  To achieve
better compression, you can use *note 'OPTIMIZE TABLE': optimize-table.
or *note 'REPAIR TABLE': repair-table.  The number of rows in 'ARCHIVE'
tables reported by *note 'SHOW TABLE STATUS': show-table-status. is
always accurate.  See *note optimize-table::, *note repair-table::, and
*note show-table-status::.

*Additional Resources*

   * A forum dedicated to the 'ARCHIVE' storage engine is available at
     <https://forums.mysql.com/list.php?112>.


File: manual.info.tmp,  Node: blackhole-storage-engine,  Next: merge-storage-engine,  Prev: archive-storage-engine,  Up: storage-engines

15.7 The BLACKHOLE Storage Engine
=================================

The 'BLACKHOLE' storage engine acts as a 'black hole' that accepts data
but throws it away and does not store it.  Retrievals always return an
empty result:

     mysql> CREATE TABLE test(i INT, c CHAR(10)) ENGINE = BLACKHOLE;
     Query OK, 0 rows affected (0.03 sec)

     mysql> INSERT INTO test VALUES(1,'record one'),(2,'record two');
     Query OK, 2 rows affected (0.00 sec)
     Records: 2  Duplicates: 0  Warnings: 0

     mysql> SELECT * FROM test;
     Empty set (0.00 sec)

To enable the 'BLACKHOLE' storage engine if you build MySQL from source,
invoke 'CMake' with the '-DWITH_BLACKHOLE_STORAGE_ENGINE' option.

To examine the source for the 'BLACKHOLE' engine, look in the 'sql'
directory of a MySQL source distribution.

When you create a 'BLACKHOLE' table, the server creates a table format
file in the database directory.  The file begins with the table name and
has an '.frm' extension.  There are no other files associated with the
table.

The 'BLACKHOLE' storage engine supports all kinds of indexes.  That is,
you can include index declarations in the table definition.

You can check whether the 'BLACKHOLE' storage engine is available with
the *note 'SHOW ENGINES': show-engines. statement.

Inserts into a 'BLACKHOLE' table do not store any data, but if statement
based binary logging is enabled, the SQL statements are logged and
replicated to slave servers.  This can be useful as a repeater or filter
mechanism.

Suppose that your application requires slave-side filtering rules, but
transferring all binary log data to the slave first results in too much
traffic.  In such a case, it is possible to set up on the master host a
'dummy' slave process whose default storage engine is 'BLACKHOLE',
depicted as follows:

FIGURE GOES HERE: Replication using BLACKHOLE for Filtering

The master writes to its binary log.  The 'dummy' *note 'mysqld':
mysqld. process acts as a slave, applying the desired combination of
'replicate-do-*' and 'replicate-ignore-*' rules, and writes a new,
filtered binary log of its own.  (See *note replication-options::.)
This filtered log is provided to the slave.

The dummy process does not actually store any data, so there is little
processing overhead incurred by running the additional *note 'mysqld':
mysqld. process on the replication master host.  This type of setup can
be repeated with additional replication slaves.

*note 'INSERT': insert. triggers for 'BLACKHOLE' tables work as
expected.  However, because the 'BLACKHOLE' table does not actually
store any data, *note 'UPDATE': update. and *note 'DELETE': delete.
triggers are not activated: The 'FOR EACH ROW' clause in the trigger
definition does not apply because there are no rows.

Other possible uses for the 'BLACKHOLE' storage engine include:

   * Verification of dump file syntax.

   * Measurement of the overhead from binary logging, by comparing
     performance using 'BLACKHOLE' with and without binary logging
     enabled.

   * 'BLACKHOLE' is essentially a 'no-op' storage engine, so it could be
     used for finding performance bottlenecks not related to the storage
     engine itself.

The 'BLACKHOLE' engine is transaction-aware, in the sense that committed
transactions are written to the binary log and rolled-back transactions
are not.

*Blackhole Engine and Auto Increment Columns*

The Blackhole engine is a no-op engine.  Any operations performed on a
table using Blackhole will have no effect.  This should be born in mind
when considering the behavior of primary key columns that auto
increment.  The engine will not automatically increment field values,
and does not retain auto increment field state.  This has important
implications in replication.

Consider the following replication scenario where all three of the
following conditions apply:

  1. On a master server there is a blackhole table with an auto
     increment field that is a primary key.

  2. On a slave the same table exists but using the MyISAM engine.

  3. Inserts are performed into the master's table without explicitly
     setting the auto increment value in the 'INSERT' statement itself
     or through using a 'SET INSERT_ID' statement.

In this scenario replication will fail with a duplicate entry error on
the primary key column.

In statement based replication, the value of 'INSERT_ID' in the context
event will always be the same.  Replication will therefore fail due to
trying insert a row with a duplicate value for a primary key column.

In row based replication, the value that the engine returns for the row
always be the same for each insert.  This will result in the slave
attempting to replay two insert log entries using the same value for the
primary key column, and so replication will fail.

*Column Filtering*

When using row-based replication, ('binlog_format=ROW'), a slave where
the last columns are missing from a table is supported, as described in
the section *note replication-features-differing-tables::.

This filtering works on the slave side, that is, the columns are copied
to the slave before they are filtered out.  There are at least two cases
where it is not desirable to copy the columns to the slave:

  1. If the data is confidential, so the slave server should not have
     access to it.

  2. If the master has many slaves, filtering before sending to the
     slaves may reduce network traffic.

Master column filtering can be achieved using the 'BLACKHOLE' engine.
This is carried out in a way similar to how master table filtering is
achieved - by using the 'BLACKHOLE' engine and the
'--replicate-do-table' or '--replicate-ignore-table' option.

The setup for the master is:

     CREATE TABLE t1 (public_col_1, ..., public_col_N,
                      secret_col_1, ..., secret_col_M) ENGINE=MyISAM;

The setup for the trusted slave is:

     CREATE TABLE t1 (public_col_1, ..., public_col_N) ENGINE=BLACKHOLE;

The setup for the untrusted slave is:

     CREATE TABLE t1 (public_col_1, ..., public_col_N) ENGINE=MyISAM;


File: manual.info.tmp,  Node: merge-storage-engine,  Next: federated-storage-engine,  Prev: blackhole-storage-engine,  Up: storage-engines

15.8 The MERGE Storage Engine
=============================

* Menu:

* merge-table-advantages::       MERGE Table Advantages and Disadvantages
* merge-table-problems::         MERGE Table Problems

The 'MERGE' storage engine, also known as the 'MRG_MyISAM' engine, is a
collection of identical 'MyISAM' tables that can be used as one.
'Identical' means that all tables have identical column data types and
index information.  You cannot merge 'MyISAM' tables in which the
columns are listed in a different order, do not have exactly the same
data types in corresponding columns, or have the indexes in different
order.  However, any or all of the 'MyISAM' tables can be compressed
with *note 'myisampack': myisampack.  See *note myisampack::.
Differences between tables such as these do not matter:

   * Names of corresponding columns and indexes can differ.

   * Comments for tables, columns, and indexes can differ.

   * Table options such as 'AVG_ROW_LENGTH', 'MAX_ROWS', or 'PACK_KEYS'
     can differ.

An alternative to a 'MERGE' table is a partitioned table, which stores
partitions of a single table in separate files.  Partitioning enables
some operations to be performed more efficiently and is not limited to
the 'MyISAM' storage engine.  For more information, see *note
partitioning::.

When you create a 'MERGE' table, MySQL creates two files on disk.  The
files have names that begin with the table name and have an extension to
indicate the file type.  An '.frm' file stores the table format, and an
'.MRG' file contains the names of the underlying 'MyISAM' tables that
should be used as one.  The tables do not have to be in the same
database as the 'MERGE' table.

You can use *note 'SELECT': select, *note 'DELETE': delete, *note
'UPDATE': update, and *note 'INSERT': insert. on 'MERGE' tables.  You
must have 'SELECT', 'DELETE', and 'UPDATE' privileges on the 'MyISAM'
tables that you map to a 'MERGE' table.

*Note*:

The use of 'MERGE' tables entails the following security issue: If a
user has access to 'MyISAM' table T, that user can create a 'MERGE'
table M that accesses T.  However, if the user's privileges on T are
subsequently revoked, the user can continue to access T by doing so
through M.

Use of *note 'DROP TABLE': drop-table. with a 'MERGE' table drops only
the 'MERGE' specification.  The underlying tables are not affected.

To create a 'MERGE' table, you must specify a 'UNION=(LIST-OF-TABLES)'
option that indicates which 'MyISAM' tables to use.  You can optionally
specify an 'INSERT_METHOD' option to control how inserts into the
'MERGE' table take place.  Use a value of 'FIRST' or 'LAST' to cause
inserts to be made in the first or last underlying table, respectively.
If you specify no 'INSERT_METHOD' option or if you specify it with a
value of 'NO', inserts into the 'MERGE' table are not permitted and
attempts to do so result in an error.

The following example shows how to create a 'MERGE' table:

     mysql> CREATE TABLE t1 (
         ->    a INT NOT NULL AUTO_INCREMENT PRIMARY KEY,
         ->    message CHAR(20)) ENGINE=MyISAM;
     mysql> CREATE TABLE t2 (
         ->    a INT NOT NULL AUTO_INCREMENT PRIMARY KEY,
         ->    message CHAR(20)) ENGINE=MyISAM;
     mysql> INSERT INTO t1 (message) VALUES ('Testing'),('table'),('t1');
     mysql> INSERT INTO t2 (message) VALUES ('Testing'),('table'),('t2');
     mysql> CREATE TABLE total (
         ->    a INT NOT NULL AUTO_INCREMENT,
         ->    message CHAR(20), INDEX(a))
         ->    ENGINE=MERGE UNION=(t1,t2) INSERT_METHOD=LAST;

Column 'a' is indexed as a 'PRIMARY KEY' in the underlying 'MyISAM'
tables, but not in the 'MERGE' table.  There it is indexed but not as a
'PRIMARY KEY' because a 'MERGE' table cannot enforce uniqueness over the
set of underlying tables.  (Similarly, a column with a 'UNIQUE' index in
the underlying tables should be indexed in the 'MERGE' table but not as
a 'UNIQUE' index.)

After creating the 'MERGE' table, you can use it to issue queries that
operate on the group of tables as a whole:

     mysql> SELECT * FROM total;
     +---+---------+
     | a | message |
     +---+---------+
     | 1 | Testing |
     | 2 | table   |
     | 3 | t1      |
     | 1 | Testing |
     | 2 | table   |
     | 3 | t2      |
     +---+---------+

To remap a 'MERGE' table to a different collection of 'MyISAM' tables,
you can use one of the following methods:

   * 'DROP' the 'MERGE' table and re-create it.

   * Use 'ALTER TABLE TBL_NAME UNION=(...)' to change the list of
     underlying tables.

     It is also possible to use 'ALTER TABLE ... UNION=()' (that is,
     with an empty *note 'UNION': union. clause) to remove all of the
     underlying tables.  However, in this case, the table is effectively
     empty and inserts fail because there is no underlying table to take
     new rows.  Such a table might be useful as a template for creating
     new 'MERGE' tables with *note 'CREATE TABLE ... LIKE':
     create-table-like.

The underlying table definitions and indexes must conform closely to the
definition of the 'MERGE' table.  Conformance is checked when a table
that is part of a 'MERGE' table is opened, not when the 'MERGE' table is
created.  If any table fails the conformance checks, the operation that
triggered the opening of the table fails.  This means that changes to
the definitions of tables within a 'MERGE' may cause a failure when the
'MERGE' table is accessed.  The conformance checks applied to each table
are:

   * The underlying table and the 'MERGE' table must have the same
     number of columns.

   * The column order in the underlying table and the 'MERGE' table must
     match.

   * Additionally, the specification for each corresponding column in
     the parent 'MERGE' table and the underlying tables are compared and
     must satisfy these checks:

        * The column type in the underlying table and the 'MERGE' table
          must be equal.

        * The column length in the underlying table and the 'MERGE'
          table must be equal.

        * The column of the underlying table and the 'MERGE' table can
          be 'NULL'.

   * The underlying table must have at least as many indexes as the
     'MERGE' table.  The underlying table may have more indexes than the
     'MERGE' table, but cannot have fewer.

     *Note*:

     A known issue exists where indexes on the same columns must be in
     identical order, in both the 'MERGE' table and the underlying
     'MyISAM' table.  See Bug #33653.

     Each index must satisfy these checks:

        * The index type of the underlying table and the 'MERGE' table
          must be the same.

        * The number of index parts (that is, multiple columns within a
          compound index) in the index definition for the underlying
          table and the 'MERGE' table must be the same.

        * For each index part:

             * Index part lengths must be equal.

             * Index part types must be equal.

             * Index part languages must be equal.

             * Check whether index parts can be 'NULL'.

If a 'MERGE' table cannot be opened or used because of a problem with an
underlying table, *note 'CHECK TABLE': check-table. displays information
about which table caused the problem.

*Additional Resources*

   * A forum dedicated to the 'MERGE' storage engine is available at
     <https://forums.mysql.com/list.php?93>.


File: manual.info.tmp,  Node: merge-table-advantages,  Next: merge-table-problems,  Prev: merge-storage-engine,  Up: merge-storage-engine

15.8.1 MERGE Table Advantages and Disadvantages
-----------------------------------------------

'MERGE' tables can help you solve the following problems:

   * Easily manage a set of log tables.  For example, you can put data
     from different months into separate tables, compress some of them
     with *note 'myisampack': myisampack, and then create a 'MERGE'
     table to use them as one.

   * Obtain more speed.  You can split a large read-only table based on
     some criteria, and then put individual tables on different disks.
     A 'MERGE' table structured this way could be much faster than using
     a single large table.

   * Perform more efficient searches.  If you know exactly what you are
     looking for, you can search in just one of the underlying tables
     for some queries and use a 'MERGE' table for others.  You can even
     have many different 'MERGE' tables that use overlapping sets of
     tables.

   * Perform more efficient repairs.  It is easier to repair individual
     smaller tables that are mapped to a 'MERGE' table than to repair a
     single large table.

   * Instantly map many tables as one.  A 'MERGE' table need not
     maintain an index of its own because it uses the indexes of the
     individual tables.  As a result, 'MERGE' table collections are
     _very_ fast to create or remap.  (You must still specify the index
     definitions when you create a 'MERGE' table, even though no indexes
     are created.)

   * If you have a set of tables from which you create a large table on
     demand, you can instead create a 'MERGE' table from them on demand.
     This is much faster and saves a lot of disk space.

   * Exceed the file size limit for the operating system.  Each 'MyISAM'
     table is bound by this limit, but a collection of 'MyISAM' tables
     is not.

   * You can create an alias or synonym for a 'MyISAM' table by defining
     a 'MERGE' table that maps to that single table.  There should be no
     really notable performance impact from doing this (only a couple of
     indirect calls and 'memcpy()' calls for each read).

The disadvantages of 'MERGE' tables are:

   * You can use only identical 'MyISAM' tables for a 'MERGE' table.

   * Some 'MyISAM' features are unavailable in 'MERGE' tables.  For
     example, you cannot create 'FULLTEXT' indexes on 'MERGE' tables.
     (You can create 'FULLTEXT' indexes on the underlying 'MyISAM'
     tables, but you cannot search the 'MERGE' table with a full-text
     search.)

   * If the 'MERGE' table is nontemporary, all underlying 'MyISAM'
     tables must be nontemporary.  If the 'MERGE' table is temporary,
     the 'MyISAM' tables can be any mix of temporary and nontemporary.

   * 'MERGE' tables use more file descriptors than 'MyISAM' tables.  If
     10 clients are using a 'MERGE' table that maps to 10 tables, the
     server uses (10 x 10) + 10 file descriptors.  (10 data file
     descriptors for each of the 10 clients, and 10 index file
     descriptors shared among the clients.)

   * Index reads are slower.  When you read an index, the 'MERGE'
     storage engine needs to issue a read on all underlying tables to
     check which one most closely matches a given index value.  To read
     the next index value, the 'MERGE' storage engine needs to search
     the read buffers to find the next value.  Only when one index
     buffer is used up does the storage engine need to read the next
     index block.  This makes 'MERGE' indexes much slower on 'eq_ref'
     searches, but not much slower on 'ref' searches.  For more
     information about 'eq_ref' and 'ref', see *note explain::.


File: manual.info.tmp,  Node: merge-table-problems,  Prev: merge-table-advantages,  Up: merge-storage-engine

15.8.2 MERGE Table Problems
---------------------------

The following are known problems with 'MERGE' tables:

   * In versions of MySQL Server prior to 5.1.23, it was possible to
     create temporary merge tables with nontemporary child MyISAM
     tables.

     From versions 5.1.23, MERGE children were locked through the parent
     table.  If the parent was temporary, it was not locked and so the
     children were not locked either.  Parallel use of the MyISAM tables
     corrupted them.

   * If you use *note 'ALTER TABLE': alter-table. to change a 'MERGE'
     table to another storage engine, the mapping to the underlying
     tables is lost.  Instead, the rows from the underlying 'MyISAM'
     tables are copied into the altered table, which then uses the
     specified storage engine.

   * The 'INSERT_METHOD' table option for a 'MERGE' table indicates
     which underlying 'MyISAM' table to use for inserts into the 'MERGE'
     table.  However, use of the 'AUTO_INCREMENT' table option for that
     'MyISAM' table has no effect for inserts into the 'MERGE' table
     until at least one row has been inserted directly into the 'MyISAM'
     table.

   * A 'MERGE' table cannot maintain uniqueness constraints over the
     entire table.  When you perform an *note 'INSERT': insert, the data
     goes into the first or last 'MyISAM' table (as determined by the
     'INSERT_METHOD' option).  MySQL ensures that unique key values
     remain unique within that 'MyISAM' table, but not over all the
     underlying tables in the collection.

   * Because the 'MERGE' engine cannot enforce uniqueness over the set
     of underlying tables, *note 'REPLACE': replace. does not work as
     expected.  The two key facts are:

        * *note 'REPLACE': replace. can detect unique key violations
          only in the underlying table to which it is going to write
          (which is determined by the 'INSERT_METHOD' option).  This
          differs from violations in the 'MERGE' table itself.

        * If *note 'REPLACE': replace. detects a unique key violation,
          it will change only the corresponding row in the underlying
          table it is writing to; that is, the first or last table, as
          determined by the 'INSERT_METHOD' option.

     Similar considerations apply for *note 'INSERT ... ON DUPLICATE KEY
     UPDATE': insert-on-duplicate.

   * 'MERGE' tables do not support partitioning.  That is, you cannot
     partition a 'MERGE' table, nor can any of a 'MERGE' table's
     underlying 'MyISAM' tables be partitioned.

   * You should not use *note 'ANALYZE TABLE': analyze-table, *note
     'REPAIR TABLE': repair-table, *note 'OPTIMIZE TABLE':
     optimize-table, *note 'ALTER TABLE': alter-table, *note 'DROP
     TABLE': drop-table, *note 'DELETE': delete. without a 'WHERE'
     clause, or *note 'TRUNCATE TABLE': truncate-table. on any of the
     tables that are mapped into an open 'MERGE' table.  If you do so,
     the 'MERGE' table may still refer to the original table and yield
     unexpected results.  To work around this problem, ensure that no
     'MERGE' tables remain open by issuing a 'FLUSH TABLES' statement
     prior to performing any of the named operations.

     The unexpected results include the possibility that the operation
     on the 'MERGE' table will report table corruption.  If this occurs
     after one of the named operations on the underlying 'MyISAM'
     tables, the corruption message is spurious.  To deal with this,
     issue a 'FLUSH TABLES' statement after modifying the 'MyISAM'
     tables.

   * *note 'DROP TABLE': drop-table. on a table that is in use by a
     'MERGE' table does not work on Windows because the 'MERGE' storage
     engine's table mapping is hidden from the upper layer of MySQL.
     Windows does not permit open files to be deleted, so you first must
     flush all 'MERGE' tables (with 'FLUSH TABLES') or drop the 'MERGE'
     table before dropping the table.

   * The definition of the 'MyISAM' tables and the 'MERGE' table are
     checked when the tables are accessed (for example, as part of a
     *note 'SELECT': select. or *note 'INSERT': insert. statement).  The
     checks ensure that the definitions of the tables and the parent
     'MERGE' table definition match by comparing column order, types,
     sizes and associated indexes.  If there is a difference between the
     tables, an error is returned and the statement fails.  Because
     these checks take place when the tables are opened, any changes to
     the definition of a single table, including column changes, column
     ordering, and engine alterations will cause the statement to fail.

   * The order of indexes in the 'MERGE' table and its underlying tables
     should be the same.  If you use *note 'ALTER TABLE': alter-table.
     to add a 'UNIQUE' index to a table used in a 'MERGE' table, and
     then use *note 'ALTER TABLE': alter-table. to add a nonunique index
     on the 'MERGE' table, the index ordering is different for the
     tables if there was already a nonunique index in the underlying
     table.  (This happens because *note 'ALTER TABLE': alter-table.
     puts 'UNIQUE' indexes before nonunique indexes to facilitate rapid
     detection of duplicate keys.)  Consequently, queries on tables with
     such indexes may return unexpected results.

   * If you encounter an error message similar to 'ERROR 1017 (HY000):
     Can't find file: 'TBL_NAME.MRG' (errno: 2)', it generally indicates
     that some of the underlying tables do not use the 'MyISAM' storage
     engine.  Confirm that all of these tables are 'MyISAM'.

   * The maximum number of rows in a 'MERGE' table is 2^64 (~1.844E+19;
     the same as for a 'MyISAM' table).  It is not possible to merge
     multiple 'MyISAM' tables into a single 'MERGE' table that would
     have more than this number of rows.

   * The 'MERGE' storage engine does not support *note 'INSERT DELAYED':
     insert-delayed. statements.

   * Use of underlying 'MyISAM' tables of differing row formats with a
     parent 'MERGE' table is currently known to fail.  See Bug #32364.

   * You cannot change the union list of a nontemporary 'MERGE' table
     when *note 'LOCK TABLES': lock-tables. is in effect.  The following
     does _not_ work:

          CREATE TABLE m1 ... ENGINE=MRG_MYISAM ...;
          LOCK TABLES t1 WRITE, t2 WRITE, m1 WRITE;
          ALTER TABLE m1 ... UNION=(t1,t2) ...;

     However, you can do this with a temporary 'MERGE' table.

   * You cannot create a 'MERGE' table with 'CREATE ... SELECT', neither
     as a temporary 'MERGE' table, nor as a nontemporary 'MERGE' table.
     For example:

          CREATE TABLE m1 ... ENGINE=MRG_MYISAM ... SELECT ...;

     Attempts to do this result in an error: TBL_NAME is not 'BASE
     TABLE'.

   * In some cases, differing 'PACK_KEYS' table option values among the
     'MERGE' and underlying tables cause unexpected results if the
     underlying tables contain 'CHAR' or 'BINARY' columns.  As a
     workaround, use 'ALTER TABLE' to ensure that all involved tables
     have the same 'PACK_KEYS' value.  (Bug #50646)


File: manual.info.tmp,  Node: federated-storage-engine,  Next: example-storage-engine,  Prev: merge-storage-engine,  Up: storage-engines

15.9 The FEDERATED Storage Engine
=================================

* Menu:

* federated-description::        FEDERATED Storage Engine Overview
* federated-create::             How to Create FEDERATED Tables
* federated-usagenotes::         FEDERATED Storage Engine Notes and Tips
* federated-storage-engine-resources::  FEDERATED Storage Engine Resources

The 'FEDERATED' storage engine lets you access data from a remote MySQL
database without using replication or cluster technology.  Querying a
local 'FEDERATED' table automatically pulls the data from the remote
(federated) tables.  No data is stored on the local tables.

To include the 'FEDERATED' storage engine if you build MySQL from
source, invoke 'CMake' with the '-DWITH_FEDERATED_STORAGE_ENGINE'
option.

The 'FEDERATED' storage engine is not enabled by default in the running
server; to enable 'FEDERATED', you must start the MySQL server binary
using the '--federated' option.

To examine the source for the 'FEDERATED' engine, look in the
'storage/federated' directory of a MySQL source distribution.


File: manual.info.tmp,  Node: federated-description,  Next: federated-create,  Prev: federated-storage-engine,  Up: federated-storage-engine

15.9.1 FEDERATED Storage Engine Overview
----------------------------------------

When you create a table using one of the standard storage engines (such
as 'MyISAM', 'CSV' or 'InnoDB'), the table consists of the table
definition and the associated data.  When you create a 'FEDERATED'
table, the table definition is the same, but the physical storage of the
data is handled on a remote server.

A 'FEDERATED' table consists of two elements:

   * A _remote server_ with a database table, which in turn consists of
     the table definition (stored in the '.frm' file) and the associated
     table.  The table type of the remote table may be any type
     supported by the remote 'mysqld' server, including 'MyISAM' or
     'InnoDB'.

   * A _local server_ with a database table, where the table definition
     matches that of the corresponding table on the remote server.  The
     table definition is stored within the '.frm' file.  However, there
     is no data file on the local server.  Instead, the table definition
     includes a connection string that points to the remote table.

When executing queries and statements on a 'FEDERATED' table on the
local server, the operations that would normally insert, update or
delete information from a local data file are instead sent to the remote
server for execution, where they update the data file on the remote
server or return matching rows from the remote server.

The basic structure of a 'FEDERATED' table setup is shown in *note
figure-se-federated-structure::.

FIGURE GOES HERE: FEDERATED Table Structure

When a client issues an SQL statement that refers to a 'FEDERATED'
table, the flow of information between the local server (where the SQL
statement is executed) and the remote server (where the data is
physically stored) is as follows:

  1. The storage engine looks through each column that the 'FEDERATED'
     table has and constructs an appropriate SQL statement that refers
     to the remote table.

  2. The statement is sent to the remote server using the MySQL client
     API.

  3. The remote server processes the statement and the local server
     retrieves any result that the statement produces (an affected-rows
     count or a result set).

  4. If the statement produces a result set, each column is converted to
     internal storage engine format that the 'FEDERATED' engine expects
     and can use to display the result to the client that issued the
     original statement.

The local server communicates with the remote server using MySQL client
C API functions.  It invokes *note 'mysql_real_query()':
mysql-real-query. to send the statement.  To read a result set, it uses
*note 'mysql_store_result()': mysql-store-result. and fetches rows one
at a time using *note 'mysql_fetch_row()': mysql-fetch-row.


File: manual.info.tmp,  Node: federated-create,  Next: federated-usagenotes,  Prev: federated-description,  Up: federated-storage-engine

15.9.2 How to Create FEDERATED Tables
-------------------------------------

* Menu:

* federated-create-connection::  Creating a FEDERATED Table Using CONNECTION
* federated-create-server::      Creating a FEDERATED Table Using CREATE SERVER

To create a 'FEDERATED' table you should follow these steps:

  1. Create the table on the remote server.  Alternatively, make a note
     of the table definition of an existing table, perhaps using the
     *note 'SHOW CREATE TABLE': show-create-table. statement.

  2. Create the table on the local server with an identical table
     definition, but adding the connection information that links the
     local table to the remote table.

For example, you could create the following table on the remote server:

     CREATE TABLE test_table (
         id     INT(20) NOT NULL AUTO_INCREMENT,
         name   VARCHAR(32) NOT NULL DEFAULT '',
         other  INT(20) NOT NULL DEFAULT '0',
         PRIMARY KEY  (id),
         INDEX name (name),
         INDEX other_key (other)
     )
     ENGINE=MyISAM
     DEFAULT CHARSET=latin1;

To create the local table that will be federated to the remote table,
there are two options available.  You can either create the local table
and specify the connection string (containing the server name, login,
password) to be used to connect to the remote table using the
'CONNECTION', or you can use an existing connection that you have
previously created using the *note 'CREATE SERVER': create-server.
statement.

*Important*:

When you create the local table it _must_ have an identical field
definition to the remote table.

*Note*:

You can improve the performance of a 'FEDERATED' table by adding indexes
to the table on the host.  The optimization will occur because the query
sent to the remote server will include the contents of the 'WHERE'
clause and will be sent to the remote server and subsequently executed
locally.  This reduces the network traffic that would otherwise request
the entire table from the server for local processing.


File: manual.info.tmp,  Node: federated-create-connection,  Next: federated-create-server,  Prev: federated-create,  Up: federated-create

15.9.2.1 Creating a FEDERATED Table Using CONNECTION
....................................................

To use the first method, you must specify the 'CONNECTION' string after
the engine type in a *note 'CREATE TABLE': create-table. statement.  For
example:

     CREATE TABLE federated_table (
         id     INT(20) NOT NULL AUTO_INCREMENT,
         name   VARCHAR(32) NOT NULL DEFAULT '',
         other  INT(20) NOT NULL DEFAULT '0',
         PRIMARY KEY  (id),
         INDEX name (name),
         INDEX other_key (other)
     )
     ENGINE=FEDERATED
     DEFAULT CHARSET=latin1
     CONNECTION='mysql://fed_user@remote_host:9306/federated/test_table';

*Note*:

'CONNECTION' replaces the 'COMMENT' used in some previous versions of
MySQL.

The 'CONNECTION' string contains the information required to connect to
the remote server containing the table that will be used to physically
store the data.  The connection string specifies the server name, login
credentials, port number and database/table information.  In the
example, the remote table is on the server 'remote_host', using port
9306.  The name and port number should match the host name (or IP
address) and port number of the remote MySQL server instance you want to
use as your remote table.

The format of the connection string is as follows:

     SCHEME://USER_NAME[:PASSWORD]@HOST_NAME[:PORT_NUM]/DB_NAME/TBL_NAME

Where:

   * SCHEME: A recognized connection protocol.  Only 'mysql' is
     supported as the SCHEME value at this point.

   * USER_NAME: The user name for the connection.  This user must have
     been created on the remote server, and must have suitable
     privileges to perform the required actions (*note 'SELECT': select,
     *note 'INSERT': insert, *note 'UPDATE': update, and so forth) on
     the remote table.

   * PASSWORD: (Optional) The corresponding password for USER_NAME.

   * HOST_NAME: The host name or IP address of the remote server.

   * PORT_NUM: (Optional) The port number for the remote server.  The
     default is 3306.

   * DB_NAME: The name of the database holding the remote table.

   * TBL_NAME: The name of the remote table.  The name of the local and
     the remote table do not have to match.

Sample connection strings:

     CONNECTION='mysql://username:password@hostname:port/database/tablename'
     CONNECTION='mysql://username@hostname/database/tablename'
     CONNECTION='mysql://username:password@hostname/database/tablename'


File: manual.info.tmp,  Node: federated-create-server,  Prev: federated-create-connection,  Up: federated-create

15.9.2.2 Creating a FEDERATED Table Using CREATE SERVER
.......................................................

If you are creating a number of 'FEDERATED' tables on the same server,
or if you want to simplify the process of creating 'FEDERATED' tables,
you can use the *note 'CREATE SERVER': create-server. statement to
define the server connection parameters, just as you would with the
'CONNECTION' string.

The format of the *note 'CREATE SERVER': create-server. statement is:

     CREATE SERVERSERVER_NAME
     FOREIGN DATA WRAPPER WRAPPER_NAME
     OPTIONS (OPTION [, OPTION] ...)

The SERVER_NAME is used in the connection string when creating a new
'FEDERATED' table.

For example, to create a server connection identical to the 'CONNECTION'
string:

     CONNECTION='mysql://fed_user@remote_host:9306/federated/test_table';

You would use the following statement:

     CREATE SERVER fedlink
     FOREIGN DATA WRAPPER mysql
     OPTIONS (USER 'fed_user', HOST 'remote_host', PORT 9306, DATABASE 'federated');

To create a 'FEDERATED' table that uses this connection, you still use
the 'CONNECTION' keyword, but specify the name you used in the *note
'CREATE SERVER': create-server. statement.

     CREATE TABLE test_table (
         id     INT(20) NOT NULL AUTO_INCREMENT,
         name   VARCHAR(32) NOT NULL DEFAULT '',
         other  INT(20) NOT NULL DEFAULT '0',
         PRIMARY KEY  (id),
         INDEX name (name),
         INDEX other_key (other)
     )
     ENGINE=FEDERATED
     DEFAULT CHARSET=latin1
     CONNECTION='fedlink/test_table';

The connection name in this example contains the name of the connection
('fedlink') and the name of the table ('test_table') to link to,
separated by a slash.  If you specify only the connection name without a
table name, the table name of the local table is used instead.

For more information on *note 'CREATE SERVER': create-server, see *note
create-server::.

The *note 'CREATE SERVER': create-server. statement accepts the same
arguments as the 'CONNECTION' string.  The *note 'CREATE SERVER':
create-server. statement updates the rows in the 'mysql.servers' table.
See the following table for information on the correspondence between
parameters in a connection string, options in the *note 'CREATE SERVER':
create-server. statement, and the columns in the 'mysql.servers' table.
For reference, the format of the 'CONNECTION' string is as follows:

     SCHEME://USER_NAME[:PASSWORD]@HOST_NAME[:PORT_NUM]/DB_NAME/TBL_NAME

Description        'CONNECTION'       *note 'CREATE SERVER': create-server.'mysql.servers'
                   string             option             column
                                                         
Connection         SCHEME             'wrapper_name'     'Wrapper'
scheme                                                   

Remote user        USER_NAME          'USER'             'Username'
                                                         
Remote password    PASSWORD           'PASSWORD'         'Password'
                                                         
Remote host        HOST_NAME          'HOST'             'Host'
                                                         
Remote port        PORT_NUM           'PORT'             'Port'
                                                         
Remote database    DB_NAME            'DATABASE'         'Db'
                                      


File: manual.info.tmp,  Node: federated-usagenotes,  Next: federated-storage-engine-resources,  Prev: federated-create,  Up: federated-storage-engine

15.9.3 FEDERATED Storage Engine Notes and Tips
----------------------------------------------

You should be aware of the following points when using the 'FEDERATED'
storage engine:

   * 'FEDERATED' tables may be replicated to other slaves, but you must
     ensure that the slave servers are able to use the user/password
     combination that is defined in the 'CONNECTION' string (or the row
     in the 'mysql.servers' table) to connect to the remote server.

The following items indicate features that the 'FEDERATED' storage
engine does and does not support:

   * The remote server must be a MySQL server.

   * The remote table that a 'FEDERATED' table points to _must_ exist
     before you try to access the table through the 'FEDERATED' table.

   * It is possible for one 'FEDERATED' table to point to another, but
     you must be careful not to create a loop.

   * A 'FEDERATED' table does not support indexes in the usual sense;
     because access to the table data is handled remotely, it is
     actually the remote table that makes use of indexes.  This means
     that, for a query that cannot use any indexes and so requires a
     full table scan, the server fetches all rows from the remote table
     and filters them locally.  This occurs regardless of any 'WHERE' or
     'LIMIT' used with this *note 'SELECT': select. statement; these
     clauses are applied locally to the returned rows.

     Queries that fail to use indexes can thus cause poor performance
     and network overload.  In addition, since returned rows must be
     stored in memory, such a query can also lead to the local server
     swapping, or even hanging.

   * Care should be taken when creating a 'FEDERATED' table since the
     index definition from an equivalent 'MyISAM' or other table may not
     be supported.  For example, creating a 'FEDERATED' table with an
     index prefix on *note 'VARCHAR': char, *note 'TEXT': blob. or *note
     'BLOB': blob. columns will fail.  The following definition in
     'MyISAM' is valid:

          CREATE TABLE `T1`(`A` VARCHAR(100),UNIQUE KEY(`A`(30))) ENGINE=MYISAM;

     The key prefix in this example is incompatible with the 'FEDERATED'
     engine, and the equivalent statement will fail:

          CREATE TABLE `T1`(`A` VARCHAR(100),UNIQUE KEY(`A`(30))) ENGINE=FEDERATED
            CONNECTION='MYSQL://127.0.0.1:3306/TEST/T1';

     If possible, you should try to separate the column and index
     definition when creating tables on both the remote server and the
     local server to avoid these index issues.

   * Internally, the implementation uses *note 'SELECT': select, *note
     'INSERT': insert, *note 'UPDATE': update, and *note 'DELETE':
     delete, but not *note 'HANDLER': handler.

   * The 'FEDERATED' storage engine supports *note 'SELECT': select,
     *note 'INSERT': insert, *note 'UPDATE': update, *note 'DELETE':
     delete, *note 'TRUNCATE TABLE': truncate-table, and indexes.  It
     does not support *note 'ALTER TABLE': alter-table, or any Data
     Definition Language statements that directly affect the structure
     of the table, other than *note 'DROP TABLE': drop-table.  The
     current implementation does not use prepared statements.

   * 'FEDERATED' accepts *note 'INSERT ... ON DUPLICATE KEY UPDATE':
     insert-on-duplicate. statements, but if a duplicate-key violation
     occurs, the statement fails with an error.

   * Transactions are not supported.

   * 'FEDERATED' performs bulk-insert handling such that multiple rows
     are sent to the remote table in a batch, which improves
     performance.  Also, if the remote table is transactional, it
     enables the remote storage engine to perform statement rollback
     properly should an error occur.  This capability has the following
     limitations:

        * The size of the insert cannot exceed the maximum packet size
          between servers.  If the insert exceeds this size, it is
          broken into multiple packets and the rollback problem can
          occur.

        * Bulk-insert handling does not occur for *note 'INSERT ... ON
          DUPLICATE KEY UPDATE': insert-on-duplicate.

   * There is no way for the 'FEDERATED' engine to know if the remote
     table has changed.  The reason for this is that this table must
     work like a data file that would never be written to by anything
     other than the database system.  The integrity of the data in the
     local table could be breached if there was any change to the remote
     database.

   * When using a 'CONNECTION' string, you cannot use an '@' character
     in the password.  You can get round this limitation by using the
     *note 'CREATE SERVER': create-server. statement to create a server
     connection.

   * The 'insert_id' and 'timestamp' options are not propagated to the
     data provider.

   * Any *note 'DROP TABLE': drop-table. statement issued against a
     'FEDERATED' table drops only the local table, not the remote table.

   * 'FEDERATED' tables do not work with the query cache.

   * User-defined partitioning is not supported for 'FEDERATED' tables.


File: manual.info.tmp,  Node: federated-storage-engine-resources,  Prev: federated-usagenotes,  Up: federated-storage-engine

15.9.4 FEDERATED Storage Engine Resources
-----------------------------------------

The following additional resources are available for the 'FEDERATED'
storage engine:

   * A forum dedicated to the 'FEDERATED' storage engine is available at
     <https://forums.mysql.com/list.php?105>.


File: manual.info.tmp,  Node: example-storage-engine,  Next: storage-engines-other,  Prev: federated-storage-engine,  Up: storage-engines

15.10 The EXAMPLE Storage Engine
================================

The 'EXAMPLE' storage engine is a stub engine that does nothing.  Its
purpose is to serve as an example in the MySQL source code that
illustrates how to begin writing new storage engines.  As such, it is
primarily of interest to developers.

To enable the 'EXAMPLE' storage engine if you build MySQL from source,
invoke 'CMake' with the '-DWITH_EXAMPLE_STORAGE_ENGINE' option.

To examine the source for the 'EXAMPLE' engine, look in the
'storage/example' directory of a MySQL source distribution.

When you create an 'EXAMPLE' table, the server creates a table format
file in the database directory.  The file begins with the table name and
has an '.frm' extension.  No other files are created.  No data can be
stored into the table.  Retrievals return an empty result.

     mysql> CREATE TABLE test (i INT) ENGINE = EXAMPLE;
     Query OK, 0 rows affected (0.78 sec)

     mysql> INSERT INTO test VALUES(1),(2),(3);
     ERROR 1031 (HY000): Table storage engine for 'test' doesn't 
                         have this option

     mysql> SELECT * FROM test;
     Empty set (0.31 sec)

The 'EXAMPLE' storage engine does not support indexing.


File: manual.info.tmp,  Node: storage-engines-other,  Prev: example-storage-engine,  Up: storage-engines

15.11 Other Storage Engines
===========================

Other storage engines may be available from third parties and community
members that have used the Custom Storage Engine interface.

Third party engines are not supported by MySQL. For further information,
documentation, installation guides, bug reporting or for any help or
assistance with these engines, please contact the developer of the
engine directly.

For more information on developing a customer storage engine that can be
used with the Pluggable Storage Engine Architecture, see MySQL
Internals: Writing a Custom Storage Engine
(https://dev.mysql.com/doc/internals/en/custom-engine.html).


File: manual.info.tmp,  Node: ha-overview,  Next: replication,  Prev: storage-engines,  Up: Top

16 High Availability and Scalability
************************************

* Menu:

* ha-zfs-replication::           Using ZFS Replication
* ha-memcached::                 Using MySQL with 'memcached'

Data is the currency of today's web, mobile, social, enterprise and
cloud applications.  Ensuring data is always available is a top priority
for any organization.  Minutes of downtime can result in significant
loss of revenue and reputation.

There is no 'one size fits all' approach to delivering High Availability
(HA). Unique application attributes, business requirements, operational
capabilities and legacy infrastructure can all influence HA technology
selection.  And technology is only one element in delivering HA: people
and processes are just as critical as the technology itself.

MySQL is deployed into many applications demanding availability and
scalability.  *Availability* refers to the ability to cope with, and if
necessary recover from, failures on the host, including failures of
MySQL, the operating system, or the hardware and maintenance activity
that may otherwise cause downtime.  Scalability refers to the ability to
spread both the database and the load of your application queries across
multiple MySQL servers.

Because each application has different operational and availability
requirements, MySQL offers a range of certified and supported solutions,
delivering the appropriate levels of High Availability (HA) and
scalability to meet service level requirements.  Such solutions extend
from replication, through virtualization and geographically redundant,
multi-data center solutions delivering 99.999% uptime.

Selecting the right high availability solution for an application
largely depends on:

   * The level of availability required.

   * The type of application being deployed.

   * Accepted best practices within your own environment.

The primary solutions supported by MySQL include:

   * MySQL Replication.  Learn more: *note replication::.

   * MySQL Cluster.  Learn more: *note mysql-cluster::.

   * Oracle MySQL Cloud Service.  Learn more about MySQL Cloud Service.
     (http://docs.oracle.com/cloud/latest/mysql-cloud/index.html)

   * Oracle Clusterware Agent for MySQL. Learn more about Oracle
     Clusterware.
     (http://www.oracle.com/technetwork/database/database-technologies/clusterware/overview/index.html)

   * MySQL with Solaris Cluster.  Learn more about Solaris Cluster.
     (http://www.oracle.com/technetwork/server-storage/solaris-cluster/overview/index.html)

Further options are available using third-party solutions.

Each architecture used to achieve highly available database services is
differentiated by the levels of uptime it offers.  These architectures
can be grouped into three main categories:

   * Data Replication.

   * Clustered & Virtualized Systems.

   * Shared-Nothing, Geographically-Replicated Clusters.

As illustrated in the following figure, each of these architectures
offers progressively higher levels of uptime, which must be balanced
against potentially greater levels of cost and complexity that each can
incur.  Simply deploying a high availability architecture is not a
guarantee of actually delivering HA. In fact, a poorly implemented and
maintained shared-nothing cluster could easily deliver lower levels of
availability than a simple data replication solution.

FIGURE GOES HERE: Tradeoffs: Cost and Complexity versus Availability

The following table compares the HA and Scalability capabilities of the
various MySQL solutions:

*Feature Comparison of MySQL HA Solutions*

Requirement              MySQL Replication        MySQL Cluster
                                                  
*Availability*

Platform Support         All Supported by MySQL   All Supported by MySQL
                         Server                   Cluster
                         (<https://www.mysql.com/support/supportedplatforms/database.html>)(<https://www.mysql.com/support/supportedplatforms/cluster.html>)
                                                  
Automated IP Failover    No                       Depends on Connector
                                                  and Configuration
                                                  
Automated Database       No                       Yes
Failover                                          

Automatic Data           No                       Yes
Resynchronization                                 

Typical Failover Time    User / Script            1 Second and Less
                         Dependent                
                         
Synchronous              No, Asynchronous and     Yes
Replication              Semisynchronous          
                         
Shared Storage           No, Distributed          No, Distributed
                                                  
Geographic redundancy    Yes                      Yes, via MySQL
support                                           Replication
                                                  
Update Schema On-Line    No                       Yes
                                                  
*Scalability*

Number of Nodes          One Master, Multiple     255
                         Slaves                   
                         
Built-in Load            Reads, via MySQL         Yes, Reads and Writes
Balancing                Replication              
                         
Supports                 Yes                      Yes
Read-Intensive                                    
Workloads

Supports                 Yes, via                 Yes, via Auto-Sharding
Write-Intensive          Application-Level        
Workloads                Sharding
                         
Scale On-Line (add       No                       Yes
nodes, repartition,      
etc.)


File: manual.info.tmp,  Node: ha-zfs-replication,  Next: ha-memcached,  Prev: ha-overview,  Up: ha-overview

16.1 Using ZFS Replication
==========================

* Menu:

* ha-zfs-config::                Using ZFS for File System Replication
* ha-zfs-mysql::                 Configuring MySQL for ZFS Replication
* ha-zfs-mysql-recovery::        Handling MySQL Recovery with ZFS

To support high availability environments, providing an instant copy of
the information on both the currently active machine and the hot backup
is a critical part of the HA solution.  There are many solutions to this
problem, such as *note replication::.

The ZFS file system provides functionality to create a snapshot of the
file system contents, transfer the snapshot to another machine, and
extract the snapshot to recreate the file system.  You can create a
snapshot at any time, and you can create as many snapshots as you like.
By continually creating, transferring, and restoring snapshots, you can
provide synchronization between one or more machines in a fashion
similar to DRBD.

The following example shows a simple Solaris system running with a
single ZFS pool, mounted at '/scratchpool':

     Filesystem             size   used  avail capacity  Mounted on
     /dev/dsk/c0d0s0        4.6G   3.7G   886M    82%    /
     /devices                 0K     0K     0K     0%    /devices
     ctfs                     0K     0K     0K     0%    /system/contract
     proc                     0K     0K     0K     0%    /proc
     mnttab                   0K     0K     0K     0%    /etc/mnttab
     swap                   1.4G   892K   1.4G     1%    /etc/svc/volatile
     objfs                    0K     0K     0K     0%    /system/object
     /usr/lib/libc/libc_hwcap1.so.1
                            4.6G   3.7G   886M    82%    /lib/libc.so.1
     fd                       0K     0K     0K     0%    /dev/fd
     swap                   1.4G    40K   1.4G     1%    /tmp
     swap                   1.4G    28K   1.4G     1%    /var/run
     /dev/dsk/c0d0s7         26G   913M    25G     4%    /export/home
     scratchpool             16G    24K    16G     1%    /scratchpool

The MySQL data is stored in a directory on '/scratchpool'.  To help
demonstrate some of the basic replication functionality, there are also
other items stored in '/scratchpool' as well:

     total 17
     drwxr-xr-x  31 root     bin           50 Jul 21 07:32 DTT/
     drwxr-xr-x   4 root     bin            5 Jul 21 07:32 SUNWmlib/
     drwxr-xr-x  14 root     sys           16 Nov  5 09:56 SUNWspro/
     drwxrwxrwx  19 1000     1000          40 Nov  6 19:16 emacs-22.1/

To create a snapshot of the file system, you use 'zfs snapshot',
specifying the pool and the snapshot name:

     root-shell> zfs snapshot scratchpool@snap1

To list the snapshots already taken:

     root-shell> zfs list -t snapshot
     NAME                USED  AVAIL  REFER  MOUNTPOINT
     scratchpool@snap1      0      -  24.5K  -
     scratchpool@snap2      0      -  24.5K  -

The snapshots themselves are stored within the file system metadata, and
the space required to keep them varies as time goes on because of the
way the snapshots are created.  The initial creation of a snapshot is
very quick, because instead of taking an entire copy of the data and
metadata required to hold the entire snapshot, ZFS records only the
point in time and metadata of when the snapshot was created.

As more changes to the original file system are made, the size of the
snapshot increases because more space is required to keep the record of
the old blocks.  If you create lots of snapshots, say one per day, and
then delete the snapshots from earlier in the week, the size of the
newer snapshots might also increase, as the changes that make up the
newer state have to be included in the more recent snapshots, rather
than being spread over the seven snapshots that make up the week.

You cannot directly back up the snapshots because they exist within the
file system metadata rather than as regular files.  To get the snapshot
into a format that you can copy to another file system, tape, and so on,
you use the 'zfs send' command to create a stream version of the
snapshot.

For example, to write the snapshot out to a file:

     root-shell> zfs send scratchpool@snap1 >/backup/scratchpool-snap1

Or tape:

     root-shell> zfs send scratchpool@snap1 >/dev/rmt/0

You can also write out the incremental changes between two snapshots
using 'zfs send':

     root-shell> zfs send scratchpool@snap1 scratchpool@snap2 >/backup/scratchpool-changes

To recover a snapshot, you use 'zfs recv', which applies the snapshot
information either to a new file system, or to an existing one.


File: manual.info.tmp,  Node: ha-zfs-config,  Next: ha-zfs-mysql,  Prev: ha-zfs-replication,  Up: ha-zfs-replication

16.1.1 Using ZFS for File System Replication
--------------------------------------------

Because 'zfs send' and 'zfs recv' use streams to exchange data, you can
use them to replicate information from one system to another by
combining 'zfs send', 'ssh', and 'zfs recv'.

For example, to copy a snapshot of the 'scratchpool' file system to a
new file system called 'slavepool' on a new server, you would use the
following command.  This sequence combines the snapshot of
'scratchpool', the transmission to the slave machine (using 'ssh' with
login credentials), and the recovery of the snapshot on the slave using
'zfs recv':

     root-shell> zfs send scratchpool@snap1 |ssh ID@HOST pfexec zfs recv -F slavepool

The first part of the pipeline, 'zfs send scratchpool@snap1', streams
the snapshot.  The 'ssh' command, and the command that it executes on
the other server, 'pfexec zfs recv -F slavepool', receives the streamed
snapshot data and writes it to slavepool.  In this instance, I've
specified the '-F' option which forces the snapshot data to be applied,
and is therefore destructive.  This is fine, as I'm creating the first
version of my replicated file system.

On the slave machine, the replicated file system contains the exact same
content:

     root-shell> ls -al /slavepool/
     total 23
     drwxr-xr-x   6 root     root           7 Nov  8 09:13 ./
     drwxr-xr-x  29 root     root          34 Nov  9 07:06 ../
     drwxr-xr-x  31 root     bin           50 Jul 21 07:32 DTT/
     drwxr-xr-x   4 root     bin            5 Jul 21 07:32 SUNWmlib/
     drwxr-xr-x  14 root     sys           16 Nov  5 09:56 SUNWspro/
     drwxrwxrwx  19 1000     1000          40 Nov  6 19:16 emacs-22.1/

Once a snapshot has been created, to synchronize the file system again,
you create a new snapshot and then use the incremental snapshot feature
of 'zfs send' to send the changes between the two snapshots to the slave
machine again:

     root-shell> zfs send -i scratchpool@snapshot1 scratchpool@snapshot2 |ssh ID@HOST pfexec zfs recv slavepool

This operation only succeeds if the file system on the slave machine has
not been modified at all.  You cannot apply the incremental changes to a
destination file system that has changed.  In the example above, the
'ls' command would cause problems by changing the metadata, such as the
last access time for files or directories.

To prevent changes on the slave file system, set the file system on the
slave to be read-only:

     root-shell> zfs set readonly=on slavepool

Setting 'readonly' means that you cannot change the file system on the
slave by normal means, including the file system metadata.  Operations
that would normally update metadata (like our 'ls') silently perform
their function without attempting to update the file system state.

In essence, the slave file system is nothing but a static copy of the
original file system.  However, even when configured to be read-only, a
file system can have snapshots applied to it.  With the file system set
to read only, re-run the initial copy:

     root-shell> zfs send scratchpool@snap1 |ssh ID@HOST pfexec zfs recv -F slavepool

Now you can make changes to the original file system and replicate them
to the slave.


File: manual.info.tmp,  Node: ha-zfs-mysql,  Next: ha-zfs-mysql-recovery,  Prev: ha-zfs-config,  Up: ha-zfs-replication

16.1.2 Configuring MySQL for ZFS Replication
--------------------------------------------

Configuring MySQL on the source file system is a case of creating the
data on the file system that you intend to replicate.  The configuration
file in the example below has been updated to use
'/scratchpool/mysql-data' as the data directory, and now you can
initialize the tables:

     root-shell> mysql_install_db --defaults-file=/etc/mysql/5.5/my.cnf --user=mysql

To synchronize the initial information, perform a new snapshot and then
send an incremental snapshot to the slave using 'zfs send':

     root-shell> zfs snapshot scratchpool@snap2
     root-shell> zfs send -i scratchpool@snap1 scratchpool@snap2|ssh ID@HOST pfexec zfs recv slavepool

Doublecheck that the slave has the data by looking at the MySQL data
directory on the 'slavepool':

     root-shell> ls -al /slavepool/mysql-data/

Now you can start up MySQL, create some data, and then replicate the
changes using 'zfs send'/' zfs recv' to the slave to synchronize the
changes.

The rate at which you perform the synchronization depends on your
application and environment.  The limitation is the speed required to
perform the snapshot and then to send the changes over the network.

To automate the process, create a script that performs the snapshot,
send, and receive operation, and use 'cron' to synchronize the changes
at set times or intervals.


File: manual.info.tmp,  Node: ha-zfs-mysql-recovery,  Prev: ha-zfs-mysql,  Up: ha-zfs-replication

16.1.3 Handling MySQL Recovery with ZFS
---------------------------------------

When using ZFS replication to provide a constant copy of your data,
ensure that you can recover your tables, either manually or
automatically, in the event of a failure of the original system.

In the event of a failure, follow this sequence:

  1. Stop the script on the master, if it is still up and running.

  2. Set the slave file system to be read/write:

          root-shell> zfs set readonly=off slavepool

  3. Start up *note 'mysqld': mysqld. on the slave.  If you are using
     'InnoDB', you get auto-recovery, if it is needed, to make sure the
     table data is correct, as shown here when I started up from our
     mid-INSERT snapshot:

          InnoDB: The log sequence number in ibdata files does not match
          InnoDB: the log sequence number in the ib_logfiles!
          081109 15:59:59  InnoDB: Database was not shut down normally!
          InnoDB: Starting crash recovery.
          InnoDB: Reading tablespace information from the .ibd files...
          InnoDB: Restoring possible half-written data pages from the doublewrite
          InnoDB: buffer...
          081109 16:00:03  InnoDB: Started; log sequence number 0 1142807951
          081109 16:00:03 [Note] /slavepool/mysql-5.0.67-solaris10-i386/bin/mysqld: ready for connections.
          Version: '5.0.67'  socket: '/tmp/mysql.sock'  port: 3306  MySQL Community Server (GPL)

Use *note 'InnoDB': innodb-storage-engine. tables and a regular
synchronization schedule to reduce the risk for significant data loss.
On MyISAM tables, you might need to run *note 'REPAIR TABLE':
repair-table, and you might even have lost some information.


File: manual.info.tmp,  Node: ha-memcached,  Prev: ha-zfs-replication,  Up: ha-overview

16.2 Using MySQL with 'memcached'
=================================

* Menu:

* ha-memcached-install::         Installing 'memcached'
* ha-memcached-using::           Using 'memcached'
* ha-memcached-interfaces::      Developing a 'memcached' Application
* ha-memcached-stats::           Getting 'memcached' Statistics
* ha-memcached-faq::             'memcached' FAQ

'memcached' is a simple, highly scalable key-based cache that stores
data and objects wherever dedicated or spare RAM is available for quick
access by applications, without going through layers of parsing or disk
I/O. To use, you run the 'memcached' command on one or more hosts and
then use the shared cache to store objects.  For more usage
instructions, see *note ha-memcached-using::

Benefits of using 'memcached' include:

   * Because all information is stored in RAM, the access speed is
     faster than loading the information each time from disk.

   * Because the 'value' portion of the key-value pair does not have any
     data type restrictions, you can cache data such as complex
     structures, documents, images, or a mixture of such things.

   * If you use the in-memory cache to hold transient information, or as
     a read-only cache for information also stored in a database, the
     failure of any 'memcached' server is not critical.  For persistent
     data, you can fall back to an alternative lookup method using
     database queries, and reload the data into RAM on a different
     server.

The typical usage environment is to modify your application so that
information is read from the cache provided by 'memcached'.  If the
information is not in 'memcached', then the data is loaded from the
MySQL database and written into the cache so that future requests for
the same object benefit from the cached data.

For a typical deployment layout, see *note ha-memcached-fig-overview::.

FIGURE GOES HERE: 'memcached' Architecture Overview

In the example structure, any of the clients can contact one of the
'memcached' servers to request a given key.  Each client is configured
to talk to all of the servers shown in the illustration.  Within the
client, when the request is made to store the information, the key used
to reference the data is hashed and this hash is then used to select one
of the 'memcached' servers.  The selection of the 'memcached' server
takes place on the client before the server is contacted, keeping the
process lightweight.

The same algorithm is used again when a client requests the same key.
The same key generates the same hash, and the same 'memcached' server is
selected as the source for the data.  Using this method, the cached data
is spread among all of the 'memcached' servers, and the cached
information is accessible from any client.  The result is a distributed,
memory-based, cache that can return information, particularly complex
data and structures, much faster than natively reading the information
from the database.

The data held within a traditional 'memcached' server is never stored on
disk (only in RAM, which means there is no persistence of data), and the
RAM cache is always populated from the backing store (a MySQL database).
If a 'memcached' server fails, the data can always be recovered from the
MySQL database.


File: manual.info.tmp,  Node: ha-memcached-install,  Next: ha-memcached-using,  Prev: ha-memcached,  Up: ha-memcached

16.2.1 Installing 'memcached'
-----------------------------

You can build and install 'memcached' from the source code directly, or
you can use an existing operating system package or installation.

*Installing 'memcached' from a Binary Distribution*

To install 'memcached' on a Red Hat, or Fedora host, use 'yum':

     root-shell> yum install memcached

*Note*:

On CentOS, you may be able to obtain a suitable RPM from another source,
or use the source tarball.

To install 'memcached' on a Debian or Ubuntu host, use 'apt-get':

     root-shell> apt-get install memcached

To install 'memcached' on a Gentoo host, use 'emerge':

     root-shell> emerge install memcached

*Building 'memcached' from Source*

On other Unix-based platforms, including Solaris, AIX, HP-UX and macOS,
and Linux distributions not mentioned already, you must install from
source.  For Linux, make sure you have a 2.6-based kernel, which
includes the improved 'epoll' interface.  For all platforms, ensure that
you have 'libevent' 1.1 or higher installed.  You can obtain 'libevent'
from 'libevent' web page (http://www.monkey.org/~provos/libevent/).

You can obtain the source for 'memcached' from 'memcached' website
(http://www.danga.com/memcached).

To build 'memcached', follow these steps:

  1. Extract the 'memcached' source package:

          shell> gunzip -c memcached-1.2.5.tar.gz | tar xf -

  2. Change to the 'memcached-1.2.5 directory:'

          shell> cd memcached-1.2.5

  3. Run 'configure'

          shell> ./configure

     Some additional options you might specify to the 'configure':

        * '--prefix'

          To specify a different installation directory, use the
          '--prefix' option:

               shell> ./configure --prefix=/opt

          The default is to use the '/usr/local' directory.

        * '--with-libevent'

          If you have installed 'libevent' and 'configure' cannot find
          the library, use the '--with-libevent' option to specify the
          location of the installed library.

        * '--enable-64bit'

          To build a 64-bit version of 'memcached' (which enables you to
          use a single instance with a large RAM allocation), use
          '--enable-64bit'.

        * '--enable-threads'

          To enable multithreading support in 'memcached', which
          improves the response times on servers with a heavy load, use
          '--enable-threads'.  You must have support for the POSIX
          threads within your operating system to enable thread support.
          For more information on the threading support, see *note
          ha-memcached-using-threads::.

        * '--enable-dtrace'

          'memcached' includes a range of DTrace threads that can be
          used to monitor and benchmark a 'memcached' instance.  For
          more information, see *note ha-memcached-using-dtrace::.

  4. Run 'make' to build 'memcached':

          shell> make

  5. Run 'make install' to install 'memcached':

          shell> make install


File: manual.info.tmp,  Node: ha-memcached-using,  Next: ha-memcached-interfaces,  Prev: ha-memcached-install,  Up: ha-memcached

16.2.2 Using 'memcached'
------------------------

* Menu:

* ha-memcached-cmdline-options::  'memcached' Command-Line Options
* ha-memcached-using-deployment::  'memcached' Deployment
* ha-memcached-using-namespaces::  Using Namespaces
* ha-memcached-using-expiry::    Data Expiry
* ha-memcached-using-hashtypes::  'memcached' Hashing/Distribution Types
* ha-memcached-using-dtrace::    Using 'memcached' and DTrace
* ha-memcached-using-memory::    Memory Allocation within 'memcached'
* ha-memcached-using-threads::   'memcached' Thread Support
* ha-memcached-using-logs::      'memcached' Logs

To start using 'memcached', start the 'memcached' service on one or more
servers.  Running 'memcached' sets up the server, allocates the memory
and starts listening for connections from clients.

*Note*:

You do not need to be a privileged user ('root') to run 'memcached'
except to listen on one of the privileged TCP/IP ports (below 1024).
You must, however, use a user that has not had their memory limits
restricted using 'setrlimit' or similar.

To start the server, run 'memcached' as a nonprivileged (that is,
non-'root') user:

     shell> memcached

By default, 'memcached' uses the following settings:

   * Memory allocation of 64MB

   * Listens for connections on all network interfaces, using port 11211

   * Supports a maximum of 1024 simultaneous connections

Typically, you would specify the full combination of options that you
want when starting 'memcached', and normally provide a startup script to
handle the initialization of 'memcached'.  For example, the following
line starts 'memcached' with a maximum of 1024MB RAM for the cache,
listening on port 11211 on the IP address 198.51.100.110, running as a
background daemon:

     shell> memcached -d -m 1024 -p 11211 -l 198.51.100.110

To ensure that 'memcached' is started up on boot, check the init script
and configuration parameters.


File: manual.info.tmp,  Node: ha-memcached-cmdline-options,  Next: ha-memcached-using-deployment,  Prev: ha-memcached-using,  Up: ha-memcached-using

16.2.2.1 'memcached' Command-Line Options
.........................................

'memcached' supports the following options:

   * '-u user'

     If you start 'memcached' as 'root', use the '-u' option to specify
     the user for executing 'memcached':

          shell> memcached -u memcache

   * '-m memory'

     Set the amount of memory allocated to 'memcached' for object
     storage.  Default is 64MB.

     To increase the amount of memory allocated for the cache, use the
     '-m' option to specify the amount of RAM to be allocated (in
     megabytes).  The more RAM you allocate, the more data you can store
     and therefore the more effective your cache is.

     *Warning*:

     Do not specify a memory allocation larger than your available RAM.
     If you specify too large a value, then some RAM allocated for
     'memcached' uses swap space, and not physical RAM. This may lead to
     delays when storing and retrieving values, because data is swapped
     to disk, instead of storing the data directly in RAM.

     You can use the output of the 'vmstat' command to get the free
     memory, as shown in 'free' column:

          shell> vmstat
          kthr      memory            page            disk          faults      cpu
          r b w   swap  free  re  mf pi po fr de sr s1 s2 -- --   in   sy   cs us sy id
          0 0 0 5170504 3450392 2  7  2  0  0  0  4  0  0  0  0  296   54  199  0  0 100

     For example, to allocate 3GB of RAM:

          shell> memcached -m 3072

     On 32-bit x86 systems where you are using PAE to access memory
     above the 4GB limit, you cannot allocate RAM beyond the maximum
     process size.  You can get around this by running multiple
     instances of 'memcached', each listening on a different port:

          shell> memcached -m 1024 -p11211
          shell> memcached -m 1024 -p11212
          shell> memcached -m 1024 -p11213

     *Note*:

     On all systems, particularly 32-bit, ensure that you leave enough
     room for both 'memcached' application in addition to the memory
     setting.  For example, if you have a dedicated 'memcached' host
     with 4GB of RAM, do not set the memory size above 3500MB. Failure
     to do this may cause either a crash or severe performance issues.

   * '-l interface'

     Specify a network interface/address to listen for connections.  The
     default is to listen on all available address ('INADDR_ANY').

          shell> memcached -l 198.51.100.110

     Support for IPv6 address support was added in 'memcached' 1.2.5.

   * '-p port'

     Specify the TCP port to use for connections.  Default is 18080.

          shell> memcached -p 18080

   * '-U port'

     Specify the UDP port to use for connections.  Default is 11211, 0
     switches UDP off.

          shell> memcached -U 18080

   * '-s socket'

     Specify a Unix socket to listen on.

     If you are running 'memcached' on the same server as the clients,
     you can disable the network interface and use a local Unix socket
     using the '-s' option:

          shell> memcached -s /tmp/memcached

     Using a Unix socket automatically disables network support, and
     saves network ports (allowing more ports to be used by your web
     server or other process).

   * '-a mask'

     Specify the access mask to be used for the Unix socket, in octal.
     Default is 0700.

   * '-c connections'

     Specify the maximum number of simultaneous connections to the
     'memcached' service.  The default is 1024.

          shell> memcached -c 2048

     Use this option, either to reduce the number of connections (to
     prevent overloading 'memcached' service) or to increase the number
     to make more effective use of the server running 'memcached'
     server.

   * '-t threads'

     Specify the number of threads to use when processing incoming
     requests.

     By default, 'memcached' is configured to use 4 concurrent threads.
     The threading improves the performance of storing and retrieving
     data in the cache, using a locking system to prevent different
     threads overwriting or updating the same values.  To increase or
     decrease the number of threads, use the '-t' option:

          shell> memcached -t 8

   * '-d'

     Run 'memcached' as a daemon (background) process:

          shell> memcached -d

   * '-r'

     Maximize the size of the core file limit.  In the event of a
     failure, this attempts to dump the entire memory space to disk as a
     core file, up to any limits imposed by 'setrlimit'.

   * '-M'

     Return an error to the client when the memory has been exhausted.
     This replaces the normal behavior of removing older items from the
     cache to make way for new items.

   * '-k'

     Lock down all paged memory.  This reserves the memory before use,
     instead of allocating new slabs of memory as new items are stored
     in the cache.

     *Note*:

     There is a user-level limit on how much memory you can lock.
     Trying to allocate more than the available memory fails.  You can
     set the limit for the user you started the daemon with (not for the
     '-u user' user) within the shell by using 'ulimit -S -l NUM_KB'

   * '-v'

     Verbose mode.  Prints errors and warnings while executing the main
     event loop.

   * '-vv'

     Very verbose mode.  In addition to information printed by '-v',
     also prints each client command and the response.

   * '-vvv'

     Extremely verbose mode.  In addition to information printed by
     '-vv', also show the internal state transitions.

   * '-h'

     Print the help message and exit.

   * '-i'

     Print the 'memcached' and 'libevent' license.

   * '-I mem'

     Specify the maximum size permitted for storing an object within the
     'memcached' instance.  The size supports a unit postfix ('k' for
     kilobytes, 'm' for megabytes).  For example, to increase the
     maximum supported object size to 32MB:

          shell> memcached -I 32m

     The maximum object size you can specify is 128MB, the default
     remains at 1MB.

     This option was added in 1.4.2.

   * '-b'

     Set the backlog queue limit.  The backlog queue configures how many
     network connections can be waiting to be processed by 'memcached'.
     Increasing this limit may reduce errors received by the client that
     it is not able to connect to the 'memcached' instance, but does not
     improve the performance of the server.  The default is 1024.

   * '-P pidfile'

     Save the process ID of the 'memcached' instance into 'file'.

   * '-f'

     Set the chunk size growth factor.  When allocating new memory
     chunks, the allocated size of new chunks is determined by
     multiplying the default slab size by this factor.

     To see the effects of this option without extensive testing, use
     the '-vv' command-line option to show the calculated slab sizes.
     For more information, see *note ha-memcached-using-logs::.

   * '-n bytes'

     The minimum space allocated for the key+value+flags information.
     The default is 48 bytes.

   * '-L'

     On systems that support large memory pages, enables large memory
     page use.  Using large memory pages enables 'memcached' to allocate
     the item cache in one large chunk, which can improve the
     performance by reducing the number misses when accessing memory.

   * '-C'

     Disable the use of compare and swap (CAS) operations.

     This option was added in 'memcached' 1.3.x.

   * '-D char'

     Set the default character to be used as a delimiter between the key
     prefixes and IDs.  This is used for the per-prefix statistics
     reporting (see *note ha-memcached-stats::).  The default is the
     colon (':').  If this option is used, statistics collection is
     turned on automatically.  If not used, you can enable stats
     collection by sending the 'stats detail on' command to the server.

     This option was added in 'memcached' 1.3.x.

   * '-R num'

     Sets the maximum number of requests per event process.  The default
     is 20.

   * '-B protocol'

     Set the binding protocol, that is, the default 'memcached' protocol
     support for client connections.  Options are 'ascii', 'binary' or
     'auto'.  Automatic ('auto') is the default.

     This option was added in 'memcached' 1.4.0.


File: manual.info.tmp,  Node: ha-memcached-using-deployment,  Next: ha-memcached-using-namespaces,  Prev: ha-memcached-cmdline-options,  Up: ha-memcached-using

16.2.2.2 'memcached' Deployment
...............................

When using 'memcached' you can use a number of different potential
deployment strategies and topologies.  The exact strategy to use depends
on your application and environment.  When developing a system for
deploying 'memcached' within your system, keep in mind the following
points:

   * 'memcached' is only a caching mechanism.  It shouldn't be used to
     store information that you cannot otherwise afford to lose and then
     load from a different location.

   * There is no security built into the 'memcached' protocol.  At a
     minimum, make sure that the servers running 'memcached' are only
     accessible from inside your network, and that the network ports
     being used are blocked (using a firewall or similar).  If the
     information that is being stored on the 'memcached' servers is
     sensitive, encrypt it before storing it in 'memcached'.

   * 'memcached' does not provide any sort of failover.  Because there
     is no communication between different 'memcached' instances.  If an
     instance fails, your application must capable of removing it from
     the list, reloading the data and then writing data to another
     'memcached' instance.

   * Latency between the clients and the 'memcached' can be a problem if
     you are using different physical machines for these tasks.  If you
     find that the latency is a problem, move the 'memcached' instances
     to be on the clients.

   * Key length is determined by the 'memcached' server.  The default
     maximum key size is 250 bytes.

   * Try to use at least two 'memcached' instances, especially for
     multiple clients, to avoid having a single point of failure.
     Ideally, create as many 'memcached' nodes as possible.  When adding
     and removing 'memcached' instances from a pool, the hashing and
     distribution of key-value pairs may be affected.  For information
     on how to avoid problems, see *note ha-memcached-using-hashtypes::.


File: manual.info.tmp,  Node: ha-memcached-using-namespaces,  Next: ha-memcached-using-expiry,  Prev: ha-memcached-using-deployment,  Up: ha-memcached-using

16.2.2.3 Using Namespaces
.........................

The 'memcached' cache is a very simple massive key-value storage system,
and as such there is no way of compartmentalizing data automatically
into different sections.  For example, if you are storing information by
the unique ID returned from a MySQL database, then storing the data from
two different tables could run into issues because the same ID might be
valid in both tables.

Some interfaces provide an automated mechanism for creating _namespaces_
when storing information into the cache.  In practice, these namespaces
are merely a prefix before a given ID that is applied every time a value
is stored or retrieve from the cache.

You can implement the same basic principle by using keys that describe
the object and the unique identifier within the key that you supply when
the object is stored.  For example, when storing user data, prefix the
ID of the user with 'user:' or 'user-'.

*Note*:

Using namespaces or prefixes only controls the keys stored/retrieved.
There is no security within 'memcached', and therefore no way to enforce
that a particular client only accesses keys with a particular namespace.
Namespaces are only useful as a method of identifying data and
preventing corruption of key-value pairs.


File: manual.info.tmp,  Node: ha-memcached-using-expiry,  Next: ha-memcached-using-hashtypes,  Prev: ha-memcached-using-namespaces,  Up: ha-memcached-using

16.2.2.4 Data Expiry
....................

There are two types of data expiry within a 'memcached' instance.  The
first type is applied at the point when you store a new key-value pair
into the 'memcached' instance.  If there is not enough space within a
suitable slab to store the value, then an existing least recently used
(LRU) object is removed (evicted) from the cache to make room for the
new item.

The LRU algorithm ensures that the object that is removed is one that is
either no longer in active use or that was used so long ago that its
data is potentially out of date or of little value.  However, in a
system where the memory allocated to 'memcached' is smaller than the
number of regularly used objects required in the cache, a lot of expired
items could be removed from the cache even though they are in active
use.  You use the statistics mechanism to get a better idea of the level
of evictions (expired objects).  For more information, see *note
ha-memcached-stats::.

You can change this eviction behavior by setting the '-M' command-line
option when starting 'memcached'.  This option forces an error to be
returned when the memory has been exhausted, instead of automatically
evicting older data.

The second type of expiry system is an explicit mechanism that you can
set when a key-value pair is inserted into the cache, or when deleting
an item from the cache.  Using an expiration time can be a useful way of
ensuring that the data in the cache is up to date and in line with your
application needs and requirements.

A typical scenario for explicitly setting the expiry time might include
caching session data for a user when accessing a website.  'memcached'
uses a lazy expiry mechanism where the explicit expiry time that has
been set is compared with the current time when the object is requested.
Only objects that have not expired are returned.

You can also set the expiry time when explicitly deleting an object from
the cache.  In this case, the expiry time is really a timeout and
indicates the period when any attempts to set the value for a given key
are rejected.


File: manual.info.tmp,  Node: ha-memcached-using-hashtypes,  Next: ha-memcached-using-dtrace,  Prev: ha-memcached-using-expiry,  Up: ha-memcached-using

16.2.2.5 'memcached' Hashing/Distribution Types
...............................................

The 'memcached' client interface supports a number of different
distribution algorithms that are used in multi-server configurations to
determine which host should be used when setting or getting data from a
given 'memcached' instance.  When you get or set a value, a hash is
constructed from the supplied key and then used to select a host from
the list of configured servers.  Because the hashing mechanism uses the
supplied key as the basis for the hash, the same server is selected
during both set and get operations.

You can think of this process as follows.  Given an array of servers (a,
b, and c), the client uses a hashing algorithm that returns an integer
based on the key being stored or retrieved.  The resulting value is then
used to select a server from the list of servers configured in the
client.  Most standard client hashing within 'memcache' clients uses a
simple modulus calculation on the value against the number of configured
'memcached' servers.  You can summarize the process in pseudocode as:

     @memcservers = ['a.memc','b.memc','c.memc'];
     $value = hash($key);
     $chosen = $value % length(@memcservers);

Replacing the above with values:

     @memcservers = ['a.memc','b.memc','c.memc'];
     $value = hash('myid');
     $chosen = 7009 % 3;

In the above example, the client hashing algorithm chooses the server at
index 1 ('7009 % 3 = 1'), and stores or retrieves the key and value with
that server.

*Note*:

This selection and hashing process is handled automatically by the
'memcached' client you are using; you need only provide the list of
'memcached' servers to use.

You can see a graphical representation of this below in *note
ha-memcached-using-hashtypes-fig-selection::.

FIGURE GOES HERE: 'memcached' Hash Selection

The same hashing and selection process takes place during any operation
on the specified key within the 'memcached' client.

Using this method provides a number of advantages:

   * The hashing and selection of the server to contact is handled
     entirely within the client.  This eliminates the need to perform
     network communication to determine the right machine to contact.

   * Because the determination of the 'memcached' server occurs entirely
     within the client, the server can be selected automatically
     regardless of the operation being executed (set, get, increment,
     etc.).

   * Because the determination is handled within the client, the hashing
     algorithm returns the same value for a given key; values are not
     affected or reset by differences in the server environment.

   * Selection is very fast.  The hashing algorithm on the key value is
     quick and the resulting selection of the server is from a simple
     array of available machines.

   * Using client-side hashing simplifies the distribution of data over
     each 'memcached' server.  Natural distribution of the values
     returned by the hashing algorithm means that keys are automatically
     spread over the available servers.

Providing that the list of servers configured within the client remains
the same, the same stored key returns the same value, and therefore
selects the same server.

However, if you do not use the same hashing mechanism then the same data
may be recorded on different servers by different interfaces, both
wasting space on your 'memcached' and leading to potential differences
in the information.

*Note*:

One way to use a multi-interface compatible hashing mechanism is to use
the 'libmemcached' library and the associated interfaces.  Because the
interfaces for the different languages (including C, Ruby, Perl and
Python) use the same client library interface, they always generate the
same hash code from the ID.

The problem with client-side selection of the server is that the list of
the servers (including their sequential order) _must_ remain consistent
on each client using the 'memcached' servers, and the servers must be
available.  If you try to perform an operation on a key when:

   * A new 'memcached' instance has been added to the list of available
     instances

   * A 'memcached' instance has been removed from the list of available
     instances

   * The order of the 'memcached' instances has changed

When the hashing algorithm is used on the given key, but with a
different list of servers, the hash calculation may choose a different
server from the list.

If a new 'memcached' instance is added into the list of servers, as
'new.memc' is in the example below, then a GET operation using the same
key, 'myid', can result in a cache-miss.  This is because the same value
is computed from the key, which selects the same index from the array of
servers, but index 2 now points to the new server, not the server
'c.memc' where the data was originally stored.  This would result in a
cache miss, even though the key exists within the cache on another
'memcached' instance.

FIGURE GOES HERE: 'memcached' Hash Selection with New 'memcached'
instance

This means that servers 'c.memc' and 'new.memc ' both contain the
information for key 'myid', but the information stored against the key
in each server may be different in each instance.  A more significant
problem is a much higher number of cache-misses when retrieving data, as
the addition of a new server changes the distribution of keys, and this
in turn requires rebuilding the cached data on the 'memcached'
instances, causing an increase in database reads.

The same effect can occur if you actively manage the list of servers
configured in your clients, adding and removing the configured
'memcached' instances as each instance is identified as being available.
For example, removing a 'memcached' instance when the client notices
that the instance can no longer be contacted can cause the server
selection to fail as described here.

To prevent this causing significant problems and invalidating your
cache, you can select the hashing algorithm used to select the server.
There are two common types of hashing algorithm, _consistent_ and
_modula_.

With _consistent_ hashing algorithms, the same key when applied to a
list of servers always uses the same server to store or retrieve the
keys, even if the list of configured servers changes.  This means that
you can add and remove servers from the configure list and always use
the same server for a given key.  There are two types of consistent
hashing algorithms available, Ketama and Wheel.  Both types are
supported by 'libmemcached', and implementations are available for PHP
and Java.

Any consistent hashing algorithm has some limitations.  When you add
servers to an existing list of configured servers, keys are distributed
to the new servers as part of the normal distribution.  When you remove
servers from the list, the keys are re-allocated to another server
within the list, meaning that the cache needs to be re-populated with
the information.  Also, a consistent hashing algorithm does not resolve
the issue where you want consistent selection of a server across
multiple clients, but where each client contains a different list of
servers.  The consistency is enforced only within a single client.

With a _modula_ hashing algorithm, the client selects a server by first
computing the hash and then choosing a server from the list of
configured servers.  As the list of servers changes, so the server
selected when using a modula hashing algorithm also changes.  The result
is the behavior described above; changes to the list of servers mean
that different servers are selected when retrieving data, leading to
cache misses and increase in database load as the cache is re-seeded
with information.

If you use only a single 'memcached' instance for each client, or your
list of 'memcached' servers configured for a client never changes, then
the selection of a hashing algorithm is irrelevant, as it has no
noticeable effect.

If you change your servers regularly, or you use a common set of servers
that are shared among a large number of clients, then using a consistent
hashing algorithm should help to ensure that your cache data is not
duplicated and the data is evenly distributed.


File: manual.info.tmp,  Node: ha-memcached-using-dtrace,  Next: ha-memcached-using-memory,  Prev: ha-memcached-using-hashtypes,  Up: ha-memcached-using

16.2.2.6 Using 'memcached' and DTrace
.....................................

'memcached' includes a number of different DTrace probes that can be
used to monitor the operation of the server.  The probes included can
monitor individual connections, slab allocations, and modifications to
the hash table when a key-value pair is added, updated, or removed.

For more information on DTrace and writing DTrace scripts, read the
DTrace User Guide (http://docs.oracle.com/cd/E19253-01/819-5488/).

Support for DTrace probes was added to 'memcached' 1.2.6 includes a
number of DTrace probes that can be used to help monitor your
application.  DTrace is supported on Solaris 10, OpenSolaris, OS X 10.5
and FreeBSD. To enable the DTrace probes in 'memcached', build from
source and use the '--enable-dtrace' option.  For more information, see
*note ha-memcached-install::.

The probes supported by 'memcached' are:

   * 'conn-allocate(connid)'

     Fired when a connection object is allocated from the connection
     pool.

        * 'connid': The connection ID.

   * 'conn-release(connid)'

     Fired when a connection object is released back to the connection
     pool.

     Arguments:

        * 'connid': The connection ID.

   * 'conn-create(ptr)'

     Fired when a new connection object is being created (that is, there
     are no free connection objects in the connection pool).

     Arguments:

        * 'ptr': A pointer to the connection.  object

   * 'conn-destroy(ptr)'

     Fired when a connection object is being destroyed.

     Arguments:

        * 'ptr': A pointer to the connection object.

   * 'conn-dispatch(connid, threadid)'

     Fired when a connection is dispatched from the main or
     connection-management thread to a worker thread.

     Arguments:

        * 'connid': The connection ID.

        * 'threadid': The thread ID.

   * 'slabs-allocate(size, slabclass, slabsize, ptr)'

     Allocate memory from the slab allocator.

     Arguments:

        * 'size': The requested size.

        * 'slabclass': The allocation is fulfilled in this class.

        * 'slabsize': The size of each item in this class.

        * 'ptr': A pointer to allocated memory.

   * 'slabs-allocate-failed(size, slabclass)'

     Failed to allocate memory (out of memory).

     Arguments:

        * 'size': The requested size.

        * 'slabclass': The class that failed to fulfill the request.

   * 'slabs-slabclass-allocate(slabclass)'

     Fired when a slab class needs more space.

     Arguments:

        * 'slabclass': The class that needs more memory.

   * 'slabs-slabclass-allocate-failed(slabclass)'

     Failed to allocate memory (out of memory).

     Arguments:

        * 'slabclass': The class that failed to grab more memory.

   * 'slabs-free(size, slabclass, ptr)'

     Release memory.

     Arguments:

        * 'size': The amount of memory to release, in bytes.

        * 'slabclass': The class the memory belongs to.

        * 'ptr': A pointer to the memory to release.

   * 'assoc-find(key, depth)'

     Fired when we have searched the hash table for a named key.  These
     two elements provide an insight into how well the hash function
     operates.  Traversals are a sign of a less optimal function,
     wasting CPU capacity.

     Arguments:

        * 'key': The key searched for.

        * 'depth': The depth in the list of hash table.

   * 'assoc-insert(key, nokeys)'

     Fired when a new item has been inserted.

     Arguments:

        * 'key': The key just inserted.

        * 'nokeys': The total number of keys currently being stored,
          including the key for which insert was called.

   * 'assoc-delete(key, nokeys)'

     Fired when a new item has been removed.

     Arguments:

        * 'key': The key just deleted.

        * 'nokeys': The total number of keys currently being stored,
          excluding the key for which delete was called.

   * 'item-link(key, size)'

     Fired when an item is being linked in the cache.

     Arguments:

        * 'key': The items key.

        * 'size': The size of the data.

   * 'item-unlink(key, size)'

     Fired when an item is being deleted.

     Arguments:

        * 'key': The items key.

        * 'size': The size of the data.

   * 'item-remove(key, size)'

     Fired when the refcount for an item is reduced.

     Arguments:

        * 'key': The item's key.

        * 'size': The size of the data.

   * 'item-update(key, size)'

     Fired when the "last referenced" time is updated.

     Arguments:

        * 'key': The item's key.

        * 'size': The size of the data.

   * 'item-replace(oldkey, oldsize, newkey, newsize)'

     Fired when an item is being replaced with another item.

     Arguments:

        * 'oldkey': The key of the item to replace.

        * 'oldsize': The size of the old item.

        * 'newkey': The key of the new item.

        * 'newsize': The size of the new item.

   * 'process-command-start(connid, request, size)'

     Fired when the processing of a command starts.

     Arguments:

        * 'connid': The connection ID.

        * 'request': The incoming request.

        * 'size': The size of the request.

   * 'process-command-end(connid, response, size)'

     Fired when the processing of a command is done.

     Arguments:

        * 'connid': The connection ID.

        * 'response': The response to send back to the client.

        * 'size': The size of the response.

   * 'command-get(connid, key, size)'

     Fired for a 'get' command.

     Arguments:

        * 'connid': The connection ID.

        * 'key': The requested key.

        * 'size': The size of the key's data (or -1 if not found).

   * 'command-gets(connid, key, size, casid)'

     Fired for a 'gets' command.

     Arguments:

        * 'connid': The connection ID.

        * 'key': The requested key.

        * 'size': The size of the key's data (or -1 if not found).

        * 'casid': The casid for the item.

   * 'command-add(connid, key, size)'

     Fired for a 'add' command.

     Arguments:

        * 'connid': The connection ID.

        * 'key': The requested key.

        * 'size': The new size of the key's data (or -1 if not found).

   * 'command-set(connid, key, size)'

     Fired for a 'set' command.

     Arguments:

        * 'connid': The connection ID.

        * 'key': The requested key.

        * 'size': The new size of the key's data (or -1 if not found).

   * 'command-replace(connid, key, size)'

     Fired for a 'replace' command.

     Arguments:

        * 'connid': The connection ID.

        * 'key': The requested key.

        * 'size': The new size of the key's data (or -1 if not found).

   * 'command-prepend(connid, key, size)'

     Fired for a 'prepend' command.

     Arguments:

        * 'connid': The connection ID.

        * 'key': The requested key.

        * 'size': The new size of the key's data (or -1 if not found).

   * 'command-append(connid, key, size)'

     Fired for a 'append' command.

     Arguments:

        * 'connid': The connection ID.

        * 'key': The requested key.

        * 'size': The new size of the key's data (or -1 if not found).

   * 'command-cas(connid, key, size, casid)'

     Fired for a 'cas' command.

     Arguments:

        * 'connid': The connection ID.

        * 'key': The requested key.

        * 'size': The size of the key's data (or -1 if not found).

        * 'casid': The cas ID requested.

   * 'command-incr(connid, key, val)'

     Fired for 'incr' command.

     Arguments:

        * 'connid': The connection ID.

        * 'key': The requested key.

        * 'val': The new value.

   * 'command-decr(connid, key, val)'

     Fired for 'decr' command.

     Arguments:

        * 'connid': The connection ID.

        * 'key': The requested key.

        * 'val': The new value.

   * 'command-delete(connid, key, exptime)'

     Fired for a 'delete' command.

     Arguments:

        * 'connid': The connection ID.

        * 'key': The requested key.

        * 'exptime': The expiry time.


File: manual.info.tmp,  Node: ha-memcached-using-memory,  Next: ha-memcached-using-threads,  Prev: ha-memcached-using-dtrace,  Up: ha-memcached-using

16.2.2.7 Memory Allocation within 'memcached'
.............................................

When you first start 'memcached', the memory that you have configured is
not automatically allocated.  Instead, 'memcached' only starts
allocating and reserving physical memory once you start saving
information into the cache.

When you start to store data into the cache, 'memcached' does not
allocate the memory for the data on an item by item basis.  Instead, a
slab allocation is used to optimize memory usage and prevent memory
fragmentation when information expires from the cache.

With slab allocation, memory is reserved in blocks of 1MB. The slab is
divided up into a number of blocks of equal size.  When you try to store
a value into the cache, 'memcached' checks the size of the value that
you are adding to the cache and determines which slab contains the right
size allocation for the item.  If a slab with the item size already
exists, the item is written to the block within the slab.

If the new item is bigger than the size of any existing blocks, then a
new slab is created, divided up into blocks of a suitable size.  If an
existing slab with the right block size already exists, but there are no
free blocks, a new slab is created.  If you update an existing item with
data that is larger than the existing block allocation for that key,
then the key is re-allocated into a suitable slab.

For example, the default size for the smallest block is 88 bytes (40
bytes of value, and the default 48 bytes for the key and flag data).  If
the size of the first item you store into the cache is less than 40
bytes, then a slab with a block size of 88 bytes is created and the
value stored.

If the size of the data that you intend to store is larger than this
value, then the block size is increased by the chunk size factor until a
block size large enough to hold the value is determined.  The block size
is always a function of the scale factor, rounded up to a block size
which is exactly divisible into the chunk size.

For a sample of the structure, see *note ha-memcached-fig-slabs::.

FIGURE GOES HERE: Memory Allocation in 'memcached'

The result is that you have multiple pages allocated within the range of
memory allocated to 'memcached'.  Each page is 1MB in size (by default),
and is split into a different number of chunks, according to the chunk
size required to store the key-value pairs.  Each instance has multiple
pages allocated, and a page is always created when a new item needs to
be created requiring a chunk of a particular size.  A slab may consist
of multiple pages, and each page within a slab contains an equal number
of chunks.

The chunk size of a new slab is determined by the base chunk size
combined with the chunk size growth factor.  For example, if the initial
chunks are 104 bytes in size, and the default chunk size growth factor
is used (1.25), then the next chunk size allocated would be the best
power of 2 fit for 104*1.25, or 136 bytes.

Allocating the pages in this way ensures that memory does not get
fragmented.  However, depending on the distribution of the objects that
you store, it may lead to an inefficient distribution of the slabs and
chunks if you have significantly different sized items.  For example,
having a relatively small number of items within each chunk size may
waste a lot of memory with just few chunks in each allocated page.

You can tune the growth factor to reduce this effect by using the '-f'
command line option, which adapts the growth factor applied to make more
effective use of the chunks and slabs allocated.  For information on how
to determine the current slab allocation statistics, see *note
ha-memcached-stats-slabs::.

If your operating system supports it, you can also start 'memcached'
with the '-L' command line option.  This option preallocates all the
memory during startup using large memory pages.  This can improve
performance by reducing the number of misses in the CPU memory cache.


File: manual.info.tmp,  Node: ha-memcached-using-threads,  Next: ha-memcached-using-logs,  Prev: ha-memcached-using-memory,  Up: ha-memcached-using

16.2.2.8 'memcached' Thread Support
...................................

If you enable the thread implementation within when building 'memcached'
from source, then 'memcached' uses multiple threads in addition to the
'libevent' system to handle requests.

When enabled, the threading implementation operates as follows:

   * Threading is handled by wrapping functions within the code to
     provide basic protection from updating the same global structures
     at the same time.

   * Each thread uses its own instance of the 'libevent' to help improve
     performance.

   * TCP/IP connections are handled with a single thread listening on
     the TCP/IP socket.  Each connection is then distributed to one of
     the active threads on a simple round-robin basis.  Each connection
     then operates solely within this thread while the connection
     remains open.

   * For UDP connections, all the threads listen to a single UDP socket
     for incoming requests.  Threads that are not currently dealing with
     another request ignore the incoming packet.  One of the remaining,
     nonbusy, threads reads the request and sends the response.  This
     implementation can lead to increased CPU load as threads wake from
     sleep to potentially process the request.

Using threads can increase the performance on servers that have multiple
CPU cores available, as the requests to update the hash table can be
spread between the individual threads.  To minimize overhead from the
locking mechanism employed, experiment with different thread values to
achieve the best performance based on the number and type of requests
within your given workload.


File: manual.info.tmp,  Node: ha-memcached-using-logs,  Prev: ha-memcached-using-threads,  Up: ha-memcached-using

16.2.2.9 'memcached' Logs
.........................

If you enable verbose mode, using the '-v', '-vv', or '-vvv' options,
then the information output by 'memcached' includes details of the
operations being performed.

Without the verbose options, 'memcached' normally produces no output
during normal operating.

   * *Output when using '-v'*

     The lowest verbosity level shows you:

        * Errors and warnings

        * Transient errors

        * Protocol and socket errors, including exhausting available
          connections

        * Each registered client connection, including the socket
          descriptor number and the protocol used.

          For example:

               32: Client using the ascii protocol
               33: Client using the ascii protocol

          The socket descriptor is only valid while the client remains
          connected.  Non-persistent connections may not be effectively
          represented.

     Examples of the error messages output at this level include:

          <%d send buffer was %d, now %d
          Can't listen for events on fd %d
          Can't read from libevent pipe
          Catastrophic: event fd doesn't match conn fd!
          Couldn't build response
          Couldn't realloc input buffer
          Couldn't update event
          Failed to build UDP headers
          Failed to read, and not due to blocking
          Too many open connections
          Unexpected state %d

   * *Output when using '-vv'*

     When using the second level of verbosity, you get more detailed
     information about protocol operations, keys updated, chunk and
     network operatings and details.

     During the initial start-up of 'memcached' with this level of
     verbosity, you are shown the sizes of the individual slab classes,
     the chunk sizes, and the number of entries per slab.  These do not
     show the allocation of the slabs, just the slabs that would be
     created when data is added.  You are also given information about
     the listen queues and buffers used to send information.  A sample
     of the output generated for a TCP/IP based system with the default
     memory and growth factors is given below:

          shell> memcached -vv
          slab class   1: chunk size     80 perslab 13107
          slab class   2: chunk size    104 perslab 10082
          slab class   3: chunk size    136 perslab  7710
          slab class   4: chunk size    176 perslab  5957
          slab class   5: chunk size    224 perslab  4681
          slab class   6: chunk size    280 perslab  3744
          slab class   7: chunk size    352 perslab  2978
          slab class   8: chunk size    440 perslab  2383
          slab class   9: chunk size    552 perslab  1899
          slab class  10: chunk size    696 perslab  1506
          slab class  11: chunk size    872 perslab  1202
          slab class  12: chunk size   1096 perslab   956
          slab class  13: chunk size   1376 perslab   762
          slab class  14: chunk size   1720 perslab   609
          slab class  15: chunk size   2152 perslab   487
          slab class  16: chunk size   2696 perslab   388
          slab class  17: chunk size   3376 perslab   310
          slab class  18: chunk size   4224 perslab   248
          slab class  19: chunk size   5280 perslab   198
          slab class  20: chunk size   6600 perslab   158
          slab class  21: chunk size   8256 perslab   127
          slab class  22: chunk size  10320 perslab   101
          slab class  23: chunk size  12904 perslab    81
          slab class  24: chunk size  16136 perslab    64
          slab class  25: chunk size  20176 perslab    51
          slab class  26: chunk size  25224 perslab    41
          slab class  27: chunk size  31536 perslab    33
          slab class  28: chunk size  39424 perslab    26
          slab class  29: chunk size  49280 perslab    21
          slab class  30: chunk size  61600 perslab    17
          slab class  31: chunk size  77000 perslab    13
          slab class  32: chunk size  96256 perslab    10
          slab class  33: chunk size 120320 perslab     8
          slab class  34: chunk size 150400 perslab     6
          slab class  35: chunk size 188000 perslab     5
          slab class  36: chunk size 235000 perslab     4
          slab class  37: chunk size 293752 perslab     3
          slab class  38: chunk size 367192 perslab     2
          slab class  39: chunk size 458992 perslab     2
          <26 server listening (auto-negotiate)
          <29 server listening (auto-negotiate)
          <30 send buffer was 57344, now 2097152
          <31 send buffer was 57344, now 2097152
          <30 server listening (udp)
          <30 server listening (udp)
          <31 server listening (udp)
          <30 server listening (udp)
          <30 server listening (udp)
          <31 server listening (udp)
          <31 server listening (udp)
          <31 server listening (udp)

     Using this verbosity level can be a useful way to check the effects
     of the growth factor used on slabs with different memory
     allocations, which in turn can be used to better tune the growth
     factor to suit the data you are storing in the cache.  For example,
     if you set the growth factor to 4 (quadrupling the size of each
     slab):

          shell> memcached -f 4 -m 1g -vv
          slab class   1: chunk size     80 perslab 13107
          slab class   2: chunk size    320 perslab  3276
          slab class   3: chunk size   1280 perslab   819
          slab class   4: chunk size   5120 perslab   204
          slab class   5: chunk size  20480 perslab    51
          slab class   6: chunk size  81920 perslab    12
          slab class   7: chunk size 327680 perslab     3
          ...

     During use of the cache, this verbosity level also prints out
     detailed information on the storage and recovery of keys and other
     information.  An example of the output during a typical set/get and
     increment/decrement operation is shown below.

          32: Client using the ascii protocol
          <32 set my_key 0 0 10
          >32 STORED
          <32 set object_key 1 0 36
          >32 STORED
          <32 get my_key
          >32 sending key my_key
          >32 END
          <32 get object_key
          >32 sending key object_key
          >32 END
          <32 set key 0 0 6
          >32 STORED
          <32 incr key 1
          >32 789544
          <32 decr key 1
          >32 789543
          <32 incr key 2
          >32 789545
          <32 set my_key 0 0 10
          >32 STORED
          <32 set object_key 1 0 36
          >32 STORED
          <32 get my_key
          >32 sending key my_key
          >32 END
          <32 get object_key
          >32 sending key object_key1 1 36

          >32 END
          <32 set key 0 0 6
          >32 STORED
          <32 incr key 1
          >32 789544
          <32 decr key 1
          >32 789543
          <32 incr key 2
          >32 789545

     During client communication, for each line, the initial character
     shows the direction of flow of the information.  The < for
     communication from the client to the 'memcached' server and > for
     communication back to the client.  The number is the numeric socket
     descriptor for the connection.

   * *Output when using '-vvv'*

     This level of verbosity includes the transitions of connections
     between different states in the event library while reading and
     writing content to/from the clients.  It should be used to diagnose
     and identify issues in client communication.  For example, you can
     use this information to determine if 'memcached' is taking a long
     time to return information to the client, during the read of the
     client operation or before returning and completing the operation.
     An example of the typical sequence for a set operation is provided
     below:

          <32 new auto-negotiating client connection
          32: going from conn_new_cmd to conn_waiting
          32: going from conn_waiting to conn_read
          32: going from conn_read to conn_parse_cmd
          32: Client using the ascii protocol
          <32 set my_key 0 0 10
          32: going from conn_parse_cmd to conn_nread
          > NOT FOUND my_key
          >32 STORED
          32: going from conn_nread to conn_write
          32: going from conn_write to conn_new_cmd
          32: going from conn_new_cmd to conn_waiting
          32: going from conn_waiting to conn_read
          32: going from conn_read to conn_closing
          <32 connection closed.

All of the verbosity levels in 'memcached' are designed to be used
during debugging or examination of issues.  The quantity of information
generated, particularly when using '-vvv', is significant, particularly
on a busy server.  Also be aware that writing the error information out,
especially to disk, may negate some of the performance gains you achieve
by using 'memcached'.  Therefore, use in production or deployment
environments is not recommended.


File: manual.info.tmp,  Node: ha-memcached-interfaces,  Next: ha-memcached-stats,  Prev: ha-memcached-using,  Up: ha-memcached

16.2.3 Developing a 'memcached' Application
-------------------------------------------

* Menu:

* ha-memcached-operations::      Basic 'memcached' Operations
* ha-memcached-mysql-frontend::  Using 'memcached' as a MySQL Caching Layer
* ha-memcached-interfaces-libmemcached::  Using 'libmemcached' with C and C++
* ha-memcached-interfaces-perl::  Using MySQL and 'memcached' with Perl
* ha-memcached-interfaces-python::  Using MySQL and 'memcached' with Python
* ha-memcached-interfaces-php::  Using MySQL and 'memcached' with PHP
* ha-memcached-interfaces-ruby::  Using MySQL and 'memcached' with Ruby
* ha-memcached-interfaces-java::  Using MySQL and 'memcached' with Java
* ha-memcached-interfaces-protocol::  Using the 'memcached' TCP Text Protocol

A number of language interfaces let applications store and retrieve
information with 'memcached' servers.  You can write 'memcached'
applications in popular languages such as Perl, PHP, Python, Ruby, C,
and Java.

Data stored into a 'memcached' server is referred to by a single string
(the key), with storage into the cache and retrieval from the cache
using the key as the reference.  The cache therefore operates like a
large associative array or hash table.  It is not possible to structure
or otherwise organize the information stored in the cache.  To emulate
database notions such as multiple tables or composite key values, you
must encode the extra information into the strings used as keys.  For
example, to store or look up the address corresponding to a specific
latitude and longitude, you might turn those two numeric values into a
single comma-separated string to use as a key.


File: manual.info.tmp,  Node: ha-memcached-operations,  Next: ha-memcached-mysql-frontend,  Prev: ha-memcached-interfaces,  Up: ha-memcached-interfaces

16.2.3.1 Basic 'memcached' Operations
.....................................

The interface to 'memcached' supports the following methods for storing
and retrieving information in the cache, and these are consistent across
all the different APIs, although the language specific mechanics might
be different:

   * 'get(KEY)': Retrieves information from the cache.  Returns the
     value associated with the key if the specified key exists.  Returns
     'NULL', 'nil', 'undefined', or the closest equivalent in the
     corresponding language, if the specified key does not exist.

   * 'set(KEY, VALUE [, EXPIRY])': Sets the item associated with a key
     in the cache to the specified value.  This either updates an
     existing item if the key already exists, or adds a new key-value
     pair if the key doesn't exist.  If the expiry time is specified,
     then the item expires (and is deleted) when the expiry time is
     reached.  The time is specified in seconds, and is taken as a
     relative time if the value is less than 30 days (30*24*60*60), or
     an absolute time (epoch) if larger than this value.

   * 'add(KEY, VALUE [, EXPIRY])': Adds the key and associated value to
     the cache, if the specified key does not already exist.

   * 'replace(KEY, VALUE [, EXPIRY])': Replaces the item associated with
     the specified 'key', only if the key already exists.  The new value
     is given by the 'value' parameter.

   * 'delete(KEY [, TIME])': Deletes the 'key' and its associated item
     from the cache.  If you supply a 'time', then adding another item
     with the specified 'key' is blocked for the specified period.

   * 'incr(KEY , VALUE)': Increments the item associated with the 'key'
     by the specified 'value'.

   * 'decr(KEY , VALUE)': Decrements the item associated with the 'key'
     by the specified 'value'.

   * 'flush_all': Invalidates (or expires) all the current items in the
     cache.  Technically they still exist (they are not deleted), but
     they are silently destroyed the next time you try to access them.

In all implementations, most or all of these functions are duplicated
through the corresponding native language interface.

When practical, use 'memcached' to store full items, rather than caching
a single column value from the database.  For example, when displaying a
record about an object (invoice, user history, or blog post), load all
the data for the associated entry from the database, and compile it into
the internal structure that would normally be required by the
application.  Save the complete object in the cache.

Complex data structures cannot be stored directly.  Most interfaces
serialize the data for you, that is, put it in a textual form that can
reconstruct the original pointers and nesting.  Perl uses 'Storable',
PHP uses 'serialize', Python uses 'cPickle' (or 'Pickle') and Java uses
the 'Serializable' interface.  In most cases, the serialization
interface used is customizable.  To share data stored in 'memcached'
instances between different language interfaces, consider using a common
serialization solution such as JSON (Javascript Object Notation).


File: manual.info.tmp,  Node: ha-memcached-mysql-frontend,  Next: ha-memcached-interfaces-libmemcached,  Prev: ha-memcached-operations,  Up: ha-memcached-interfaces

16.2.3.2 Using 'memcached' as a MySQL Caching Layer
...................................................

When using 'memcached' to cache MySQL data, your application must
retrieve data from the database and load the appropriate key-value pairs
into the cache.  Then, subsequent lookups can be done directly from the
cache.

Because MySQL has its own in-memory caching mechanisms for queried data,
such as the 'InnoDB' buffer pool and the MySQL query cache, look for
opportunities beyond loading individual column values or rows into the
cache.  Prefer to cache composite values, such as those retrieved from
multiple tables through a join query, or result sets assembled from
multiple rows.

*Caution*:

Limit the information in the cache to non-sensitive data, because there
is no security required to access or update the information within a
'memcached' instance.  Anybody with access to the machine has the
ability to read, view and potentially update the information.  To keep
the data secure, encrypt the information before caching it.  To restrict
the users capable of connecting to the server, either disable network
access, or use IPTables or similar techniques to restrict access to the
'memcached' ports to a select set of hosts.

You can introduce 'memcached' to an existing application, even if
caching was not part of the original design.  In many languages and
environments the changes to the application will be just a few lines,
first to attempt to read from the cache when loading data, fall back to
the old method if the information is not cached, and to update the cache
with information once the data has been read.

The general sequence for using 'memcached' in any language as a caching
solution for MySQL is as follows:

  1. Request the item from the cache.

  2. If the item exists, use the item data.

  3. If the item does not exist, load the data from MySQL, and store the
     value into the cache.  This means the value is available to the
     next client that requests it from the cache.

For a flow diagram of this sequence, see *note
ha-memcached-fig-basicflow::.

FIGURE GOES HERE: Typical 'memcached' Application Flowchart

*Adapting Database Best Practices to 'memcached' Applications*

The most direct way to cache MySQL data is to use a 2-column table,
where the first column is a primary key.  Because of the uniqueness
requirements for 'memcached' keys, make sure your database schema makes
appropriate use of primary keys and unique constraints.

If you combine multiple column values into a single 'memcached' item
value, choose data types to make it easy to parse the value back into
its components, for example by using a separator character between
numeric values.

The queries that map most easily to 'memcached' lookups are those with a
single 'WHERE' clause, using an '=' or 'IN' operator.  For complicated
'WHERE' clauses, or those using operators such as '<', '>', 'BETWEEN',
or 'LIKE', 'memcached' does not provide a simple or efficient way to
scan through or filter the keys or associated values, so typically you
perform those operations as SQL queries on the underlying database.


File: manual.info.tmp,  Node: ha-memcached-interfaces-libmemcached,  Next: ha-memcached-interfaces-perl,  Prev: ha-memcached-mysql-frontend,  Up: ha-memcached-interfaces

16.2.3.3 Using 'libmemcached' with C and C++
............................................

* Menu:

* ha-memcached-interfaces-libmemcached-base::  'libmemcached' Base Functions
* ha-memcached-interfaces-libmemcached-servers::  'libmemcached' Server Functions
* ha-memcached-interfaces-libmemcached-set::  'libmemcached' Set Functions
* ha-memcached-interfaces-libmemcached-get::  'libmemcached' Get Functions
* ha-memcached-interfaces-libmemcached-behaviors::  Controlling 'libmemcached' Behaviors
* ha-memcached-interfaces-libmemcached-utilities::  'libmemcached' Command-Line Utilities

The 'libmemcached' library provides both C and C++ interfaces to
'memcached' and is also the basis for a number of different additional
API implementations, including Perl, Python and Ruby.  Understanding the
core 'libmemcached' functions can help when using these other
interfaces.

The C library is the most comprehensive interface library for
'memcached' and provides functions and operational systems not always
exposed in interfaces not based on the 'libmemcached' library.

The different functions can be divided up according to their basic
operation.  In addition to functions that interface to the core API, a
number of utility functions provide extended functionality, such as
appending and prepending data.

To build and install 'libmemcached', download the 'libmemcached'
package, run 'configure', and then build and install:

     shell> tar xjf libmemcached-0.21.tar.gz
     shell> cd libmemcached-0.21
     shell> ./configure
     shell> make
     shell> make install

On many Linux operating systems, you can install the corresponding
'libmemcached' package through the usual 'yum', 'apt-get', or similar
commands.

To build an application that uses the library, first set the list of
servers.  Either directly manipulate the servers configured within the
main 'memcached_st' structure, or separately populate a list of servers,
and then add this list to the 'memcached_st' structure.  The latter
method is used in the following example.  Once the server list has been
set, you can call the functions to store or retrieve data.  A simple
application for setting a preset value to 'localhost' is provided here:

     #include <stdio.h>
     #include <string.h>
     #include <unistd.h>
     #include <libmemcached/memcached.h>

     int main(int argc, char *argv[])
     {
       memcached_server_st *servers = NULL;
       memcached_st *memc;
       memcached_return rc;
       char *key= "keystring";
       char *value= "keyvalue";

       memcached_server_st *memcached_servers_parse (char *server_strings);
       memc= memcached_create(NULL);

       servers= memcached_server_list_append(servers, "localhost", 11211, &rc);
       rc= memcached_server_push(memc, servers);

       if (rc == MEMCACHED_SUCCESS)
         fprintf(stderr,"Added server successfully\n");
       else
         fprintf(stderr,"Couldn't add server: %s\n",memcached_strerror(memc, rc));

       rc= memcached_set(memc, key, strlen(key), value, strlen(value), (time_t)0, (uint32_t)0);

       if (rc == MEMCACHED_SUCCESS)
         fprintf(stderr,"Key stored successfully\n");
       else
         fprintf(stderr,"Couldn't store key: %s\n",memcached_strerror(memc, rc));

       return 0;
     }

To test the success of an operation, use the return value, or populated
result code, for a given function.  The value is always set to
'MEMCACHED_SUCCESS' if the operation succeeded.  In the event of a
failure, use the 'memcached_strerror()' function to translate the result
code into a printable string.

To build the application, specify the 'memcached' library:

     shell> gcc -o memc_basic memc_basic.c -lmemcached

Running the above sample application, after starting a 'memcached'
server, should return a success message:

     shell> memc_basic
     Added server successfully
     Key stored successfully


File: manual.info.tmp,  Node: ha-memcached-interfaces-libmemcached-base,  Next: ha-memcached-interfaces-libmemcached-servers,  Prev: ha-memcached-interfaces-libmemcached,  Up: ha-memcached-interfaces-libmemcached

16.2.3.4 'libmemcached' Base Functions
......................................

The base 'libmemcached' functions let you create, destroy and clone the
main 'memcached_st' structure that is used to interface with the
'memcached' servers.  The main functions are defined below:

     memcached_st *memcached_create (memcached_st *ptr);

Creates a new 'memcached_st' structure for use with the other
'libmemcached' API functions.  You can supply an existing, static,
'memcached_st' structure, or 'NULL' to have a new structured allocated.
Returns a pointer to the created structure, or 'NULL' on failure.

     void memcached_free (memcached_st *ptr);

Frees the structure and memory allocated to a previously created
'memcached_st' structure.

     memcached_st *memcached_clone(memcached_st *clone, memcached_st *source);

Clones an existing 'memcached' structure from the specified 'source',
copying the defaults and list of servers defined in the structure.


File: manual.info.tmp,  Node: ha-memcached-interfaces-libmemcached-servers,  Next: ha-memcached-interfaces-libmemcached-set,  Prev: ha-memcached-interfaces-libmemcached-base,  Up: ha-memcached-interfaces-libmemcached

16.2.3.5 'libmemcached' Server Functions
........................................

The 'libmemcached' API uses a list of servers, stored within the
'memcached_server_st' structure, to act as the list of servers used by
the rest of the functions.  To use 'memcached', you first create the
server list, and then apply the list of servers to a valid
'libmemcached' object.

Because the list of servers, and the list of servers within an active
'libmemcached' object can be manipulated separately, you can update and
manage server lists while an active 'libmemcached' interface is running.

The functions for manipulating the list of servers within a
'memcached_st' structure are:

     memcached_return
        memcached_server_add (memcached_st *ptr,
                              char *hostname,
                              unsigned int port);

Adds a server, using the given 'hostname' and 'port' into the
'memcached_st' structure given in 'ptr'.

     memcached_return
        memcached_server_add_unix_socket (memcached_st *ptr,
                                          char *socket);

Adds a Unix socket to the list of servers configured in the
'memcached_st' structure.

     unsigned int memcached_server_count (memcached_st *ptr);

Returns a count of the number of configured servers within the
'memcached_st' structure.

     memcached_server_st *
        memcached_server_list (memcached_st *ptr);

Returns an array of all the defined hosts within a 'memcached_st'
structure.

     memcached_return
        memcached_server_push (memcached_st *ptr,
                               memcached_server_st *list);

Pushes an existing list of servers onto list of servers configured for a
current 'memcached_st' structure.  This adds servers to the end of the
existing list, and duplicates are not checked.

The 'memcached_server_st' structure can be used to create a list of
'memcached' servers which can then be applied individually to
'memcached_st' structures.

     memcached_server_st *
        memcached_server_list_append (memcached_server_st *ptr,
                                      char *hostname,
                                      unsigned int port,
                                      memcached_return *error);

Adds a server, with 'hostname' and 'port', to the server list in 'ptr'.
The result code is handled by the 'error' argument, which should point
to an existing 'memcached_return' variable.  The function returns a
pointer to the returned list.

     unsigned int memcached_server_list_count (memcached_server_st *ptr);

Returns the number of the servers in the server list.

     void memcached_server_list_free (memcached_server_st *ptr);

Frees the memory associated with a server list.

     memcached_server_st *memcached_servers_parse (char *server_strings);

Parses a string containing a list of servers, where individual servers
are separated by a comma, space, or both, and where individual servers
are of the form 'SERVER[:PORT]'.  The return value is a server list
structure.


File: manual.info.tmp,  Node: ha-memcached-interfaces-libmemcached-set,  Next: ha-memcached-interfaces-libmemcached-get,  Prev: ha-memcached-interfaces-libmemcached-servers,  Up: ha-memcached-interfaces-libmemcached

16.2.3.6 'libmemcached' Set Functions
.....................................

The set-related functions within 'libmemcached' provide the same
functionality as the core functions supported by the 'memcached'
protocol.  The full definition for the different functions is the same
for all the base functions ('add', 'replace', 'prepend', 'append').  For
example, the function definition for 'memcached_set()' is:

     memcached_return
        memcached_set (memcached_st *ptr,
                       const char *key,
                       size_t key_length,
                       const char *value,
                       size_t value_length,
                       time_t expiration,
                       uint32_t flags);

The 'ptr' is the 'memcached_st' structure.  The 'key' and 'key_length'
define the key name and length, and 'value' and 'value_length' the
corresponding value and length.  You can also set the expiration and
optional flags.  For more information, see *note
ha-memcached-interfaces-libmemcached-behaviors::.

The following table outlines the remainder of the set-related
'libmemcached' functions and the equivalent core functions supported by
the 'memcached' protocol.

'libmemcached' Function              Equivalent Core Function
                                     
'memcached_set(memc, key,            Generic 'set()' operation.
key_length, value, value_length,     
expiration, flags)'

'memcached_add(memc, key,            Generic 'add()' function.
key_length, value, value_length,     
expiration, flags)'

'memcached_replace(memc, key,        Generic 'replace()'.
key_length, value, value_length,     
expiration, flags)'

'memcached_prepend(memc, key,        Prepends the specified 'value'
key_length, value, value_length,     before the current value of the
expiration, flags)'                  specified 'key'.
                                     
'memcached_append(memc, key,         Appends the specified 'value'
key_length, value, value_length,     after the current value of the
expiration, flags)'                  specified 'key'.
                                     
'memcached_cas(memc, key,            Overwrites the data for a given
key_length, value, value_length,     key as long as the corresponding
expiration, flags, cas)'             'cas' value is still the same
                                     within the server.
                                     
'memcached_set_by_key(memc,          Similar to the generic 'set()',
master_key, master_key_length,       but has the option of an
key, key_length, value,              additional master key that can be
value_length, expiration, flags)'    used to identify an individual
                                     server.
                                     
'memcached_add_by_key(memc,          Similar to the generic 'add()',
master_key, master_key_length,       but has the option of an
key, key_length, value,              additional master key that can be
value_length, expiration, flags)'    used to identify an individual
                                     server.
                                     
'memcached_replace_by_key(memc,      Similar to the generic
master_key, master_key_length,       'replace()', but has the option of
key, key_length, value,              an additional master key that can
value_length, expiration, flags)'    be used to identify an individual
                                     server.
                                     
'memcached_prepend_by_key(memc,      Similar to the
master_key, master_key_length,       'memcached_prepend()', but has the
key, key_length, value,              option of an additional master key
value_length, expiration, flags)'    that can be used to identify an
                                     individual server.
                                     
'memcached_append_by_key(memc,       Similar to the
master_key, master_key_length,       'memcached_append()', but has the
key, key_length, value,              option of an additional master key
value_length, expiration, flags)'    that can be used to identify an
                                     individual server.
                                     
'memcached_cas_by_key(memc,          Similar to the 'memcached_cas()',
master_key, master_key_length,       but has the option of an
key, key_length, value,              additional master key that can be
value_length, expiration, flags)'    used to identify an individual
                                     server.

The 'by_key' methods add two further arguments that define the master
key, to be used and applied during the hashing stage for selecting the
servers.  You can see this in the following definition:

     memcached_return
        memcached_set_by_key(memcached_st *ptr,
                             const char *master_key,
                             size_t master_key_length,
                             const char *key,
                             size_t key_length,
                             const char *value,
                             size_t value_length,
                             time_t expiration,
                             uint32_t flags);

All the functions return a value of type 'memcached_return', which you
can compare against the 'MEMCACHED_SUCCESS' constant.


File: manual.info.tmp,  Node: ha-memcached-interfaces-libmemcached-get,  Next: ha-memcached-interfaces-libmemcached-behaviors,  Prev: ha-memcached-interfaces-libmemcached-set,  Up: ha-memcached-interfaces-libmemcached

16.2.3.7 'libmemcached' Get Functions
.....................................

The 'libmemcached' functions provide both direct access to a single
item, and a multiple-key request mechanism that provides much faster
responses when fetching a large number of keys simultaneously.

The main get-style function, which is equivalent to the generic 'get()'
is 'memcached_get()'.  This function returns a string pointer, pointing
to the value associated with the specified key.

     char *memcached_get (memcached_st *ptr,
                          const char *key, size_t key_length,
                          size_t *value_length,
                          uint32_t *flags,
                          memcached_return *error);

A multi-key get, 'memcached_mget()', is also available.  Using a
multiple key get operation is much quicker to do in one block than
retrieving the key values with individual calls to 'memcached_get()'.
To start the multi-key get, call 'memcached_mget()':

     memcached_return
         memcached_mget (memcached_st *ptr,
                         char **keys, size_t *key_length,
                         unsigned int number_of_keys);

The return value is the success of the operation.  The 'keys' parameter
should be an array of strings containing the keys, and 'key_length' an
array containing the length of each corresponding key.  'number_of_keys'
is the number of keys supplied in the array.

To fetch the individual values, use 'memcached_fetch()' to get each
corresponding value.

     char *memcached_fetch (memcached_st *ptr,
                            const char *key, size_t *key_length,
                            size_t *value_length,
                            uint32_t *flags,
                            memcached_return *error);

The function returns the key value, with the 'key', 'key_length' and
'value_length' parameters being populated with the corresponding key and
length information.  The function returns 'NULL' when there are no more
values to be returned.  A full example, including the populating of the
key data and the return of the information is provided here.

     #include <stdio.h>
     #include <sstring.h>
     #include <unistd.h>
     #include <libmemcached/memcached.h>

     int main(int argc, char *argv[])
     {
       memcached_server_st *servers = NULL;
       memcached_st *memc;
       memcached_return rc;
       char *keys[]= {"huey", "dewey", "louie"};
       size_t key_length[3];
       char *values[]= {"red", "blue", "green"};
       size_t value_length[3];
       unsigned int x;
       uint32_t flags;

       char return_key[MEMCACHED_MAX_KEY];
       size_t return_key_length;
       char *return_value;
       size_t return_value_length;

       memc= memcached_create(NULL);

       servers= memcached_server_list_append(servers, "localhost", 11211, &rc);
       rc= memcached_server_push(memc, servers);

       if (rc == MEMCACHED_SUCCESS)
         fprintf(stderr,"Added server successfully\n");
       else
         fprintf(stderr,"Couldn't add server: %s\n",memcached_strerror(memc, rc));

       for(x= 0; x < 3; x++)
         {
           key_length[x] = strlen(keys[x]);
           value_length[x] = strlen(values[x]);

           rc= memcached_set(memc, keys[x], key_length[x], values[x],
                             value_length[x], (time_t)0, (uint32_t)0);
           if (rc == MEMCACHED_SUCCESS)
             fprintf(stderr,"Key %s stored successfully\n",keys[x]);
           else
             fprintf(stderr,"Couldn't store key: %s\n",memcached_strerror(memc, rc));
         }

       rc= memcached_mget(memc, keys, key_length, 3);

       if (rc == MEMCACHED_SUCCESS)
         {
           while ((return_value= memcached_fetch(memc, return_key, &return_key_length,
                                                 &return_value_length, &flags, &rc)) != NULL)
             {
               if (rc == MEMCACHED_SUCCESS)
                 {
                   fprintf(stderr,"Key %s returned %s\n",return_key, return_value);
                 }
             }
         }

       return 0;
     }

Running the above application produces the following output:

     shell> memc_multi_fetch
     Added server successfully
     Key huey stored successfully
     Key dewey stored successfully
     Key louie stored successfully
     Key huey returned red
     Key dewey returned blue
     Key louie returned green


File: manual.info.tmp,  Node: ha-memcached-interfaces-libmemcached-behaviors,  Next: ha-memcached-interfaces-libmemcached-utilities,  Prev: ha-memcached-interfaces-libmemcached-get,  Up: ha-memcached-interfaces-libmemcached

16.2.3.8 Controlling 'libmemcached' Behaviors
.............................................

The behavior of 'libmemcached' can be modified by setting one or more
behavior flags.  These can either be set globally, or they can be
applied during the call to individual functions.  Some behaviors also
accept an additional setting, such as the hashing mechanism used when
selecting servers.

To set global behaviors:

     memcached_return
        memcached_behavior_set (memcached_st *ptr,
                                memcached_behavior flag,
                                uint64_t data);

To get the current behavior setting:

     uint64_t
        memcached_behavior_get (memcached_st *ptr,
                                memcached_behavior flag);

The following table describes 'libmemcached' behavior flags.

Behavior                      Description
                              
'MEMCACHED_BEHAVIOR_NO_BLOCK' Caused 'libmemcached' to use asynchronous
                              I/O.
                              
'MEMCACHED_BEHAVIOR_TCP_NODELAY'Turns on no-delay for network sockets.
                              
'MEMCACHED_BEHAVIOR_HASH'     Without a value, sets the default hashing
                              algorithm for keys to use MD5.  Other
                              valid values include
                              'MEMCACHED_HASH_DEFAULT',
                              'MEMCACHED_HASH_MD5',
                              'MEMCACHED_HASH_CRC',
                              'MEMCACHED_HASH_FNV1_64',
                              'MEMCACHED_HASH_FNV1A_64',
                              'MEMCACHED_HASH_FNV1_32', and
                              'MEMCACHED_HASH_FNV1A_32'.
                              
'MEMCACHED_BEHAVIOR_DISTRIBUTION'Changes the method of selecting the
                              server used to store a given value.  The
                              default method is
                              'MEMCACHED_DISTRIBUTION_MODULA'.  You can
                              enable consistent hashing by setting
                              'MEMCACHED_DISTRIBUTION_CONSISTENT'.
                              'MEMCACHED_DISTRIBUTION_CONSISTENT' is an
                              alias for the value
                              'MEMCACHED_DISTRIBUTION_CONSISTENT_KETAMA'.
                              
'MEMCACHED_BEHAVIOR_CACHE_LOOKUPS'Cache the lookups made to the DNS
                              service.  This can improve the
                              performance if you are using names
                              instead of IP addresses for individual
                              hosts.
                              
'MEMCACHED_BEHAVIOR_SUPPORT_CAS'Support CAS operations.  By default, this
                              is disabled because it imposes a
                              performance penalty.
                              
'MEMCACHED_BEHAVIOR_KETAMA'   Sets the default distribution to
                              'MEMCACHED_DISTRIBUTION_CONSISTENT_KETAMA'
                              and the hash to 'MEMCACHED_HASH_MD5'.
                              
'MEMCACHED_BEHAVIOR_POLL_TIMEOUT'Modify the timeout value used by
                              'poll()'.  Supply a 'signed int' pointer
                              for the timeout value.
                              
'MEMCACHED_BEHAVIOR_BUFFER_REQUESTS'Buffers IO requests instead of them being
                              sent.  A get operation, or closing the
                              connection causes the data to be flushed.
                              
'MEMCACHED_BEHAVIOR_VERIFY_KEY'Forces 'libmemcached' to verify that a
                              specified key is valid.
                              
'MEMCACHED_BEHAVIOR_SORT_HOSTS'If set, hosts added to the list of
                              configured hosts for a 'memcached_st'
                              structure are placed into the host list
                              in sorted order.  This breaks consistent
                              hashing if that behavior has been
                              enabled.
                              
'MEMCACHED_BEHAVIOR_CONNECT_TIMEOUT'In nonblocking mode this changes the
                              value of the timeout during socket
                              connection.


File: manual.info.tmp,  Node: ha-memcached-interfaces-libmemcached-utilities,  Prev: ha-memcached-interfaces-libmemcached-behaviors,  Up: ha-memcached-interfaces-libmemcached

16.2.3.9 'libmemcached' Command-Line Utilities
..............................................

In addition to the main C library interface, 'libmemcached' also
includes a number of command-line utilities that can be useful when
working with and debugging 'memcached' applications.

All of the command-line tools accept a number of arguments, the most
critical of which is 'servers', which specifies the list of servers to
connect to when returning information.

The main tools are:

   * 'memcat': Display the value for each ID given on the command line:

          shell> memcat --servers=localhost hwkey
          Hello world

   * 'memcp': Copy the contents of a file into the cache, using the file
     name as the key:

          shell> echo "Hello World" > hwkey
          shell> memcp --servers=localhost hwkey
          shell> memcat --servers=localhost hwkey
          Hello world

   * 'memrm': Remove an item from the cache:

          shell> memcat --servers=localhost hwkey
          Hello world
          shell> memrm --servers=localhost hwkey
          shell> memcat --servers=localhost hwkey

   * 'memslap': Test the load on one or more 'memcached' servers,
     simulating get/set and multiple client operations.  For example,
     you can simulate the load of 100 clients performing get operations:

          shell> memslap --servers=localhost --concurrency=100 --flush --test=get
          memslap --servers=localhost --concurrency=100 --flush --test=get	Threads connecting to servers 100
          	Took 13.571 seconds to read data

   * 'memflush': Flush (empty) the contents of the 'memcached' cache.

          shell> memflush --servers=localhost


File: manual.info.tmp,  Node: ha-memcached-interfaces-perl,  Next: ha-memcached-interfaces-python,  Prev: ha-memcached-interfaces-libmemcached,  Up: ha-memcached-interfaces

16.2.3.10 Using MySQL and 'memcached' with Perl
...............................................

The 'Cache::Memcached' module provides a native interface to the
Memcache protocol, and provides support for the core functions offered
by 'memcached'.  Install the module using your operating system's
package management system, or using 'CPAN':

     root-shell> perl -MCPAN -e 'install Cache::Memcached'

To use 'memcached' from Perl through the 'Cache::Memcached' module,
first create a new 'Cache::Memcached' object that defines the list of
servers and other parameters for the connection.  The only argument is a
hash containing the options for the cache interface.  For example, to
create a new instance that uses three 'memcached' servers:

     use Cache::Memcached;

     my $cache = new Cache::Memcached {
         'servers' => [
             '198.51.100.100:11211',
             '198.51.100.101:11211',
             '198.51.100.102:11211',
     	],
     };

*Note*:

When using the 'Cache::Memcached' interface with multiple servers, the
API automatically performs certain operations across all the servers in
the group.  For example, getting statistical information through
'Cache::Memcached' returns a hash that contains data on a host-by-host
basis, as well as generalized statistics for all the servers in the
group.

You can set additional properties on the cache object instance when it
is created by specifying the option as part of the option hash.
Alternatively, you can use a corresponding method on the instance:

   * 'servers' or method 'set_servers()': Specifies the list of the
     servers to be used.  The servers list should be a reference to an
     array of servers, with each element as the address and port number
     combination (separated by a colon).  You can also specify a local
     connection through a Unix socket (for example
     '/tmp/sock/memcached').  To specify the server with a weight
     (indicating how much more frequently the server should be used
     during hashing), specify an array reference with the 'memcached'
     server instance and a weight number.  Higher numbers give higher
     priority.

   * 'compress_threshold' or method 'set_compress_threshold()':
     Specifies the threshold when values are compressed.  Values larger
     than the specified number are automatically compressed (using
     'zlib') during storage and retrieval.

   * 'no_rehash' or method 'set_norehash()': Disables finding a new
     server if the original choice is unavailable.

   * 'readonly' or method 'set_readonly()': Disables writes to the
     'memcached' servers.

Once the 'Cache::Memcached' object instance has been configured, you can
use the 'set()' and 'get()' methods to store and retrieve information
from the 'memcached' servers.  Objects stored in the cache are
automatically serialized and deserialized using the 'Storable' module.

The 'Cache::Memcached' interface supports the following methods for
storing/retrieving data, and relate to the generic methods as shown in
the table.

'Cache::Memcached' Function          Equivalent Generic Method
                                     
'get()'                              Generic 'get()'.
                                     
'get_multi(keys)'                    Gets multiple 'keys' from memcache
                                     using just one query.  Returns a
                                     hash reference of key-value pairs.
                                     
'set()'                              Generic 'set()'.
                                     
'add()'                              Generic 'add()'.
                                     
'replace()'                          Generic 'replace()'.
                                     
'delete()'                           Generic 'delete()'.
                                     
'incr()'                             Generic 'incr()'.
                                     
'decr()'                             Generic 'decr()'.

Below is a complete example for using 'memcached' with Perl and the
'Cache::Memcached' module:

     #!/usr/bin/perl

     use Cache::Memcached;
     use DBI;
     use Data::Dumper;

     # Configure the memcached server

     my $cache = new Cache::Memcached {
         'servers' => [
                        'localhost:11211',
                        ],
         };

     # Get the film name from the command line
     # memcached keys must not contain spaces, so create
     # a key name by replacing spaces with underscores

     my $filmname = shift or die "Must specify the film name\n";
     my $filmkey = $filmname;
     $filmkey =~ s/ /_/;

     # Load the data from the cache

     my $filmdata = $cache->get($filmkey);

     # If the data wasn't in the cache, then we load it from the database

     if (!defined($filmdata))
     {
         $filmdata = load_filmdata($filmname);

         if (defined($filmdata))
         {

     # Set the data into the cache, using the key

     	if ($cache->set($filmkey,$filmdata))
             {
                 print STDERR "Film data loaded from database and cached\n";
             }
             else
             {
                 print STDERR "Couldn't store to cache\n";
     	}
         }
         else
         {
          	die "Couldn't find $filmname\n";
         }
     }
     else
     {
         print STDERR "Film data loaded from Memcached\n";
     }

     sub load_filmdata
     {
         my ($filmname) = @_;

         my $dsn = "DBI:mysql:database=sakila;host=localhost;port=3306";

         $dbh = DBI->connect($dsn, 'sakila','PASSWORD');

         my ($filmbase) = $dbh->selectrow_hashref(sprintf('select * from film where title = %s',
                                                          $dbh->quote($filmname)));

         if (!defined($filmname))
         {
          	return (undef);
         }

         $filmbase->{stars} =
     	$dbh->selectall_arrayref(sprintf('select concat(first_name," ",last_name) ' .
                                              'from film_actor left join (actor) ' .
                                              'on (film_actor.actor_id = actor.actor_id) ' .
                                              ' where film_id=%s',
                                              $dbh->quote($filmbase->{film_id})));

         return($filmbase);
     }

The example uses the Sakila database, obtaining film data from the
database and writing a composite record of the film and actors to
'memcached'.  When calling it for a film does not exist, you get this
result:

     shell> memcached-sakila.pl "ROCK INSTINCT"
     Film data loaded from database and cached

When accessing a film that has already been added to the cache:

     shell> memcached-sakila.pl "ROCK INSTINCT"
     Film data loaded from Memcached


File: manual.info.tmp,  Node: ha-memcached-interfaces-python,  Next: ha-memcached-interfaces-php,  Prev: ha-memcached-interfaces-perl,  Up: ha-memcached-interfaces

16.2.3.11 Using MySQL and 'memcached' with Python
.................................................

The Python 'memcache' module interfaces to 'memcached' servers, and is
written in pure Python (that is, without using one of the C APIs).  You
can download and install a copy from Python Memcached
(http://www.tummy.com/Community/software/python-memcached/).

To install, download the package and then run the Python installer:

     python setup.py install
     running install
     running bdist_egg
     running egg_info
     creating python_memcached.egg-info
     ...
     removing 'build/bdist.linux-x86_64/egg' (and everything under it)
     Processing python_memcached-1.43-py2.4.egg
     creating /usr/lib64/python2.4/site-packages/python_memcached-1.43-py2.4.egg
     Extracting python_memcached-1.43-py2.4.egg to /usr/lib64/python2.4/site-packages
     Adding python-memcached 1.43 to easy-install.pth file

     Installed /usr/lib64/python2.4/site-packages/python_memcached-1.43-py2.4.egg
     Processing dependencies for python-memcached==1.43
     Finished processing dependencies for python-memcached==1.43

Once installed, the 'memcache' module provides a class-based interface
to your 'memcached' servers.  When you store Python data structures as
'memcached' items, they are automatically serialized (turned into string
values) using the Python 'cPickle' or 'pickle' modules.

To create a new 'memcache' interface, import the 'memcache' module and
create a new instance of the 'memcache.Client' class.  For example, if
the 'memcached' daemon is running on localhost using the default port:

     import memcache
     memc = memcache.Client(['127.0.0.1:11211'])

The first argument is an array of strings containing the server and port
number for each 'memcached' instance to use.  To enable debugging, set
the optional 'debug' parameter to 1.

By default, the hashing mechanism used to divide the items among
multiple servers is 'crc32'.  To change the function used, set the value
of 'memcache.serverHashFunction' to the alternate function to use.  For
example:

     from zlib import adler32
     memcache.serverHashFunction = adler32

Once you have defined the servers to use within the 'memcache' instance,
the core functions provide the same functionality as in the generic
interface specification.  The following table provides a summary of the
supported functions.

Python 'memcache' Function           Equivalent Generic Function
                                     
'get()'                              Generic 'get()'.
                                     
'get_multi(keys)'                    Gets multiple values from the
                                     supplied array of 'keys'.  Returns
                                     a hash reference of key-value
                                     pairs.
                                     
'set()'                              Generic 'set()'.
                                     
'set_multi(dict [, expiry [,         Sets multiple key-value pairs from
key_prefix]])'                       the supplied 'dict'.
                                     
'add()'                              Generic 'add()'.
                                     
'replace()'                          Generic 'replace()'.
                                     
'prepend(key, value [, expiry])'     Prepends the supplied 'value' to
                                     the value of the existing 'key'.
                                     
'append(key, value [, expiry[)'      Appends the supplied 'value' to
                                     the value of the existing 'key'.
                                     
'delete()'                           Generic 'delete()'.
                                     
'delete_multi(keys [, expiry [,      Deletes all the keys from the hash
key_prefix]] )'                      matching each string in the array
                                     'keys'.
                                     
'incr()'                             Generic 'incr()'.
                                     
'decr()'                             Generic 'decr()'.

*Note*:

Within the Python 'memcache' module, all the '*_multi()'functions
support an optional 'key_prefix' parameter.  If supplied, then the
string is used as a prefix to all key lookups.  For example, if you
call:

     memc.get_multi(['a','b'], key_prefix='users:')

The function retrieves the keys 'users:a' and 'users:b' from the
servers.

Here is an example showing the storage and retrieval of information to a
'memcache' instance, loading the raw data from MySQL:

     import sys
     import MySQLdb
     import memcache

     memc = memcache.Client(['127.0.0.1:11211'], debug=1);

     try:
         conn = MySQLdb.connect (host = "localhost",
                                 user = "sakila",
                                 passwd = "PASSWORD",
                                 db = "sakila")
     except MySQLdb.Error, e:
          print "Error %d: %s" % (e.args[0], e.args[1])
          sys.exit (1)

     popularfilms = memc.get('top5films')

     if not popularfilms:
         cursor = conn.cursor()
         cursor.execute('select film_id,title from film order by rental_rate desc limit 5')
         rows = cursor.fetchall()
         memc.set('top5films',rows,60)
         print "Updated memcached with MySQL data"
     else:
         print "Loaded data from memcached"
         for row in popularfilms:
             print "%s, %s" % (row[0], row[1])

When executed for the first time, the data is loaded from the MySQL
database and stored to the 'memcached' server.

     shell> python memc_python.py
     Updated memcached with MySQL data

Because the data is automatically serialized using 'cPickle'/'pickle',
when you load the data back from 'memcached', you can use the object
directly.  In the example above, the information stored to 'memcached'
is in the form of rows from a Python DB cursor.  When accessing the
information (within the 60 second expiry time), the data is loaded from
'memcached' and dumped:

     shell> python memc_python.py
     Loaded data from memcached
     2, ACE GOLDFINGER
     7, AIRPLANE SIERRA
     8, AIRPORT POLLOCK
     10, ALADDIN CALENDAR
     13, ALI FOREVER

The serialization and deserialization happens automatically.  Because
serialization of Python data may be incompatible with other interfaces
and languages, you can change the serialization module used during
initialization.  For example, you might use JSON format when you store
complex data structures using a script written in one language, and
access them in a script written in a different language.


File: manual.info.tmp,  Node: ha-memcached-interfaces-php,  Next: ha-memcached-interfaces-ruby,  Prev: ha-memcached-interfaces-python,  Up: ha-memcached-interfaces

16.2.3.12 Using MySQL and 'memcached' with PHP
..............................................

PHP provides support for the Memcache functions through a PECL
extension.  To enable the PHP 'memcache' extensions, build PHP using the
'--enable-memcache' option to 'configure' when building from source.

If you are installing on a Red Hat-based server, you can install the
'php-pecl-memcache' RPM:

     root-shell> yum --install php-pecl-memcache

On Debian-based distributions, use the 'php-memcache' package.

To set global runtime configuration options, specify the configuration
option values within your 'php.ini' file.  The following table provides
the name, default value, and a description for each global runtime
configuration option.

Configuration option      Default        Description
                                         
'memcache.allow_failover' 1              Specifies whether another
                                         server in the list should be
                                         queried if the first server
                                         selected fails.
                                         
'memcache.max_failover_attempts'20       Specifies the number of
                                         servers to try before
                                         returning a failure.
                                         
'memcache.chunk_size'     8192           Defines the size of network
                                         chunks used to exchange data
                                         with the 'memcached' server.
                                         
'memcache.default_port'   11211          Defines the default port to
                                         use when communicating with
                                         the 'memcached' servers.
                                         
'memcache.hash_strategy'  standard       Specifies which hash strategy
                                         to use.  Set to 'consistent'
                                         to enable servers to be added
                                         or removed from the pool
                                         without causing the keys to be
                                         remapped to other servers.
                                         When set to 'standard', an
                                         older (modula) strategy is
                                         used that potentially uses
                                         different servers for storage.
                                         
'memcache.hash_function'  crc32          Specifies which function to
                                         use when mapping keys to
                                         servers.  'crc32' uses the
                                         standard CRC32 hash.  'fnv'
                                         uses the FNV-1a hashing
                                         algorithm.

To create a connection to a 'memcached' server, create a new 'Memcache'
object and then specify the connection options.  For example:

     <?php

     $cache = new Memcache;
     $cache->connect('localhost',11211);
     ?>

This opens an immediate connection to the specified server.

To use multiple 'memcached' servers, you need to add servers to the
memcache object using 'addServer()':

     bool Memcache::addServer ( string $host [, int $port [, bool $persistent
                      [, int $weight [, int $timeout [, int $retry_interval
                      [, bool $status [, callback $failure_callback
                      ]]]]]]] )

The server management mechanism within the 'php-memcache' module is a
critical part of the interface as it controls the main interface to the
'memcached' instances and how the different instances are selected
through the hashing mechanism.

To create a simple connection to two 'memcached' instances:

     <?php

     $cache = new Memcache;
     $cache->addServer('198.51.100.100',11211);
     $cache->addServer('198.51.100.101',11211);
     ?>

In this scenario, the instance connection is not explicitly opened, but
only opened when you try to store or retrieve a value.  To enable
persistent connections to 'memcached' instances, set the '$persistent'
argument to true.  This is the default setting, and causes the
connections to remain open.

To help control the distribution of keys to different instances, use the
global 'memcache.hash_strategy' setting.  This sets the hashing
mechanism used to select.  You can also add another weight to each
server, which effectively increases the number of times the instance
entry appears in the instance list, therefore increasing the likelihood
of the instance being chosen over other instances.  To set the weight,
set the value of the '$weight' argument to more than one.

The functions for setting and retrieving information are identical to
the generic functional interface offered by 'memcached', as shown in the
following table.

PECL 'memcache' Function             Generic Function
                                     
'get()'                              Generic 'get()'.
                                     
'set()'                              Generic 'set()'.
                                     
'add()'                              Generic 'add()'.
                                     
'replace()'                          Generic 'replace()'.
                                     
'delete()'                           Generic 'delete()'.
                                     
'increment()'                        Generic 'incr()'.
                                     
'decrement()'                        Generic 'decr()'.

A full example of the PECL 'memcache' interface is provided below.  The
code loads film data from the Sakila database when the user provides a
film name.  The data stored into the 'memcached' instance is recorded as
a 'mysqli' result row, and the API automatically serializes the
information for you.


     <?php

     $memc = new Memcache;
     $memc->addServer('localhost','11211');

     if(empty($_POST['film'])) {
     ?>
       <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
         <head>
           <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
           <title>Simple Memcache Lookup</title>
         </head>
         <body>
           <form method="post">
             <p><b>Film</b>: <input type="text" size="20" name="film"></p>
             <input type="submit">
           </form>
           <hr/>
     <?php

     } else {

         echo "Loading data...\n";

         $film   = htmlspecialchars($_POST['film'], ENT_QUOTES, 'UTF-8');
         $mfilms = $memc->get($film);

         if ($mfilms) {

             printf("<p>Film data for %s loaded from memcache</p>", $mfilms['title']);

             foreach (array_keys($mfilms) as $key) {
                 printf("<p><b>%s</b>: %s</p>", $key, $mfilms[$key]);
             }

         } else {

             $mysqli = mysqli('localhost','sakila','<replaceable>password</replaceable>','sakila');

             if (mysqli_connect_error()) {
                 sprintf("Database error: (%d) %s", mysqli_connect_errno(), mysqli_connect_error());
                 exit;
             }

             $sql = sprintf('SELECT * FROM film WHERE title="%s"', $mysqli->real_escape_string($film));

             $result = $mysqli->query($sql);

             if (!$result) {
                 sprintf("Database error: (%d) %s", $mysqli->errno, $mysqli->error);
                 exit;
             }

             $row = $result->fetch_assoc();

             $memc->set($row['title'], $row);

             printf("<p>Loaded (%s) from MySQL</p>", htmlspecialchars($row['title'], ENT_QUOTES, 'UTF-8');
         }
     }
     ?>
       </body>
     </html>

With PHP, the connections to the 'memcached' instances are kept open as
long as the PHP and associated Apache instance remain running.  When
adding or removing servers from the list in a running instance (for
example, when starting another script that mentions additional servers),
the connections are shared, but the script only selects among the
instances explicitly configured within the script.

To ensure that changes to the server list within a script do not cause
problems, make sure to use the consistent hashing mechanism.


File: manual.info.tmp,  Node: ha-memcached-interfaces-ruby,  Next: ha-memcached-interfaces-java,  Prev: ha-memcached-interfaces-php,  Up: ha-memcached-interfaces

16.2.3.13 Using MySQL and 'memcached' with Ruby
...............................................

There are a number of different modules for interfacing to 'memcached'
within Ruby.  The 'Ruby-MemCache' client library provides a native
interface to 'memcached' that does not require any external libraries,
such as 'libmemcached'.  You can obtain the installer package from
<http://www.deveiate.org/projects/RMemCache>.

To install, extract the package and then run 'install.rb':

     shell> install.rb

If you have RubyGems, you can install the 'Ruby-MemCache' gem:

     shell> gem install Ruby-MemCache
     Bulk updating Gem source index for: http://gems.rubyforge.org
     Install required dependency io-reactor? [Yn]  y
     Successfully installed Ruby-MemCache-0.0.1
     Successfully installed io-reactor-0.05
     Installing ri documentation for io-reactor-0.05...
     Installing RDoc documentation for io-reactor-0.05...

To use a 'memcached' instance from within Ruby, create a new instance of
the 'MemCache' object.

     require 'memcache'
     memc = MemCache::new '198.51.100.100:11211'

You can add a weight to each server to increase the likelihood of the
server being selected during hashing by appending the weight count to
the server host name/port string:

     require 'memcache'
     memc = MemCache::new '198.51.100.100:11211:3'

To add servers to an existing list, you can append them directly to the
'MemCache' object:

     memc += ["198.51.100.101:11211"]

To set data into the cache, you can just assign a value to a key within
the new cache object, which works just like a standard Ruby hash object:

     memc["key"] = "value"

Or to retrieve the value:

     print memc["key"]

For more explicit actions, you can use the method interface, which
mimics the main 'memcached' API functions, as summarized in the
following table.

Ruby 'MemCache' Method               Equivalent 'memcached' API
                                     Functions
                                     
'get()'                              Generic 'get()'.
                                     
'get_hash(keys)'                     Get the values of multiple 'keys',
                                     returning the information as a
                                     hash of the keys and their values.
                                     
'set()'                              Generic 'set()'.
                                     
'set_many(pairs)'                    Set the values of the keys and
                                     values in the hash 'pairs'.
                                     
'add()'                              Generic 'add()'.
                                     
'replace()'                          Generic 'replace()'.
                                     
'delete()'                           Generic 'delete()'.
                                     
'incr()'                             Generic 'incr()'.
                                     
'decr()'                             Generic 'decr()'.


File: manual.info.tmp,  Node: ha-memcached-interfaces-java,  Next: ha-memcached-interfaces-protocol,  Prev: ha-memcached-interfaces-ruby,  Up: ha-memcached-interfaces

16.2.3.14 Using MySQL and 'memcached' with Java
...............................................

The 'com.danga.MemCached' class within Java provides a native interface
to 'memcached' instances.  You can obtain the client from
<https://github.com/gwhalin/Memcached-Java-Client/downloads>.  The Java
class uses hashes that are compatible with 'libmemcached', so you can
mix and match Java and 'libmemcached' applications accessing the same
'memcached' instances.  The serialization between Java and other
interfaces are not compatible.  If this is a problem, use JSON or a
similar nonbinary serialization format.

On most systems, you can download the package and use the 'jar'
directly.

To use the 'com.danga.MemCached' interface, you create a
'MemCachedClient' instance and then configure the list of servers by
configuring the 'SockIOPool'.  Through the pool specification you set up
the server list, weighting, and the connection parameters to optimized
the connections between your client and the 'memcached' instances that
you configure.

Generally, you can configure the 'memcached' interface once within a
single class, then use this interface throughout the rest of your
application.

For example, to create a basic interface, first configure the
'MemCachedClient' and base 'SockIOPool' settings:

     public class MyClass {

         protected static MemCachedClient mcc = new MemCachedClient();

         static {

             String[] servers =
                 {
                     "localhost:11211",
                 };

             Integer[] weights = { 1 };

             SockIOPool pool = SockIOPool.getInstance();

             pool.setServers( servers );
             pool.setWeights( weights );

In the above sample, the list of servers is configured by creating an
array of the 'memcached' instances to use.  You can then configure
individual weights for each server.

The remainder of the properties for the connection are optional, but you
can set the connection numbers (initial connections, minimum
connections, maximum connections, and the idle timeout) by setting the
pool parameters:

     pool.setInitConn( 5 );
     pool.setMinConn( 5 );
     pool.setMaxConn( 250 );
     pool.setMaxIdle( 1000 * 60 * 60 * 6

Once the parameters have been configured, initialize the connection
pool:

     pool.initialize();

The pool, and the connection to your 'memcached' instances should now be
ready to use.

To set the hashing algorithm used to select the server used when storing
a given key, use 'pool.setHashingAlg()':

     pool.setHashingAlg( SockIOPool.NEW_COMPAT_HASH );

Valid values are 'NEW_COMPAT_HASH', 'OLD_COMPAT_HASH' and 'NATIVE_HASH'
are also basic modula hashing algorithms.  For a consistent hashing
algorithm, use 'CONSISTENT_HASH'.  These constants are equivalent to the
corresponding hash settings within 'libmemcached'.

The following table outlines the Java 'com.danga.MemCached' methods and
the equivalent generic methods in the 'memcached' interface
specification.

Java 'com.danga.MemCached' Method    Equivalent Generic Method
                                     
'get()'                              Generic 'get()'.
                                     
'getMulti(keys)'                     Get the values of multiple 'keys',
                                     returning the information as Hash
                                     map using 'java.lang.String' for
                                     the keys and 'java.lang.Object'
                                     for the corresponding values.
                                     
'set()'                              Generic 'set()'.
                                     
'add()'                              Generic 'add()'.
                                     
'replace()'                          Generic 'replace()'.
                                     
'delete()'                           Generic 'delete()'.
                                     
'incr()'                             Generic 'incr()'.
                                     
'decr()'                             Generic 'decr()'.


File: manual.info.tmp,  Node: ha-memcached-interfaces-protocol,  Prev: ha-memcached-interfaces-java,  Up: ha-memcached-interfaces

16.2.3.15 Using the 'memcached' TCP Text Protocol
.................................................

Communicating with a 'memcached' server can be achieved through either
the TCP or UDP protocols.  When using the TCP protocol, you can use a
simple text based interface for the exchange of information.

When communicating with 'memcached', you can connect to the server using
the port configured for the server.  You can open a connection with the
server without requiring authorization or login.  As soon as you have
connected, you can start to send commands to the server.  When you have
finished, you can terminate the connection without sending any specific
disconnection command.  Clients are encouraged to keep their connections
open to decrease latency and improve performance.

Data is sent to the 'memcached' server in two forms:

   * Text lines, which are used to send commands to the server, and
     receive responses from the server.

   * Unstructured data, which is used to receive or send the value
     information for a given key.  Data is returned to the client in
     exactly the format it was provided.

Both text lines (commands and responses) and unstructured data are
always terminated with the string '\r\n'.  Because the data being stored
may contain this sequence, the length of the data (returned by the
client before the unstructured data is transmitted should be used to
determine the end of the data.

Commands to the server are structured according to their operation:

   * *Storage commands*: 'set', 'add', 'replace', 'append', 'prepend',
     'cas'

     Storage commands to the server take the form:

          command key [flags] [exptime] length [noreply]

     Or when using compare and swap (cas):

          cas key [flags] [exptime] length [casunique] [noreply]

     Where:

        * 'command': The command name.

             * 'set': Store value against key

             * 'add': Store this value against key if the key does not
               already exist

             * 'replace': Store this value against key if the key
               already exists

             * 'append': Append the supplied value to the end of the
               value for the specified key.  The 'flags' and 'exptime'
               arguments should not be used.

             * 'prepend': Append value currently in the cache to the end
               of the supplied value for the specified key.  The 'flags'
               and 'exptime' arguments should not be used.

             * 'cas': Set the specified key to the supplied value, only
               if the supplied 'casunique' matches.  This is effectively
               the equivalent of change the information if nobody has
               updated it since I last fetched it.

        * 'key': The key.  All data is stored using a the specific key.
          The key cannot contain control characters or whitespace, and
          can be up to 250 characters in size.

        * 'flags': The flags for the operation (as an integer).  Flags
          in 'memcached' are transparent.  The 'memcached' server
          ignores the contents of the flags.  They can be used by the
          client to indicate any type of information.  In 'memcached'
          1.2.0 and lower the value is a 16-bit integer value.  In
          'memcached' 1.2.1 and higher the value is a 32-bit integer.

        * 'exptime': The expiry time, or zero for no expiry.

        * 'length': The length of the supplied value block in bytes,
          excluding the terminating '\r\n' characters.

        * 'casunique': A unique 64-bit value of an existing entry.  This
          is used to compare against the existing value.  Use the value
          returned by the 'gets' command when issuing 'cas' updates.

        * 'noreply': Tells the server not to reply to the command.

     For example, to store the value 'abcdef' into the key 'xyzkey', you
     would use:

          set xyzkey 0 0 6\r\nabcdef\r\n

     The return value from the server is one line, specifying the status
     or error information.  For more information, see *note
     ha-memcached-interfaces-protocol-responses::.

   * *Retrieval commands*: 'get', 'gets'

     Retrieval commands take the form:

          get key1 [key2 .... keyn]
          gets key1 [key2 ... keyn]

     You can supply multiple keys to the commands, with each requested
     key separated by whitespace.

     The server responds with an information line of the form:

          VALUE key flags bytes [casunique]

     Where:

        * 'key': The key name.

        * 'flags': The value of the flag integer supplied to the
          'memcached' server when the value was stored.

        * 'bytes': The size (excluding the terminating '\r\n' character
          sequence) of the stored value.

        * 'casunique': The unique 64-bit integer that identifies the
          item.

     The information line is immediately followed by the value data
     block.  For example:

          get xyzkey\r\n
          VALUE xyzkey 0 6\r\n
          abcdef\r\n

     If you have requested multiple keys, an information line and data
     block is returned for each key found.  If a requested key does not
     exist in the cache, no information is returned.

   * *Delete commands*: 'delete'

     Deletion commands take the form:

          delete key [time] [noreply]

     Where:

        * 'key': The key name.

        * 'time': The time in seconds (or a specific Unix time) for
          which the client wishes the server to refuse 'add' or
          'replace' commands on this key.  All 'add', 'replace', 'get',
          and 'gets' commands fail during this period.  'set' operations
          succeed.  After this period, the key is deleted permanently
          and all commands are accepted.

          If not supplied, the value is assumed to be zero (delete
          immediately).

        * 'noreply': Tells the server not to reply to the command.

     Responses to the command are either 'DELETED' to indicate that the
     key was successfully removed, or 'NOT_FOUND' to indicate that the
     specified key could not be found.

   * *Increment/Decrement*: 'incr', 'decr'

     The increment and decrement commands change the value of a key
     within the server without performing a separate get/set sequence.
     The operations assume that the currently stored value is a 64-bit
     integer.  If the stored value is not a 64-bit integer, then the
     value is assumed to be zero before the increment or decrement
     operation is applied.

     Increment and decrement commands take the form:

          incr key value [noreply]
          decr key value [noreply]

     Where:

        * 'key': The key name.

        * 'value': An integer to be used as the increment or decrement
          value.

        * 'noreply': Tells the server not to reply to the command.

     The response is:

        * 'NOT_FOUND': The specified key could not be located.

        * 'value': The new value associated with the specified key.

     Values are assumed to be unsigned.  For 'decr' operations, the
     value is never decremented below 0.  For 'incr' operations, the
     value wraps around the 64-bit maximum.

   * *Statistics commands*: 'stats'

     The 'stats' command provides detailed statistical information about
     the current status of the 'memcached' instance and the data it is
     storing.

     Statistics commands take the form:

          STAT [name] [value]

     Where:

        * 'name': The optional name of the statistics to return.  If not
          specified, the general statistics are returned.

        * 'value': A specific value to be used when performing certain
          statistics operations.

     The return value is a list of statistics data, formatted as
     follows:

          STAT name value

     The statistics are terminated with a single line, 'END'.

     For more information, see *note ha-memcached-stats::.

For reference, a list of the different commands supported and their
formats is provided below.

*memcached Command Reference*

Command            Command Formats
                   
'set'              'set key flags exptime length', 'set key flags
                   exptime length noreply'
                   
'add'              'add key flags exptime length', 'add key flags
                   exptime length noreply'
                   
'replace'          'replace key flags exptime length', 'replace key
                   flags exptime length noreply'
                   
'append'           'append key length', 'append key length noreply'
                   
'prepend'          'prepend key length', 'prepend key length noreply'
                   
'cas'              'cas key flags exptime length casunique', 'cas key
                   flags exptime length casunique noreply'
                   
'get'              'get key1 [key2 ... keyn]'
                   
'gets'             ''
                   
'delete'           'delete key', 'delete key noreply', 'delete key
                   expiry', 'delete key expiry noreply'
                   
'incr'             'incr key', 'incr key noreply', 'incr key value',
                   'incr key value noreply'
                   
'decr'             'decr key', 'decr key noreply', 'decr key value',
                   'decr key value noreply'
                   
'stat'             'stat', 'stat name', 'stat name value'

When sending a command to the server, the response from the server is
one of the settings in the following table.  All response values from
the server are terminated by '\r\n':

*memcached Protocol Responses*

String             Description
                   
'STORED'           Value has successfully been stored.
                   
'NOT_STORED'       The value was not stored, but not because of an
                   error.  For commands where you are adding a or
                   updating a value if it exists (such as 'add' and
                   'replace'), or where the item has already been set
                   to be deleted.
                   
'EXISTS'           When using a 'cas' command, the item you are trying
                   to store already exists and has been modified since
                   you last checked it.
                   
'NOT_FOUND'        The item you are trying to store, update or delete
                   does not exist or has already been deleted.
                   
'ERROR'            You submitted a nonexistent command name.
                   
'CLIENT_ERROR      There was an error in the input line, the detail is
errorstring'       contained in 'errorstring'.
                   
'SERVER_ERROR      There was an error in the server that prevents it
errorstring'       from returning the information.  In extreme
                   conditions, the server may disconnect the client
                   after this error occurs.
                   
'VALUE keys        The requested key has been found, and the stored
flags length'      'key', 'flags' and data block are returned, of the
                   specified 'length'.
                   
'DELETED'          The requested key was deleted from the server.
                   
'STAT name         A line of statistics data.
value'             

'END'              The end of the statistics data.


File: manual.info.tmp,  Node: ha-memcached-stats,  Next: ha-memcached-faq,  Prev: ha-memcached-interfaces,  Up: ha-memcached

16.2.4 Getting 'memcached' Statistics
-------------------------------------

* Menu:

* ha-memcached-stats-general::   'memcached' General Statistics
* ha-memcached-stats-slabs::     'memcached' Slabs Statistics
* ha-memcached-stats-items::     'memcached' Item Statistics
* ha-memcached-stats-sizes::     'memcached' Size Statistics
* ha-memcached-stats-detail::    'memcached' Detail Statistics
* ha-memcached-stats-memcached-tool::  Using 'memcached-tool'

The 'memcached' system has a built-in statistics system that collects
information about the data being stored into the cache, cache hit
ratios, and detailed information on the memory usage and distribution of
information through the slab allocation used to store individual items.
Statistics are provided at both a basic level that provide the core
statistics, and more specific statistics for specific areas of the
'memcached' server.

This information can be useful to ensure that you are getting the
correct level of cache and memory usage, and that your slab allocation
and configuration properties are set at an optimal level.

The stats interface is available through the standard 'memcached'
protocol, so the reports can be accessed by using 'telnet' to connect to
the 'memcached'.  The supplied 'memcached-tool' includes support for
obtaining the *note ha-memcached-stats-slabs:: and *note
ha-memcached-stats-general:: information.  For more information, see
*note ha-memcached-stats-memcached-tool::.

Alternatively, most of the language API interfaces provide a function
for obtaining the statistics from the server.

For example, to get the basic stats using 'telnet':

     shell> telnet localhost 11211
     Trying ::1...
     Connected to localhost.
     Escape character is '^]'.
     stats
     STAT pid 23599
     STAT uptime 675
     STAT time 1211439587
     STAT version 1.2.5
     STAT pointer_size 32
     STAT rusage_user 1.404992
     STAT rusage_system 4.694685
     STAT curr_items 32
     STAT total_items 56361
     STAT bytes 2642
     STAT curr_connections 53
     STAT total_connections 438
     STAT connection_structures 55
     STAT cmd_get 113482
     STAT cmd_set 80519
     STAT get_hits 78926
     STAT get_misses 34556
     STAT evictions 0
     STAT bytes_read 6379783
     STAT bytes_written 4860179
     STAT limit_maxbytes 67108864
     STAT threads 1
     END

When using Perl and the 'Cache::Memcached' module, the 'stats()'
function returns information about all the servers currently configured
in the connection object, and total statistics for all the 'memcached'
servers as a whole.

For example, the following Perl script obtains the stats and dumps the
hash reference that is returned:

     use Cache::Memcached;
     use Data::Dumper;

     my $memc = new Cache::Memcached;
     $memc->set_servers(\@ARGV);

     print Dumper($memc->stats());

When executed on the same 'memcached' as used in the 'Telnet' example
above we get a hash reference with the host by host and total
statistics:

     $VAR1 = {
         'hosts' => {
                'localhost:11211' => {
                           'misc' => {
                                 'bytes' => '2421',
                                 'curr_connections' => '3',
                                 'connection_structures' => '56',
                                 'pointer_size' => '32',
                                 'time' => '1211440166',
                                 'total_items' => '410956',
                                 'cmd_set' => '588167',
                                 'bytes_written' => '35715151',
                                 'evictions' => '0',
                                 'curr_items' => '31',
                                 'pid' => '23599',
                                 'limit_maxbytes' => '67108864',
                                 'uptime' => '1254',
                                 'rusage_user' => '9.857805',
                                 'cmd_get' => '838451',
                                 'rusage_system' => '34.096988',
                                 'version' => '1.2.5',
                                 'get_hits' => '581511',
                                 'bytes_read' => '46665716',
                                 'threads' => '1',
                                 'total_connections' => '3104',
                                 'get_misses' => '256940'
                               },
                           'sizes' => {
                                  '128' => '16',
                                  '64' => '15'
                                }
                         }
              },
         'self' => {},
         'total' => {
                'cmd_get' => 838451,
                'bytes' => 2421,
                'get_hits' => 581511,
                'connection_structures' => 56,
                'bytes_read' => 46665716,
                'total_items' => 410956,
                'total_connections' => 3104,
                'cmd_set' => 588167,
                'bytes_written' => 35715151,
                'curr_items' => 31,
                'get_misses' => 256940
              }
             };

The statistics are divided up into a number of distinct sections, and
then can be requested by adding the type to the 'stats' command.  Each
statistics output is covered in more detail in the following sections.

   * General statistics, see *note ha-memcached-stats-general::.

   * Slab statistics ('slabs'), see *note ha-memcached-stats-slabs::.

   * Item statistics ('items'), see *note ha-memcached-stats-items::.

   * Size statistics ('sizes'), see *note ha-memcached-stats-sizes::.

   * Detailed status ('detail'), see *note ha-memcached-stats-detail::.


File: manual.info.tmp,  Node: ha-memcached-stats-general,  Next: ha-memcached-stats-slabs,  Prev: ha-memcached-stats,  Up: ha-memcached-stats

16.2.4.1 'memcached' General Statistics
.......................................

The output of the general statistics provides an overview of the
performance and use of the 'memcached' instance.  The statistics
returned by the command and their meaning is shown in the following
table.

The following terms are used to define the value type for each
statistics value:

   * '32u': 32-bit unsigned integer

   * '64u': 64-bit unsigned integer

   * '32u:32u': Two 32-bit unsigned integers separated by a colon

   * 'String': Character string

Statistic      Data type   Description                              Version
                                                                    
'pid'          32u         Process ID of the 'memcached'
                           instance.
                           
'uptime'       32u         Uptime (in seconds) for this
                           'memcached' instance.
                           
'time'         32u         Current time (as epoch).
                           
'version'      string      Version string of this instance.
                           
'pointer_size' string      Size of pointers for this host
                           specified in bits (32 or 64).
                           
'rusage_user'  32u:32u     Total user time for this instance
                           (seconds:microseconds).
                           
'rusage_system'32u:32u     Total system time for this instance
                           (seconds:microseconds).
                           
'curr_items'   32u         Current number of items stored by this
                           instance.
                           
'total_items'  32u         Total number of items stored during
                           the life of this instance.
                           
'bytes'        64u         Current number of bytes used by this
                           server to store items.
                           
'curr_connections'32u      Current number of open connections.
                           
'total_connections'32u     Total number of connections opened
                           since the server started running.
                           
'connection_structures'32u Number of connection structures
                           allocated by the server.
                           
'cmd_get'      64u         Total number of retrieval requests
                           ('get' operations).
                           
'cmd_set'      64u         Total number of storage requests
                           ('set' operations).
                           
'get_hits'     64u         Number of keys that have been
                           requested and found present.
                           
'get_misses'   64u         Number of items that have been
                           requested and not found.
                           
'delete_hits'  64u         Number of keys that have been deleted    1.3.x
                           and found present.                       
                           
'delete_misses'64u         Number of items that have been delete    1.3.x
                           and not found.                           
                           
'incr_hits'    64u         Number of keys that have been            1.3.x
                           incremented and found present.           
                           
'incr_misses'  64u         Number of items that have been           1.3.x
                           incremented and not found.               
                           
'decr_hits'    64u         Number of keys that have been            1.3.x
                           decremented and found present.           
                           
'decr_misses'  64u         Number of items that have been           1.3.x
                           decremented and not found.               
                           
'cas_hits'     64u         Number of keys that have been compared   1.3.x
                           and swapped and found present.           
                           
'cas_misses'   64u         Number of items that have been           1.3.x
                           compared and swapped and not found.      
                           
'cas_badvalue' 64u         Number of keys that have been compared   1.3.x
                           and swapped, but the comparison          
                           (original) value did not match the
                           supplied value.
                           
'evictions'    64u         Number of valid items removed from
                           cache to free memory for new items.
                           
'bytes_read'   64u         Total number of bytes read by this
                           server from network.
                           
'bytes_written'64u         Total number of bytes sent by this
                           server to network.
                           
'limit_maxbytes'32u        Number of bytes this server is
                           permitted to use for storage.
                           
'threads'      32u         Number of worker threads requested.
                           
'conn_yields'  64u         Number of yields for connections         1.4.0
                           (related to the '-R' option).
                           

The most useful statistics from those given here are the number of cache
hits, misses, and evictions.

A large number of 'get_misses' may just be an indication that the cache
is still being populated with information.  The number should, over
time, decrease in comparison to the number of cache 'get_hits'.  If,
however, you have a large number of cache misses compared to cache hits
after an extended period of execution, it may be an indication that the
size of the cache is too small and you either need to increase the total
memory size, or increase the number of the 'memcached' instances to
improve the hit ratio.

A large number of 'evictions' from the cache, particularly in comparison
to the number of items stored is a sign that your cache is too small to
hold the amount of information that you regularly want to keep cached.
Instead of items being retained in the cache, items are being evicted to
make way for new items keeping the turnover of items in the cache high,
reducing the efficiency of the cache.


File: manual.info.tmp,  Node: ha-memcached-stats-slabs,  Next: ha-memcached-stats-items,  Prev: ha-memcached-stats-general,  Up: ha-memcached-stats

16.2.4.2 'memcached' Slabs Statistics
.....................................

To get the 'slabs' statistics, use the 'stats slabs' command, or the API
equivalent.

The slab statistics provide you with information about the slabs that
have created and allocated for storing information within the cache.
You get information both on each individual slab-class and total
statistics for the whole slab.

     STAT 1:chunk_size 104
     STAT 1:chunks_per_page 10082
     STAT 1:total_pages 1
     STAT 1:total_chunks 10082
     STAT 1:used_chunks 10081
     STAT 1:free_chunks 1
     STAT 1:free_chunks_end 10079
     STAT 9:chunk_size 696
     STAT 9:chunks_per_page 1506
     STAT 9:total_pages 63
     STAT 9:total_chunks 94878
     STAT 9:used_chunks 94878
     STAT 9:free_chunks 0
     STAT 9:free_chunks_end 0
     STAT active_slabs 2
     STAT total_malloced 67083616
     END

Individual stats for each slab class are prefixed with the slab ID. A
unique ID is given to each allocated slab from the smallest size up to
the largest.  The prefix number indicates the slab class number in
relation to the calculated chunk from the specified growth factor.
Hence in the example, 1 is the first chunk size and 9 is the 9th chunk
allocated size.

The parameters returned for each chunk size and a description of each
parameter are provided in the following table.

Statistic          Description                              Version
                                                            
'chunk_size'       Space allocated to each chunk within
                   this slab class.
                   
'chunks_per_page'  Number of chunks within a single page
                   for this slab class.
                   
'total_pages'      Number of pages allocated to this slab
                   class.
                   
'total_chunks'     Number of chunks allocated to the slab
                   class.
                   
'used_chunks'      Number of chunks allocated to an
                   item..
                   
'free_chunks'      Number of chunks not yet allocated to
                   items.
                   
'free_chunks_end'  Number of free chunks at the end of
                   the last allocated page.
                   
'get_hits'         Number of get hits to this chunk         1.3.x
                                                            
'cmd_set'          Number of set commands on this chunk     1.3.x
                                                            
'delete_hits'      Number of delete hits to this chunk      1.3.x
                                                            
'incr_hits'        Number of increment hits to this chunk   1.3.x
                                                            
'decr_hits'        Number of decrement hits to this chunk   1.3.x
                                                            
'cas_hits'         Number of CAS hits to this chunk         1.3.x
                                                            
'cas_badval'       Number of CAS hits on this chunk where   1.3.x
                   the existing value did not match         
                   
'mem_requested'    The true amount of memory of memory      1.4.1
                   requested within this chunk
                   

The following additional statistics cover the information for the entire
server, rather than on a chunk by chunk basis:

Statistic          Description                              Version
                                                            
'active_slabs'     Total number of slab classes
                   allocated.
                   
'total_malloced'   Total amount of memory allocated to
                   slab pages.

The key values in the slab statistics are the 'chunk_size', and the
corresponding 'total_chunks' and 'used_chunks' parameters.  These given
an indication of the size usage of the chunks within the system.
Remember that one key-value pair is placed into a chunk of a suitable
size.

From these stats, you can get an idea of your size and chunk allocation
and distribution.  If you store many items with a number of largely
different sizes, consider adjusting the chunk size growth factor to
increase in larger steps to prevent chunk and memory wastage.  A good
indication of a bad growth factor is a high number of different slab
classes, but with relatively few chunks actually in use within each
slab.  Increasing the growth factor creates fewer slab classes and
therefore makes better use of the allocated pages.


File: manual.info.tmp,  Node: ha-memcached-stats-items,  Next: ha-memcached-stats-sizes,  Prev: ha-memcached-stats-slabs,  Up: ha-memcached-stats

16.2.4.3 'memcached' Item Statistics
....................................

To get the 'items' statistics, use the 'stats items' command, or the API
equivalent.

The 'items' statistics give information about the individual items
allocated within a given slab class.

     STAT items:2:number 1
     STAT items:2:age 452
     STAT items:2:evicted 0
     STAT items:2:evicted_nonzero 0
     STAT items:2:evicted_time 2
     STAT items:2:outofmemory 0
     STAT items:2:tailrepairs 0
     ...
     STAT items:27:number 1
     STAT items:27:age 452
     STAT items:27:evicted 0
     STAT items:27:evicted_nonzero 0
     STAT items:27:evicted_time 2
     STAT items:27:outofmemory 0
     STAT items:27:tailrepairs 0

The prefix number against each statistics relates to the corresponding
chunk size, as returned by the 'stats slabs' statistics.  The result is
a display of the number of items stored within each chunk within each
slab size, and specific statistics about their age, eviction counts, and
out of memory counts.  A summary of the statistics is given in the
following table.

Statistic          Description
                   
'number'           The number of items currently stored in this
                   slab class.
             