ct using encryption
like this:

     mysql --ssl-ca=ca.pem

To require that a client certificate also be specified, create the
account using a 'REQUIRE X509' clause.  Then the client must also
specify the proper client key and certificate files or the server will
reject the connection:

     mysql --ssl-ca=ca.pem \
           --ssl-cert=client-cert.pem \
           --ssl-key=client-key.pem

For additional information about the 'REQUIRE' clause, see *note
grant::.

To prevent use of encryption and override other '--ssl-XXX' options,
invoke the client program with '--ssl=0' or a synonym ('--skip-ssl',
'--disable-ssl'):

     mysql --ssl=0

To determine whether the current connection with the server uses
encryption, check the session value of the 'Ssl_cipher' status variable.
If the value is empty, the connection is not encrypted.  Otherwise, the
connection is encrypted and the value indicates the encryption cipher.
For example:

     mysql> SHOW SESSION STATUS LIKE 'Ssl_cipher';
     +---------------+--------------------+
     | Variable_name | Value              |
     +---------------+--------------------+
     | Ssl_cipher    | DHE-RSA-AES256-SHA |
     +---------------+--------------------+

For the *note 'mysql': mysql. client, an alternative is to use the
'STATUS' or '\s' command and check the 'SSL' line:

     mysql> \s
     ...
     SSL: Not in use
     ...

Or:

     mysql> \s
     ...
     SSL: Cipher in use is DHE-RSA-AES256-SHA
     ...


File: manual.info.tmp,  Node: encrypted-connection-protocols-ciphers,  Next: creating-ssl-files-using-openssl,  Prev: using-encrypted-connections,  Up: encrypted-connections

6.3.2 Encrypted Connection TLS Protocols and Ciphers
----------------------------------------------------

MySQL supports TLS encryption, and enables configuring which ciphers to
permit for encrypted connections.  It is also possible to determine
which TLS protocol and cipher the current session uses.

   * *note encrypted-connection-supported-protocols::

   * *note encrypted-connection-cipher-configuration::

   * *note encrypted-connection-protocol-monitoring::

*Supported Connection TLS Protocols*

MySQL supports encrypted connections using the TLSv1 protocol.  As of
MySQL 5.5.42, SSL 2.0 and SSL 3.0 are explicitly disabled because they
provide weak encryption.

*Connection Cipher Configuration*

A default set of ciphers applies to encrypted connections, which can be
overridden by explicitly configuring the permitted ciphers.  During
connection establishment, both sides of a connection must permit some
cipher in common or the connection fails.  Of the permitted ciphers
common to both sides, the SSL library chooses the one supported by the
provided certificate that has the highest priority.

To specify a cipher or ciphers for encrypted connections, use the
'--ssl-cipher' option, which is available for the server and for client
programs.

For master/slave replication, the 'MASTER_SSL_CIPHER' option for the
*note 'CHANGE MASTER TO': change-master-to. statement specifies which
ciphers a slave server permits for connections to the master.

To determine which ciphers a given server supports, check the session
value of the 'Ssl_cipher_list' status variable:

     SHOW SESSION STATUS LIKE 'Ssl_cipher_list';

The 'Ssl_cipher_list' status variable lists the possible SSL ciphers
(empty for non-SSL connections).  The set of available ciphers depends
on your MySQL version and whether MySQL was compiled using OpenSSL or
yaSSL, and (for OpenSSL) the library version used to compile MySQL.

MySQL passes a default cipher list to the SSL library.

MySQL passes this default cipher list to OpenSSL:

     AES256-GCM-SHA384
     AES256-SHA
     AES256-SHA256
     CAMELLIA256-SHA
     DES-CBC3-SHA
     DHE-DSS-AES256-GCM-SHA384
     DHE-DSS-AES256-SHA
     DHE-DSS-AES256-SHA256
     DHE-DSS-CAMELLIA256-SHA
     DHE-RSA-AES256-GCM-SHA384
     DHE-RSA-AES256-SHA
     DHE-RSA-AES256-SHA256
     DHE-RSA-CAMELLIA256-SHA
     ECDH-ECDSA-AES256-GCM-SHA384
     ECDH-ECDSA-AES256-SHA
     ECDH-ECDSA-AES256-SHA384
     ECDH-ECDSA-DES-CBC3-SHA
     ECDH-RSA-AES256-GCM-SHA384
     ECDH-RSA-AES256-SHA
     ECDH-RSA-AES256-SHA384
     ECDH-RSA-DES-CBC3-SHA
     ECDHE-ECDSA-AES128-GCM-SHA256
     ECDHE-ECDSA-AES128-SHA
     ECDHE-ECDSA-AES128-SHA256
     ECDHE-ECDSA-AES256-GCM-SHA384
     ECDHE-ECDSA-AES256-SHA
     ECDHE-ECDSA-AES256-SHA384
     ECDHE-ECDSA-DES-CBC3-SHA
     ECDHE-RSA-AES128-GCM-SHA256
     ECDHE-RSA-AES128-SHA
     ECDHE-RSA-AES128-SHA256
     ECDHE-RSA-AES256-GCM-SHA384
     ECDHE-RSA-AES256-SHA
     ECDHE-RSA-AES256-SHA384
     ECDHE-RSA-DES-CBC3-SHA
     EDH-DSS-DES-CBC3-SHA
     EDH-RSA-DES-CBC3-SHA
     PSK-3DES-EDE-CBC-SHA
     PSK-AES256-CBC-SHA
     SRP-DSS-3DES-EDE-CBC-SHA
     SRP-DSS-AES-128-CBC-SHA
     SRP-DSS-AES-256-CBC-SHA
     SRP-RSA-3DES-EDE-CBC-SHA
     SRP-RSA-AES-128-CBC-S
     SRP-RSA-AES-256-CBC-SHA

MySQL passes this default cipher list to yaSSL:

     AES128-RMD
     AES128-SHA
     AES256-RMD
     AES256-SHA
     DES-CBC-SHA
     DES-CBC3-RMD
     DES-CBC3-SHA
     DHE-RSA-AES128-RMD
     DHE-RSA-AES128-SHA
     DHE-RSA-AES256-RMD
     DHE-RSA-AES256-SHA
     DHE-RSA-DES-CBC3-RMD
     EDH-RSA-DES-CBC-SHA
     EDH-RSA-DES-CBC3-SHA
     RC4-MD5
     RC4-SHA

*Monitoring Current Session TLS Protocol and Cipher*

To determine which encryption TLS protocol and cipher the current
session uses, check the session values of the 'Ssl_version' and
'Ssl_cipher' status variables:

     mysql> SHOW SESSION STATUS LIKE 'Ssl_version';
     +---------------+-------+
     | Variable_name | Value |
     +---------------+-------+
     | Ssl_version   | TLSv1 |
     +---------------+-------+
     mysql> SHOW SESSION STATUS LIKE 'Ssl_cipher';
     +---------------+--------------------+
     | Variable_name | Value              |
     +---------------+--------------------+
     | Ssl_cipher    | DHE-RSA-AES256-SHA |
     +---------------+--------------------+

If the connection is not encrypted, both variables have an empty value.


File: manual.info.tmp,  Node: creating-ssl-files-using-openssl,  Next: ssl-libraries,  Prev: encrypted-connection-protocols-ciphers,  Up: encrypted-connections

6.3.3 Creating SSL Certificates and Keys Using openssl
------------------------------------------------------

This section describes how to use the 'openssl' command to set up SSL
certificate and key files for use by MySQL servers and clients.  The
first example shows a simplified procedure such as you might use from
the command line.  The second shows a script that contains more detail.
The first two examples are intended for use on Unix and both use the
'openssl' command that is part of OpenSSL. The third example describes
how to set up SSL files on Windows.

*Important*:

Whatever method you use to generate the certificate and key files, the
Common Name value used for the server and client certificates/keys must
each differ from the Common Name value used for the CA certificate.
Otherwise, the certificate and key files will not work for servers
compiled using OpenSSL. A typical error in this case is:

     ERROR 2026 (HY000): SSL connection error:
     error:00000001:lib(0):func(0):reason(1)

   * *note creating-ssl-files-using-openssl-unix-command-line::

   * *note creating-ssl-files-using-openssl-unix-script::

   * *note creating-ssl-files-using-openssl-windows::

*Example 1: Creating SSL Files from the Command Line on Unix*

The following example shows a set of commands to create MySQL server and
client certificate and key files.  You will need to respond to several
prompts by the 'openssl' commands.  To generate test files, you can
press Enter to all prompts.  To generate files for production use, you
should provide nonempty responses.

     # Create clean environment
     rm -rf newcerts
     mkdir newcerts && cd newcerts

     # Create CA certificate
     openssl genrsa 2048 > ca-key.pem
     openssl req -new -x509 -nodes -days 3600 \
             -key ca-key.pem -out ca.pem

     # Create server certificate, remove passphrase, and sign it
     # server-cert.pem = public key, server-key.pem = private key
     openssl req -newkey rsa:2048 -days 3600 \
             -nodes -keyout server-key.pem -out server-req.pem
     openssl rsa -in server-key.pem -out server-key.pem
     openssl x509 -req -in server-req.pem -days 3600 \
             -CA ca.pem -CAkey ca-key.pem -set_serial 01 -out server-cert.pem

     # Create client certificate, remove passphrase, and sign it
     # client-cert.pem = public key, client-key.pem = private key
     openssl req -newkey rsa:2048 -days 3600 \
             -nodes -keyout client-key.pem -out client-req.pem
     openssl rsa -in client-key.pem -out client-key.pem
     openssl x509 -req -in client-req.pem -days 3600 \
             -CA ca.pem -CAkey ca-key.pem -set_serial 01 -out client-cert.pem

After generating the certificates, verify them:

     openssl verify -CAfile ca.pem server-cert.pem client-cert.pem

You should see a response like this:

     server-cert.pem: OK
     client-cert.pem: OK

Now you have a set of files that can be used as follows:

   * 'ca.pem': Use this as the argument to '--ssl-ca' on the server and
     client sides.  (The CA certificate, if used, must be the same on
     both sides.)

   * 'server-cert.pem', 'server-key.pem': Use these as the arguments to
     '--ssl-cert' and '--ssl-key' on the server side.

   * 'client-cert.pem', 'client-key.pem': Use these as the arguments to
     '--ssl-cert' and '--ssl-key' on the client side.

For additional usage instructions, see *note
using-encrypted-connections::.

*Example 2: Creating SSL Files Using a Script on Unix*

Here is an example script that shows how to set up SSL certificate and
key files for MySQL. After executing the script, use the files for SSL
connections as described in *note using-encrypted-connections::.

     DIR=`pwd`/openssl
     PRIV=$DIR/private

     mkdir $DIR $PRIV $DIR/newcerts
     cp /usr/share/ssl/openssl.cnf $DIR
     replace ./demoCA $DIR -- $DIR/openssl.cnf

     # Create necessary files: $database, $serial and $new_certs_dir
     # directory (optional)

     touch $DIR/index.txt
     echo "01" > $DIR/serial

     #
     # Generation of Certificate Authority(CA)
     #

     openssl req -new -x509 -keyout $PRIV/cakey.pem -out $DIR/ca.pem \
         -days 3600 -config $DIR/openssl.cnf

     # Sample output:
     # Using configuration from /home/finley/openssl/openssl.cnf
     # Generating a 1024 bit RSA private key
     # ................++++++
     # .........++++++
     # writing new private key to '/home/finley/openssl/private/cakey.pem'
     # Enter PEM pass phrase:
     # Verifying password - Enter PEM pass phrase:
     # -----
     # You are about to be asked to enter information that will be
     # incorporated into your certificate request.
     # What you are about to enter is what is called a Distinguished Name
     # or a DN.
     # There are quite a few fields but you can leave some blank
     # For some fields there will be a default value,
     # If you enter '.', the field will be left blank.
     # -----
     # Country Name (2 letter code) [AU]:FI
     # State or Province Name (full name) [Some-State]:.
     # Locality Name (eg, city) []:
     # Organization Name (eg, company) [Internet Widgits Pty Ltd]:MySQL AB
     # Organizational Unit Name (eg, section) []:
     # Common Name (eg, YOUR name) []:MySQL admin
     # Email Address []:

     #
     # Create server request and key
     #
     openssl req -new -keyout $DIR/server-key.pem -out \
         $DIR/server-req.pem -days 3600 -config $DIR/openssl.cnf

     # Sample output:
     # Using configuration from /home/finley/openssl/openssl.cnf
     # Generating a 1024 bit RSA private key
     # ..++++++
     # ..........++++++
     # writing new private key to '/home/finley/openssl/server-key.pem'
     # Enter PEM pass phrase:
     # Verifying password - Enter PEM pass phrase:
     # -----
     # You are about to be asked to enter information that will be
     # incorporated into your certificate request.
     # What you are about to enter is what is called a Distinguished Name
     # or a DN.
     # There are quite a few fields but you can leave some blank
     # For some fields there will be a default value,
     # If you enter '.', the field will be left blank.
     # -----
     # Country Name (2 letter code) [AU]:FI
     # State or Province Name (full name) [Some-State]:.
     # Locality Name (eg, city) []:
     # Organization Name (eg, company) [Internet Widgits Pty Ltd]:MySQL AB
     # Organizational Unit Name (eg, section) []:
     # Common Name (eg, YOUR name) []:MySQL server
     # Email Address []:
     #
     # Please enter the following 'extra' attributes
     # to be sent with your certificate request
     # A challenge password []:
     # An optional company name []:

     #
     # Remove the passphrase from the key
     #
     openssl rsa -in $DIR/server-key.pem -out $DIR/server-key.pem

     #
     # Sign server cert
     #
     openssl ca -cert $DIR/ca.pem -policy policy_anything \
         -out $DIR/server-cert.pem -config $DIR/openssl.cnf \
         -infiles $DIR/server-req.pem

     # Sample output:
     # Using configuration from /home/finley/openssl/openssl.cnf
     # Enter PEM pass phrase:
     # Check that the request matches the signature
     # Signature ok
     # The Subjects Distinguished Name is as follows
     # countryName           :PRINTABLE:'FI'
     # organizationName      :PRINTABLE:'MySQL AB'
     # commonName            :PRINTABLE:'MySQL admin'
     # Certificate is to be certified until Sep 13 14:22:46 2003 GMT
     # (365 days)
     # Sign the certificate? [y/n]:y
     #
     #
     # 1 out of 1 certificate requests certified, commit? [y/n]y
     # Write out database with 1 new entries
     # Data Base Updated

     #
     # Create client request and key
     #
     openssl req -new -keyout $DIR/client-key.pem -out \
         $DIR/client-req.pem -days 3600 -config $DIR/openssl.cnf

     # Sample output:
     # Using configuration from /home/finley/openssl/openssl.cnf
     # Generating a 1024 bit RSA private key
     # .....................................++++++
     # .............................................++++++
     # writing new private key to '/home/finley/openssl/client-key.pem'
     # Enter PEM pass phrase:
     # Verifying password - Enter PEM pass phrase:
     # -----
     # You are about to be asked to enter information that will be
     # incorporated into your certificate request.
     # What you are about to enter is what is called a Distinguished Name
     # or a DN.
     # There are quite a few fields but you can leave some blank
     # For some fields there will be a default value,
     # If you enter '.', the field will be left blank.
     # -----
     # Country Name (2 letter code) [AU]:FI
     # State or Province Name (full name) [Some-State]:.
     # Locality Name (eg, city) []:
     # Organization Name (eg, company) [Internet Widgits Pty Ltd]:MySQL AB
     # Organizational Unit Name (eg, section) []:
     # Common Name (eg, YOUR name) []:MySQL user
     # Email Address []:
     #
     # Please enter the following 'extra' attributes
     # to be sent with your certificate request
     # A challenge password []:
     # An optional company name []:

     #
     # Remove the passphrase from the key
     #
     openssl rsa -in $DIR/client-key.pem -out $DIR/client-key.pem

     #
     # Sign client cert
     #

     openssl ca -cert $DIR/ca.pem -policy policy_anything \
         -out $DIR/client-cert.pem -config $DIR/openssl.cnf \
         -infiles $DIR/client-req.pem

     # Sample output:
     # Using configuration from /home/finley/openssl/openssl.cnf
     # Enter PEM pass phrase:
     # Check that the request matches the signature
     # Signature ok
     # The Subjects Distinguished Name is as follows
     # countryName           :PRINTABLE:'FI'
     # organizationName      :PRINTABLE:'MySQL AB'
     # commonName            :PRINTABLE:'MySQL user'
     # Certificate is to be certified until Sep 13 16:45:17 2003 GMT
     # (365 days)
     # Sign the certificate? [y/n]:y
     #
     #
     # 1 out of 1 certificate requests certified, commit? [y/n]y
     # Write out database with 1 new entries
     # Data Base Updated

     #
     # Create a my.cnf file that you can use to test the certificates
     #

     cat <<EOF > $DIR/my.cnf
     [client]
     ssl-ca=$DIR/ca.pem
     ssl-cert=$DIR/client-cert.pem
     ssl-key=$DIR/client-key.pem
     [mysqld]
     ssl-ca=$DIR/ca.pem
     ssl-cert=$DIR/server-cert.pem
     ssl-key=$DIR/server-key.pem
     EOF

*Example 3: Creating SSL Files on Windows*

Download OpenSSL for Windows if it is not installed on your system.  An
overview of available packages can be seen here:

     <http://www.slproweb.com/products/Win32OpenSSL.html>

Choose the Win32 OpenSSL Light or Win64 OpenSSL Light package, depending
on your architecture (32-bit or 64-bit).  The default installation
location will be 'C:\OpenSSL-Win32' or 'C:\OpenSSL-Win64', depending on
which package you downloaded.  The following instructions assume a
default location of 'C:\OpenSSL-Win32'.  Modify this as necessary if you
are using the 64-bit package.

If a message occurs during setup indicating ''...critical component is
missing: Microsoft Visual C++ 2008 Redistributables'', cancel the setup
and download one of the following packages as well, again depending on
your architecture (32-bit or 64-bit):

   * Visual C++ 2008 Redistributables (x86), available at:

          <http://www.microsoft.com/downloads/details.aspx?familyid=9B2DA534-3E03-4391-8A4D-074B9F2BC1BF>

   * Visual C++ 2008 Redistributables (x64), available at:

          <http://www.microsoft.com/downloads/details.aspx?familyid=bd2a6171-e2d6-4230-b809-9a8d7548c1b6>

After installing the additional package, restart the OpenSSL setup
procedure.

During installation, leave the default 'C:\OpenSSL-Win32' as the install
path, and also leave the default option ''Copy OpenSSL DLL files to the
Windows system directory'' selected.

When the installation has finished, add 'C:\OpenSSL-Win32\bin' to the
Windows System Path variable of your server:

  1. On the Windows desktop, right-click the 'My Computer' icon, and
     select 'Properties'.

  2. Select the 'Advanced' tab from the 'System Properties' menu that
     appears, and click the 'Environment Variables' button.

  3. Under 'System Variables', select 'Path', then click the 'Edit'
     button.  The 'Edit System Variable' dialogue should appear.

  4. Add '';C:\OpenSSL-Win32\bin'' to the end (notice the semicolon).

  5. Press OK 3 times.

  6. Check that OpenSSL was correctly integrated into the Path variable
     by opening a new command console ('Start>Run>cmd.exe') and
     verifying that OpenSSL is available (depending on your version of
     Windows, the following path-setting instructions might differ
     slightly):

          Microsoft Windows [Version ...]
          Copyright (c) 2006 Microsoft Corporation. All rights reserved.

          C:\Windows\system32>cd \

          C:\>openssl
          OpenSSL> exit <<< If you see the OpenSSL prompt, installation was successful.

          C:\>

After OpenSSL has been installed, use instructions similar to those from
Example 1 (shown earlier in this section), with the following changes:

   * Change the following Unix commands:

          # Create clean environment
          rm -rf newcerts
          mkdir newcerts && cd newcerts

     On Windows, use these commands instead:

          # Create clean environment
          md c:\newcerts
          cd c:\newcerts

   * When a ''\'' character is shown at the end of a command line, this
     ''\'' character must be removed and the command lines entered all
     on a single line.

After generating the certificate and key files, to use them for SSL
connections, see *note using-encrypted-connections::.


File: manual.info.tmp,  Node: ssl-libraries,  Next: windows-and-ssh,  Prev: creating-ssl-files-using-openssl,  Up: encrypted-connections

6.3.4 SSL Library-Dependent Capabilities
----------------------------------------

MySQL can be compiled using OpenSSL or yaSSL, both of which enable
encrypted connections based on the OpenSSL API:

   * MySQL Enterprise Edition binary distributions are compiled using
     yaSSL.

   * MySQL Community Edition binary distributions are compiled using
     yaSSL.

   * MySQL Community Edition source distributions can be compiled using
     either OpenSSL or yaSSL (see *note
     source-ssl-library-configuration::).

OpenSSL and yaSSL offer the same basic functionality, but additional
features are available in MySQL distributions compiled using OpenSSL:

   * OpenSSL supports a more flexible syntax for specifying ciphers for
     the '--ssl-cipher' option, and supports a wider range of encryption
     ciphers from which to choose.  See *note
     encrypted-connection-options::, and *note
     encrypted-connection-protocols-ciphers::.

   * OpenSSL supports the '--ssl-capath' option.  MySQL distributions
     compiled using yaSSL do not because yaSSL does not look in any
     directory and do not follow a chained certificate tree.  yaSSL
     requires that all components of the CA certificate tree be
     contained within a single CA certificate tree and that each
     certificate in the file has a unique SubjectName value.  To work
     around this limitation, concatenate the individual certificate
     files comprising the certificate tree into a new file and specify
     that file as the value of the '--ssl-ca' option.


File: manual.info.tmp,  Node: windows-and-ssh,  Prev: ssl-libraries,  Up: encrypted-connections

6.3.5 Connecting to MySQL Remotely from Windows with SSH
--------------------------------------------------------

This section describes how to get an encrypted connection to a remote
MySQL server with SSH. The information was provided by David Carlson
<dcarlson@mplcomm.com>.

  1. Install an SSH client on your Windows machine.  For a comparison of
     SSH clients, see
     <http://en.wikipedia.org/wiki/Comparison_of_SSH_clients>.

  2. Start your Windows SSH client.  Set 'Host_Name =
     YOURMYSQLSERVER_URL_OR_IP'.  Set 'userid=YOUR_USERID' to log in to
     your server.  This 'userid' value might not be the same as the user
     name of your MySQL account.

  3. Set up port forwarding.  Either do a remote forward (Set
     'local_port: 3306', 'remote_host: YOURMYSQLSERVERNAME_OR_IP',
     'remote_port: 3306' ) or a local forward (Set 'port: 3306', 'host:
     localhost', 'remote port: 3306').

  4. Save everything, otherwise you will have to redo it the next time.

  5. Log in to your server with the SSH session you just created.

  6. On your Windows machine, start some ODBC application (such as
     Access).

  7. Create a new file in Windows and link to MySQL using the ODBC
     driver the same way you normally do, except type in 'localhost' for
     the MySQL host server, not YOURMYSQLSERVERNAME.

At this point, you should have an ODBC connection to MySQL, encrypted
using SSH.


File: manual.info.tmp,  Node: security-plugins,  Prev: encrypted-connections,  Up: security

6.4 Security Plugins
====================

* Menu:

* authentication-plugins::       Authentication Plugins
* audit-log::                    MySQL Enterprise Audit

MySQL includes several plugins that implement security features:

   * Plugins for authenticating attempts by clients to connect to MySQL
     Server.  Plugins are available for several authentication
     protocols.  For general discussion of the authentication process,
     see *note pluggable-authentication::.  For characteristics of
     specific authentication plugins, see *note
     authentication-plugins::.

   * (MySQL Enterprise Edition only) MySQL Enterprise Audit, implemented
     using a server plugin, uses the open MySQL Audit API to enable
     standard, policy-based monitoring and logging of connection and
     query activity executed on specific MySQL servers.  Designed to
     meet the Oracle audit specification, MySQL Enterprise Audit
     provides an out of box, easy to use auditing and compliance
     solution for applications that are governed by both internal and
     external regulatory guidelines.  See *note audit-log::.


File: manual.info.tmp,  Node: authentication-plugins,  Next: audit-log,  Prev: security-plugins,  Up: security-plugins

6.4.1 Authentication Plugins
----------------------------

* Menu:

* native-pluggable-authentication::  Native Pluggable Authentication
* old-native-pluggable-authentication::  Old Native Pluggable Authentication
* cleartext-pluggable-authentication::  Client-Side Cleartext Pluggable Authentication
* pam-pluggable-authentication::  PAM Pluggable Authentication
* windows-pluggable-authentication::  Windows Pluggable Authentication
* socket-pluggable-authentication::  Socket Peer-Credential Pluggable Authentication
* test-pluggable-authentication::  Test Pluggable Authentication

The following sections describe pluggable authentication methods
available in MySQL and the plugins that implement these methods.  For
general discussion of the authentication process, see *note
pluggable-authentication::.


File: manual.info.tmp,  Node: native-pluggable-authentication,  Next: old-native-pluggable-authentication,  Prev: authentication-plugins,  Up: authentication-plugins

6.4.1.1 Native Pluggable Authentication
.......................................

MySQL includes two plugins that implement native authentication; that
is, authentication based on the password hashing methods in use from
before the introduction of pluggable authentication.  This section
describes 'mysql_native_password', which implements authentication
against the 'mysql.user' system table using the native password hashing
method.  For information about 'mysql_old_password', which implements
authentication using the older (pre-4.1) native password hashing method,
see *note old-native-pluggable-authentication::.  For information about
these password hashing methods, see *note password-hashing::.

The 'mysql_native_password' native authentication plugin is backward
compatible.  Older clients that do not support authentication _plugins_
do use the native authentication _protocol_, so they can connect to
servers that support pluggable authentication.

The following table shows the plugin names on the server and client
sides.

*Plugin and Library Names for Native Password Authentication*

Plugin or File         Plugin or File Name
                       
Server-side plugin     'mysql_native_password'
                       
Client-side plugin     'mysql_native_password'
                       
Library file           None (plugins are built in)

The following sections provide installation and usage information
specific to native pluggable authentication:

   * *note native-pluggable-authentication-installation::

   * *note native-pluggable-authentication-usage::

For general information about pluggable authentication in MySQL, see
*note pluggable-authentication::.

*Installing Native Pluggable Authentication*

The 'mysql_native_password' plugin exists in server and client forms:

   * The server-side plugin is built into the server, need not be loaded
     explicitly, and cannot be disabled by unloading it.

   * The client-side plugin is built into the 'libmysqlclient' client
     library and is available to any program linked against
     'libmysqlclient'.

*Using Native Pluggable Authentication*

MySQL client programs use 'mysql_native_password' by default.  The
'--default-auth' option can be used as a hint about which client-side
plugin the program can expect to use:

     shell> mysql --default-auth=mysql_native_password ...

If an account row specifies no plugin name, the server authenticates the
account using either the 'mysql_native_password' or 'mysql_old_password'
plugin, depending on whether the password hash value in the 'Password'
column used native hashing or the older pre-4.1 hashing method.  Clients
must match the password in the 'Password' column of the account row.


File: manual.info.tmp,  Node: old-native-pluggable-authentication,  Next: cleartext-pluggable-authentication,  Prev: native-pluggable-authentication,  Up: authentication-plugins

6.4.1.2 Old Native Pluggable Authentication
...........................................

MySQL includes two plugins that implement native authentication; that
is, authentication based on the password hashing methods in use from
before the introduction of pluggable authentication.  This section
describes 'mysql_old_password', which implements authentication against
the 'mysql.user' system table using the older (pre-4.1) native password
hashing method.  For information about 'mysql_native_password', which
implements authentication using the native password hashing method, see
*note native-pluggable-authentication::.  For information about these
password hashing methods, see *note password-hashing::.

*Note*:

Passwords that use the pre-4.1 hashing method are less secure than
passwords that use the native password hashing method and should be
avoided.

The 'mysql_old_password' native authentication plugin is backward
compatible.  Older clients that do not support authentication _plugins_
do use the native authentication _protocol_, so they can connect to
servers that support pluggable authentication.

The following table shows the plugin names on the server and client
sides.

*Plugin and Library Names for Old Native Password Authentication*

Plugin or File         Plugin or File Name
                       
Server-side plugin     'mysql_old_password'
                       
Client-side plugin     'mysql_old_password'
                       
Library file           None (plugins are built in)

The following sections provide installation and usage information
specific to old native pluggable authentication:

   * *note old-native-pluggable-authentication-installation::

   * *note old-native-pluggable-authentication-usage::

For general information about pluggable authentication in MySQL, see
*note pluggable-authentication::.

*Installing Old Native Pluggable Authentication*

The 'mysql_old_password' plugin exists in server and client forms:

   * The server-side plugin is built into the server, need not be loaded
     explicitly, and cannot be disabled by unloading it.

   * The client-side plugin is built into the 'libmysqlclient' client
     library and is available to any program linked against
     'libmysqlclient'.

*Using Old Native Pluggable Authentication*

MySQL client programs can use the '--default-auth' option to specify the
'mysql_old_password' plugin as a hint about which client-side plugin the
program can expect to use:

     shell> mysql --default-auth=mysql_old_password ...

If an account row specifies no plugin name, the server authenticates the
account using either the 'mysql_native_password' or 'mysql_old_password'
plugin, depending on whether the password hash value in the 'Password'
column used native hashing or the older pre-4.1 hashing method.  Clients
must match the password in the 'Password' column of the account row.


File: manual.info.tmp,  Node: cleartext-pluggable-authentication,  Next: pam-pluggable-authentication,  Prev: old-native-pluggable-authentication,  Up: authentication-plugins

6.4.1.3 Client-Side Cleartext Pluggable Authentication
......................................................

As of MySQL 5.5.10, a client-side authentication plugin is available
that enables clients to send passwords to the server as cleartext,
without hashing or encryption.  This plugin is built into the MySQL
client library.

The following table shows the plugin name.

*Plugin and Library Names for Cleartext Authentication*

Plugin or File         Plugin or File Name
                       
Server-side plugin     None, see discussion
                       
Client-side plugin     'mysql_clear_password'
                       
Library file           None (plugin is built in)

Many client-side authentication plugins perform hashing or encryption of
a password before the client sends it to the server.  This enables
clients to avoid sending passwords as cleartext.

Hashing or encryption cannot be done for authentication schemes that
require the server to receive the password as entered on the client
side.  In such cases, the client-side 'mysql_clear_password' plugin is
used to send the password to the server as cleartext.  There is no
corresponding server-side plugin.  Rather, the client-side plugin can be
used by any server-side plugin that needs a cleartext

Hashing or encryption cannot be done for authentication schemes that
require the server to receive the password as entered on the client
side.  In such cases, the client-side 'mysql_clear_password' plugin is
used, which enables the client to send the password to the server as
cleartext.  There is no corresponding server-side plugin.  Rather,
'mysql_clear_password' can be used on the client side in concert with
any server-side plugin that needs a cleartext password.  (The PAM
authentication plugin is one such; see *note
pam-pluggable-authentication::.)

The following discussion provides usage information specific to
cleartext pluggable authentication.  For general information about
pluggable authentication in MySQL, see *note pluggable-authentication::.

*Note*:

Sending passwords as cleartext may be a security problem in some
configurations.  To avoid problems if there is any possibility that the
password would be intercepted, clients should connect to MySQL Server
using a method that protects the password.  Possibilities include SSL
(see *note encrypted-connections::), IPsec, or a private network.

As of MySQL 5.5.27, to make inadvertent use of the
'mysql_clear_password' plugin less likely, MySQL clients must explicitly
enable it.  This can be done in several ways:

   * Set the 'LIBMYSQL_ENABLE_CLEARTEXT_PLUGIN' environment variable to
     a value that begins with '1', 'Y', or 'y'.  This enables the plugin
     for all client connections.

   * The *note 'mysql': mysql, *note 'mysqladmin': mysqladmin, and *note
     'mysqlslap': mysqlslap. client programs (also *note 'mysqlcheck':
     mysqlcheck, *note 'mysqldump': mysqldump, and *note 'mysqlshow':
     mysqlshow. for MySQL 5.5.47 and later) support an
     '--enable-cleartext-plugin' option that enables the plugin on a
     per-invocation basis.

   * The *note 'mysql_options()': mysql-options. C API function supports
     a 'MYSQL_ENABLE_CLEARTEXT_PLUGIN' option that enables the plugin on
     a per-connection basis.  Also, any program that uses
     'libmysqlclient' and reads option files can enable the plugin by
     including an 'enable-cleartext-plugin' option in an option group
     read by the client library.


File: manual.info.tmp,  Node: pam-pluggable-authentication,  Next: windows-pluggable-authentication,  Prev: cleartext-pluggable-authentication,  Up: authentication-plugins

6.4.1.4 PAM Pluggable Authentication
....................................

*Note*:

PAM pluggable authentication is an extension included in MySQL
Enterprise Edition, a commercial product.  To learn more about
commercial products, see <https://www.mysql.com/products/>.

As of MySQL 5.5.16, MySQL Enterprise Edition supports an authentication
method that enables MySQL Server to use PAM (Pluggable Authentication
Modules) to authenticate MySQL users.  PAM enables a system to use a
standard interface to access various kinds of authentication methods,
such as traditional Unix passwords or an LDAP directory.

PAM pluggable authentication provides these capabilities:

   * External authentication: PAM authentication enables MySQL Server to
     accept connections from users defined outside the MySQL grant
     tables and that authenticate using methods supported by PAM.

   * Proxy user support: PAM authentication can return to MySQL a user
     name different from the external user name passed by the client
     program, based on the PAM groups the external user is a member of
     and the authentication string provided.  This means that the plugin
     can return the MySQL user that defines the privileges the external
     PAM-authenticated user should have.  For example, an operating
     sytem user named 'joe' can connect and have the privileges of a
     MySQL user named 'developer'.

PAM pluggable authentication has been tested on Linux and macOS.

The following table shows the plugin and library file names.  The file
name suffix might differ on your system.  The file must be located in
the directory named by the 'plugin_dir' system variable.  For
installation information, see *note
pam-pluggable-authentication-installation::.

*Plugin and Library Names for PAM Authentication*

Plugin or File         Plugin or File Name
                       
Server-side plugin     'authentication_pam'
                       
Client-side plugin     'mysql_clear_password'
                       
Library file           'authentication_pam.so'

As of MySQL 5.5.10, the client-side 'mysql_clear_password' cleartext
plugin that communicates with the server-side PAM plugin is built into
the 'libmysqlclient' client library and is included in all
distributions, including community distributions.  Inclusion of the
client-side cleartext plugin in all MySQL distributions enables clients
from any distribution to connect to a server that has the server-side
PAM plugin loaded.

The following sections provide installation and usage information
specific to PAM pluggable authentication:

   * *note pam-pluggable-authentication-how-it-works::

   * *note pam-pluggable-authentication-installation::

   * *note pam-pluggable-authentication-uninstallation::

   * *note pam-pluggable-authentication-usage::

   * *note pam-authentication-unix-without-proxy::

   * *note pam-authentication-ldap-without-proxy::

   * *note pam-authentication-unix-with-proxy::

   * *note pam-authentication-unix-password-store::

   * *note pam-pluggable-authentication-debugging::

For general information about pluggable authentication in MySQL, see
*note pluggable-authentication::.  For information about the
'mysql_clear_password' plugin, see *note
cleartext-pluggable-authentication::.  For proxy user information, see
*note proxy-users::.

*How PAM Authentication of MySQL Users Works*

This section provides a general overview of how MySQL and PAM work
together to authenticate MySQL users.  For examples showing how to set
up MySQL accounts to use specific PAM services, see *note
pam-pluggable-authentication-usage::.

  1. The client program and the server communicate, with the client
     sending to the server the client user name (the operating system
     user name by default) and password:

        * The client user name is the external user name.

        * For accounts that use the PAM server-side authentication
          plugin, the corresponding client-side plugin is
          'mysql_clear_password'.  This client-side plugin performs no
          password hashing, with the result that the client sends the
          password to the server as cleartext.

  2. The server finds a matching MySQL account based on the external
     user name and the host from which the client connects.  The PAM
     plugin uses the information passed to it by MySQL Server (such as
     user name, host name, password, and authentication string).  When
     you define a MySQL account that authenticates using PAM, the
     authentication string contains:

        * A PAM service name, which is a name that the system
          administrator can use to refer to an authentication method for
          a particular application.  There can be multiple applications
          associated with a single database server instance, so the
          choice of service name is left to the SQL application
          developer.

        * Optionally, if proxying is to be used, a mapping from PAM
          groups to MySQL user names.

  3. The plugin uses the PAM service named in the authentication string
     to check the user credentials and returns ''Authentication
     succeeded, Username is USER_NAME'' or ''Authentication failed''.
     The password must be appropriate for the password store used by the
     PAM service.  Examples:

        * For traditional Unix passwords, the service looks up passwords
          stored in the '/etc/shadow' file.

        * For LDAP, the service looks up passwords stored in an LDAP
          directory.

     If the credentials check fails, the server refuses the connection.

  4. Otherwise, the authentication string indicates whether proxying
     occurs.  If the string contains no PAM group mapping, proxying does
     not occur.  In this case, the MySQL user name is the same as the
     external user name.

  5. Otherwise, proxying is indicated based on the PAM group mapping,
     with the MySQL user name determined based on the first matching
     group in the mapping list.  The meaning of 'PAM group' depends on
     the PAM service.  Examples:

        * For traditional Unix passwords, groups are Unix groups defined
          in the '/etc/group' file, possibly supplemented with
          additional PAM information in a file such as
          '/etc/security/group.conf'.

        * For LDAP, groups are LDAP groups defined in an LDAP directory.

     If the proxy user (the external user) has the 'PROXY' privilege for
     the proxied MySQL user name, proxying occurs, with the proxy user
     assuming the privileges of the proxied user.

*Installing PAM Pluggable Authentication*

This section describes how to install the PAM authentication plugin.
For general information about installing plugins, see *note
plugin-loading::.

To be usable by the server, the plugin library file must be located in
the MySQL plugin directory (the directory named by the 'plugin_dir'
system variable).  If necessary, configure the plugin directory location
by setting the value of 'plugin_dir' at server startup.

The plugin library file base name is 'authentication_pam'.  The file
name suffix differs per platform (for example, '.so' for Unix and
Unix-like systems, '.dll' for Windows).

To load the plugin at server startup, use the '--plugin-load' option to
name the library file that contains it.  With this plugin-loading
method, the option must be given each time the server starts.  For
example, put these lines in the server 'my.cnf' file (adjust the '.so'
suffix for your platform as necessary):

     [mysqld]
     plugin-load=authentication_pam.so

After modifying 'my.cnf', restart the server to cause the new settings
to take effect.

Alternatively, to load the plugin at runtime, use this statement (adjust
the '.so' suffix for your platform as necessary):

     INSTALL PLUGIN authentication_pam SONAME 'authentication_pam.so';

*note 'INSTALL PLUGIN': install-plugin. loads the plugin immediately,
and also registers it in the 'mysql.plugins' system table to cause the
server to load it for each subsequent normal startup without the need
for '--plugin-load'.

To verify plugin installation, examine the *note
'INFORMATION_SCHEMA.PLUGINS': plugins-table. table or use the *note
'SHOW PLUGINS': show-plugins. statement (see *note
obtaining-plugin-information::).  For example:

     mysql> SELECT PLUGIN_NAME, PLUGIN_STATUS
            FROM INFORMATION_SCHEMA.PLUGINS
            WHERE PLUGIN_NAME LIKE '%pam%';
     +--------------------+---------------+
     | PLUGIN_NAME        | PLUGIN_STATUS |
     +--------------------+---------------+
     | authentication_pam | ACTIVE        |
     +--------------------+---------------+

If the plugin failed to initialize, check the server error log for
diagnostic messages.

To associate MySQL accounts with the PAM plugin, see *note
pam-pluggable-authentication-usage::.

*Uninstalling PAM Pluggable Authentication*

The method used to uninstall the PAM authentication plugin depends on
how you installed it:

   * If you installed the plugin at server startup using a
     '--plugin-load' option, restart the server without the option.

   * If you installed the plugin at runtime using an *note 'INSTALL
     PLUGIN': install-plugin. statement, it remains installed across
     server restarts.  To uninstall it, use *note 'UNINSTALL PLUGIN':
     uninstall-plugin.:

          UNINSTALL PLUGIN authentication_pam;

*Using PAM Pluggable Authentication*

This section describes in general terms how to use the PAM
authentication plugin to connect from MySQL client programs to the
server.  The following sections provide instructions for using PAM
authentication in specific ways.  It is assumed that the server is
running with the server-side PAM plugin enabled, as described in *note
pam-pluggable-authentication-installation::, and that client programs
are recent enough to include the client-side plugin.

To refer to the PAM authentication plugin in the 'IDENTIFIED WITH'
clause of a *note 'CREATE USER': create-user. or *note 'GRANT': grant.
statement, use the name 'authentication_pam'.  For example:

     CREATE USER USER
       IDENTIFIED WITH authentication_pam
       AS 'AUTH_STRING';

The authentication string specifies the following types of information:

   * The PAM service name (see *note
     pam-pluggable-authentication-how-it-works::).  Examples in the
     following discussion use a service name of 'mysql-unix' for
     authentication using traditional Unix passwords, and 'mysql-ldap'
     for authentication using LDAP.

   * For proxy support, PAM provides a way for a PAM module to return to
     the server a MySQL user name other than the external user name
     passed by the client program when it connects to the server.  Use
     the authentication string to control the mapping from external user
     names to MySQL user names.  If you want to take advantage of proxy
     user capabilities, the authentication string must include this kind
     of mapping.

For example, if an account uses the 'mysql-unix' PAM service name and
should map operating system users in the 'root' and 'users' PAM groups
to the 'developer' and 'data_entry' MySQL users, respectively, use a
statement like this:

     CREATE USER USER
       IDENTIFIED WITH authentication_pam
       AS 'mysql-unix, root=developer, users=data_entry';

Authentication string syntax for the PAM authentication plugin follows
these rules:

   * The string consists of a PAM service name, optionally followed by a
     PAM group mapping list consisting of one or more keyword/value
     pairs each specifying a PAM group name and a MySQL user name:

          PAM_SERVICE_NAME[,PAM_GROUP_NAME=MYSQL_USER_NAME]...

     The plugin parses the authentication string for each connection
     attempt that uses the account.  To minimize overhead, keep the
     string as short as possible.

   * Each 'PAM_GROUP_NAME=MYSQL_USER_NAME' pair must be preceded by a
     comma.

   * Leading and trailing spaces not inside double quotation marks are
     ignored.

   * Unquoted PAM_SERVICE_NAME, PAM_GROUP_NAME, and MYSQL_USER_NAME
     values can contain anything except equal sign, comma, or space.

   * If a PAM_SERVICE_NAME, PAM_GROUP_NAME, or MYSQL_USER_NAME value is
     quoted with double quotation marks, everything between the
     quotation marks is part of the value.  This is necessary, for
     example, if the value contains space characters.  All characters
     are legal except double quotation mark and backslash ('\').  To
     include either character, escape it with a backslash.

If the plugin successfully authenticates the external user name (the
name passed by the client), it looks for a PAM group mapping list in the
authentication string and, if present, uses it to return a different
MySQL user name to the MySQL server based on which PAM groups the
external user is a member of:

   * If the authentication string contains no PAM group mapping list,
     the plugin returns the external name.

   * If the authentication string does contain a PAM group mapping list,
     the plugin examines each 'PAM_GROUP_NAME=MYSQL_USER_NAME' pair in
     the list from left to right and tries to find a match for the
     PAM_GROUP_NAME value in a non-MySQL directory of the groups
     assigned to the authenticated user and returns MYSQL_USER_NAME for
     the first match it finds.  If the plugin finds no match for any PAM
     group, it returns the external name.  If the plugin is not capable
     of looking up a group in a directory, it ignores the PAM group
     mapping list and returns the external name.

The following sections describe how to set up several authentication
scenarios that use the PAM authentication plugin:

   * No proxy users.  This uses PAM only to check login names and
     passwords.  Every external user permitted to connect to MySQL
     Server should have a matching MySQL account that is defined to use
     PAM authentication.  (For a MySQL account of
     ''USER_NAME'@'HOST_NAME'' to match the external user, USER_NAME
     must be the external user name and HOST_NAME must match the host
     from which the client connects.)  Authentication can be performed
     by various PAM-supported methods.  Later discussion shows how to
     authenticate client credentials using traditional Unix passwords,
     and passwords in LDAP.

     PAM authentication, when not done through proxy users or PAM
     groups, requires the MySQL user name to be same as the operating
     system user name.  MySQL user names are limited to 16 characters
     (see *note grant-tables::), which limits PAM nonproxy
     authentication to Unix accounts with names of at most 16
     characters.

   * Proxy users only, with PAM group mapping.  For this scenario,
     create one or more MySQL accounts that define different sets of
     privileges.  (Ideally, nobody should connect using those accounts
     directly.)  Then define a default user authenticating through PAM
     that uses some mapping scheme (usually based on the external PAM
     groups the users are members of) to map all the external user names
     to the few MySQL accounts holding the privilege sets.  Any client
     who connects and specifies an external user name as the client user
     name is mapped to one of the MySQL accounts and uses its
     privileges.  The discussion shows how to set this up using
     traditional Unix passwords, but other PAM methods such as LDAP
     could be used instead.

Variations on these scenarios are possible:

   * You can permit some users to log in directly (without proxying) but
     require others to connect through proxy accounts.

   * You can use one PAM authentication method for some users, and
     another method for other users, by using differing PAM service
     names among your PAM-authenticated accounts.  For example, you can
     use the 'mysql-unix' PAM service for some users, and 'mysql-ldap'
     for others.

The examples make the following assumptions.  You might need to make
some adjustments if your system is set up differently.

   * The login name and password are 'antonio' and ANTONIO_PASSWORD,
     respectively.  Change these to correspond to the user you want to
     authenticate.

   * The PAM configuration directory is '/etc/pam.d'.

   * The PAM service name corresponds to the authentication method
     ('mysql-unix' or 'mysql-ldap' in this discussion).  To use a given
     PAM service, you must set up a PAM file with the same name in the
     PAM configuration directory (creating the file if it does not
     exist).  In addition, you must use the PAM service name in the
     authentication string of *note 'CREATE USER': create-user. or *note
     'GRANT': grant. statements for any account that authenticates using
     that service.

The PAM authentication plugin checks at initialization time whether the
'AUTHENTICATION_PAM_LOG' environment value is set in the server's
startup environment.  If so, the plugin enables logging of diagnostic
messages to the standard output.  Depending on how your server is
started, the message might appear on the console or in the error log.
These messages can be helpful for debugging PAM-related issues that
occur when the plugin performs authentication.  For more information,
see *note pam-pluggable-authentication-debugging::.

*PAM Unix Password Authentication without Proxy Users*

This authentication scenario uses PAM to check external users defined in
terms of operating system user names and Unix passwords, without
proxying.  Every such external user permitted to connect to MySQL Server
should have a matching MySQL account that is defined to use PAM
authentication through traditional Unix password store.

*Note*:

Traditional Unix passwords are checked using the '/etc/shadow' file.
For information regarding possible issues related to this file, see
*note pam-authentication-unix-password-store::.

  1. Verify that Unix authentication permits logins to the operating
     system with the user name 'antonio' and password ANTONIO_PASSWORD.

  2. Set up PAM to authenticate MySQL connections using traditional Unix
     passwords by creating a 'mysql-unix' PAM service file named
     '/etc/pam.d/mysql-unix'.  The file contents are system dependent,
     so check existing login-related files in the '/etc/pam.d' directory
     to see what they look like.  On Linux, the 'mysql-unix' file might
     look like this:

          #%PAM-1.0
          auth            include         password-auth
          account         include         password-auth

     For macOS, use 'login' rather than 'password-auth'.

     The PAM file format might differ on some systems.  For example, on
     Ubuntu and other Debian-based systems, use these file contents
     instead:

          @include common-auth
          @include common-account
          @include common-session-noninteractive

  3. Create a MySQL account with the same user name as the operating
     system user name and define it to authenticate using the PAM plugin
     and the 'mysql-unix' PAM service:

          CREATE USER 'antonio'@'localhost'
            IDENTIFIED WITH authentication_pam
            AS 'mysql-unix';
          GRANT ALL PRIVILEGES
            ON mydb.*
            TO 'antonio'@'localhost';

     Here, the authentication string contains only the PAM service name,
     'mysql-unix', which authenticates Unix passwords.

  4. Use the *note 'mysql': mysql. command-line client to connect to the
     MySQL server as 'antonio'.  For example:

          shell> mysql --user=antonio --password --enable-cleartext-plugin
          Enter password: ANTONIO_PASSWORD

     The server should permit the connection and the following query
     should return output as shown:

          mysql> SELECT USER(), CURRENT_USER(), @@proxy_user;
          +-------------------+-------------------+--------------+
          | USER()            | CURRENT_USER()    | @@proxy_user |
          +-------------------+-------------------+--------------+
          | antonio@localhost | antonio@localhost | NULL         |
          +-------------------+-------------------+--------------+

     This demonstrates that the 'antonio' operating system user is
     authenticated to have the privileges granted to the 'antonio' MySQL
     user, and that no proxying has occurred.

*Note*:

The client-side 'mysql_clear_password' authentication plugin leaves the
password untouched, so client programs send it to the MySQL server as
cleartext.  This enables the password to be passed as is to PAM. A
cleartext password is necessary to use the server-side PAM library, but
may be a security problem in some configurations.  These measures
minimize the risk:

   * To make inadvertent use of the 'mysql_clear_password' plugin less
     likely, MySQL clients must explicitly enable it (for example, with
     the '--enable-cleartext-plugin' option).  See *note
     cleartext-pluggable-authentication::.

   * To avoid password exposure with the 'mysql_clear_password' plugin
     enabled, MySQL clients should connect to the MySQL server using an
     encrypted connection.  See *note using-encrypted-connections::.

*PAM LDAP Authentication without Proxy Users*

This authentication scenario uses PAM to check external users defined in
terms of operating system user names and LDAP passwords, without
proxying.  Every such external user permitted to connect to MySQL Server
should have a matching MySQL account that is defined to use PAM
authentication through LDAP.

To use PAM LDAP pluggable authentication for MySQL, these prerequisites
must be satisfied:

   * An LDAP server must be available for the PAM LDAP service to
     communicate with.

   * LDAP users to be authenticated by MySQL must be present in the
     directory managed by the LDAP server.

Configure MySQL for PAM LDAP authentication as follows:

  1. Verify that Unix authentication permits logins to the operating
     system with the user name 'antonio' and password ANTONIO_PASSWORD.

  2. Set up PAM to authenticate MySQL connections using LDAP by creating
     a 'mysql-ldap' PAM service file named '/etc/pam.d/mysql-ldap'.  The
     file contents are system dependent, so check existing login-related
     files in the '/etc/pam.d' directory to see what they look like.  On
     Linux, the 'mysql-ldap' file might look like this:

          #%PAM-1.0
          auth        required    pam_ldap.so
          account     required    pam_ldap.so

     If PAM object files have a suffix different from '.so' on your
     system, substitute the correct suffix.

     The PAM file format might differ on some systems.

  3. Create a MySQL account with the same user name as the operating
     system user name and define it to authenticate using the PAM plugin
     and the 'mysql-ldap' PAM service:

          CREATE USER 'antonio'@'localhost'
            IDENTIFIED WITH authentication_pam
            AS 'mysql-ldap';
          GRANT ALL PRIVILEGES
            ON mydb.*
            TO 'antonio'@'localhost';

     Here, the authentication string contains only the PAM service name,
     'mysql-ldap', which authenticates using LDAP.

  4. Connecting to the server is the same as described in *note
     pam-authentication-unix-without-proxy::.

*PAM Unix Password Authentication with Proxy Users and Group Mapping*

The authentication scheme described here uses proxying and PAM group
mapping to map connecting MySQL users who authenticate using PAM onto
other MySQL accounts that define different sets of privileges.  Users do
not connect directly through the accounts that define the privileges.
Instead, they connect through a default proxy account authenticated
using PAM, such that all the external users are mapped to the MySQL
accounts that hold the privileges.  Any user who connects using the
proxy account is mapped to one of those MySQL accounts, the privileges
for which determine the database operations permitted to the external
user.

The procedure shown here uses Unix password authentication.  To use LDAP
instead, see the early steps of *note
pam-authentication-ldap-without-proxy::.

*Note*:

Traditional Unix passwords are checked using the '/etc/shadow' file.
For information regarding possible issues related to this file, see
*note pam-authentication-unix-password-store::.

  1. Verify that Unix authentication permits logins to the operating
     system with the user name 'antonio' and password ANTONIO_PASSWORD.

  2. Verify that 'antonio' is a member of the 'root' or 'users' PAM
     group.

  3. Set up PAM to authenticate the 'mysql-unix' PAM service through
     operating system users by creating a file named
     '/etc/pam.d/mysql-unix'.  The file contents are system dependent,
     so check existing login-related files in the '/etc/pam.d' directory
     to see what they look like.  On Linux, the 'mysql-unix' file might
     look like this:

          #%PAM-1.0
          auth            include         password-auth
          account         include         password-auth

     For macOS, use 'login' rather than 'password-auth'.

     The PAM file format might differ on some systems.  For example, on
     Ubuntu and other Debian-based systems, use these file contents
     instead:

          @include common-auth
          @include common-account
          @include common-session-noninteractive

  4. Create a default proxy user ('''@''') that maps external PAM users
     to the proxied accounts:

          CREATE USER ''@''
            IDENTIFIED WITH authentication_pam
            AS 'mysql-unix, root=developer, users=data_entry';

     Here, the authentication string contains the PAM service name,
     'mysql-unix', which authenticates Unix passwords.  The
     authentication string also maps external users in the 'root' and
     'users' PAM groups to the 'developer' and 'data_entry' MySQL user
     names, respectively.

     The PAM group mapping list following the PAM service name is
     required when you set up proxy users.  Otherwise, the plugin cannot
     tell how to perform mapping from external user names to the proper
     proxied MySQL user names.

     *Note*:

     If your MySQL installation has anonymous users, they might conflict
     with the default proxy user.  For more information about this
     issue, and ways of dealing with it, see *note
     proxy-users-conflicts::.

  5. Create the proxied accounts and grant to them the privileges
     required for MySQL access:

          CREATE USER 'developer'@'localhost'
            IDENTIFIED BY 'VERY SECRET PASSWORD';
          CREATE USER 'data_entry'@'localhost'
            IDENTIFIED BY 'VERY SECRET PASSWORD';

          GRANT ALL PRIVILEGES
            ON mydevdb.*
            TO 'developer'@'localhost';
          GRANT ALL PRIVILEGES
            ON mydb.*
            TO 'data_entry'@'localhost';

     If you do not let anyone know the passwords for these accounts,
     clients cannot use them to connect directly to the MySQL server.
     Instead, it is expected that users who authenticate using PAM will
     use the 'developer' or 'data_entry' account by proxy based on their
     PAM group.

  6. Grant to the proxy account the 'PROXY' privilege for each proxied
     account:

          GRANT PROXY
            ON 'developer'@'localhost'
            TO ''@'';
          GRANT PROXY
            ON 'data_entry'@'localhost'
            TO ''@'';

  7. Use the *note 'mysql': mysql. command-line client to connect to the
     MySQL server as 'antonio'.

          shell> mysql --user=antonio --password --enable-cleartext-plugin
          Enter password: ANTONIO_PASSWORD

     The server authenticates the connection using the default '''@'''
     proxy account.  The resulting privileges for 'antonio' depend on
     which PAM groups 'antonio' is a member of.  If 'antonio' is a
     member of the 'root' PAM group, the PAM plugin maps 'root' to the
     'developer' MySQL user name and returns that name to the server.
     The server verifies that '''@''' has the 'PROXY' privilege for
     'developer' and permits the connection.  The following query should
     return output as shown:

          mysql> SELECT USER(), CURRENT_USER(), @@proxy_user;
          +-------------------+---------------------+--------------+
          | USER()            | CURRENT_USER()      | @@proxy_user |
          +-------------------+---------------------+--------------+
          | antonio@localhost | developer@localhost | ''@''        |
          +-------------------+---------------------+--------------+

     This demonstrates that the 'antonio' operating system user is
     authenticated to have the privileges granted to the 'developer'
     MySQL user, and that proxying occurred through the default proxy
     account.

     If 'antonio' is not a member of the 'root' PAM group but is a
     member of the 'users' PAM group, a similar process occurs, but the
     plugin maps 'user' PAM group membership to the 'data_entry' MySQL
     user name and returns that name to the server:

          mysql> SELECT USER(), CURRENT_USER(), @@proxy_user;
          +-------------------+----------------------+--------------+
          | USER()            | CURRENT_USER()       | @@proxy_user |
          +-------------------+----------------------+--------------+
          | antonio@localhost | data_entry@localhost | ''@''        |
          +-------------------+----------------------+--------------+

     This demonstrates that the 'antonio' operating system user is
     authenticated to have the privileges of the 'data_entry' MySQL
     user, and that proxying occurred through the default proxy account.

*Note*:

The client-side 'mysql_clear_password' authentication plugin leaves the
password untouched, so client programs send it to the MySQL server as
cleartext.  This enables the password to be passed as is to PAM. A
cleartext password is necessary to use the server-side PAM library, but
may be a security problem in some configurations.  These measures
minimize the risk:

   * To make inadvertent use of the 'mysql_clear_password' plugin less
     likely, MySQL clients must explicitly enable it (for example, with
     the '--enable-cleartext-plugin' option).  See *note
     cleartext-pluggable-authentication::.

   * To avoid password exposure with the 'mysql_clear_password' plugin
     enabled, MySQL clients should connect to the MySQL server using an
     encrypted connection.  See *note using-encrypted-connections::.

*PAM Authentication Access to Unix Password Store*

On some systems, Unix authentication uses a password store such as
'/etc/shadow', a file that typically has restricted access permissions.
This can cause MySQL PAM-based authentication to fail.  Unfortunately,
the PAM implementation does not permit distinguishing 'password could
not be checked' (due, for example, to inability to read '/etc/shadow')
from 'password does not match.' If you are using Unix password store for
PAM authentication, you may be able to enable access to it from MySQL
using one of the following methods:

   * Assuming that the MySQL server is run from the 'mysql' operating
     system account, put that account in the 'shadow' group that has
     '/etc/shadow' access:

       1. Create a 'shadow' group in '/etc/group'.

       2. Add the 'mysql' operating system user to the 'shadow' group in
          '/etc/group'.

       3. Assign '/etc/group' to the 'shadow' group and enable the group
          read permission:

               chgrp shadow /etc/shadow
               chmod g+r /etc/shadow

       4. Restart the MySQL server.

   * If you are using the 'pam_unix' module and the 'unix_chkpwd'
     utility, enable password store access as follows:

          chmod u-s /usr/sbin/unix_chkpwd
          setcap cap_dac_read_search+ep /usr/sbin/unix_chkpwd

     Adjust the path to 'unix_chkpwd' as necessary for your platform.

*PAM Authentication Debugging*

The PAM authentication plugin checks at initialization time whether the
'AUTHENTICATION_PAM_LOG' environment value is set (the value does not
matter).  If so, the plugin enables logging of diagnostic messages to
the standard output.  These messages may be helpful for debugging
PAM-related issues that occur when the plugin performs authentication.

Some messages include reference to PAM plugin source files and line
numbers, which enables plugin actions to be tied more closely to the
location in the code where they occur.

Another technique for debugging connection failures and determining what
is happening during connection attempts is to configure PAM
authentication to permit all connections, then check the system log
files.  This technique should be used only on a _temporary_ basis, and
not on a production server.

Configure a PAM service file named '/etc/pam.d/mysql-any-password' with
these contents (the format may differ on some systems):

     #%PAM-1.0
     auth        required    pam_permit.so
     account     required    pam_permit.so

Create an account that uses the PAM plugin and names the
'mysql-any-password' PAM service:

     CREATE USER 'testuser'@'localhost'
       IDENTIFIED WITH authentication_pam
       AS 'mysql-any-password';

The 'mysql-any-password' service file causes any authentication attempt
to return true, even for incorrect passwords.  If an authentication
attempt fails, that tells you the configuration problem is on the MySQL
side.  Otherwise, the problem is on the operating system/PAM side.  To
see what might be happening, check system log files such as
'/var/log/secure', '/var/log/audit.log', '/var/log/syslog', or
'/var/log/messages'.

After determining what the problem is, remove the 'mysql-any-password'
PAM service file to disable any-password access.


File: manual.info.tmp,  Node: windows-pluggable-authentication,  Next: socket-pluggable-authentication,  Prev: pam-pluggable-authentication,  Up: authentication-plugins

6.4.1.5 Windows Pluggable Authentication
........................................

*Note*:

Windows pluggable authentication is an extension included in MySQL
Enterprise Edition, a commercial product.  To learn more about
commercial products, see <https://www.mysql.com/products/>.

As of MySQL 5.5.16, MySQL Enterprise Edition for Windows supports an
authentication method that performs external authentication on Windows,
enabling MySQL Server to use native Windows services to authenticate
client connections.  Users who have logged in to Windows can connect
from MySQL client programs to the server based on the information in
their environment without specifying an additional password.

The client and server exchange data packets in the authentication
handshake.  As a result of this exchange, the server creates a security
context object that represents the identity of the client in the Windows
OS. This identity includes the name of the client account.  Windows
pluggable authentication uses the identity of the client to check
whether it is a given account or a member of a group.  By default,
negotiation uses Kerberos to authenticate, then NTLM if Kerberos is
unavailable.

Windows pluggable authentication provides these capabilities:

   * External authentication: Windows authentication enables MySQL
     Server to accept connections from users defined outside the MySQL
     grant tables who have logged in to Windows.

   * Proxy user support: Windows authentication can return to MySQL a
     user name different from the external user name passed by the
     client program.  This means that the plugin can return the MySQL
     user that defines the privileges the external Windows-authenticated
     user should have.  For example, a Windows user named 'joe' can
     connect and have the privileges of a MySQL user named 'developer'.

The following table shows the plugin and library file names.  The file
must be located in the directory named by the 'plugin_dir' system
variable.

*Plugin and Library Names for Windows Authentication*

Plugin or File         Plugin or File Name
                       
Server-side plugin     'authentication_windows'
                       
Client-side plugin     'authentication_windows_client'
                       
Library file           'authentication_windows.dll'

The library file includes only the server-side plugin.  As of MySQL
5.5.13, the client-side plugin is built into the 'libmysqlclient' client
library.

The server-side Windows authentication plugin is included only in MySQL
Enterprise Edition.  It is not included in MySQL community
distributions.  The client-side plugin is included in all distributions,
including community distributions.  This permits clients from any 5.5.13
or higher distribution to connect to a server that has the server-side
plugin loaded.

The Windows authentication plugin is supported on any version of Windows
supported by MySQL 5.5 (see
<https://www.mysql.com/support/supportedplatforms/database.html>).  It
requires MySQL Server 5.5.16 or higher.

The following sections provide installation and usage information
specific to Windows pluggable authentication:

   * *note windows-pluggable-authentication-installation::

   * *note windows-pluggable-authentication-uninstallation::

   * *note windows-pluggable-authentication-usage::

For general information about pluggable authentication in MySQL, see
*note pluggable-authentication::.  For proxy user information, see *note
proxy-users::.

*Installing Windows Pluggable Authentication*

This section describes how to install the Windows authentication plugin.
For general information about installing plugins, see *note
plugin-loading::.

To be usable by the server, the plugin library file must be located in
the MySQL plugin directory (the directory named by the 'plugin_dir'
system variable).  If necessary, configure the plugin directory location
by setting the value of 'plugin_dir' at server startup.

To load the plugin at server startup, use the '--plugin-load' option to
name the library file that contains it.  With this plugin-loading
method, the option must be given each time the server starts.  For
example, put these lines in the server 'my.cnf' file:

     [mysqld]
     plugin-load=authentication_windows.dll

After modifying 'my.cnf', restart the server to cause the new settings
to take effect.

Alternatively, to load the plugin at runtime, use this statement:

     INSTALL PLUGIN authentication_windows SONAME 'authentication_windows.dll';

*note 'INSTALL PLUGIN': install-plugin. loads the plugin immediately,
and also registers it in the 'mysql.plugins' system table to cause the
server to load it for each subsequent normal startup without the need
for '--plugin-load'.

To verify plugin installation, examine the *note
'INFORMATION_SCHEMA.PLUGINS': plugins-table. table or use the *note
'SHOW PLUGINS': show-plugins. statement (see *note
obtaining-plugin-information::).  For example:

     mysql> SELECT PLUGIN_NAME, PLUGIN_STATUS
            FROM INFORMATION_SCHEMA.PLUGINS
            WHERE PLUGIN_NAME LIKE '%windows%';
     +------------------------+---------------+
     | PLUGIN_NAME            | PLUGIN_STATUS |
     +------------------------+---------------+
     | authentication_windows | ACTIVE        |
     +------------------------+---------------+

If the plugin failed to initialize, check the server error log for
diagnostic messages.

To associate MySQL accounts with the Windows authentication plugin, see
*note windows-pluggable-authentication-usage::.  Additional plugin
control is provided by the 'authentication_windows_use_principal_name'
and 'authentication_windows_log_level' system variables.  See *note
server-system-variables::.

*Uninstalling Windows Pluggable Authentication*

The method used to uninstall the Windows authentication plugin depends
on how you installed it:

   * If you installed the plugin at server startup using a
     '--plugin-load' option, restart the server without the option.

   * If you installed the plugin at runtime using an *note 'INSTALL
     PLUGIN': install-plugin. statement, it remains installed across
     server restarts.  To uninstall it, use *note 'UNINSTALL PLUGIN':
     uninstall-plugin.:

          UNINSTALL PLUGIN authentication_windows;

In addition, remove any startup options that set Windows plugin-related
system variables.

*Using Windows Pluggable Authentication*

The Windows authentication plugin supports the use of MySQL accounts
such that users who have logged in to Windows can connect to the MySQL
server without having to specify an additional password.  It is assumed
that the server is running with the server-side plugin enabled, as
described in *note windows-pluggable-authentication-installation::, and
that client programs are recent enough to include the client-side plugin
built into 'libmysqlclient' (MySQL 5.5.13 or higher).  Once the DBA has
enabled the server-side plugin and set up accounts to use it, clients
can connect using those accounts with no other setup required on their
part.

To refer to the Windows authentication plugin in the 'IDENTIFIED WITH'
clause of a *note 'CREATE USER': create-user. or *note 'GRANT': grant.
statement, use the name 'authentication_windows'.  Suppose that the
Windows users 'Rafal' and 'Tasha' should be permitted to connect to
MySQL, as well as any users in the 'Administrators' or 'Power Users'
group.  To set this up, create a MySQL account named 'sql_admin' that
uses the Windows plugin for authentication:

     CREATE USER sql_admin
       IDENTIFIED WITH authentication_windows
       AS 'Rafal, Tasha, Administrators, "Power Users"';

The plugin name is 'authentication_windows'.  The string following the
'AS' keyword is the authentication string.  It specifies that the
Windows users named 'Rafal' or 'Tasha' are permitted to authenticate to
the server as the MySQL user 'sql_admin', as are any Windows users in
the 'Administrators' or 'Power Users' group.  The latter group name
contains a space, so it must be quoted with double quote characters.

After you create the 'sql_admin' account, a user who has logged in to
Windows can attempt to connect to the server using that account:

     C:\> mysql --user=sql_admin

No password is required here.  The 'authentication_windows' plugin uses
the Windows security API to check which Windows user is connecting.  If
that user is named 'Rafal' or 'Tasha', or is a member of the
'Administrators' or 'Power Users' group, the server grants access and
the client is authenticated as 'sql_admin' and has whatever privileges
are granted to the 'sql_admin' account.  Otherwise, the server denies
access.

Authentication string syntax for the Windows authentication plugin
follows these rules:

   * The string consists of one or more user mappings separated by
     commas.

   * Each user mapping associates a Windows user or group name with a
     MySQL user name:

          WIN_USER_OR_GROUP_NAME=MYSQL_USER_NAME
          WIN_USER_OR_GROUP_NAME

     For the latter syntax, with no MYSQL_USER_NAME value given, the
     implicit value is the MySQL user created by the *note 'CREATE
     USER': create-user. statement.  Thus, these statements are
     equivalent:

          CREATE USER sql_admin
            IDENTIFIED WITH authentication_windows
            AS 'Rafal, Tasha, Administrators, "Power Users"';

          CREATE USER sql_admin
            IDENTIFIED WITH authentication_windows
            AS 'Rafal=sql_admin, Tasha=sql_admin, Administrators=sql_admin,
                "Power Users"=sql_admin';

   * Each backslash character ('\') in a value must be doubled because
     backslash is the escape character in MySQL strings.

   * Leading and trailing spaces not inside double quotation marks are
     ignored.

   * Unquoted WIN_USER_OR_GROUP_NAME and MYSQL_USER_NAME values can
     contain anything except equal sign, comma, or space.

   * If a WIN_USER_OR_GROUP_NAME and or MYSQL_USER_NAME value is quoted
     with double quotation marks, everything between the quotation marks
     is part of the value.  This is necessary, for example, if the name
     contains space characters.  All characters within double quotes are
     legal except double quotation mark and backslash.  To include
     either character, escape it with a backslash.

   * WIN_USER_OR_GROUP_NAME values use conventional syntax for Windows
     principals, either local or in a domain.  Examples (note the
     doubling of backslashes):

          domain\\user
          .\\user
          domain\\group
          .\\group
          BUILTIN\\WellKnownGroup

When invoked by the server to authenticate a client, the plugin scans
the authentication string left to right for a user or group match to the
Windows user.  If there is a match, the plugin returns the corresponding
MYSQL_USER_NAME to the MySQL server.  If there is no match,
authentication fails.

A user name match takes preference over a group name match.  Suppose
that the Windows user named 'win_user' is a member of 'win_group' and
the authentication string looks like this:

     'win_group = sql_user1, win_user = sql_user2'

When 'win_user' connects to the MySQL server, there is a match both to
'win_group' and to 'win_user'.  The plugin authenticates the user as
'sql_user2' because the more-specific user match takes precedence over
the group match, even though the group is listed first in the
authentication string.

Windows authentication always works for connections from the same
computer on which the server is running.  For cross-computer
connections, both computers must be registered with Windows Active
Directory.  If they are in the same Windows domain, it is unnecessary to
specify a domain name.  It is also possible to permit connections from a
different domain, as in this example:

     CREATE USER sql_accounting
       IDENTIFIED WITH authentication_windows
       AS 'SomeDomain\\Accounting';

Here 'SomeDomain' is the name of the other domain.  The backslash
character is doubled because it is the MySQL escape character within
strings.

MySQL supports the concept of proxy users whereby a client can connect
and authenticate to the MySQL server using one account but while
connected has the privileges of another account (see *note
proxy-users::).  Suppose that you want Windows users to connect using a
single user name but be mapped based on their Windows user and group
names onto specific MySQL accounts as follows:

   * The 'local_user' and 'MyDomain\domain_user' local and domain
     Windows users should map to the 'local_wlad' MySQL account.

   * Users in the 'MyDomain\Developers' domain group should map to the
     'local_dev' MySQL account.

   * Local machine administrators should map to the 'local_admin' MySQL
     account.

To set this up, create a proxy account for Windows users to connect to,
and configure this account so that users and groups map to the
appropriate MySQL accounts ('local_wlad', 'local_dev', 'local_admin').
In addition, grant the MySQL accounts the privileges appropriate to the
operations they need to perform.  The following instructions use
'win_proxy' as the proxy account, and 'local_wlad', 'local_dev', and
'local_admin' as the proxied accounts.

  1. Create the proxy MySQL account:

          CREATE USER win_proxy
            IDENTIFIED WITH  authentication_windows
            AS 'local_user = local_wlad,
                MyDomain\\domain_user = local_wlad,
                MyDomain\\Developers = local_dev,
                BUILTIN\\Administrators = local_admin';

  2. For proxying to work, the proxied accounts must exist, so create
     them:

          CREATE USER local_wlad IDENTIFIED BY 'SECRET_WLAD_PASS';
          CREATE USER local_dev IDENTIFIED BY 'SECRET_DEV_PASS';
          CREATE USER local_admin IDENTIFIED BY  'SECRET_ADMIN_PASS';

     If you do not let anyone know the passwords for these accounts,
     clients cannot use them to connect directly to the MySQL server.
     Instead, it is expected that users who authenticate using Windows
     will use the 'win_proxy' proxy account.

     You should also execute *note 'GRANT': grant. statements (not
     shown) that grant each proxied account the privileges required for
     MySQL access.

  3. Grant to the proxy account the 'PROXY' privilege for each proxied
     account:

          GRANT PROXY ON local_wlad TO win_proxy;
          GRANT PROXY ON local_dev TO win_proxy;
          GRANT PROXY ON local_admin TO win_proxy;

Now the Windows users 'local_user' and 'MyDomain\domain_user' can
connect to the MySQL server as 'win_proxy' and when authenticated have
the privileges of the account given in the authentication string (in
this case, 'local_wlad').  A user in the 'MyDomain\Developers' group who
connects as 'win_proxy' has the privileges of the 'local_dev' account.
A user in the 'BUILTIN\Administrators' group has the privileges of the
'local_admin' account.

To configure authentication so that all Windows users who do not have
their own MySQL account go through a proxy account, substitute the
default proxy account ('''@''') for 'win_proxy' in the preceding
instructions.  For information about default proxy accounts, see *note
proxy-users::.

*Note*:

If your MySQL installation has anonymous users, they might conflict with
the default proxy user.  For more information about this issue, and ways
of dealing with it, see *note proxy-users-conflicts::.

To use the Windows authentication plugin with Connector/NET connection
strings in Connector/NET 6.4.4 and higher, see Using the Windows Native
Authentication Plugin
(https://dev.mysql.com/doc/connector-net/en/connector-net-programming-authentication-windows-native.html).


File: manual.info.tmp,  Node: socket-pluggable-authentication,  Next: test-pluggable-authentication,  Prev: windows-pluggable-authentication,  Up: authentication-plugins

6.4.1.6 Socket Peer-Credential Pluggable Authentication
.......................................................

As of MySQL 5.5.10, a server-side 'auth_socket' authentication plugin is
available that authenticates clients that connect from the local host
through the Unix socket file.  The plugin uses the 'SO_PEERCRED' socket
option to obtain information about the user running the client program.
Thus, the plugin can be used only on systems that support the
'SO_PEERCRED' option, such as Linux.

The source code for this plugin can be examined as a relatively simple
example demonstrating how to write a loadable authentication plugin.

The following table shows the plugin and library file names.  The file
must be located in the directory named by the 'plugin_dir' system
variable.

*Plugin and Library Names for Socket Peer-Credential Authentication*

Plugin or File         Plugin or File Name
                       
Server-side plugin     'auth_socket'
                       
Client-side plugin     None, see discussion
                       
Library file           'auth_socket.so'

The following sections provide installation and usage information
specific to socket pluggable authentication:

   * *note socket-pluggable-authentication-installation::

   * *note socket-pluggable-authentication-uninstallation::

   * *note socket-pluggable-authentication-usage::

For general information about pluggable authentication in MySQL, see
*note pluggable-authentication::.

*Installing Socket Pluggable Authentication*

This section describes how to install the socket authentication plugin.
For general information about installing plugins, see *note
plugin-loading::.

To be usable by the server, the plugin library file must be located in
the MySQL plugin directory (the directory named by the 'plugin_dir'
system variable).  If necessary, configure the plugin directory location
by setting the value of 'plugin_dir' at server startup.

To load the plugin at server startup, use the '--plugin-load' option to
name the library file that contains it.  With this plugin-loading
method, the option must be given each time the server starts.  For
example, put these lines in the server 'my.cnf' file:

     [mysqld]
     plugin-load=auth_socket.so

After modifying 'my.cnf', restart the server to cause the new settings
to take effect.

Alternatively, to load the plugin at runtime, use this statement:

     INSTALL PLUGIN auth_socket SONAME 'auth_socket.so';

*note 'INSTALL PLUGIN': install-plugin. loads the plugin immediately,
and also registers it in the 'mysql.plugins' system table to cause the
server to load it for each subsequent normal startup without the need
for '--plugin-load'.

To verify plugin installation, examine the *note
'INFORMATION_SCHEMA.PLUGINS': plugins-table. table or use the *note
'SHOW PLUGINS': show-plugins. statement (see *note
obtaining-plugin-information::).  For example:

     mysql> SELECT PLUGIN_NAME, PLUGIN_STATUS
            FROM INFORMATION_SCHEMA.PLUGINS
            WHERE PLUGIN_NAME LIKE '%socket%';
     +-------------+---------------+
     | PLUGIN_NAME | PLUGIN_STATUS |
     +-------------+---------------+
     | auth_socket | ACTIVE        |
     +-------------+---------------+

If the plugin failed to initialize, check the server error log for
diagnostic messages.

To associate MySQL accounts with the socket plugin, see *note
socket-pluggable-authentication-usage::.

*Uninstalling Socket Pluggable Authentication*

The method used to uninstall the socket authentication plugin depends on
how you installed it:

   * If you installed the plugin at server startup using a
     '--plugin-load' option, restart the server without the option.

   * If you installed the plugin at runtime using an *note 'INSTALL
     PLUGIN': install-plugin. statement, it remains installed across
     server restarts.  To uninstall it, use *note 'UNINSTALL PLUGIN':
     uninstall-plugin.:

          UNINSTALL PLUGIN auth_socket;

*Using Socket Pluggable Authentication*

The socket plugin checks whether the socket user name (the operating
system user name) matches the MySQL user name specified by the client
program to the server, and permits the connection only if the names
match.

Suppose that a MySQL account is created for an operating system user
named 'valerie' who is to be authenticated by the 'auth_socket' plugin
for connections from the local host through the socket file:

     CREATE USER 'valerie'@'localhost' IDENTIFIED WITH auth_socket;

If a user on the local host with a login name of 'stefanie' invokes
*note 'mysql': mysql. with the option '--user=valerie' to connect
through the socket file, the server uses 'auth_socket' to authenticate
the client.  The plugin determines that the '--user' option value
('valerie') differs from the client user's name ('stephanie') and
refuses the connection.  If a user named 'valerie' tries the same thing,
the plugin finds that the user name and the MySQL user name are both
'valerie' and permits the connection.  However, the plugin refuses the
connection even for 'valerie' if the connection is made using a
different protocol, such as TCP/IP.


File: manual.info.tmp,  Node: test-pluggable-authentication,  Prev: socket-pluggable-authentication,  Up: authentication-plugins

6.4.1.7 Test Pluggable Authentication
.....................................

MySQL includes a test plugin that checks account credentials and logs
success or failure to the server error log.  This is a loadable plugin
(not built in) and must be installed prior to use.

The test plugin source code is separate from the server source, unlike
the built-in native plugin, so it can be examined as a relatively simple
example demonstrating how to write a loadable authentication plugin.

*Note*:

This plugin is intended for testing and development purposes, and is not
for use in production environments or on servers that are exposed to
public networks.

The following table shows the plugin and library file names.  The file
name suffix might differ on your system.  The file must be located in
the directory named by the 'plugin_dir' system variable.

*Plugin and Library Names for Test Authentication*

Plugin or File         Plugin or File Name
                       
Server-side plugin     'test_plugin_server'
                       
Client-side plugin     'auth_test_plugin'
                       
Library file           'auth_test_plugin.so'

The following sections provide installation and usage information
specific to test pluggable authentication:

   * *note test-pluggable-authentication-installation::

   * *note test-pluggable-authentication-uninstallation::

   * *note test-pluggable-authentication-usage::

For general information about pluggable authentication in MySQL, see
*note pluggable-authentication::.

*Installing Test Pluggable Authentication*

This section describes how to install the test authentication plugin.
For general information about installing plugins, see *note
plugin-loading::.

To be usable by the server, the plugin library file must be located in
the MySQL plugin directory (the directory named by the 'plugin_dir'
system variable).  If necessary, configure the plugin directory location
by setting the value of 'plugin_dir' at server startup.

To load the plugin at server startup, use the '--plugin-load' option to
name the library file that contains it.  With this plugin-loading
method, the option must be given each time the server starts.  For
example, put these lines in the server 'my.cnf' file (adjust the '.so'
suffix for your platform as necessary):

     [mysqld]
     plugin-load=auth_test_plugin.so

After modifying 'my.cnf', restart the server to cause the new settings
to take effect.

Alternatively, to load the plugin at runtime, use this statement (adjust
the '.so' suffix for your platform as necessary):

     INSTALL PLUGIN test_plugin_server SONAME 'auth_test_plugin.so';

*note 'INSTALL PLUGIN': install-plugin. loads the plugin immediately,
and also registers it in the 'mysql.plugins' system table to cause the
server to load it for each subsequent normal startup without the need
for '--plugin-load'.

To verify plugin installation, examine the *note
'INFORMATION_SCHEMA.PLUGINS': plugins-table. table or use the *note
'SHOW PLUGINS': show-plugins. statement (see *note
obtaining-plugin-information::).  For example:

     mysql> SELECT PLUGIN_NAME, PLUGIN_STATUS
            FROM INFORMATION_SCHEMA.PLUGINS
            WHERE PLUGIN_NAME LIKE '%test_plugin%';
     +--------------------+---------------+
     | PLUGIN_NAME        | PLUGIN_STATUS |
     +--------------------+---------------+
     | test_plugin_server | ACTIVE        |
     +--------------------+---------------+

If the plugin failed to initialize, check the server error log for
diagnostic messages.

To associate MySQL accounts with the test plugin, see *note
test-pluggable-authentication-usage::.

*Uninstalling Test Pluggable Authentication*

The method used to uninstall the test authentication plugin depends on
how you installed it:

   * If you installed the plugin at server startup using a
     '--plugin-load' option, restart the server without the option.

   * If you installed the plugin at runtime using an *note 'INSTALL
     PLUGIN': install-plugin. statement, it remains installed across
     server restarts.  To uninstall it, use *note 'UNINSTALL PLUGIN':
     uninstall-plugin.:

          UNINSTALL PLUGIN test_plugin_server;

*Using Test Pluggable Authentication*

To use the test authentication plugin, create an account and name that
plugin in the 'IDENTIFIED WITH' clause:

     CREATE USER 'testuser'@'localhost' IDENTIFIED WITH test_plugin_server;
     SET PASSWORD FOR 'testuser'@'localhost' = PASSWORD('TESTPASSWORD');

Then provide the '--user' and '--password' options for that account when
you connect to the server.  For example:

     shell> mysql --user=testuser --password
     Enter password: TESTPASSWORD

The plugin fetches the password as received from the client and compares
it with the value stored in the 'authentication_string' column of the
account row in the 'mysql.user' system table.  If the two values match,
the plugin returns the 'authentication_string' value as the new
effective user ID.

You can look in the server error log for a message indicating whether
authentication succeeded (notice that the password is reported as the
'user'):

     [Note] Plugin test_plugin_server reported:
     'successfully authenticated user TESTPASSWORD'


File: manual.info.tmp,  Node: audit-log,  Prev: authentication-plugins,  Up: security-plugins

6.4.2 MySQL Enterprise Audit
----------------------------

* Menu:

* audit-log-installation::       Installing MySQL Enterprise Audit
* audit-log-security::           MySQL Enterprise Audit Security Considerations
* audit-log-file-formats::       Audit Log File Formats
* audit-log-logging-control::    Audit Log Logging Control
* audit-log-filtering::          Audit Log Filtering
* audit-log-option-variable-reference::  Audit Log Option and Variable Reference
* audit-log-options-variables::  Audit Log Options and System Variables
* audit-log-restrictions::       Audit Log Restrictions

*Note*:

MySQL Enterprise Audit is an extension included in MySQL Enterprise
Edition, a commercial product.  To learn more about commercial products,
see <https://www.mysql.com/products/>.

As of MySQL 5.5.28, MySQL Enterprise Edition includes MySQL Enterprise
Audit, implemented using a server plugin named 'audit_log'.  MySQL
Enterprise Audit uses the open MySQL Audit API to enable standard,
policy-based monitoring and logging of connection and query activity
executed on specific MySQL servers.  Designed to meet the Oracle audit
specification, MySQL Enterprise Audit provides an out of box, easy to
use auditing and compliance solution for applications that are governed
by both internal and external regulatory guidelines.

When installed, the audit plugin enables MySQL Server to produce a log
file containing an audit record of server activity.  The log contents
include when clients connect and disconnect, and what actions they
perform while connected, such as which databases and tables they access.

After you install the plugin (see *note audit-log-installation::), it
writes an audit log file.  By default, the file is named 'audit.log' in
the server data directory.  To change the name of the file, set the
'audit_log_file' system variable at server startup.

Audit log file contents are not encrypted.  See *note
audit-log-security::.

The audit log file is written in XML, with auditable events encoded as
'<AUDIT_RECORD>' elements.  To select the file format, set the
'audit_log_format' system variable at server startup.  For details on
file format and contents, see *note audit-log-file-formats::.

To control what information the audit log plugin writes to its log file,
set the 'audit_log_policy' system variable.  By default, this variable
is set to 'ALL' (write all auditable events), but also permits values of
'LOGINS' or 'QUERIES' to log only login or query events, or 'NONE' to
disable logging.

For more information about controlling how logging occurs, see *note
audit-log-logging-control::.  For descriptions of the parameters used to
configure the audit log plugin, see *note audit-log-options-variables::.

If the audit log plugin is enabled, the Performance Schema (see *note
performance-schema::) has instrumentation for it.  To identify the
relevant instruments, use this query:

     SELECT NAME FROM performance_schema.setup_instruments
     WHERE NAME LIKE '%/alog/%';

*Changes from Older MySQL Enterprise Audit Versions*

Several changes were made to the audit log plugin in MySQL 5.5.34 for
better compatibility with Oracle Audit Vault.

MySQL 5.7 changed audit log file output to a new format.  This format
has been backported to MySQL 5.5 and it is possible to select either the
old or new format using the 'audit_log_format' system variable, which
has permitted values of 'OLD' and 'NEW' (default 'OLD').  The two
formats differ as follows:

   * Information within '<AUDIT_RECORD>' elements written in the old
     format using attributes is written in the new format using
     subelements.

   * The new format includes more information in '<AUDIT_RECORD>'
     elements.  Every element includes a 'RECORD_ID' value providing a
     unique identifier.  The 'TIMESTAMP' value includes time zone
     information.  Query records include 'HOST', 'IP', 'OS_LOGIN', and
     'USER' information, as well as 'COMMAND_CLASS' and 'STATUS_CODE'
     values.

Example of old '<AUDIT_RECORD>' format:

     <AUDIT_RECORD
      TIMESTAMP="2013-09-15T15:27:27"
      NAME="Query"
      CONNECTION_ID="3"
      STATUS="0"
      SQLTEXT="SELECT 1"
     />

Example of new '<AUDIT_RECORD>' format:

     <AUDIT_RECORD>
      <TIMESTAMP>2013-09-15T15:27:27 UTC</TIMESTAMP>
      <RECORD_ID>3998_2013-09-15T15:27:27</RECORD_ID>
      <NAME>Query</NAME>
      <CONNECTION_ID>3</CONNECTION_ID>
      <STATUS>0</STATUS>
      <STATUS_CODE>0</STATUS_CODE>
      <USER>root[root] @ localhost [127.0.0.1]</USER>
      <OS_LOGIN></OS_LOGIN>
      <HOST>localhost</HOST>
      <IP>127.0.0.1</IP>
      <COMMAND_CLASS>select</COMMAND_CLASS>
      <SQLTEXT>SELECT 1</SQLTEXT>
     </AUDIT_RECORD>

When the audit log plugin rotates the audit log file, it uses a
different file name format.  For a log file named 'audit.log', the
plugin previously renamed the file to 'audit.log.TIMESTAMP'.  The plugin
now renames the file to 'audit.log.TIMESTAMP.xml' to indicate that it is
an XML file.

If you change the value of 'audit_log_format', use this procedure to
avoid writing log entries in one format to an existing log file that
contains entries in a different format:

  1. Stop the server.

  2. Rename the current audit log file manually.

  3. Restart the server with the new value of 'audit_log_format'.  The
     audit log plugin will create a new log file, which will contain log
     entries in the selected format.

The API for writing audit plugins has also changed.  The
'mysql_event_general' structure has new members to represent client host
name and IP address, command class, and external user.  For more
information, see *note writing-audit-plugins::.


File: manual.info.tmp,  Node: audit-log-installation,  Next: audit-log-security,  Prev: audit-log,  Up: audit-log

6.4.2.1 Installing MySQL Enterprise Audit
.........................................

This section describes how to install MySQL Enterprise Audit, which is
implemented using the 'audit_log' plugin.  For general information about
installing plugins, see *note plugin-loading::.

*Note*:

If installed, the 'audit_log' plugin involves some minimal overhead even
when disabled.  To avoid this overhead, do not install MySQL Enterprise
Audit unless you plan to use it.

To be usable by the server, the plugin library file must be located in
the MySQL plugin directory (the directory named by the 'plugin_dir'
system variable).  If necessary, configure the plugin directory location
by setting the value of 'plugin_dir' at server startup.

The plugin library file base name is 'audit_log'.  The file name suffix
differs per platform (for example, '.so' for Unix and Unix-like systems,
'.dll' for Windows).

To load the plugin at server startup, use the '--plugin-load' option to
name the library file that contains it.  With this plugin-loading
method, the option must be given each time the server starts.  For
example, put the following lines in the server 'my.cnf' file (adjust the
'.so' suffix for your platform as necessary):

     [mysqld]
     plugin-load=audit_log.so

After modifying 'my.cnf', restart the server to cause the new settings
to take effect.

Alternatively, to load the plugin at runtime, use this statement (adjust
the '.so' suffix for your platform as necessary):

     INSTALL PLUGIN audit_log SONAME 'audit_log.so';

*note 'INSTALL PLUGIN': install-plugin. loads the plugin, and also
registers it in the 'mysql.plugins' system table to cause the plugin to
be loaded for each subsequent normal server startup without the need for
'--plugin-load'.

To verify plugin installation, examine the *note
'INFORMATION_SCHEMA.PLUGINS': plugins-table. table or use the *note
'SHOW PLUGINS': show-plugins. statement (see *note
obtaining-plugin-information::).  For example:

     mysql> SELECT PLUGIN_NAME, PLUGIN_STATUS
            FROM INFORMATION_SCHEMA.PLUGINS
            WHERE PLUGIN_NAME LIKE 'audit%';
     +-------------+---------------+
     | PLUGIN_NAME | PLUGIN_STATUS |
     +-------------+---------------+
     | audit_log   | ACTIVE        |
     +-------------+---------------+

If the plugin failed to initialize, check the server error log for
diagnostic messages.

If the plugin has been previously registered with *note 'INSTALL
PLUGIN': install-plugin. or is loaded with '--plugin-load', you can use
the '--audit-log' option at server startup to control plugin activation.
For example, to load the plugin at startup and prevent it from being
removed at runtime, use these options:

     [mysqld]
     plugin-load=audit_log.so
     audit-log=FORCE_PLUS_PERMANENT

If it is desired to prevent the server from running without the audit
plugin, use '--audit-log' with a value of 'FORCE' or
'FORCE_PLUS_PERMANENT' to force server startup to fail if the plugin
does not initialize successfully.

For additional information about the parameters used to configure
operation of the 'audit_log' plugin, see *note
audit-log-options-variables::.

Audit log file contents are not encrypted.  See *note
audit-log-security::.


File: manual.info.tmp,  Node: audit-log-security,  Next: audit-log-file-formats,  Prev: audit-log-installation,  Up: audit-log

6.4.2.2 MySQL Enterprise Audit Security Considerations
......................................................

Contents of audit log files produced by the audit log plugin are not
encrypted and may contain sensitive information, such as the text of SQL
statements.  For security reasons, audit log files should be written to
a directory accessible only to the MySQL server and to users with a
legitimate reason to view the log.  The default file name is 'audit.log'
in the data directory.  This can be changed by setting the
'audit_log_file' system variable at server startup.  Other audit log
files may exist due to log rotation.


File: manual.info.tmp,  Node: audit-log-file-formats,  Next: audit-log-logging-control,  Prev: audit-log-security,  Up: audit-log

6.4.2.3 Audit Log File Formats
..............................

The MySQL server calls the audit log plugin to write an audit record to
its log file whenever an auditable event occurs.  Typically the first
audit record written after plugin startup contains the server
description and startup options.  Elements following that one represent
events such as client connect and disconnect events, executed SQL
statements, and so forth.  Only top-level statements are logged, not
statements within stored programs such as triggers or stored procedures.
Contents of files referenced by statements such as *note 'LOAD DATA':
load-data. are not logged.

To select the log format that the audit log plugin uses to write its log
file, set the 'audit_log_format' system variable at server startup.
These formats are available:

   * New-style XML format ('audit_log_format=NEW'): An XML format that
     has better compatibility with Oracle Audit Vault than old-style XML
     format.  MySQL 5.7 introduced this format, which was backported to
     MySQL 5.5 as of MySQL 5.5.34.

   * Old-style XML format ('audit_log_format=OLD'): The original audit
     log format used by default in older MySQL series.  MySQL 5.5 uses
     old-style XML format by default.

*Note*:

Changing the value of 'audit_log_format' can result in writing log
entries in one format to an existing log file that contains entries in a
different format.  To avoid this issue, use the procedure described at
*note audit-log-file-format::.

Audit log file contents are not encrypted.  See *note
audit-log-security::.

The following sections describe the available audit logging formats:

   * *note audit-log-file-new-style-xml-format::

   * *note audit-log-file-old-style-xml-format::

*New-Style XML Audit Log File Format*

Here is a sample log file in new-style XML format
('audit_log_format=NEW'), reformatted slightly for readability:

     <?xml version="1.0" encoding="utf-8"?>
     <AUDIT>
      <AUDIT_RECORD>
       <TIMESTAMP>2017-10-16T14:06:33 UTC</TIMESTAMP>
       <RECORD_ID>1_2017-10-16T14:06:33</RECORD_ID>
       <NAME>Audit</NAME>
       <SERVER_ID>1</SERVER_ID>
       <VERSION>1</VERSION>
       <STARTUP_OPTIONS>/usr/local/mysql/bin/mysqld
         --socket=/usr/local/mysql/mysql.sock
         --port=3306</STARTUP_OPTIONS>
       <OS_VERSION>i686-Linux</OS_VERSION>
       <MYSQL_VERSION>5.5.59-log</MYSQL_VERSION>
      </AUDIT_RECORD>
      <AUDIT_RECORD>
       <TIMESTAMP>2017-10-16T14:09:38 UTC</TIMESTAMP>
       <RECORD_ID>2_2017-10-16T14:06:33</RECORD_ID>
       <NAME>Connect</NAME>
       <CONNECTION_ID>5</CONNECTION_ID>
       <STATUS>0</STATUS>
       <STATUS_CODE>0</STATUS_CODE>
       <USER>root</USER>
       <OS_LOGIN/>
       <HOST>localhost</HOST>
       <IP>127.0.0.1</IP>
       <COMMAND_CLASS>connect</COMMAND_CLASS>
       <PRIV_USER>root</PRIV_USER>
       <PROXY_USER/>
       <DB>test</DB>
      </AUDIT_RECORD>

     ...

      <AUDIT_RECORD>
       <TIMESTAMP>2017-10-16T14:09:38 UTC</TIMESTAMP>
       <RECORD_ID>6_2017-10-16T14:06:33</RECORD_ID>
       <NAME>Query</NAME>
       <CONNECTION_ID>5</CONNECTION_ID>
       <STATUS>0</STATUS>
       <STATUS_CODE>0</STATUS_CODE>
       <USER>root[root] @ localhost [127.0.0.1]</USER>
       <OS_LOGIN/>
       <HOST>localhost</HOST>
       <IP>127.0.0.1</IP>
       <COMMAND_CLASS>drop_table</COMMAND_CLASS>
       <SQLTEXT>DROP TABLE IF EXISTS t</SQLTEXT>
      </AUDIT_RECORD>

     ...

      <AUDIT_RECORD>
       <TIMESTAMP>2017-10-16T14:09:39 UTC</TIMESTAMP>
       <RECORD_ID>8_2017-10-16T14:06:33</RECORD_ID>
       <NAME>Quit</NAME>
       <CONNECTION_ID>5</CONNECTION_ID>
       <STATUS>0</STATUS>
       <STATUS_CODE>0</STATUS_CODE>
       <USER>root</USER>
       <OS_LOGIN/>
       <HOST>localhost</HOST>
       <IP>127.0.0.1</IP>
       <COMMAND_CLASS>connect</COMMAND_CLASS>
      </AUDIT_RECORD>

     ...

      <AUDIT_RECORD>
       <TIMESTAMP>2017-10-16T14:09:43 UTC</TIMESTAMP>
       <RECORD_ID>11_2017-10-16T14:06:33</RECORD_ID>
       <NAME>Quit</NAME>
       <CONNECTION_ID>6</CONNECTION_ID>
       <STATUS>0</STATUS>
       <STATUS_CODE>0</STATUS_CODE>
       <USER>root</USER>
       <OS_LOGIN/>
       <HOST>localhost</HOST>
       <IP>127.0.0.1</IP>
       <COMMAND_CLASS>connect</COMMAND_CLASS>
      </AUDIT_RECORD>
      <AUDIT_RECORD>
       <TIMESTAMP>2017-10-16T14:09:45 UTC</TIMESTAMP>
       <RECORD_ID>12_2017-10-16T14:06:33</RECORD_ID>
       <NAME>NoAudit</NAME>
       <SERVER_ID>1</SERVER_ID>
      </AUDIT_RECORD>
     </AUDIT>

The audit log file is written as XML, using UTF-8 (up to 4 bytes per
character).  The root element is '<AUDIT>'.  The root element contains
'<AUDIT_RECORD>' elements, each of which provides information about an
audited event.  When the audit log plugin begins writing a new log file,
it writes the XML declaration and opening '<AUDIT>' root element tag.
When the plugin closes a log file, it writes the closing '</AUDIT>' root
element tag.  The closing tag is not present while the file is open.

Elements within '<AUDIT_RECORD>' elements have these characteristics:

   * Some elements appear in every '<AUDIT_RECORD>' element.  Others are
     optional and may appear depending on the audit record type.

   * Order of elements within an '<AUDIT_RECORD>' element is not
     guaranteed.

   * Element values are not fixed length.  Long values may be truncated
     as indicated in the element descriptions given later.

   * The '<', '>', '"', and '&' characters are encoded as '&lt;',
     '&gt;', '&quot;', and '&amp;', respectively.  NUL bytes (U+00) are
     encoded as the '?' character.

   * Characters not valid as XML characters are encoded using numeric
     character references.  Valid XML characters are:

          #x9 | #xA | #xD | [#x20-#xD7FF] | [#xE000-#xFFFD] | [#x10000-#x10FFFF]

The following elements are mandatory in every '<AUDIT_RECORD>' element:

   * '<NAME>'

     A string representing the type of instruction that generated the
     audit event, such as a command that the server received from a
     client.

     Example:

          <NAME>Query</NAME>

     Some common '<NAME>' values:

          Audit    When auditing starts, which may be server startup time
          Connect  When a client connects, also known as logging in
          Query    An SQL statement (executed directly)
          Prepare  Preparation of an SQL statement; usually followed by Execute
          Execute  Execution of an SQL statement; usually follows Prepare
          Shutdown Server shutdown
          Quit     When a client disconnects
          NoAudit  Auditing has been turned off

     The possible values are 'Audit', 'Binlog Dump', 'Change user',
     'Close stmt', 'Connect Out', 'Connect', 'Create DB', 'Daemon',
     'Debug', 'Delayed insert', 'Drop DB', 'Execute', 'Fetch', 'Field
     List', 'Init DB', 'Kill', 'Long Data', 'NoAudit', 'Ping',
     'Prepare', 'Processlist', 'Query', 'Quit', 'Refresh', 'Register
     Slave', 'Reset stmt', 'Set option', 'Shutdown', 'Sleep',
     'Statistics', 'Table Dump', 'Time'.

     With the exception of 'Audit' and 'NoAudit', these values
     correspond to the 'COM_XXX' command values listed in the
     'mysql_com.h' header file.  For example, 'Create DB' and 'Change
     user' correspond to 'COM_CREATE_DB' and 'COM_CHANGE_USER',
     respectively.

   * '<RECORD_ID>'

     A unique identifier for the audit record.  The value is composed
     from a sequence number and timestamp, in the format
     'SEQ_TIMESTAMP'.  When the audit log plugin opens the audit log
     file, it initializes the sequence number to the size of the audit
     log file, then increments the sequence by 1 for each record logged.
     The timestamp is a UTC value in 'YYYY-MM-DDTHH:MM:SS' format
     indicating the date and time when the audit log plugin opened the
     file.

     Example:

          <RECORD_ID>12_2017-10-16T14:06:33</RECORD_ID>

   * '<TIMESTAMP>'

     A string representing a UTC value in 'YYYY-MM-DDTHH:MM:SS UTC'
     format indicating the date and time when the audit event was
     generated.  For example, the event corresponding to execution of an
     SQL statement received from a client has a '<TIMESTAMP>' value
     occurring after the statement finishes, not when it was received.

     Example:

          <TIMESTAMP>2017-10-16T14:09:45 UTC</TIMESTAMP>

The following elements are optional in '<AUDIT_RECORD>' elements.  Many
of them occur only with specific '<NAME>' element values.

   * '<COMMAND_CLASS>'

     A string that indicates the type of action performed.

     Example:

          <COMMAND_CLASS>drop_table</COMMAND_CLASS>

     The values correspond to the 'Com_XXX' status variables that
     indicate command counts; for example 'Com_drop_table' and
     'Com_select' count *note 'DROP TABLE': drop-table. and *note
     'SELECT': select. statements, respectively.  The following
     statement displays the possible names:

          SELECT LOWER(REPLACE(VARIABLE_NAME, 'COM_', '')) AS name
          FROM INFORMATION_SCHEMA.GLOBAL_STATUS
          WHERE VARIABLE_NAME LIKE 'COM%'
          ORDER BY name;

   * '<CONNECTION_ID>'

     An unsigned integer representing the client connection identifier.
     This is the same as the value returned by the 'CONNECTION_ID()'
     function within the session.

     Example:

          <CONNECTION_ID>127</CONNECTION_ID>

   * '<DB>'

     A string representing the default database name.

     Example:

          <DB>test</DB>

   * '<HOST>'

     A string representing the client host name.

     Example:

          <HOST>localhost</HOST>

   * '<IP>'

     A string representing the client IP address.

     Example:

          <IP>127.0.0.1</IP>

   * '<MYSQL_VERSION>'

     A string representing the MySQL server version.  This is the same
     as the value of the 'VERSION()' function or 'version' system
     variable.

     Example:

          <MYSQL_VERSION>5.5.59-log</MYSQL_VERSION>

   * '<OS_LOGIN>'

     A string representing the external user (empty if none).  The value
     may differ from the '<USER>' value, for example, if the server
     authenticates the client using an external authentication method.

     Example:

          <OS_LOGIN>jeffrey</OS_LOGIN>

   * '<OS_VERSION>'

     A string representing the operating system on which the server was
     built or is running.

     Example:

          <OS_VERSION>x86_64-Linux</OS_VERSION>

   * '<PRIV_USER>'

     A string representing the user that the server authenticated the
     client as.  This is the user name that the server uses for
     privilege checking, and may differ from the '<USER>' value.

     Example:

          <PRIV_USER>jeffrey</PRIV_USER>

   * '<PROXY_USER>'

     A string representing the proxy user (see *note proxy-users::).
     The value is empty if user proxying is not in effect.

     Example:

          <PROXY_USER>developer</PROXY_USER>

   * '<SERVER_ID>'

     An unsigned integer representing the server ID. This is the same as
     the value of the 'server_id' system variable.

     Example:

          <SERVER_ID>1</SERVER_ID>

   * '<SQLTEXT>'

     A string representing the text of an SQL statement.  The value can
     be empty.  Long values may be truncated.  The string, like the
     audit log file itself, is written using UTF-8 (up to 4 bytes per
     character), so the value may be the result of conversion.  For
     example, the original statement might have been received from the
     client as an SJIS string.

     Example:

          <SQLTEXT>DELETE FROM t1</SQLTEXT>

   * '<STARTUP_OPTIONS>'

     A string representing the options that were given on the command
     line or in option files when the MySQL server was started.  The
     first option is the path to the server executable.

     Example:

          <STARTUP_OPTIONS>/usr/local/mysql/bin/mysqld
            --port=3306 --log_output=FILE</STARTUP_OPTIONS>

   * '<STATUS>'

     An unsigned integer representing the command status: 0 for success,
     nonzero if an error occurred.  This is the same as the value of the
     *note 'mysql_errno()': mysql-errno. C API function.  See the
     description for '<STATUS_CODE>' for information about how it
     differs from '<STATUS>'.

     The audit log does not contain the SQLSTATE value or error message.
     To see the associations between error codes, SQLSTATE values, and
     messages, see *note server-error-reference::.

     Warnings are not logged.

     Example:

          <STATUS>1051</STATUS>

   * '<STATUS_CODE>'

     An unsigned integer representing the command status: 0 for success,
     1 if an error occurred.

     The 'STATUS_CODE' value differs from the 'STATUS' value:
     'STATUS_CODE' is 0 for success and 1 for error, which is compatible
     with the EZ_collector consumer for Audit Vault.  'STATUS' is the
     value of the *note 'mysql_errno()': mysql-errno. C API function.
     This is 0 for success and nonzero for error, and thus is not
     necessarily 1 for error.

     Example:

          <STATUS_CODE>0</STATUS_CODE>

   * '<USER>'

     A string representing the user name sent by the client.  This may
     differ from the '<PRIV_USER>' value.

     Example:

          <USER>root[root] @ localhost [127.0.0.1]</USER>

   * '<VERSION>'

     An unsigned integer representing the version of the audit log file
     format.

     Example:

          <VERSION>1</VERSION>

*Old-Style XML Audit Log File Format*

Here is a sample log file in old-style XML format
('audit_log_format=OLD'), reformatted slightly for readability:

     <?xml version="1.0" encoding="utf-8"?>
     <AUDIT>
       <AUDIT_RECORD
         TIMESTAMP="2017-10-16T14:25:00 UTC"
         RECORD_ID="1_2017-10-16T14:25:00"
         NAME="Audit"
         SERVER_ID="1"
         VERSION="1"
         STARTUP_OPTIONS="--port=3306"
         OS_VERSION="i686-Linux"
         MYSQL_VERSION="5.5.59-log"/>
       <AUDIT_RECORD
         TIMESTAMP="2017-10-16T14:25:24 UTC"
         RECORD_ID="2_2017-10-16T14:25:00"
         NAME="Connect"
         CONNECTION_ID="4"
         STATUS="0"
         STATUS_CODE="0"
         USER="root"
         OS_LOGIN=""
         HOST="localhost"
         IP="127.0.0.1"
         COMMAND_CLASS="connect"
         PRIV_USER="root"
         PROXY_USER=""
         DB="test"/>

     ...

       <AUDIT_RECORD
         TIMESTAMP="2017-10-16T14:25:24 UTC"
         RECORD_ID="6_2017-10-16T14:25:00"
         NAME="Query"
         CONNECTION_ID="4"
         STATUS="0"
         STATUS_CODE="0"
         USER="root[root] @ localhost [127.0.0.1]"
         OS_LOGIN=""
         HOST="localhost"
         IP="127.0.0.1"
         COMMAND_CLASS="drop_table"
         SQLTEXT="DROP TABLE IF EXISTS t"/>

     ...

       <AUDIT_RECORD
         TIMESTAMP="2017-10-16T14:25:24 UTC"
         RECORD_ID="8_2017-10-16T14:25:00"
         NAME="Quit"
         CONNECTION_ID="4"
         STATUS="0"
         STATUS_CODE="0"
         USER="root"
         OS_LOGIN=""
         HOST="localhost"
         IP="127.0.0.1"
         COMMAND_CLASS="connect"
       <AUDIT_RECORD
         TIMESTAMP="2017-10-16T14:25:32 UTC"
         RECORD_ID="12_2017-10-16T14:25:00"
         NAME="NoAudit"
         SERVER_ID="1"/>
     </AUDIT>

The audit log file is written as XML, using UTF-8 (up to 4 bytes per
character).  The root element is '<AUDIT>'.  The root element contains
'<AUDIT_RECORD>' elements, each of which provides information about an
audited event.  When the audit log plugin begins writing a new log file,
it writes the XML declaration and opening '<AUDIT>' root element tag.
When the plugin closes a log file, it writes the closing '</AUDIT>' root
element tag.  The closing tag is not present while the file is open.

Attributes of '<AUDIT_RECORD>' elements have these characteristics:

   * Some attributes appear in every '<AUDIT_RECORD>' element.  Others
     are optional and may appear depending on the audit record type.

   * Order of attributes within an '<AUDIT_RECORD>' element is not
     guaranteed.

   * Attribute values are not fixed length.  Long values may be
     truncated as indicated in the attribute descriptions given later.

   * The '<', '>', '"', and '&' characters are encoded as '&lt;',
     '&gt;', '&quot;', and '&amp;', respectively.  NUL bytes (U+00) are
     encoded as the '?' character.

   * Characters not valid as XML characters are encoded using numeric
     character references.  Valid XML characters are:

          #x9 | #xA | #xD | [#x20-#xD7FF] | [#xE000-#xFFFD] | [#x10000-#x10FFFF]

The following attributes are mandatory in every '<AUDIT_RECORD>'
element:

   * 'NAME'

     A string representing the type of instruction that generated the
     audit event, such as a command that the server received from a
     client.

     Example: 'NAME="Query"'

     Some common 'NAME' values:

          Audit    When auditing starts, which may be server startup time
          Connect  When a client connects, also known as logging in
          Query    An SQL statement (executed directly)
          Prepare  Preparation of an SQL statement; usually followed by Execute
          Execute  Execution of an SQL statement; usually follows Prepare
          Shutdown Server shutdown
          Quit     When a client disconnects
          NoAudit  Auditing has been turned off

     The possible values are 'Audit', 'Binlog Dump', 'Change user',
     'Close stmt', 'Connect Out', 'Connect', 'Create DB', 'Daemon',
     'Debug', 'Delayed insert', 'Drop DB', 'Execute', 'Fetch', 'Field
     List', 'Init DB', 'Kill', 'Long Data', 'NoAudit', 'Ping',
     'Prepare', 'Processlist', 'Query', 'Quit', 'Refresh', 'Register
     Slave', 'Reset stmt', 'Set option', 'Shutdown', 'Sleep',
     'Statistics', 'Table Dump', 'Time'.

     With the exception of '"Audit"' and '"NoAudit"', these values
     correspond to the 'COM_XXX' command values listed in the
     'mysql_com.h' header file.  For example, '"Create DB"' and '"Change
     user"' correspond to 'COM_CREATE_DB' and 'COM_CHANGE_USER',
     respectively.

   * 'RECORD_ID'

     A unique identifier for the audit record.  The value is composed
     from a sequence number and timestamp, in the format
     'SEQ_TIMESTAMP'.  When the audit log plugin opens the audit log
     file, it initializes the sequence number to the size of the audit
     log file, then increments the sequence by 1 for each record logged.
     The timestamp is a UTC value in 'YYYY-MM-DDTHH:MM:SS' format
     indicating the date and time when the audit log plugin opened the
     file.

     Example: 'RECORD_ID="12_2017-10-16T14:25:00"'

   * 'TIMESTAMP'

     A string representing a UTC value in 'YYYY-MM-DDTHH:MM:SS UTC'
     format indicating the date and time when the audit event was
     generated.  For example, the event corresponding to execution of an
     SQL statement received from a client has a 'TIMESTAMP' value
     occurring after the statement finishes, not when it was received.

     Example: 'TIMESTAMP="2017-10-16T14:25:32 UTC"'

The following attributes are optional in '<AUDIT_RECORD>' elements.
Many of them occur only for elements with specific values of the 'NAME'
attribute.

   * 'COMMAND_CLASS'

     A string that indicates the type of action performed.

     Example: 'COMMAND_CLASS="drop_table"'

     The values correspond to the 'Com_XXX' status variables that
     indicate command counts; for example 'Com_drop_table' and
     'Com_select' count *note 'DROP TABLE': drop-table. and *note
     'SELECT': select. statements, respectively.  The following
     statement displays the possible names:

          SELECT LOWER(REPLACE(VARIABLE_NAME, 'COM_', '')) AS name
          FROM INFORMATION_SCHEMA.GLOBAL_STATUS
          WHERE VARIABLE_NAME LIKE 'COM%'
          ORDER BY name;

   * 'CONNECTION_ID'

     An unsigned integer representing the client connection identifier.
     This is the same as the value returned by the 'CONNECTION_ID()'
     function within the session.

     Example: 'CONNECTION_ID="127"'

   * 'DB'

     A string representing the default database name.

     Example: 'DB="test"'

   * 'HOST'

     A string representing the client host name.

     Example: 'HOST="localhost"'

   * 'IP'

     A string representing the client IP address.

     Example: 'IP="127.0.0.1"'

   * 'MYSQL_VERSION'

     A string representing the MySQL server version.  This is the same
     as the value of the 'VERSION()' function or 'version' system
     variable.

     Example: 'MYSQL_VERSION="5.5.31-log"'

   * 'OS_LOGIN'

     A string representing the external user (empty if none).  The value
     may differ from 'USER', for example, if the server authenticates
     the client using an external authentication method.

     Example: 'OS_LOGIN="jeffrey"'

   * 'OS_VERSION'

     A string representing the operating system on which the server was
     built or is running.

     Example: 'OS_VERSION="x86_64-Linux"'

   * 'PRIV_USER'

     A string representing the user that the server authenticated the
     client as.  This is the user name that the server uses for
     privilege checking, and it may differ from the 'USER' value.

     Example: 'PRIV_USER="jeffrey"'

   * 'PROXY_USER'

     A string representing the proxy user (see *note proxy-users::).
     The value is empty if user proxying is not in effect.

     Example: 'PROXY_USER="developer"'

   * 'SERVER_ID'

     An unsigned integer representing the server ID. This is the same as
     the value of the 'server_id' system variable.

     Example: 'SERVER_ID="1"'

   * 'SQLTEXT'

     A string representing the text of an SQL statement.  The value can
     be empty.  Long values may be truncated.  The string, like the
     audit log file itself, is written using UTF-8 (up to 4 bytes per
     character), so the value may be the result of conversion.  For
     example, the original statement might have been received from the
     client as an SJIS string.

     Example: 'SQLTEXT="DELETE FROM t1"'

   * 'STARTUP_OPTIONS'

     A string representing the options that were given on the command
     line or in option files when the MySQL server was started.

     Example: 'STARTUP_OPTIONS="--port=3306 --log_output=FILE"'

   * 'STATUS'

     An unsigned integer representing the command status: 0 for success,
     nonzero if an error occurred.  This is the same as the value of the
     *note 'mysql_errno()': mysql-errno. C API function.  See the
     description for 'STATUS_CODE' for information about how it differs
     from 'STATUS'.

     The audit log does not contain the SQLSTATE value or error message.
     To see the associations between error codes, SQLSTATE values, and
     messages, see *note server-error-reference::.

     Warnings are not logged.

     Example: 'STATUS="1051"'

   * 'STATUS_CODE'

     An unsigned integer representing the command status: 0 for success,
     1 if an error occurred.

     The 'STATUS_CODE' value differs from the 'STATUS' value:
     'STATUS_CODE' is 0 for success and 1 for error, which is compatible
     with the EZ_collector consumer for Audit Vault.  'STATUS' is the
     value of the *note 'mysql_errno()': mysql-errno. C API function.
     This is 0 for success and nonzero for error, and thus is not
     necessarily 1 for error.

     Example: 'STATUS_CODE="0"'

   * 'USER'

     A string representing the user name sent by the client.  This may
     differ from the 'PRIV_USER' value.

   * 'VERSION'

     An unsigned integer representing the version of the audit log file
     format.

     Example: 'VERSION="1"'


File: manual.info.tmp,  Node: audit-log-logging-control,  Next: audit-log-filtering,  Prev: audit-log-file-formats,  Up: audit-log

6.4.2.4 Audit Log Logging Control
.................................

This section describes how to control general characteristics of audit
logging, such as the file to which the audit log plugin writes events
and the format of written events.

   * *note audit-log-file-name::

   * *note audit-log-file-format::

   * *note audit-log-strategy::

   * *note audit-log-space-management::

For additional information about the system variables that affect audit
logging, see *note audit-log-options-variables::.

*Audit Log File Name*

To control the audit log file name, set the 'audit_log_file' system
variable at server startup.  By default, the name is 'audit.log' in the
server data directory.  For security reasons, the audit log file should
be written to a directory accessible only to the MySQL server and to
users with a legitimate reason to view the log.

When the audit plugin initializes, it checks whether a file with the
audit log file name already exists.  If so, the plugin checks whether
the file ends with an '</AUDIT>' tag and truncates the tag before
writing any '<AUDIT_RECORD>' elements.  If the log file exists but does
not end with '</AUDIT>' or the '</AUDIT>' tag cannot be truncated, the
plugin considers the file malformed and fails to initialize.  This can
occur if the server exits unexpectedly with the audit log plugin
running.  No logging occurs until the problem is rectified.  Check the
error log for diagnostic information:

     [ERROR] Plugin 'audit_log' init function returned error.

To deal with this problem, either remove or rename the malformed log
file and restart the server.

*Audit Log File Format*

To control the audit log file format, set the 'audit_log_format' system
variable at server startup.  By default, the format is 'OLD' (old-style
XML format).  For information about available formats, see *note
audit-log-file-formats::.

*Note*:

Changing the value of 'audit_log_format' can result in writing log
entries in one format to an existing log file that contains entries in a
different format.  To avoid this issue, use the following procedure:

  1. Stop the server.

  2. Either change the value of the 'audit_log_file' system variable so
     the plugin writes to a different file, or rename the current audit
     log file manually.

  3. Restart the server with the new value of 'audit_log_format'.  The
     audit log plugin creates a new log file and writes entries to it in
     the selected format.

*Audit Logging Write Strategy*

The audit log plugin can use any of several strategies for log writes.
Regardless of strategy, logging occurs on a best-effort basis, with no
guarantee of consistency.

To specify a write strategy, set the 'audit_log_strategy' system
variable at server startup.  By default, the strategy value is
'ASYNCHRONOUS' and the plugin logs asynchronously to a buffer, waiting
if the buffer is full.  It's possible to tell the plugin not to wait
('PERFORMANCE') or to log synchronously, either using file system
caching ('SEMISYNCHRONOUS') or forcing output with a 'sync()' call after
each write request ('SYNCHRONOUS').

For asynchronous write strategy, the 'audit_log_buffer_size' system
variable is the buffer size in bytes.  Set this variable at server
startup to change the buffer size.  The plugin uses a single buffer,
which it allocates when it initializes and removes when it terminates.
The plugin does not allocate this buffer for nonasynchronous write
strategies.

Asynchronous logging strategy has these characteristics:

   * Minimal impact on server performance and scalability.

   * Blocking of threads that generate audit events for the shortest
     possible time; that is, time to allocate the buffer plus time to
     copy the event to the buffer.

   * Output goes to the buffer.  A separate thread handles writes from
     the buffer to the log file.

With asynchronous logging, the integrity of the log file may be
compromised if a problem occurs during a write to the file or if the
plugin does not shut down cleanly (for example, in the event that the
server host exits unexpectedly).  To reduce this risk, set
'audit_log_strategy' to use synchronous logging.

A disadvantage of 'PERFORMANCE' strategy is that it drops events when
the buffer is full.  For a heavily loaded server, the audit log may have
events missing.

*Audit Log File Space Management and Name Rotation*

The audit log file has the potential to grow very large and consume a
lot of disk space.  To enable management of the space used by its log
files, the audit log plugin provides the 'audit_log_rotate_on_size' and
'audit_log_flush' system variables, which control audit log file
rotation and flushing.  Rotation can be done manually, or automatically
based on file size.

Manual audit log file rotation

By default, 'audit_log_rotate_on_size=0' and there is no log rotation
except that which you perform manually.  In this case, the audit log
plugin closes and reopens the log file when the 'audit_log_flush' value
changes from disabled to enabled.  Log file renaming must be done
externally to the server.  Suppose that the log file name is 'audit.log'
and you want to maintain the three most recent log files, cycling
through the names 'audit.log.1' through 'audit.log.3'.  On Unix, perform
rotation manually like this:

  1. From the command line, rename the current log files:

          mv audit.log.2 audit.log.3
          mv audit.log.1 audit.log.2
          mv audit.log audit.log.1

     At this point, the plugin is still writing to the current log file,
     which has been renamed to 'audit.log.1'.

  2. Connect to the server and flush the log file so the plugin closes
     it and reopens a new 'audit.log' file:

          SET GLOBAL audit_log_flush = ON;

Automatic size-based audit log file rotation

If 'audit_log_rotate_on_size' is greater than 0, setting
'audit_log_flush' has no effect.  Instead, whenever a write to the log
file causes its size to exceed the 'audit_log_rotate_on_size' value, the
audit log plugin closes the file, renames it, and opens a new log file.

The renamed file has a timestamp and '.xml' added to the end.  For
example, if the file name is 'audit.log', the plugin renames it to a
value such as 'audit.log.15081807937726520.xml'.  The last 7 digits are
a fractional second part.  The first 10 digits are a Unix timestamp
value that can be interpreted using the 'FROM_UNIXTIME()' function:

     mysql> SELECT FROM_UNIXTIME(1508180793);
     +---------------------------+
     | FROM_UNIXTIME(1508180793) |
     +---------------------------+
     | 2017-10-16 14:06:33       |
     +---------------------------+

*Note*:

With size-based log file rotation, renamed log files do not rotate off
the end of the name sequence.  Instead, they have unique names and
accumulate indefinitely.  To avoid excessive space use, remove old files
periodically, backing them up first as necessary.


File: manual.info.tmp,  Node: audit-log-filtering,  Next: audit-log-option-variable-reference,  Prev: audit-log-logging-control,  Up: audit-log

6.4.2.5 Audit Log Filtering
...........................

The 'audit_log_policy' system variable controls what kinds of
information the plugin writes.  By default, this variable is set to
'ALL' (write all auditable events), but also permits values of 'LOGINS'
or 'QUERIES' to log only login or query events, or 'NONE' to disable
logging.


File: manual.info.tmp,  Node: audit-log-option-variable-reference,  Next: audit-log-options-variables,  Prev: audit-log-filtering,  Up: audit-log

6.4.2.6 Audit Log Option and Variable Reference
...............................................

*Audit Log Option and Variable Reference*

Name           Cmd-Line    Option      System      Status      Var Scope   Dynamic
                           File        Var         Var                     
                                                   
audit-log      Yes         Yes                                             
                           
audit_log_buffer_sizeYes   Yes         Yes                     Global      No
                                                                           
audit_log_file Yes         Yes         Yes                     Global      No
                                                                           
audit_log_flush                        Yes                     Global      Yes
                                                                           
audit_log_formatYes        Yes         Yes                     Global      No
                                                                           
audit_log_policyYes        Yes         Yes                     Global      Yes
                                                                           
audit_log_rotate_on_sizeYesYes         Yes                     Global      Yes
                                                                           
audit_log_strategyYes      Yes         Yes                     Global      No
                                                               


File: manual.info.tmp,  Node: audit-log-options-variables,  Next: audit-log-restrictions,  Prev: audit-log-option-variable-reference,  Up: audit-log

6.4.2.7 Audit Log Options and System Variables
..............................................

This section describes the command options and system variables that
control operation of MySQL Enterprise Audit.  If values specified at
startup time are incorrect, the audit log plugin may fail to initialize
properly and the server does not load it.  In this case, the server may
also produce error messages for other audit log settings because it will
not recognize them.

To control activation of the 'audit_log' plugin, use this option:

   * 
     '--audit-log[=VALUE]'

     Property               Value
                            
     *Command-Line          '--audit-log[=value]'
     Format*                

     *Introduced*           5.5.28
                            
     *Type*                 Enumeration
                            
     *Default Value*        'ON'
                            
     *Valid Values*         'ON' 'OFF' 'FORCE' 'FORCE_PLUS_PERMANENT'

     This option controls how the server loads the 'audit_log' plugin at
     startup.  It is available only if the plugin has been previously
     registered with *note 'INSTALL PLUGIN': install-plugin. or is
     loaded with '--plugin-load'.  See *note audit-log-installation::.

     The option value should be one of those available for
     plugin-loading options, as described in *note plugin-loading::.
     For example, '--audit-log=FORCE_PLUS_PERMANENT' tells the server to
     load the plugin at startup and prevents it from being removed while
     the server is running.

     This option was added in MySQL 5.5.28.

If the audit log plugin is enabled, it exposes several system variables
that permit control over logging:

     mysql> SHOW VARIABLES LIKE 'audit_log%';
     +--------------------------+--------------+
     | Variable_name            | Value        |
     +--------------------------+--------------+
     | audit_log_buffer_size    | 1048576      |
     | audit_log_file           | audit.log    |
     | audit_log_flush          | OFF          |
     | audit_log_policy         | ALL          |
     | audit_log_rotate_on_size | 0            |
     | audit_log_strategy       | ASYNCHRONOUS |
     +--------------------------+--------------+

You can set any of these variables at server startup, and some of them
at runtime.

   * 
     'audit_log_buffer_size'

     Property               Value
                            
     *Command-Line          '--audit-log-buffer-size=#'
     Format*                

     *Introduced*           5.5.28
                            
     *System Variable*      'audit_log_buffer_size'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Integer
                            
     *Default Value*        '1048576'
                            
     *Minimum Value*        '4096'
                            
     *Maximum Value*        '18446744073709547520'
     (64-bit platforms)     

     *Maximum Value*        '4294967295'
     (32-bit platforms)

     When the audit log plugin writes events to the log asynchronously,
     it uses a buffer to store event contents prior to writing them.
     This variable controls the size of that buffer, in bytes.  The
     server adjusts the value to a multiple of 4096.  The plugin uses a
     single buffer, which it allocates when it initializes and removes
     when it terminates.  The plugin allocates this buffer only if
     logging is asynchronous.

     This variable was added in MySQL 5.5.28.

   * 
     'audit_log_file'

     Property               Value
                            
     *Command-Line          '--audit-log-file=file_name'
     Format*                

     *Introduced*           5.5.28
                            
     *System Variable*      'audit_log_file'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 File name
                            
     *Default Value*        'audit.log'

     The name of the file to which the audit log plugin writes events.
     The default value is 'audit.log'.  If the file name is a relative
     path, the server interprets it relative to the data directory.  For
     security reasons, the audit log file should be written to a
     directory accessible only to the MySQL server and to users with a
     legitimate reason to view the log.

     This variable was added in MySQL 5.5.28.

   * 
     'audit_log_flush'

     Property               Value
                            
     *Introduced*           5.5.28
                            
     *System Variable*      'audit_log_flush'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Boolean
                            
     *Default Value*        'OFF'

     When this variable is set to enabled (1 or 'ON'), the audit log
     plugin closes and reopens its log file to flush it.  (The value
     remains 'OFF' so that you need not disable it explicitly before
     enabling it again to perform another flush.)  Enabling this
     variable has no effect unless 'audit_log_rotate_on_size' is 0.

     This variable was added in MySQL 5.5.28.

   * 
     'audit_log_format'

     Property               Value
                            
     *Command-Line          '--audit-log-format=value'
     Format*                

     *Introduced*           5.5.34
                            
     *System Variable*      'audit_log_format'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Enumeration
                            
     *Default Value*        'OLD'
                            
     *Valid Values*         'OLD' 'NEW'

     The audit log file format.  Permitted values are 'OLD' and 'NEW'
     (default 'OLD').  For details about each format, see *note
     audit-log-file-formats::.

     Changing the value of 'audit_log_format' can result in writing log
     entries in one format to an existing log file that contains entries
     in a different format.  To avoid this issue, use the procedure
     described at *note audit-log-file-format::.

     This variable was added in MySQL 5.5.34.

   * 
     'audit_log_policy'

     Property               Value
                            
     *Command-Line          '--audit-log-policy=value'
     Format*                

     *Introduced*           5.5.28
                            
     *System Variable*      'audit_log_policy'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Enumeration
                            
     *Default Value*        'ALL'
                            
     *Valid Values*         'ALL' 'LOGINS' 'QUERIES' 'NONE'

     The policy controlling the information written by the audit log
     plugin to its log file.  The following table shows the permitted
     values.

     Value       Description
                 
     'ALL'       Log all events
                 
     'NONE'      Log nothing (disable the audit stream)
                 
     'LOGINS'    Log only login events
                 
     'QUERIES'   Log only query events

     This variable was added in MySQL 5.5.28.

   * 
     'audit_log_rotate_on_size'

     Property               Value
                            
     *Command-Line          '--audit-log-rotate-on-size=#'
     Format*                

     *Introduced*           5.5.28
                            
     *System Variable*      'audit_log_rotate_on_size'
                            
     *Scope*                Global
                            
     *Dynamic*              Yes
                            
     *Type*                 Integer
                            
     *Default Value*        '0'

     If the 'audit_log_rotate_on_size' value is 0, the audit log plugin
     does not perform automatic log file rotation.  Instead, use
     'audit_log_flush' to close and reopen the log on demand.  In this
     case, manually rename the file externally to the server before
     flushing it.

     If the 'audit_log_rotate_on_size' value is greater than 0,
     automatic size-based log file rotation occurs.  Whenever a write to
     the log file causes its size to exceed the
     'audit_log_rotate_on_size' value, the audit log plugin closes the
     current log file, renames it, and opens a new log file.

     For more information about audit log file rotation, see *note
     audit-log-space-management::.

     If you set this variable to a value that is not a multiple of 4096,
     it is truncated to the nearest multiple.  (Thus, setting it to a
     value less than 4096 has the effect of setting it to 0 and no
     rotation occurs, except manually.)

     This variable was added in MySQL 5.5.28.

   * 
     'audit_log_strategy'

     Property               Value
                            
     *Command-Line          '--audit-log-strategy=value'
     Format*                

     *Introduced*           5.5.28
                            
     *System Variable*      'audit_log_strategy'
                            
     *Scope*                Global
                            
     *Dynamic*              No
                            
     *Type*                 Enumeration
                            
     *Default Value*        'ASYNCHRONOUS'
                            
     *Valid Values*         'ASYNCHRONOUS' 'PERFORMANCE' 'SEMISYNCHRONOUS'
                            'SYNCHRONOUS'

     The logging method used by the audit log plugin.  These strategy
     values are permitted:

        * 'ASYNCHRONOUS': Log asynchronously.  Wait for space in the
          output buffer.

        * 'PERFORMANCE': Log asynchronously.  Drop requests for which
          there is insufficient space in the output buffer.

        * 'SEMISYNCHRONOUS': Log synchronously.  Permit caching by the
          operating system.

        * 'SYNCHRONOUS': Log synchronously.  Call 'sync()' after each
          request.

     This variable was added in MySQL 5.5.28.


File: manual.info.tmp,  Node: audit-log-restrictions,  Prev: audit-log-options-variables,  Up: audit-log

6.4.2.8 Audit Log Restrictions
..............................

MySQL Enterprise Audit is subject to these general restrictions:

   * Only SQL statements are logged.  Changes made by no-SQL APIs, such
     as memcached, Node.JS, and the NDB API, are not logged.

   * Only top-level statements are logged, not statements within stored
     programs such as triggers or stored procedures.

   * Contents of files referenced by statements such as *note 'LOAD
     DATA': load-data. are not logged.

NDB Cluster

It is possible to use MySQL Enterprise Audit with MySQL NDB Cluster,
subject to the following conditions:

   * All changes to be logged must be done using the SQL interface.
     Changes using no-SQL interfaces, such as those provided by the NDB
     API, memcached, or ClusterJ, are not logged.

   * The plugin must be installed on each MySQL server that is used to
     execute SQL on the cluster.

   * Audit plugin data must be aggregated amongst all MySQL servers used
     with the cluster.  This aggregation is the responsibility of the
     application or user.


File: manual.info.tmp,  Node: backup-and-recovery,  Next: optimization,  Prev: security,  Up: Top

7 Backup and Recovery
*********************

* Menu:

* backup-types::                 Backup and Recovery Types
* backup-methods::               Database Backup Methods
* backup-strategy-example::      Example Backup and Recovery Strategy
* using-mysqldump::              Using mysqldump for Backups
* point-in-time-recovery::       Point-in-Time (Incremental) Recovery Using the Binary Log
* myisam-table-maintenance::     MyISAM Table Maintenance and Crash Recovery

It is important to back up your databases so that you can recover your
data and be up and running again in case problems occur, such as system
crashes, hardware failures, or users deleting data by mistake.  Backups
are also essential as a safeguard before upgrading a MySQL installation,
and they can be used to transfer a MySQL installation to another system
or to set up replication slave servers.

MySQL offers a variety of backup strategies from which you can choose
the methods that best suit the requirements for your installation.  This
chapter discusses several backup and recovery topics with which you
should be familiar:

   * Types of backups: Logical versus physical, full versus incremental,
     and so forth.

   * Methods for creating backups.

   * Recovery methods, including point-in-time recovery.

   * Backup scheduling, compression, and encryption.

   * Table maintenance, to enable recovery of corrupt tables.

*Additional Resources*

Resources related to backup or to maintaining data availability include
the following:

   * Customers of MySQL Enterprise Edition can use the MySQL Enterprise
     Backup product for backups.  For an overview of the MySQL
     Enterprise Backup product, see *note mysql-enterprise-backup::.

   * A forum dedicated to backup issues is available at
     <https://forums.mysql.com/list.php?28>.

   * Details for *note 'mysqldump': mysqldump, *note 'mysqlhotcopy':
     mysqlhotcopy, and other MySQL backup programs can be found in *note
     programs::.

   * The syntax of the SQL statements described here is given in *note
     sql-statements::.

   * For additional information about 'InnoDB' backup procedures, see
     *note innodb-backup::.

   * Replication enables you to maintain identical data on multiple
     servers.  This has several benefits, such as enabling client query
     load to be distributed over servers, availability of data even if a
     given server is taken offline or fails, and the ability to make
     backups with no impact on the master by using a slave server.  See
     *note replication::.

   * NDB Cluster provides a high-availability, high-redundancy version
     of MySQL adapted for the distributed computing environment.  See
     *note mysql-cluster::.  For information specifically about NDB
     Cluster backup, see *note mysql-cluster-backup::.

   * Distributed Replicated Block Device (DRBD) is another
     high-availability solution.  It works by replicating a block device
     from a primary server to a secondary server at the block level.
     See *note ha-overview::


File: manual.info.tmp,  Node: backup-types,  Next: backup-methods,  Prev: backup-and-recovery,  Up: backup-and-recovery

7.1 Backup and Recovery Types
=============================

This section describes the characteristics of different types of
backups.

*Physical (Raw) Versus Logical Backups*

Physical backups consist of raw copies of the directories and files that
store database contents.  This type of backup is suitable for large,
important databases that need to be recovered quickly when problems
occur.

Logical backups save information represented as logical database
structure (*note 'CREATE DATABASE': create-database, *note 'CREATE
TABLE': create-table. statements) and content (*note 'INSERT': insert.
statements or delimited-text files).  This type of backup is suitable
for smaller amounts of data where you might edit the data values or
table structure, or recreate the data on a different machine
architecture.

Physical backup methods have these characteristics:

   * The backup consists of exact copies of database directories and
     files.  Typically this is a copy of all or part of the MySQL data
     directory.

   * Physical backup methods are faster than logical because they
     involve only file copying without conversion.

   * Output is more compact than for logical backup.

   * Because backup speed and compactness are important for busy,
     important databases, the MySQL Enterprise Backup product performs
     physical backups.  For an overview of the MySQL Enterprise Backup
     product, see *note mysql-enterprise-backup::.

   * Backup and restore granularity ranges from the level of the entire
     data directory down to the level of individual files.  This may or
     may not provide for table-level granularity, depending on storage
     engine.  For example, 'InnoDB' tables can each be in a separate
     file, or share file storage with other 'InnoDB' tables; each
     'MyISAM' table corresponds uniquely to a set of files.

   * In addition to databases, the backup can include any related files
     such as log or configuration files.

   * Data from 'MEMORY' tables is tricky to back up this way because
     their contents are not stored on disk.  (The MySQL Enterprise
     Backup product has a feature where you can retrieve data from
     'MEMORY' tables during a backup.)

   * Backups are portable only to other machines that have identical or
     similar hardware characteristics.

   * Backups can be performed while the MySQL server is not running.  If
     the server is running, it is necessary to perform appropriate
     locking so that the server does not change database contents during
     the backup.  MySQL Enterprise Backup does this locking
     automatically for tables that require it.

   * Physical backup tools include the 'mysqlbackup' of MySQL Enterprise
     Backup for 'InnoDB' or any other tables, file system-level commands
     (such as 'cp', 'scp', 'tar', 'rsync'), or *note 'mysqlhotcopy':
     mysqlhotcopy. for 'MyISAM' tables.

   * For restore:

        * MySQL Enterprise Backup restores 'InnoDB' and other tables
          that it backed up.

        * *note 'ndb_restore': mysql-cluster-programs-ndb-restore.
          restores *note 'NDB': mysql-cluster. tables.

        * Files copied at the file system level or with *note
          'mysqlhotcopy': mysqlhotcopy. can be copied back to their
          original locations with file system commands.

Logical backup methods have these characteristics:

   * The backup is done by querying the MySQL server to obtain database
     structure and content information.

   * Backup is slower than physical methods because the server must
     access database information and convert it to logical format.  If
     the output is written on the client side, the server must also send
     it to the backup program.

   * Output is larger than for physical backup, particularly when saved
     in text format.

   * Backup and restore granularity is available at the server level
     (all databases), database level (all tables in a particular
     database), or table level.  This is true regardless of storage
     engine.

   * The backup does not include log or configuration files, or other
     database-related files that are not part of databases.

   * Backups stored in logical format are machine independent and highly
     portable.

   * Logical backups are performed with the MySQL server running.  The
     server is not taken offline.

   * Logical backup tools include the *note 'mysqldump': mysqldump.
     program and the *note 'SELECT ... INTO OUTFILE': select. statement.
     These work for any storage engine, even 'MEMORY'.

   * To restore logical backups, SQL-format dump files can be processed
     using the *note 'mysql': mysql. client.  To load delimited-text
     files, use the *note 'LOAD DATA': load-data. statement or the *note
     'mysqlimport': mysqlimport. client.

*Online Versus Offline Backups*

Online backups take place while the MySQL server is running so that the
database information can be obtained from the server.  Offline backups
take place while the server is stopped.  This distinction can also be
described as 'hot' versus 'cold' backups; a 'warm' backup is one where
the server remains running but locked against modifying data while you
access database files externally.

Online backup methods have these characteristics:

   * The backup is less intrusive to other clients, which can connect to
     the MySQL server during the backup and may be able to access data
     depending on what operations they need to perform.

   * Care must be taken to impose appropriate locking so that data
     modifications do not take place that would compromise backup
     integrity.  The MySQL Enterprise Backup product does such locking
     automatically.

Offline backup methods have these characteristics:

   * Clients can be affected adversely because the server is unavailable
     during backup.  For that reason, such backups are often taken from
     a replication slave server that can be taken offline without
     harming availability.

   * The backup procedure is simpler because there is no possibility of
     interference from client activity.

A similar distinction between online and offline applies for recovery
operations, and similar characteristics apply.  However, it is more
likely that clients will be affected for online recovery than for online
backup because recovery requires stronger locking.  During backup,
clients might be able to read data while it is being backed up.
Recovery modifies data and does not just read it, so clients must be
prevented from accessing data while it is being restored.

*Local Versus Remote Backups*

A local backup is performed on the same host where the MySQL server
runs, whereas a remote backup is done from a different host.  For some
types of backups, the backup can be initiated from a remote host even if
the output is written locally on the server.  host.

   * *note 'mysqldump': mysqldump. can connect to local or remote
     servers.  For SQL output ('CREATE' and *note 'INSERT': insert.
     statements), local or remote dumps can be done and generate output
     on the client.  For delimited-text output (with the '--tab'
     option), data files are created on the server host.

   * *note 'mysqlhotcopy': mysqlhotcopy. performs only local backups: It
     connects to the server to lock it against data modifications and
     then copies local table files.

   * *note 'SELECT ... INTO OUTFILE': select-into. can be initiated from
     a local or remote client host, but the output file is created on
     the server host.

   * Physical backup methods typically are initiated locally on the
     MySQL server host so that the server can be taken offline, although
     the destination for copied files might be remote.

*Snapshot Backups*

Some file system implementations enable 'snapshots' to be taken.  These
provide logical copies of the file system at a given point in time,
without requiring a physical copy of the entire file system.  (For
example, the implementation may use copy-on-write techniques so that
only parts of the file system modified after the snapshot time need be
copied.)  MySQL itself does not provide the capability for taking file
system snapshots.  It is available through third-party solutions such as
Veritas, LVM, or ZFS.

*Full Versus Incremental Backups*

A full backup includes all data managed by a MySQL server at a given
point in time.  An incremental backup consists of the changes made to
the data during a given time span (from one point in time to another).
MySQL has different ways to perform full backups, such as those
described earlier in this section.  Incremental backups are made
possible by enabling the server's binary log, which the server uses to
record data changes.

*Full Versus Point-in-Time (Incremental) Recovery*

A full recovery restores all data from a full backup.  This restores the
server instance to the state that it had when the backup was made.  If
that state is not sufficiently current, a full recovery can be followed
by recovery of incremental backups made since the full backup, to bring
the server to a more up-to-date state.

Incremental recovery is recovery of changes made during a given time
span.  This is also called point-in-time recovery because it makes a
server's state current up to a given time.  Point-in-time recovery is
based on the binary log and typically follows a full recovery from the
backup files that restores the server to its state when the backup was
made.  Then the data changes written in the binary log files are applied
as incremental recovery to redo data modifications and bring the server
up to the desired point in time.

*Table Maintenance*

Data integrity can be compromised if tables become corrupt.  For *note
'InnoDB': innodb-storage-engine. tables, this is not a typical issue.
For programs to check *note 'MyISAM': myisam-storage-engine. tables and
repair them if problems are found, see *note myisam-table-maintenance::.

*Backup Scheduling, Compression, and Encryption*

Backup scheduling is valuable for automating backup procedures.
Compression of backup output reduces space requirements, and encryption
of the output provides better security against unauthorized access of
backed-up data.  MySQL itself does not provide these capabilities.  The
MySQL Enterprise Backup product can compress 'InnoDB' backups, and
compression or encryption of backup output can be achieved using file
system utilities.  Other third-party solutions may be available.


File: manual.info.tmp,  Node: backup-methods,  Next: backup-strategy-example,  Prev: backup-types,  Up: backup-and-recovery

7.2 Database Backup Methods
===========================

This section summarizes some general methods for making backups.

*Making a Hot Backup with MySQL Enterprise Backup*

Customers of MySQL Enterprise Edition can use the MySQL Enterprise
Backup product to do physical backups of entire instances or selected
databases, tables, or both.  This product includes features for
incremental and compressed backups.  Backing up the physical database
files makes restore much faster than logical techniques such as the
'mysqldump' command.  'InnoDB' tables are copied using a hot backup
mechanism.  (Ideally, the 'InnoDB' tables should represent a substantial
majority of the data.)  Tables from other storage engines are copied
using a warm backup mechanism.  For an overview of the MySQL Enterprise
Backup product, see *note mysql-enterprise-backup::.

*Making Backups with mysqldump or mysqlhotcopy*

The *note 'mysqldump': mysqldump. program and the *note 'mysqlhotcopy':
mysqlhotcopy. script can make backups.  *note 'mysqldump': mysqldump. is
more general because it can back up all kinds of tables.  *note
'mysqlhotcopy': mysqlhotcopy. works only with some storage engines.
(See *note using-mysqldump::, and *note mysqlhotcopy::.)

For 'InnoDB' tables, it is possible to perform an online backup that
takes no locks on tables using the '--single-transaction' option to
*note 'mysqldump': mysqldump.  See *note backup-policy::.

*Making Backups by Copying Table Files*

For storage engines that represent each table using its own files,
tables can be backed up by copying those files.  For example, 'MyISAM'
tables are stored as files, so it is easy to do a backup by copying
files ('*.frm', '*.MYD', and '*.MYI' files).  To get a consistent
backup, stop the server or lock and flush the relevant tables:

     FLUSH TABLES TBL_LIST WITH READ LOCK;

You need only a read lock; this enables other clients to continue to
query the tables while you are making a copy of the files in the
database directory.  The flush is needed to ensure that the all active
index pages are written to disk before you start the backup.  See *note
lock-tables::, and *note flush::.

You can also create a binary backup simply by copying all table files,
as long as the server isn't updating anything.  The *note
'mysqlhotcopy': mysqlhotcopy. script uses this method.  (But note that
table file copying methods do not work if your database contains
'InnoDB' tables.  *note 'mysqlhotcopy': mysqlhotcopy. does not work for
'InnoDB' tables because 'InnoDB' does not necessarily store table
contents in database directories.  Also, even if the server is not
actively updating data, 'InnoDB' may still have modified data cached in
memory and not flushed to disk.)

*Making Delimited-Text File Backups*

To create a text file containing a table's data, you can use *note
'SELECT * INTO OUTFILE 'FILE_NAME' FROM TBL_NAME': select-into.  The
file is created on the MySQL server host, not the client host.  For this
statement, the output file cannot already exist because permitting files
to be overwritten constitutes a security risk.  See *note select::.
This method works for any kind of data file, but saves only table data,
not the table structure.

Another way to create text data files (along with files containing *note
'CREATE TABLE': create-table. statements for the backed up tables) is to
use *note 'mysqldump': mysqldump. with the '--tab' option.  See *note
mysqldump-delimited-text::.

To reload a delimited-text data file, use *note 'LOAD DATA': load-data.
or *note 'mysqlimport': mysqlimport.

*Making Incremental Backups by Enabling the Binary Log*

MySQL supports incremental backups: You must start the server with the
'--log-bin' option to enable binary logging; see *note binary-log::.
The binary log files provide you with the information you need to
replicate changes to the database that are made subsequent to the point
at which you performed a backup.  At the moment you want to make an
incremental backup (containing all changes that happened since the last
full or incremental backup), you should rotate the binary log by using
'FLUSH LOGS'.  This done, you need to copy to the backup location all
binary logs which range from the one of the moment of the last full or
incremental backup to the last but one.  These binary logs are the
incremental backup; at restore time, you apply them as explained in
*note point-in-time-recovery::.  The next time you do a full backup, you
should also rotate the binary log using 'FLUSH LOGS', *note 'mysqldump
--flush-logs': mysqldump, or *note 'mysqlhotcopy --flushlog':
mysqlhotcopy.  See *note mysqldump::, and *note mysqlhotcopy::.

*Making Backups Using Replication Slaves*

If you have performance problems with your master server while making
backups, one strategy that can help is to set up replication and perform
backups on the slave rather than on the master.  See *note
replication-solutions-backups::.

If you are backing up a slave replication server, you should back up its
'master.info' and 'relay-log.info' files when you back up the slave's
databases, regardless of the backup method you choose.  These
information files are always needed to resume replication after you
restore the slave's data.  If your slave is replicating *note 'LOAD
DATA': load-data. statements, you should also back up any 'SQL_LOAD-*'
files that exist in the directory that the slave uses for this purpose.
The slave needs these files to resume replication of any interrupted
*note 'LOAD DATA': load-data. operations.  The location of this
directory is the value of the 'slave_load_tmpdir' system variable.  If
the server was not started with that variable set, the directory
location is the value of the 'tmpdir' system variable.

*Recovering Corrupt Tables*

If you have to restore 'MyISAM' tables that have become corrupt, try to
recover them using *note 'REPAIR TABLE': repair-table. or *note
'myisamchk -r': myisamchk. first.  That should work in 99.9% of all
cases.  If *note 'myisamchk': myisamchk. fails, see *note
myisam-table-maintenance::.

*Making Backups Using a File System Snapshot*

If you are using a Veritas file system, you can make a backup like this:

  1. From a client program, execute 'FLUSH TABLES WITH READ LOCK'.

  2. From another shell, execute 'mount vxfs snapshot'.

  3. From the first client, execute *note 'UNLOCK TABLES': lock-tables.

  4. Copy files from the snapshot.

  5. Unmount the snapshot.

Similar snapshot capabilities may be available in other file systems,
such as LVM or ZFS.


File: manual.info.tmp,  Node: backup-strategy-example,  Next: using-mysqldump,  Prev: backup-methods,  Up: backup-and-recovery

7.3 Example Backup and Recovery Strategy
========================================

* Menu:

* backup-policy::                Establishing a Backup Policy
* recovery-from-backups::        Using Backups for Recovery
* backup-strategy-summary::      Backup Strategy Summary

This section discusses a procedure for performing backups that enables
you to recover data after several types of crashes:

   * Operating system crash

   * Power failure

   * File system crash

   * Hardware problem (hard drive, motherboard, and so forth)

The example commands do not include options such as '--user' and
'--password' for the *note 'mysqldump': mysqldump. and *note 'mysql':
mysql. client programs.  You should include such options as necessary to
enable client programs to connect to the MySQL server.

Assume that data is stored in the 'InnoDB' storage engine, which has
support for transactions and automatic crash recovery.  Assume also that
the MySQL server is under load at the time of the crash.  If it were
not, no recovery would ever be needed.

For cases of operating system crashes or power failures, we can assume
that MySQL's disk data is available after a restart.  The 'InnoDB' data
files might not contain consistent data due to the crash, but 'InnoDB'
reads its logs and finds in them the list of pending committed and
noncommitted transactions that have not been flushed to the data files.
'InnoDB' automatically rolls back those transactions that were not
committed, and flushes to its data files those that were committed.
Information about this recovery process is conveyed to the user through
the MySQL error log.  The following is an example log excerpt:

     InnoDB: Database was not shut down normally.
     InnoDB: Starting recovery from log files...
     InnoDB: Starting log scan based on checkpoint at
     InnoDB: log sequence number 0 13674004
     InnoDB: Doing recovery: scanned up to log sequence number 0 13739520
     InnoDB: Doing recovery: scanned up to log sequence number 0 13805056
     InnoDB: Doing recovery: scanned up to log sequence number 0 13870592
     InnoDB: Doing recovery: scanned up to log sequence number 0 13936128
     ...
     InnoDB: Doing recovery: scanned up to log sequence number 0 20555264
     InnoDB: Doing recovery: scanned up to log sequence number 0 20620800
     InnoDB: Doing recovery: scanned up to log sequence number 0 20664692
     InnoDB: 1 uncommitted transaction(s) which must be rolled back
     InnoDB: Starting rollback of uncommitted transactions
     InnoDB: Rolling back trx no 16745
     InnoDB: Rolling back of trx no 16745 completed
     InnoDB: Rollback of uncommitted transactions completed
     InnoDB: Starting an apply batch of log records to the database...
     InnoDB: Apply batch completed
     InnoDB: Started
     mysqld: ready for connections

For the cases of file system crashes or hardware problems, we can assume
that the MySQL disk data is _not_ available after a restart.  This means
that MySQL fails to start successfully because some blocks of disk data
are no longer readable.  In this case, it is necessary to reformat the
disk, install a new one, or otherwise correct the underlying problem.
Then it is necessary to recover our MySQL data from backups, which means
that backups must already have been made.  To make sure that is the
case, design and implement a backup policy.


File: manual.info.tmp,  Node: backup-policy,  Next: recovery-from-backups,  Prev: backup-strategy-example,  Up: backup-strategy-example

7.3.1 Establishing a Backup Policy
----------------------------------

To be useful, backups must be scheduled regularly.  A full backup (a
snapshot of the data at a point in time) can be done in MySQL with
several tools.  For example, *note MySQL Enterprise Backup:
mysql-enterprise-backup. can perform a physical backup of an entire
instance, with optimizations to minimize overhead and avoid disruption
when backing up 'InnoDB' data files; *note 'mysqldump': mysqldump.
provides online logical backup.  This discussion uses *note 'mysqldump':
mysqldump.

Assume that we make a full backup of all our 'InnoDB' tables in all
databases using the following command on Sunday at 1 p.m., when load is
low:

     shell> mysqldump --all-databases --master-data --single-transaction > backup_sunday_1_PM.sql

The resulting '.sql' file produced by *note 'mysqldump': mysqldump.
contains a set of SQL *note 'INSERT': insert. statements that can be
used to reload the dumped tables at a later time.

This backup operation acquires a global read lock on all tables at the
beginning of the dump (using 'FLUSH TABLES WITH READ LOCK').  As soon as
this lock has been acquired, the binary log coordinates are read and the
lock is released.  If long updating statements are running when the
*note 'FLUSH': flush. statement is issued, the backup operation may
stall until those statements finish.  After that, the dump becomes
lock-free and does not disturb reads and writes on the tables.

It was assumed earlier that the tables to back up are 'InnoDB' tables,
so '--single-transaction' uses a consistent read and guarantees that
data seen by *note 'mysqldump': mysqldump. does not change.  (Changes
made by other clients to 'InnoDB' tables are not seen by the *note
'mysqldump': mysqldump. process.)  If the backup operation includes
nontransactional tables, consistency requires that they do not change
during the backup.  For example, for the 'MyISAM' tables in the 'mysql'
database, there must be no administrative changes to MySQL accounts
during the backup.

Full backups are necessary, but it is not always convenient to create
them.  They produce large backup files and take time to generate.  They
are not optimal in the sense that each successive full backup includes
all data, even that part that has not changed since the previous full
backup.  It is more efficient to make an initial full backup, and then
to make incremental backups.  The incremental backups are smaller and
take less time to produce.  The tradeoff is that, at recovery time, you
cannot restore your data just by reloading the full backup.  You must
also process the incremental backups to recover the incremental changes.

To make incremental backups, we need to save the incremental changes.
In MySQL, these changes are represented in the binary log, so the MySQL
server should always be started with the '--log-bin' option to enable
that log.  With binary logging enabled, the server writes each data
change into a file while it updates data.  Looking at the data directory
of a MySQL server that was started with the '--log-bin' option and that
has been running for some days, we find these MySQL binary log files:

     -rw-rw---- 1 guilhem  guilhem   1277324 Nov 10 23:59 gbichot2-bin.000001
     -rw-rw---- 1 guilhem  guilhem         4 Nov 10 23:59 gbichot2-bin.000002
     -rw-rw---- 1 guilhem  guilhem        79 Nov 11 11:06 gbichot2-bin.000003
     -rw-rw---- 1 guilhem  guilhem       508 Nov 11 11:08 gbichot2-bin.000004
     -rw-rw---- 1 guilhem  guilhem 220047446 Nov 12 16:47 gbichot2-bin.000005
     -rw-rw---- 1 guilhem  guilhem    998412 Nov 14 10:08 gbichot2-bin.000006
     -rw-rw---- 1 guilhem  guilhem       361 Nov 14 10:07 gbichot2-bin.index

Each time it restarts, the MySQL server creates a new binary log file
using the next number in the sequence.  While the server is running, you
can also tell it to close the current binary log file and begin a new
one manually by issuing a 'FLUSH LOGS' SQL statement or with a *note
'mysqladmin flush-logs': mysqladmin. command.  *note 'mysqldump':
mysqldump. also has an option to flush the logs.  The '.index' file in
the data directory contains the list of all MySQL binary logs in the
directory.

The MySQL binary logs are important for recovery because they form the
set of incremental backups.  If you make sure to flush the logs when you
make your full backup, the binary log files created afterward contain
all the data changes made since the backup.  Let's modify the previous
*note 'mysqldump': mysqldump. command a bit so that it flushes the MySQL
binary logs at the moment of the full backup, and so that the dump file
contains the name of the new current binary log:

     shell> mysqldump --single-transaction --flush-logs --master-data=2 \
              --all-databases > backup_sunday_1_PM.sql

After executing this command, the data directory contains a new binary
log file, 'gbichot2-bin.000007', because the '--flush-logs' option
causes the server to flush its logs.  The '--master-data' option causes
*note 'mysqldump': mysqldump. to write binary log information to its
output, so the resulting '.sql' dump file includes these lines:

     -- Position to start replication or point-in-time recovery from
     -- CHANGE MASTER TO MASTER_LOG_FILE='gbichot2-bin.000007',MASTER_LOG_POS=4;

Because the *note 'mysqldump': mysqldump. command made a full backup,
those lines mean two things:

   * The dump file contains all changes made before any changes written
     to the 'gbichot2-bin.000007' binary log file or higher.

   * All data changes logged after the backup are not present in the
     dump file, but are present in the 'gbichot2-bin.000007' binary log
     file or higher.

On Monday at 1 p.m., we can create an incremental backup by flushing the
logs to begin a new binary log file.  For example, executing a *note
'mysqladmin flush-logs': mysqladmin. command creates
'gbichot2-bin.000008'.  All changes between the Sunday 1 p.m.  full
backup and Monday 1 p.m.  will be in the 'gbichot2-bin.000007' file.
This incremental backup is important, so it is a good idea to copy it to
a safe place.  (For example, back it up on tape or DVD, or copy it to
another machine.)  On Tuesday at 1 p.m., execute another *note
'mysqladmin flush-logs': mysqladmin. command.  All changes between
Monday 1 p.m.  and Tuesday 1 p.m.  will be in the 'gbichot2-bin.000008'
file (which also should be copied somewhere safe).

The MySQL binary logs take up disk space.  To free up space, purge them
from time to time.  One way to do this is by deleting the binary logs
that are no longer needed, such as when we make a full backup:

     shell> mysqldump --single-transaction --flush-logs --master-data=2 \
              --all-databases --delete-master-logs > backup_sunday_1_PM.sql

*Note*:

Deleting the MySQL binary logs with *note 'mysqldump
--delete-master-logs': mysqldump. can be dangerous if your server is a
replication master server, because slave servers might not yet fully
have processed the contents of the binary log.  The description for the
*note 'PURGE BINARY LOGS': purge-binary-logs. statement explains what
should be verified before deleting the MySQL binary logs.  See *note
purge-binary-logs::.


File: manual.info.tmp,  Node: recovery-from-backups,  Next: backup-strategy-summary,  Prev: backup-policy,  Up: backup-strategy-example

7.3.2 Using Backups for Recovery
--------------------------------

Now, suppose that we have a catastrophic crash on Wednesday at 8 a.m.
that requires recovery from backups.  To recover, first we restore the
last full backup we have (the one from Sunday 1 p.m.).  The full backup
file is just a set of SQL statements, so restoring it is very easy:

     shell> mysql < backup_sunday_1_PM.sql

At this point, the data is restored to its state as of Sunday 1 p.m..
To restore the changes made since then, we must use the incremental
backups; that is, the 'gbichot2-bin.000007' and 'gbichot2-bin.000008'
binary log files.  Fetch the files if necessary from where they were
backed up, and then process their contents like this:

     shell> mysqlbinlog gbichot2-bin.000007 gbichot2-bin.000008 | mysql

We now have recovered the data to its state as of Tuesday 1 p.m., but
still are missing the changes from that date to the date of the crash.
To not lose them, we would have needed to have the MySQL server store
its MySQL binary logs into a safe location (RAID disks, SAN, ...)
different from the place where it stores its data files, so that these
logs were not on the destroyed disk.  (That is, we can start the server
with a '--log-bin' option that specifies a location on a different
physical device from the one on which the data directory resides.  That
way, the logs are safe even if the device containing the directory is
lost.)  If we had done this, we would have the 'gbichot2-bin.000009'
file (and any subsequent files) at hand, and we could apply them using
*note 'mysqlbinlog': mysqlbinlog. and *note 'mysql': mysql. to restore
the most recent data changes with no loss up to the moment of the crash:

     shell> mysqlbinlog gbichot2-bin.000009 ... | mysql

For more information about using *note 'mysqlbinlog': mysqlbinlog. to
process binary log files, see *note point-in-time-recovery::.


File: manual.info.tmp,  Node: backup-strategy-summary,  Prev: recovery-from-backups,  Up: backup-strategy-example

7.3.3 Backup Strategy Summary
-----------------------------

In case of an operating system crash or power failure, 'InnoDB' itself
does all the job of recovering data.  But to make sure that you can
sleep well, observe the following guidelines:

   * Always run the MySQL server with the '--log-bin' option, or even
     '--log-bin=LOG_NAME', where the log file name is located on some
     safe media different from the drive on which the data directory is
     located.  If you have such safe media, this technique can also be
     good for disk load balancing (which results in a performance
     improvement).

   * Make periodic full backups, using the *note 'mysqldump': mysqldump.
     command shown earlier in *note backup-policy::, that makes an
     online, nonblocking backup.

   * Make periodic incremental backups by flushing the logs with 'FLUSH
     LOGS' or *note 'mysqladmin flush-logs': mysqladmin.


File: manual.info.tmp,  Node: using-mysqldump,  Next: point-in-time-recovery,  Prev: backup-strategy-example,  Up: backup-and-recovery

7.4 Using mysqldump for Backups
===============================

* Menu:

* mysqldump-sql-format::         Dumping Data in SQL Format with mysqldump
* reloading-sql-format-dumps::   Reloading SQL-Format Backups
* mysqldump-delimited-text::     Dumping Data in Delimited-Text Format with mysqldump
* reloading-delimited-text-dumps::  Reloading Delimited-Text Format Backups
* mysqldump-tips::               mysqldump Tips

This section describes how to use *note 'mysqldump': mysqldump. to
produce dump files, and how to reload dump files.  A dump file can be
used in several ways:

   * As a backup to enable data recovery in case of data loss.

   * As a source of data for setting up replication slaves.

   * As a source of data for experimentation:

        * To make a copy of a database that you can use without changing
          the original data.

        * To test potential upgrade incompatibilities.

*note 'mysqldump': mysqldump. produces two types of output, depending on
whether the '--tab' option is given:

   * Without '--tab', *note 'mysqldump': mysqldump. writes SQL
     statements to the standard output.  This output consists of
     'CREATE' statements to create dumped objects (databases, tables,
     stored routines, and so forth), and 'INSERT' statements to load
     data into tables.  The output can be saved in a file and reloaded
     later using *note 'mysql': mysql. to recreate the dumped objects.
     Options are available to modify the format of the SQL statements,
     and to control which objects are dumped.

   * With '--tab', *note 'mysqldump': mysqldump. produces two output
     files for each dumped table.  The server writes one file as
     tab-delimited text, one line per table row.  This file is named
     'TBL_NAME.txt' in the output directory.  The server also sends a
     *note 'CREATE TABLE': create-table. statement for the table to
     *note 'mysqldump': mysqldump, which writes it as a file named
     'TBL_NAME.sql' in the output directory.


File: manual.info.tmp,  Node: mysqldump-sql-format,  Next: reloading-sql-format-dumps,  Prev: using-mysqldump,  Up: using-mysqldump

7.4.1 Dumping Data in SQL Format with mysqldump
-----------------------------------------------

This section describes how to use *note 'mysqldump': mysqldump. to
create SQL-format dump files.  For information about reloading such dump
files, see *note reloading-sql-format-dumps::.

By default, *note 'mysqldump': mysqldump. writes information as SQL
statements to the standard output.  You can save the output in a file:

     shell> mysqldump [ARGUMENTS] > FILE_NAME

To dump all databases, invoke *note 'mysqldump': mysqldump. with the
'--all-databases' option:

     shell> mysqldump --all-databases > dump.sql

To dump only specific databases, name them on the command line and use
the '--databases' option:

     shell> mysqldump --databases db1 db2 db3 > dump.sql

The '--databases' option causes all names on the command line to be
treated as database names.  Without this option, *note 'mysqldump':
mysqldump. treats the first name as a database name and those following
as table names.

With '--all-databases' or '--databases', *note 'mysqldump': mysqldump.
writes *note 'CREATE DATABASE': create-database. and *note 'USE': use.
statements prior to the dump output for each database.  This ensures
that when the dump file is reloaded, it creates each database if it does
not exist and makes it the default database so database contents are
loaded into the same database from which they came.  If you want to
cause the dump file to force a drop of each database before recreating
it, use the '--add-drop-database' option as well.  In this case, *note
'mysqldump': mysqldump. writes a *note 'DROP DATABASE': drop-database.
statement preceding each *note 'CREATE DATABASE': create-database.
statement.

To dump a single database, name it on the command line:

     shell> mysqldump --databases test > dump.sql

In the single-database case, it is permissible to omit the '--databases'
option:

     shell> mysqldump test > dump.sql

The difference between the two preceding commands is that without
'--databases', the dump output contains no *note 'CREATE DATABASE':
create-database. or *note 'USE': use. statements.  This has several
implications:

   * When you reload the dump file, you must specify a default database
     name so that the server knows which database to reload.

   * For reloading, you can specify a database name different from the
     original name, which enables you to reload the data into a
     different database.

   * If the database to be reloaded does not exist, you must create it
     first.

   * Because the output will contain no *note 'CREATE DATABASE':
     create-database. statement, the '--add-drop-database' option has no
     effect.  If you use it, it produces no *note 'DROP DATABASE':
     drop-database. statement.

To dump only specific tables from a database, name them on the command
line following the database name:

     shell> mysqldump test t1 t3 t7 > dump.sql


File: manual.info.tmp,  Node: reloading-sql-format-dumps,  Next: mysqldump-delimited-text,  Prev: mysqldump-sql-format,  Up: using-mysqldump

7.4.2 Reloading SQL-Format Backups
----------------------------------

To reload a dump file written by *note 'mysqldump': mysqldump. that
consists of SQL statements, use it as input to the *note 'mysql': mysql.
client.  If the dump file was created by *note 'mysqldump': mysqldump.
with the '--all-databases' or '--databases' option, it contains *note
'CREATE DATABASE': create-database. and *note 'USE': use. statements and
it is not necessary to specify a default database into which to load the
data:

     shell> mysql < dump.sql

Alternatively, from within *note 'mysql': mysql, use a 'source' command:

     mysql> source dump.sql

If the file is a single-database dump not containing *note 'CREATE
DATABASE': create-database. and *note 'USE': use. statements, create the
database first (if necessary):

     shell> mysqladmin create db1

Then specify the database name when you load the dump file:

     shell> mysql db1 < dump.sql

Alternatively, from within *note 'mysql': mysql, create the database,
select it as the default database, and load the dump file:

     mysql> CREATE DATABASE IF NOT EXISTS db1;
     mysql> USE db1;
     mysql> source dump.sql

*Note*:

For Windows PowerShell users: Because the "<" character is reserved for
future use in PowerShell, an alternative approach is required, such as
using quotes 'cmd.exe /c "mysql < dump.sql"'.


File: manual.info.tmp,  Node: mysqldump-delimited-text,  Next: reloading-delimited-text-dumps,  Prev: reloading-sql-format-dumps,  Up: using-mysqldump

7.4.3 Dumping Data in Delimited-Text Format with mysqldump
----------------------------------------------------------

This section describes how to use *note 'mysqldump': mysqldump. to
create delimited-text dump files.  For information about reloading such
dump files, see *note reloading-delimited-text-dumps::.

If you invoke *note 'mysqldump': mysqldump. with the '--tab=DIR_NAME'
option, it uses DIR_NAME as the output directory and dumps tables
individually in that directory using two files for each table.  The
table name is the base name for these files.  For a table named 't1',
the files are named 't1.sql' and 't1.txt'.  The '.sql' file contains a
*note 'CREATE TABLE': create-table. statement for the table.  The '.txt'
file contains the table data, one line per table row.

The following command dumps the contents of the 'db1' database to files
in the '/tmp' database:

     shell> mysqldump --tab=/tmp db1

The '.txt' files containing table data are written by the server, so
they are owned by the system account used for running the server.  The
server uses *note 'SELECT ... INTO OUTFILE': select-into. to write the
files, so you must have the 'FILE' privilege to perform this operation,
and an error occurs if a given '.txt' file already exists.

The server sends the 'CREATE' definitions for dumped tables to *note
'mysqldump': mysqldump, which writes them to '.sql' files.  These files
therefore are owned by the user who executes *note 'mysqldump':
mysqldump.

It is best that '--tab' be used only for dumping a local server.  If you
use it with a remote server, the '--tab' directory must exist on both
the local and remote hosts, and the '.txt' files will be written by the
server in the remote directory (on the server host), whereas the '.sql'
files will be written by *note 'mysqldump': mysqldump. in the local
directory (on the client host).

For *note 'mysqldump --tab': mysqldump, the server by default writes
table data to '.txt' files one line per row with tabs between column
values, no quotation marks around column values, and newline as the line
terminator.  (These are the same defaults as for *note 'SELECT ... INTO
OUTFILE': select-into.)

To enable data files to be written using a different format, *note
'mysqldump': mysqldump. supports these options:

   * '--fields-terminated-by=STR'

     The string for separating column values (default: tab).

   * '--fields-enclosed-by=CHAR'

     The character within which to enclose column values (default: no
     character).

   * '--fields-optionally-enclosed-by=CHAR'

     The character within which to enclose non-numeric column values
     (default: no character).

   * '--fields-escaped-by=CHAR'

     The character for escaping special characters (default: no
     escaping).

   * '--lines-terminated-by=STR'

     The line-termination string (default: newline).

Depending on the value you specify for any of these options, it might be
necessary on the command line to quote or escape the value appropriately
for your command interpreter.  Alternatively, specify the value using
hex notation.  Suppose that you want *note 'mysqldump': mysqldump. to
quote column values within double quotation marks.  To do so, specify
double quote as the value for the '--fields-enclosed-by' option.  But
this character is often special to command interpreters and must be
treated specially.  For example, on Unix, you can quote the double quote
like this:

     --fields-enclosed-by='"'

On any platform, you can specify the value in hex:

     --fields-enclosed-by=0x22

It is common to use several of the data-formatting options together.
For example, to dump tables in comma-separated values format with lines
terminated by carriage-return/newline pairs ('\r\n'), use this command
(enter it on a single line):

     shell> mysqldump --tab=/tmp --fields-terminated-by=,
              --fields-enclosed-by='"' --lines-terminated-by=0x0d0a db1

Should you use any of the data-formatting options to dump table data,
you will need to specify the same format when you reload data files
later, to ensure proper interpretation of the file contents.


File: manual.info.tmp,  Node: reloading-delimited-text-dumps,  Next: mysqldump-tips,  Prev: mysqldump-delimited-text,  Up: using-mysqldump

7.4.4 Reloading Delimited-Text Format Backups
---------------------------------------------

For backups produced with *note 'mysqldump --tab': mysqldump, each table
is represented in the output directory by an '.sql' file containing the
*note 'CREATE TABLE': create-table. statement for the table, and a
'.txt' file containing the table data.  To reload a table, first change
location into the output directory.  Then process the '.sql' file with
*note 'mysql': mysql. to create an empty table and process the '.txt'
file to load the data into the table:

     shell> mysql db1 < t1.sql
     shell> mysqlimport db1 t1.txt

An alternative to using *note 'mysqlimport': mysqlimport. to load the
data file is to use the *note 'LOAD DATA': load-data. statement from
within the *note 'mysql': mysql. client:

     mysql> USE db1;
     mysql> LOAD DATA INFILE 't1.txt' INTO TABLE t1;

If you used any data-formatting options with *note 'mysqldump':
mysqldump. when you initially dumped the table, you must use the same
options with *note 'mysqlimport': mysqlimport. or *note 'LOAD DATA':
load-data. to ensure proper interpretation of the data file contents:

     shell> mysqlimport --fields-terminated-by=,
              --fields-enclosed-by='"' --lines-terminated-by=0x0d0a db1 t1.txt

Or:

     mysql> USE db1;
     mysql> LOAD DATA INFILE 't1.txt' INTO TABLE t1
            FIELDS TERMINATED BY ',' FIELDS ENCLOSED BY '"'
            LINES TERMINATED BY '\r\n';


File: manual.info.tmp,  Node: mysqldump-tips,  Prev: reloading-delimited-text-dumps,  Up: using-mysqldump

7.4.5 mysqldump Tips
--------------------

* Menu:

* mysqldump-copying-database::   Making a Copy of a Database
* mysqldump-copying-to-other-server::  Copy a Database from one Server to Another
* mysqldump-stored-programs::    Dumping Stored Programs
* mysqldump-definition-data-dumps::  Dumping Table Definitions and Content Separately
* mysqldump-upgrade-testing::    Using mysqldump to Test for Upgrade Incompatibilities

This section surveys techniques that enable you to use *note
'mysqldump': mysqldump. to solve specific problems:

   * How to make a copy a database

   * How to copy a database from one server to another

   * How to dump stored programs (stored procedures and functions,
     triggers, and events)

   * How to dump definitions and data separately


File: manual.info.tmp,  Node: mysqldump-copying-database,  Next: mysqldump-copying-to-other-server,  Prev: mysqldump-tips,  Up: mysqldump-tips

7.4.5.1 Making a Copy of a Database
...................................

     shell> mysqldump db1 > dump.sql
     shell> mysqladmin create db2
     shell> mysql db2 < dump.sql

Do not use '--databases' on the *note 'mysqldump': mysqldump. command
line because that causes 'USE db1' to be included in the dump file,
which overrides the effect of naming 'db2' on the *note 'mysql': mysql.
command line.


File: manual.info.tmp,  Node: mysqldump-copying-to-other-server,  Next: mysqldump-stored-programs,  Prev: mysqldump-copying-database,  Up: mysqldump-tips

7.4.5.2 Copy a Database from one Server to Another
..................................................

On Server 1:

     shell> mysqldump --databases db1 > dump.sql

Copy the dump file from Server 1 to Server 2.

On Server 2:

     shell> mysql < dump.sql

Use of '--databases' with the *note 'mysqldump': mysqldump. command line
causes the dump file to include *note 'CREATE DATABASE':
create-database. and *note 'USE': use. statements that create the
database if it does exist and make it the default database for the
reloaded data.

Alternatively, you can omit '--databases' from the *note 'mysqldump':
mysqldump. command.  Then you will need to create the database on Server
2 (if necessary) and specify it as the default database when you reload
the dump file.

On Server 1:

     shell> mysqldump db1 > dump.sql

On Server 2:

     shell> mysqladmin create db1
     shell> mysql db1 < dump.sql

You can specify a different database name in this case, so omitting
'--databases' from the *note 'mysqldump': mysqldump. command enables you
to dump data from one database and load it into another.


File: manual.info.tmp,  Node: mysqldump-stored-programs,  Next: mysqldump-definition-data-dumps,  Prev: mysqldump-copying-to-other-server,  Up: mysqldump-tips

7.4.5.3 Dumping Stored Programs
...............................

Several options control how *note 'mysqldump': mysqldump. handles stored
programs (stored procedures and functions, triggers, and events):

   * '--events': Dump Event Scheduler events

   * '--routines': Dump stored procedures and functions

   * '--triggers': Dump triggers for tables

The '--triggers' option is enabled by default so that when tables are
dumped, they are accompanied by any triggers they have.  The other
options are disabled by default and must be specified explicitly to dump
the corresponding objects.  To disable any of these options explicitly,
use its skip form: '--skip-events', '--skip-routines', or
'--skip-triggers'.


File: manual.info.tmp,  Node: mysqldump-definition-data-dumps,  Next: mysqldump-upgrade-testing,  Prev: mysqldump-stored-programs,  Up: mysqldump-tips

7.4.5.4 Dumping Table Definitions and Content Separately
........................................................

The '--no-data' option tells *note 'mysqldump': mysqldump. not to dump
table data, resulting in the dump file containing only statements to
create the tables.  Conversely, the '--no-create-info' option tells
*note 'mysqldump': mysqldump. to suppress 'CREATE' statements from the
output, so that the dump file contains only table data.

For example, to dump table definitions and data separately for the
'test' database, use these commands:

     shell> mysqldump --no-data test > dump-defs.sql
     shell> mysqldump --no-create-info test > dump-data.sql

For a definition-only dump, add the '--routines' and '--events' options
to also include stored routine and event definitions:

     shell> mysqldump --no-data --routines --events test > dump-defs.sql


File: manual.info.tmp,  Node: mysqldump-upgrade-testing,  Prev: mysqldump-definition-data-dumps,  Up: mysqldump-tips

7.4.5.5 Using mysqldump to Test for Upgrade Incompatibilities
.............................................................

When contemplating a MySQL upgrade, it is prudent to install the newer
version separately from your current production version.  Then you can
dump the database and database object definitions from the production
server and load them into the new server to verify that they are handled
properly.  (This is also useful for testing downgrades.)

On the production server:

     shell> mysqldump --all-databases --no-data --routines --events > dump-defs.sql

On the upgraded server:

     shell> mysql < dump-defs.sql

Because the dump file does not contain table data, it can be processed
quickly.  This enables you to spot potential incompatibilities without
waiting for lengthy data-loading operations.  Look for warnings or
errors while the dump file is being processed.

After you have verified that the definitions are handled properly, dump
the data and try to load it into the upgraded server.

On the production server:

     shell> mysqldump --all-databases --no-create-info > dump-data.sql

On the upgraded server:

     shell> mysql < dump-data.sql

Now check the table contents and run some test queries.


File: manual.info.tmp,  Node: point-in-time-recovery,  Next: myisam-table-maintenance,  Prev: using-mysqldump,  Up: backup-and-recovery

7.5 Point-in-Time (Incremental) Recovery Using the Binary Log
=============================================================

* Menu:

* point-in-time-recovery-times::  Point-in-Time Recovery Using Event Times
* point-in-time-recovery-positions::  Point-in-Time Recovery Using Event Positions

Point-in-time recovery refers to recovery of data changes made since a
given point in time.  Typically, this type of recovery is performed
after restoring a full backup that brings the server to its state as of
the time the backup was made.  (The full backup can be made in several
ways, such as those listed in *note backup-methods::.)  Point-in-time
recovery then brings the server up to date incrementally from the time
of the full backup to a more recent time.

*Note*:

Many of the examples here use the *note 'mysql': mysql. client to
process binary log output produced by *note 'mysqlbinlog': mysqlbinlog.
If your binary log contains '\0' (null) characters, that output cannot
be parsed by *note 'mysql': mysql. unless you invoke it with the
'--binary-mode'
(https://dev.mysql.com/doc/refman/5.6/en/mysql-command-options.html#option_mysql_binary-mode)
option (available in MySQL 5.6).

Point-in-time recovery is based on these principles:

   * The source of information for point-in-time recovery is the set of
     incremental backups represented by the binary log files generated
     subsequent to the full backup operation.  Therefore, the server
     must be started with the '--log-bin' option to enable binary
     logging (see *note binary-log::).

     To restore data from the binary log, you must know the name and
     location of the current binary log files.  By default, the server
     creates binary log files in the data directory, but a path name can
     be specified with the '--log-bin' option to place the files in a
     different location.  *note binary-log::.

     To see a listing of all binary log files, use this statement:

          mysql> SHOW BINARY LOGS;

     To determine the name of the current binary log file, issue the
     following statement:

          mysql> SHOW MASTER STATUS;

   * The *note 'mysqlbinlog': mysqlbinlog. utility converts the events
     in the binary log files from binary format to text so that they can
     be executed or viewed.  *note 'mysqlbinlog': mysqlbinlog. has
     options for selecting sections of the binary log based on event
     times or position of events within the log.  See *note
     mysqlbinlog::.

   * Executing events from the binary log causes the data modifications
     they represent to be redone.  This enables recovery of data changes
     for a given span of time.  To execute events from the binary log,
     process *note 'mysqlbinlog': mysqlbinlog. output using the *note
     'mysql': mysql. client:

          shell> mysqlbinlog BINLOG_FILES | mysql -u root -p

   * Viewing log contents can be useful when you need to determine event
     times or positions to select partial log contents prior to
     executing events.  To view events from the log, send *note
     'mysqlbinlog': mysqlbinlog. output into a paging program:

          shell> mysqlbinlog BINLOG_FILES | more

     Alternatively, save the output in a file and view the file in a
     text editor:

          shell> mysqlbinlog BINLOG_FILES > tmpfile
          shell> ... EDIT TMPFILE ...

   * Saving the output in a file is useful as a preliminary to executing
     the log contents with certain events removed, such as an accidental
     *note 'DROP DATABASE': drop-database.  You can delete from the file
     any statements not to be executed before executing its contents.
     After editing the file, execute the contents as follows:

          shell> mysql -u root -p < tmpfile

If you have more than one binary log to execute on the MySQL server, the
safe method is to process them all using a single connection to the
server.  Here is an example that demonstrates what may be _unsafe_:

     shell> mysqlbinlog binlog.000001 | mysql -u root -p # DANGER!!
     shell> mysqlbinlog binlog.000002 | mysql -u root -p # DANGER!!

Processing binary logs this way using different connections to the
server causes problems if the first log file contains a *note 'CREATE
TEMPORARY TABLE': create-table. statement and the second log contains a
statement that uses the temporary table.  When the first *note 'mysql':
mysql. process terminates, the server drops the temporary table.  When
the second *note 'mysql': mysql. process attempts to use the table, the
server reports 'unknown table.'

To avoid problems like this, use a _single_ connection to execute the
contents of all binary logs that you want to process.  Here is one way
to do so:

     shell> mysqlbinlog binlog.000001 binlog.000002 | mysql -u root -p

Another approach is to write all the logs to a single file and then
process the file:

     shell> mysqlbinlog binlog.000001 >  /tmp/statements.sql
     shell> mysqlbinlog binlog.000002 >> /tmp/statements.sql
     shell> mysql -u root -p -e "source /tmp/statements.sql"


File: manual.info.tmp,  Node: point-in-time-recovery-times,  Next: point-in-time-recovery-positions,  Prev: point-in-time-recovery,  Up: point-in-time-recovery

7.5.1 Point-in-Time Recovery Using Event Times
----------------------------------------------

To indicate the start and end times for recovery, specify the
'--start-datetime' and '--stop-datetime' options for *note
'mysqlbinlog': mysqlbinlog, in *note 'DATETIME': datetime. format.  As
an example, suppose that exactly at 10:00 a.m.  on April 20, 2005 an SQL
statement was executed that deleted a large table.  To restore the table
and data, you could restore the previous night's backup, and then
execute the following command:

     shell> mysqlbinlog --stop-datetime="2005-04-20 9:59:59" \
              /var/log/mysql/bin.123456 | mysql -u root -p

This command recovers all of the data up until the date and time given
by the '--stop-datetime' option.  If you did not detect the erroneous
SQL statement that was entered until hours later, you will probably also
want to recover the activity that occurred afterward.  Based on this,
you could run *note 'mysqlbinlog': mysqlbinlog. again with a start date
and time, like so:

     shell> mysqlbinlog --start-datetime="2005-04-20 10:01:00" \
              /var/log/mysql/bin.123456 | mysql -u root -p

In this command, the SQL statements logged from 10:01 a.m.  on will be
re-executed.  The combination of restoring of the previous night's dump
file and the two *note 'mysqlbinlog': mysqlbinlog. commands restores
everything up until one second before 10:00 a.m.  and everything from
10:01 a.m.  on.

To use this method of point-in-time recovery, you should examine the log
to be sure of the exact times to specify for the commands.  To display
the log file contents without executing them, use this command:

     shell> mysqlbinlog /var/log/mysql/bin.123456 > /tmp/mysql_restore.sql

Then open the '/tmp/mysql_restore.sql' file with a text editor to
examine it.

Excluding specific changes by specifying times for *note 'mysqlbinlog':
mysqlbinlog. does not work well if multiple statements executed at the
same time as the one to be excluded.


File: manual.info.tmp,  Node: point-in-time-recovery-positions,  Prev: point-in-time-recovery-times,  Up: point-in-time-recovery

7.5.2 Point-in-Time Recovery Using Event Positions
--------------------------------------------------

Instead of specifying dates and times, the '--start-position' and
'--stop-position' options for *note 'mysqlbinlog': mysqlbinlog. can be
used for specifying log positions.  They work the same as the start and
stop date options, except that you specify log position numbers rather
than dates.  Using positions may enable you to be more precise about
which part of the log to recover, especially if many transactions
occurred around the same time as a damaging SQL statement.  To determine
the position numbers, run *note 'mysqlbinlog': mysqlbinlog. for a range
of times near the time when the unwanted transaction was executed, but
redirect the results to a text file for examination.  This can be done
like so:

     shell> mysqlbinlog --start-datetime="2005-04-20 9:55:00" \
              --stop-datetime="2005-04-20 10:05:00" \
              /var/log/mysql/bin.123456 > /tmp/mysql_restore.sql

This command creates a small text file in the '/tmp' directory that
contains the SQL statements around the time that the deleterious SQL
statement was executed.  Open this file with a text editor and look for
the statement that you do not want to repeat.  Determine the positions
in the binary log for stopping and resuming the recovery and make note
of them.  Positions are labeled as 'log_pos' followed by a number.
After restoring the previous backup file, use the position numbers to
process the binary log file.  For example, you would use commands
something like these:

     shell> mysqlbinlog --stop-position=368312 /var/log/mysql/bin.123456 \
              | mysql -u root -p

     shell> mysqlbinlog --start-position=368315 /var/log/mysql/bin.123456 \
              | mysql -u root -p

The first command recovers all the transactions up until the stop
position given.  The second command recovers all transactions from the
starting position given until the end of the binary log.  Because the
output of *note 'mysqlbinlog': mysqlbinlog. includes 'SET TIMESTAMP'
statements before each SQL statement recorded, the recovered data and
related MySQL logs will reflect the original times at which the
transactions were executed.


File: manual.info.tmp,  Node: myisam-table-maintenance,  Prev: point-in-time-recovery,  Up: backup-and-recovery

7.6 MyISAM Table Maintenance and Crash Recovery
===============================================

* Menu:

* myisam-crash-recovery::        Using myisamchk for Crash Recovery
* myisam-check::                 How to Check MyISAM Tables for Errors
* myisam-repair::                How to Repair MyISAM Tables
* myisam-optimization::          MyISAM Table Optimization
* myisam-maintenance-schedule::  Setting Up a MyISAM Table Maintenance Schedule

This section discusses how to use *note 'myisamchk': myisamchk. to check
or repair 'MyISAM' tables (tables that have '.MYD' and '.MYI' files for
storing data and indexes).  For general *note 'myisamchk': myisamchk.
background, see *note myisamchk::.  Other table-repair information can
be found at *note rebuilding-tables::.

You can use *note 'myisamchk': myisamchk. to check, repair, or optimize
database tables.  The following sections describe how to perform these
operations and how to set up a table maintenance schedule.  For
information about using *note 'myisamchk': myisamchk. to get information
about your tables, see *note myisamchk-table-info::.

Even though table repair with *note 'myisamchk': myisamchk. is quite
secure, it is always a good idea to make a backup _before_ doing a
repair or any maintenance operation that could make a lot of changes to
a table.

*note 'myisamchk': myisamchk. operations that affect indexes can cause
'FULLTEXT' indexes to be rebuilt with full-text parameters that are
incompatible with the values used by the MySQL server.  To avoid this
problem, follow the guidelines in *note myisamchk-general-options::.

'MyISAM' table maintenance can also be done using the SQL statements
that perform operations similar to what *note 'myisamchk': myisamchk.
can do:

   * To check 'MyISAM' tables, use *note 'CHECK TABLE': check-table.

   * To repair 'MyISAM' tables, use *note 'REPAIR TABLE': repair-table.

   * To optimize 'MyISAM' tables, use *note 'OPTIMIZE TABLE':
     optimize-table.

   * To analyze 'MyISAM' tables, use *note 'ANALYZE TABLE':
     analyze-table.

For additional information about these statements, see *note
table-maintenance-statements::.

These statements can be used directly or by means of the *note
'mysqlcheck': mysqlcheck. client program.  One advantage of these
statements over *note 'myisamchk': myisamchk. is that the server does
all the work.  With *note 'myisamchk': myisamchk, you must make sure
that the server does not use the tables at the same time so that there
is no unwanted interaction between *note 'myisamchk': myisamchk. and the
server.


File: manual.info.tmp,  Node: myisam-crash-recovery,  Next: myisam-check,  Prev: myisam-table-maintenance,  Up: myisam-table-maintenance

7.6.1 Using myisamchk for Crash Recovery
----------------------------------------

This section describes how to check for and deal with data corruption in
MySQL databases.  If your tables become corrupted frequently, you should
try to find the reason why.  See *note crashing::.

For an explanation of how 'MyISAM' tables can become corrupted, see
*note myisam-table-problems::.

If you run *note 'mysqld': mysqld. with external locking disabled (which
is the default), you cannot reliably use *note 'myisamchk': myisamchk.
to check a table when *note 'mysqld': mysqld. is using the same table.
If you can be certain that no one will access the tables through *note
'mysqld': mysqld. while you run *note 'myisamchk': myisamchk, you only
have to execute *note 'mysqladmin flush-tables': mysqladmin. before you
start checking the tables.  If you cannot guarantee this, you must stop
*note 'mysqld': mysqld. while you check the tables.  If you run *note
'myisamchk': myisamchk. to check tables that *note 'mysqld': mysqld. is
updating at the same time, you may get a warning that a table is corrupt
even when it is not.

If the server is run with external locking enabled, you can use *note
'myisamchk': myisamchk. to check tables at any time.  In this case, if
the server tries to update a table that *note 'myisamchk': myisamchk. is
using, the server will wait for *note 'myisamchk': myisamchk. to finish
before it continues.

If you use *note 'myisamchk': myisamchk. to repair or optimize tables,
you _must_ always ensure that the *note 'mysqld': mysqld. server is not
using the table (this also applies if external locking is disabled).  If
you do not stop *note 'mysqld': mysqld, you should at least do a *note
'mysqladmin flush-tables': mysqladmin. before you run *note 'myisamchk':
myisamchk.  Your tables _may become corrupted_ if the server and *note
'myisamchk': myisamchk. access the tables simultaneously.

When performing crash recovery, it is important to understand that each
'MyISAM' table TBL_NAME in a database corresponds to the three files in
the database directory shown in the following table.

File           Purpose
               
'TBL_NAME.frm' Definition (format) file
               
'TBL_NAME.MYD' Data file
               
'TBL_NAME.MYI' Index file

Each of these three file types is subject to corruption in various ways,
but problems occur most often in data files and index files.

*note 'myisamchk': myisamchk. works by creating a copy of the '.MYD'
data file row by row.  It ends the repair stage by removing the old
'.MYD' file and renaming the new file to the original file name.  If you
use '--quick', *note 'myisamchk': myisamchk. does not create a temporary
'.MYD' file, but instead assumes that the '.MYD' file is correct and
generates only a new index file without touching the '.MYD' file.  This
is safe, because *note 'myisamchk': myisamchk. automatically detects
whether the '.MYD' file is corrupt and aborts the repair if it is.  You
can also specify the '--quick' option twice to *note 'myisamchk':
myisamchk.  In this case, *note 'myisamchk': myisamchk. does not abort
on some errors (such as duplicate-key errors) but instead tries to
resolve them by modifying the '.MYD' file.  Normally the use of two
'--quick' options is useful only if you have too little free disk space
to perform a normal repair.  In this case, you should at least make a
backup of the table before running *note 'myisamchk': myisamchk.


File: manual.info.tmp,  Node: myisam-check,  Next: myisam-repair,  Prev: myisam-crash-recovery,  Up: myisam-table-maintenance

7.6.2 How to Check MyISAM Tables for Errors
-------------------------------------------

To check a 'MyISAM' table, use the following commands:

   * *note 'myisamchk TBL_NAME': myisamchk.

     This finds 99.99% of all errors.  What it cannot find is corruption
     that involves _only_ the data file (which is very unusual).  If you
     want to check a table, you should normally run *note 'myisamchk':
     myisamchk. without options or with the '-s' (silent) option.

   * *note 'myisamchk -m TBL_NAME': myisamchk.

     This finds 99.999% of all errors.  It first checks all index
     entries for errors and then reads through all rows.  It calculates
     a checksum for all key values in the rows and verifies that the
     checksum matches the checksum for the keys in the index tree.

   * *note 'myisamchk -e TBL_NAME': myisamchk.

     This does a complete and thorough check of all data ('-e' means
     'extended check').  It does a check-read of every key for each row
     to verify that they indeed point to the correct row.  This may take
     a long time for a large table that has many indexes.  Normally,
     *note 'myisamchk': myisamchk. stops after the first error it finds.
     If you want to obtain more information, you can add the '-v'
     (verbose) option.  This causes *note 'myisamchk': myisamchk. to
     keep going, up through a maximum of 20 errors.

   * *note 'myisamchk -e -i TBL_NAME': myisamchk.

     This is like the previous command, but the '-i' option tells *note
     'myisamchk': myisamchk. to print additional statistical
     information.

In most cases, a simple *note 'myisamchk': myisamchk. command with no
arguments other than the table name is sufficient to check a table.


File: manual.info.tmp,  Node: myisam-repair,  Next: myisam-optimization,  Prev: myisam-check,  Up: myisam-table-maintenance

7.6.3 How to Repair MyISAM Tables
---------------------------------

The discussion in this section describes how to use *note 'myisamchk':
myisamchk. on 'MyISAM' tables (extensions '.MYI' and '.MYD').

You can also use the *note 'CHECK TABLE': check-table. and *note 'REPAIR
TABLE': repair-table. statements to check and repair 'MyISAM' tables.
See *note check-table::, and *note repair-table::.

Symptoms of corrupted tables include queries that abort unexpectedly and
observable errors such as these:

   * 'TBL_NAME.frm' is locked against change

   * Can't find file 'TBL_NAME.MYI' (Errcode: NNN)

   * Unexpected end of file

   * Record file is crashed

   * Got error NNN from table handler

To get more information about the error, run *note 'perror': perror.
NNN, where NNN is the error number.  The following example shows how to
use *note 'perror': perror. to find the meanings for the most common
error numbers that indicate a problem with a table:

     shell> perror 126 127 132 134 135 136 141 144 145
     MySQL error code 126 = Index file is crashed
     MySQL error code 127 = Record-file is crashed
     MySQL error code 132 = Old database file
     MySQL error code 134 = Record was already deleted (or record file crashed)
     MySQL error code 135 = No more room in record file
     MySQL error code 136 = No more room in index file
     MySQL error code 141 = Duplicate unique key or constraint on write or update
     MySQL error code 144 = Table is crashed and last repair failed
     MySQL error code 145 = Table was marked as crashed and should be repaired

Note that error 135 (no more room in record file) and error 136 (no more
room in index file) are not errors that can be fixed by a simple repair.
In this case, you must use *note 'ALTER TABLE': alter-table. to increase
the 'MAX_ROWS' and 'AVG_ROW_LENGTH' table option values:

     ALTER TABLE TBL_NAME MAX_ROWS=XXX AVG_ROW_LENGTH=YYY;

If you do not know the current table option values, use *note 'SHOW
CREATE TABLE': show-create-table.

For the other errors, you must repair your tables.  *note 'myisamchk':
myisamchk. can usually detect and fix most problems that occur.

The repair process involves up to four stages, described here.  Before
you begin, you should change location to the database directory and
check the permissions of the table files.  On Unix, make sure that they
are readable by the user that *note 'mysqld': mysqld. runs as (and to
you, because you need to access the files you are checking).  If it
turns out you need to modify files, they must also be writable by you.

This section is for the cases where a table check fails (such as those
described in *note myisam-check::), or you want to use the extended
features that *note 'myisamchk': myisamchk. provides.

The *note 'myisamchk': myisamchk. options used for table maintenance
with are described in *note myisamchk::.  *note 'myisamchk': myisamchk.
also has variables that you can set to control memory allocation that
may improve performance.  See *note myisamchk-memory::.

If you are going to repair a table from the command line, you must first
stop the *note 'mysqld': mysqld. server.  Note that when you do *note
'mysqladmin shutdown': mysqladmin. on a remote server, the *note
'mysqld': mysqld. server is still available for a while after *note
'mysqladmin': mysqladmin. returns, until all statement-processing has
stopped and all index changes have been flushed to disk.

*Stage 1: Checking your tables*

Run *note 'myisamchk *.MYI': myisamchk. or *note 'myisamchk -e *.MYI':
myisamchk. if you have more time.  Use the '-s' (silent) option to
suppress unnecessary information.

If the *note 'mysqld': mysqld. server is stopped, you should use the
'--update-state' option to tell *note 'myisamchk': myisamchk. to mark
the table as 'checked.'

You have to repair only those tables for which *note 'myisamchk':
myisamchk. announces an error.  For such tables, proceed to Stage 2.

If you get unexpected errors when checking (such as 'out of memory'
errors), or if *note 'myisamchk': myisamchk. crashes, go to Stage 3.

*Stage 2: Easy safe repair*

First, try *note 'myisamchk -r -q TBL_NAME': myisamchk. ('-r -q' means
'quick recovery mode').  This attempts to repair the index file without
touching the data file.  If the data file contains everything that it
should and the delete links point at the correct locations within the
data file, this should work, and the table is fixed.  Start repairing
the next table.  Otherwise, use the following procedure:

  1. Make a backup of the data file before continuing.

  2. Use *note 'myisamchk -r TBL_NAME': myisamchk. ('-r' means 'recovery
     mode').  This removes incorrect rows and deleted rows from the data
     file and reconstructs the index file.

  3. If the preceding step fails, use *note 'myisamchk --safe-recover
     TBL_NAME': myisamchk.  Safe recovery mode uses an old recovery
     method that handles a few cases that regular recovery mode does not
     (but is slower).

*Note*:

If you want a repair operation to go much faster, you should set the
values of the 'sort_buffer_size' and 'key_buffer_size' variables each to
about 25% of your available memory when running *note 'myisamchk':
myisamchk.

If you get unexpected errors when repairing (such as 'out of memory'
errors), or if *note 'myisamchk': myisamchk. crashes, go to Stage 3.

*Stage 3: Difficult repair*

You should reach this stage only if the first 16KB block in the index
file is destroyed or contains incorrect information, or if the index
file is missing.  In this case, it is necessary to create a new index
file.  Do so as follows:

  1. Move the data file to a safe place.

  2. Use the table description file to create new (empty) data and index
     files:

          shell> mysql DB_NAME

          mysql> SET autocommit=1;
          mysql> TRUNCATE TABLE TBL_NAME;
          mysql> quit

  3. Copy the old data file back onto the newly created data file.  (Do
     not just move the old file back onto the new file.  You want to
     retain a copy in case something goes wrong.)

*Important*:

If you are using replication, you should stop it prior to performing the
above procedure, since it involves file system operations, and these are
not logged by MySQL.

Go back to Stage 2.  *note 'myisamchk -r -q': myisamchk. should work.
(This should not be an endless loop.)

You can also use the 'REPAIR TABLE TBL_NAME USE_FRM' SQL statement,
which performs the whole procedure automatically.  There is also no
possibility of unwanted interaction between a utility and the server,
because the server does all the work when you use *note 'REPAIR TABLE':
repair-table.  See *note repair-table::.

*Stage 4: Very difficult repair*

You should reach this stage only if the '.frm' description file has also
crashed.  That should never happen, because the description file is not
changed after the table is created:

  1. Restore the description file from a backup and go back to Stage 3.
     You can also restore the index file and go back to Stage 2.  In the
     latter case, you should start with *note 'myisamchk -r': myisamchk.

  2. If you do not have a backup but know exactly how the table was
     created, create a copy of the table in another database.  Remove
     the new data file, and then move the '.frm' description and '.MYI'
     index files from the other database to your crashed database.  This
     gives you new description and index files, but leaves the '.MYD'
     data file alone.  Go back to Stage 2 and attempt to reconstruct the
     index file.


File: manual.info.tmp,  Node: myisam-optimization,  Next: myisam-maintenance-schedule,  Prev: myisam-repair,  Up: myisam-table-maintenance

7.6.4 MyISAM Table Optimization
-------------------------------

To coalesce fragmented rows and eliminate wasted space that results from
deleting or updating rows, run *note 'myisamchk': myisamchk. in recovery
mode:

     shell> myisamchk -r TBL_NAME

You can optimize a table in the same way by using the *note 'OPTIMIZE
TABLE': optimize-table. SQL statement.  *note 'OPTIMIZE TABLE':
optimize-table. does a table repair and a key analysis, and also sorts
the index tree so that key lookups are faster.  There is also no
possibility of unwanted interaction between a utility and the server,
because the server does all the work when you use *note 'OPTIMIZE
TABLE': optimize-table.  See *note optimize-table::.

*note 'myisamchk': myisamchk. has a number of other options that you can
use to improve the performance of a table:

   * '--analyze' or '-a': Perform key distribution analysis.  This
     improves join performance by enabling the join optimizer to better
     choose the order in which to join the tables and which indexes it
     should use.

   * '--sort-index' or '-S': Sort the index blocks.  This optimizes
     seeks and makes table scans that use indexes faster.

   * '--sort-records=INDEX_NUM' or '-R INDEX_NUM': Sort data rows
     according to a given index.  This makes your data much more
     localized and may speed up range-based *note 'SELECT': select. and
     'ORDER BY' operations that use this index.

For a full description of all available options, see *note myisamchk::.


File: manual.info.tmp,  Node: myisam-maintenance-schedule,  Prev: myisam-optimization,  Up: myisam-table-maintenance

7.6.5 Setting Up a MyISAM Table Maintenance Schedule
----------------------------------------------------

It is a good idea to perform table checks on a regular basis rather than
waiting for problems to occur.  One way to check and repair 'MyISAM'
tables is with the *note 'CHECK TABLE': check-table. and *note 'REPAIR
TABLE': repair-table. statements.  See *note
table-maintenance-statements::.

Another way to check tables is to use *note 'myisamchk': myisamchk.  For
maintenance purposes, you can use *note 'myisamchk -s': myisamchk.  The
'-s' option (short for '--silent') causes *note 'myisamchk': myisamchk.
to run in silent mode, printing messages only when errors occur.

It is also a good idea to enable automatic 'MyISAM' table checking.  For
example, whenever the machine has done a restart in the middle of an
update, you usually need to check each table that could have been
affected before it is used further.  (These are 'expected crashed
tables.') To cause the server to check 'MyISAM' tables automatically,
start it with the 'myisam_recover_options' system variable set.  See
*note server-system-variables::.

You should also check your tables regularly during normal system
operation.  For example, you can run a 'cron' job to check important
tables once a week, using a line like this in a 'crontab' file:

     35 0 * * 0 /PATH/TO/MYISAMCHK --fast --silent /PATH/TO/DATADIR/*/*.MYI

This prints out information about crashed tables so that you can examine
and repair them as necessary.

To start with, execute *note 'myisamchk -s': myisamchk. each night on
all tables that have been updated during the last 24 hours.  As you see
that problems occur infrequently, you can back off the checking
frequency to once a week or so.

Normally, MySQL tables need little maintenance.  If you are performing
many updates to 'MyISAM' tables with dynamic-sized rows (tables with
*note 'VARCHAR': char, *note 'BLOB': blob, or *note 'TEXT': blob.
columns) or have tables with many deleted rows you may want to
defragment/reclaim space from the tables from time to time.  You can do
this by using *note 'OPTIMIZE TABLE': optimize-table. on the tables in
question.  Alternatively, if you can stop the *note 'mysqld': mysqld.
server for a while, change location into the data directory and use this
command while the server is stopped:

     shell> myisamchk -r -s --sort-index --myisam_sort_buffer_size=16M */*.MYI


File: manual.info.tmp,  Node: optimization,  Next: language-structure,  Prev: backup-and-recovery,  Up: Top

8 Optimization
**************

* Menu:

* optimize-overview::            Optimization Overview
* statement-optimization::       Optimizing SQL Statements
* optimization-indexes::         Optimization and Indexes
* optimizing-database-structure::  Optimizing Database Structure
* optimizing-innodb::            Optimizing for InnoDB Tables
* optimizing-myisam::            Optimizing for MyISAM Tables
* optimizing-memory-tables::     Optimizing for MEMORY Tables
* execution-plan-information::   Understanding the Query Execution Plan
* controlling-optimizer::        Controlling the Query Optimizer
* buffering-caching::            Buffering and Caching
* locking-issues::               Optimizing Locking Operations
* optimizing-server::            Optimizing the MySQL Server
* optimize-benchmarking::        Measuring Performance (Benchmarking)
* thread-information::           Examining Thread Information

This chapter explains how to optimize MySQL performance and provides
examples.  Optimization involves configuring, tuning, and measuring
performance, at several levels.  Depending on your job role (developer,
DBA, or a combination of both), you might optimize at the level of
individual SQL statements, entire applications, a single database
server, or multiple networked database servers.  Sometimes you can be
proactive and plan in advance for performance, while other times you
might troubleshoot a configuration or code issue after a problem occurs.
Optimizing CPU and memory usage can also improve scalability, allowing
the database to handle more load without slowing down.


File: manual.info.tmp,  Node: optimize-overview,  Next: statement-optimization,  Prev: optimization,  Up: optimization

8.1 Optimization Overview
=========================

Database performance depends on several factors at the database level,
such as tables, queries, and configuration settings.  These software
constructs result in CPU and I/O operations at the hardware level, which
you must minimize and make as efficient as possible.  As you work on
database performance, you start by learning the high-level rules and
guidelines for the software side, and measuring performance using
wall-clock time.  As you become an expert, you learn more about what
happens internally, and start measuring things such as CPU cycles and
I/O operations.

Typical users aim to get the best database performance out of their
existing software and hardware configurations.  Advanced users look for
opportunities to improve the MySQL software itself, or develop their own
storage engines and hardware appliances to expand the MySQL ecosystem.

   * *note optimize-database-level::

   * *note optimize-hardware-level::

   * *note optimize-portability-performance::

*Optimizing at the Database Level*

The most important factor in making a database application fast is its
basic design:

   * Are the tables structured properly?  In particular, do the columns
     have the right data types, and does each table have the appropriate
     columns for the type of work?  For example, applications that
     perform frequent updates often have many tables with few columns,
     while applications that analyze large amounts of data often have
     few tables with many columns.

   * Are the right *note indexes: optimization-indexes. in place to make
     queries efficient?

   * Are you using the appropriate storage engine for each table, and
     taking advantage of the strengths and features of each storage
     engine you use?  In particular, the choice of a transactional
     storage engine such as '*note InnoDB: optimizing-innodb.' or a
     nontransactional one such as '*note MyISAM: optimizing-myisam.' can
     be very important for performance and scalability.

     *Note*:

     'InnoDB' is the default storage engine for new tables.  In
     practice, the advanced 'InnoDB' performance features mean that
     'InnoDB' tables often outperform the simpler 'MyISAM' tables,
     especially for a busy database.

   * Does each table use an appropriate row format?  This choice also
     depends on the storage engine used for the table.  In particular,
     compressed tables use less disk space and so require less disk I/O
     to read and write the data.  Compression is available for all kinds
     of workloads with 'InnoDB' tables, and for read-only 'MyISAM'
     tables.

   * Does the application use an appropriate *note locking strategy:
     locking-issues.?  For example, by allowing shared access when
     possible so that database operations can run concurrently, and
     requesting exclusive access when appropriate so that critical
     operations get top priority.  Again, the choice of storage engine
     is significant.  The 'InnoDB' storage engine handles most locking
     issues without involvement from you, allowing for better
     concurrency in the database and reducing the amount of
     experimentation and tuning for your code.

   * Are all *note memory areas used for caching: buffering-caching.
     sized correctly?  That is, large enough to hold frequently accessed
     data, but not so large that they overload physical memory and cause
     paging.  The main memory areas to configure are the 'InnoDB' buffer
     pool, the 'MyISAM' key cache, and the MySQL query cache.

*Optimizing at the Hardware Level*

Any database application eventually hits hardware limits as the database
becomes more and more busy.  A DBA must evaluate whether it is possible
to tune the application or reconfigure the server to avoid these
bottlenecks, or whether more hardware resources are required.  System
bottlenecks typically arise from these sources:

   * Disk seeks.  It takes time for the disk to find a piece of data.
     With modern disks, the mean time for this is usually lower than
     10ms, so we can in theory do about 100 seeks a second.  This time
     improves slowly with new disks and is very hard to optimize for a
     single table.  The way to optimize seek time is to distribute the
     data onto more than one disk.

   * Disk reading and writing.  When the disk is at the correct
     position, we need to read or write the data.  With modern disks,
     one disk delivers at least 10-20MB/s throughput.  This is easier to
     optimize than seeks because you can read in parallel from multiple
     disks.

   * CPU cycles.  When the data is in main memory, we must process it to
     get our result.  Having large tables compared to the amount of
     memory is the most common limiting factor.  But with small tables,
     speed is usually not the problem.

   * Memory bandwidth.  When the CPU needs more data than can fit in the
     CPU cache, main memory bandwidth becomes a bottleneck.  This is an
     uncommon bottleneck for most systems, but one to be aware of.

*Balancing Portability and Performance*

To use performance-oriented SQL extensions in a portable MySQL program,
you can wrap MySQL-specific keywords in a statement within '/*! */'
comment delimiters.  Other SQL servers ignore the commented keywords.
For information about writing comments, see *note comments::.


File: manual.info.tmp,  Node: statement-optimization,  Next: optimization-indexes,  Prev: optimize-overview,  Up: optimization

8.2 Optimizing SQL Statements
=============================

* Menu:

* select-optimization::          Optimizing SELECT Statements
* subquery-optimization::        Subquery Optimization
* information-schema-optimization::  Optimizing INFORMATION_SCHEMA Queries
* data-change-optimization::     Optimizing Data Change Statements
* permission-optimization::      Optimizing Database Privileges
* miscellaneous-optimization-tips::  Other Optimization Tips

The core logic of a database application is performed through SQL
statements, whether issued directly through an interpreter or submitted
behind the scenes through an API. The tuning guidelines in this section
help to speed up all kinds of MySQL applications.  The guidelines cover
SQL operations that read and write data, the behind-the-scenes overhead
for SQL operations in general, and operations used in specific scenarios
such as database monitoring.


File: manual.info.tmp,  Node: select-optimization,  Next: subquery-optimization,  Prev: statement-optimization,  Up: statement-optimization

8.2.1 Optimizing SELECT Statements
----------------------------------

* Menu:

* where-optimization::           WHERE Clause Optimization
* range-optimization::           Range Optimization
* index-merge-optimization::     Index Merge Optimization
* condition-pushdown-optimization::  Engine Condition Pushdown Optimization
* nested-loop-joins::            Nested-Loop Join Algorithms
* nested-join-optimization::     Nested Join Optimization
* outer-join-optimization::      Outer Join Optimization
* outer-join-simplification::    Outer Join Simplification
* is-null-optimization::         IS NULL Optimization
* order-by-optimization::        ORDER BY Optimization
* group-by-optimization::        GROUP BY Optimization
* distinct-optimization::        DISTINCT Optimization
* limit-optimization::           LIMIT Query Optimization
* function-optimization::        Function Call Optimization
* row-constructor-optimization::  Row Constructor Expression Optimization
* table-scan-avoidance::         Avoiding Full Table Scans

Queries, in the form of *note 'SELECT': select. statements, perform all
the lookup operations in the database.  Tuning these statements is a top
priority, whether to achieve sub-second response times for dynamic web
pages, or to chop hours off the time to generate huge overnight reports.

Besides *note 'SELECT': select. statements, the tuning techniques for
queries also apply to constructs such as *note 'CREATE TABLE...AS
SELECT': create-table-select, *note 'INSERT INTO...SELECT':
insert-select, and 'WHERE' clauses in *note 'DELETE': delete.
statements.  Those statements have additional performance considerations
because they combine write operations with the read-oriented query
operations.

In MySQL NDB Cluster 7.2 and later, the *note 'NDB': mysql-cluster.
storage engine supports a join pushdown optimization whereby a
qualifying join is sent in its entirety to NDB Cluster data nodes, where
it can be distributed among them and executed in parallel.  For more
information about this optimization, see *note
ndb_join_pushdown-conditions::.

The main considerations for optimizing queries are:

   * To make a slow 'SELECT ... WHERE' query faster, the first thing to
     check is whether you can add an index.  Set up indexes on columns
     used in the 'WHERE' clause, to speed up evaluation, filtering, and
     the final retrieval of results.  To avoid wasted disk space,
     construct a small set of indexes that speed up many related queries
     used in your application.

     Indexes are especially important for queries that reference
     different tables, using features such as joins and foreign keys.
     You can use the *note 'EXPLAIN': explain. statement to determine
     which indexes are used for a *note 'SELECT': select.  See *note
     mysql-indexes:: and *note using-explain::.

   * Isolate and tune any part of the query, such as a function call,
     that takes excessive time.  Depending on how the query is
     structured, a function could be called once for every row in the
     result set, or even once for every row in the table, greatly
     magnifying any inefficiency.

   * Minimize the number of full table scans in your queries,
     particularly for big tables.

   * Keep table statistics up to date by using the *note 'ANALYZE
     TABLE': analyze-table. statement periodically, so the optimizer has
     the information needed to construct an efficient execution plan.

   * Learn the tuning techniques, indexing techniques, and configuration
     parameters that are specific to the storage engine for each table.
     Both 'InnoDB' and 'MyISAM' have sets of guidelines for enabling and
     sustaining high performance in queries.  For details, see *note
     optimizing-innodb-queries:: and *note optimizing-queries-myisam::.

   * Avoid transforming the query in ways that make it hard to
     understand, especially if the optimizer does some of the same
     transformations automatically.

   * If a performance issue is not easily solved by one of the basic
     guidelines, investigate the internal details of the specific query
     by reading the *note 'EXPLAIN': explain. plan and adjusting your
     indexes, 'WHERE' clauses, join clauses, and so on.  (When you reach
     a certain level of expertise, reading the *note 'EXPLAIN': explain.
     plan might be your first step for every query.)

   * Adjust the size and properties of the memory areas that MySQL uses
     for caching.  With efficient use of the 'InnoDB' buffer pool,
     'MyISAM' key cache, and the MySQL query cache, repeated queries run
     faster because the results are retrieved from memory the second and
     subsequent times.

   * Even for a query that runs fast using the cache memory areas, you
     might still optimize further so that they require less cache
     memory, making your application more scalable.  Scalability means
     that your application can handle more simultaneous users, larger
     requests, and so on without experiencing a big drop in performance.

   * Deal with locking issues, where the speed of your query might be
     affected by other sessions accessing the tables at the same time.


File: manual.info.tmp,  Node: where-optimization,  Next: range-optimization,  Prev: select-optimization,  Up: select-optimization

8.2.1.1 WHERE Clause Optimization
.................................

This section discusses optimizations that can be made for processing
'WHERE' clauses.  The examples use *note 'SELECT': select. statements,
but the same optimizations apply for 'WHERE' clauses in *note 'DELETE':
delete. and *note 'UPDATE': update. statements.

Some examples of queries that are very fast:

     SELECT COUNT(*) FROM TBL_NAME;

     SELECT MIN(KEY_PART1),MAX(KEY_PART1) FROM TBL_NAME;

     SELECT MAX(KEY_PART2) FROM TBL_NAME
       WHERE KEY_PART1=CONSTANT;

     SELECT ... FROM TBL_NAME
       ORDER BY KEY_PART1,KEY_PART2,... LIMIT 10;

     SELECT ... FROM TBL_NAME
       ORDER BY KEY_PART1 DESC, KEY_PART2 DESC, ... LIMIT 10;

MySQL resolves the following queries using only the entries from a
secondary index, if the indexed columns are numeric:

     SELECT KEY_PART1,KEY_PART2 FROM TBL_NAME WHERE KEY_PART1=VAL;

     SELECT COUNT(*) FROM TBL_NAME
       WHERE KEY_PART1=VAL1 AND KEY_PART2=VAL2;

     SELECT KEY_PART2 FROM TBL_NAME GROUP BY KEY_PART1;

The following queries use the index data to retrieve the rows in sorted
order without a separate sorting pass:

     SELECT ... FROM TBL_NAME
       ORDER BY KEY_PART1,KEY_PART2,... ;

     SELECT ... FROM TBL_NAME
       ORDER BY KEY_PART1 DESC, KEY_PART2 DESC, ... ;

You might be tempted to rewrite your queries to make arithmetic
operations faster, while sacrificing readability.  Because MySQL does
similar optimizations automatically, you can often avoid this work, and
leave the query in a more understandable and maintainable form.  Some of
the optimizations performed by MySQL follow:

*Note*:

Because work on the MySQL optimizer is ongoing, not all of the
optimizations that MySQL performs are documented here.

   * Removal of unnecessary parentheses:

             ((a AND b) AND c OR (((a AND b) AND (c AND d))))
          -> (a AND b AND c) OR (a AND b AND c AND d)

   * Constant folding:

             (a<b AND b=c) AND a=5
          -> b>5 AND b=c AND a=5

   * Constant condition removal:

             (b>=5 AND b=5) OR (b=6 AND 5=5) OR (b=7 AND 5=6)
          -> b=5 OR b=6

   * Constant expressions used by indexes are evaluated only once.

   * 'COUNT(*)' on a single table without a 'WHERE' is retrieved
     directly from the table information for 'MyISAM' and 'MEMORY'
     tables.  This is also done for any 'NOT NULL' expression when used
     with only one table.

   * Early detection of invalid constant expressions.  MySQL quickly
     detects that some *note 'SELECT': select. statements are impossible
     and returns no rows.

   * 'HAVING' is merged with 'WHERE' if you do not use 'GROUP BY' or
     aggregate functions ('COUNT()', 'MIN()', and so on).

   * For each table in a join, a simpler 'WHERE' is constructed to get a
     fast 'WHERE' evaluation for the table and also to skip rows as soon
     as possible.

   * 
     All constant tables are read first before any other tables in the
     query.  A constant table is any of the following:

        * An empty table or a table with one row.

        * A table that is used with a 'WHERE' clause on a 'PRIMARY KEY'
          or a 'UNIQUE' index, where all index parts are compared to
          constant expressions and are defined as 'NOT NULL'.

     All of the following tables are used as constant tables:

          SELECT * FROM t WHERE PRIMARY_KEY=1;
          SELECT * FROM t1,t2
            WHERE t1.PRIMARY_KEY=1 AND t2.PRIMARY_KEY=t1.id;

   * The best join combination for joining the tables is found by trying
     all possibilities.  If all columns in 'ORDER BY' and 'GROUP BY'
     clauses come from the same table, that table is preferred first
     when joining.

   * If there is an 'ORDER BY' clause and a different 'GROUP BY' clause,
     or if the 'ORDER BY' or 'GROUP BY' contains columns from tables
     other than the first table in the join queue, a temporary table is
     created.

   * If you use the 'SQL_SMALL_RESULT' modifier, MySQL uses an in-memory
     temporary table.

   * Each table index is queried, and the best index is used unless the
     optimizer believes that it is more efficient to use a table scan.
     At one time, a scan was used based on whether the best index
     spanned more than 30% of the table, but a fixed percentage no
     longer determines the choice between using an index or a scan.  The
     optimizer now is more complex and bases its estimate on additional
     factors such as table size, number of rows, and I/O block size.

   * In some cases, MySQL can read rows from the index without even
     consulting the data file.  If all columns used from the index are
     numeric, only the index tree is used to resolve the query.

   * Before each row is output, those that do not match the 'HAVING'
     clause are skipped.


File: manual.info.tmp,  Node: range-optimization,  Next: index-merge-optimization,  Prev: where-optimization,  Up: select-optimization

8.2.1.2 Range Optimization
..........................

The 'range' access method uses a single index to retrieve a subset of
table rows that are contained within one or several index value
intervals.  It can be used for a single-part or multiple-part index.
The following sections give a detailed description of how intervals are
extracted from the 'WHERE' clause.

   * *note range-access-single-part::

   * *note range-access-multi-part::

*Range Access Method for Single-Part Indexes*

For a single-part index, index value intervals can be conveniently
represented by corresponding conditions in the 'WHERE' clause, denoted
as _range conditions_ rather than 'intervals.'

The definition of a range condition for a single-part index is as
follows:

   * For both 'BTREE' and 'HASH' indexes, comparison of a key part with
     a constant value is a range condition when using the '=', '<=>',
     'IN()', 'IS NULL', or 'IS NOT NULL' operators.

   * Additionally, for 'BTREE' indexes, comparison of a key part with a
     constant value is a range condition when using the '>', '<', '>=',
     '<=', 'BETWEEN', '!=', or '<>' operators, or 'LIKE' comparisons if
     the argument to 'LIKE' is a constant string that does not start
     with a wildcard character.

   * For all index types, multiple range conditions combined with 'OR'
     or 'AND' form a range condition.

'Constant value' in the preceding descriptions means one of the
following:

   * A constant from the query string

   * A column of a 'const' or 'system' table from the same join

   * The result of an uncorrelated subquery

   * Any expression composed entirely from subexpressions of the
     preceding types

Here are some examples of queries with range conditions in the 'WHERE'
clause:

     SELECT * FROM t1
       WHERE KEY_COL > 1
       AND KEY_COL < 10;

     SELECT * FROM t1
       WHERE KEY_COL = 1
       OR KEY_COL IN (15,18,20);

     SELECT * FROM t1
       WHERE KEY_COL LIKE 'ab%'
       OR KEY_COL BETWEEN 'bar' AND 'foo';

Some nonconstant values may be converted to constants during the
optimizer constant propagation phase.

MySQL tries to extract range conditions from the 'WHERE' clause for each
of the possible indexes.  During the extraction process, conditions that
cannot be used for constructing the range condition are dropped,
conditions that produce overlapping ranges are combined, and conditions
that produce empty ranges are removed.

Consider the following statement, where 'key1' is an indexed column and
'nonkey' is not indexed:

     SELECT * FROM t1 WHERE
       (key1 < 'abc' AND (key1 LIKE 'abcde%' OR key1 LIKE '%b')) OR
       (key1 < 'bar' AND nonkey = 4) OR
       (key1 < 'uux' AND key1 > 'z');

The extraction process for key 'key1' is as follows:

  1. Start with original 'WHERE' clause:

          (key1 < 'abc' AND (key1 LIKE 'abcde%' OR key1 LIKE '%b')) OR
          (key1 < 'bar' AND nonkey = 4) OR
          (key1 < 'uux' AND key1 > 'z')

  2. Remove 'nonkey = 4' and 'key1 LIKE '%b'' because they cannot be
     used for a range scan.  The correct way to remove them is to
     replace them with 'TRUE', so that we do not miss any matching rows
     when doing the range scan.  Replacing them with 'TRUE' yields:

          (key1 < 'abc' AND (key1 LIKE 'abcde%' OR TRUE)) OR
          (key1 < 'bar' AND TRUE) OR
          (key1 < 'uux' AND key1 > 'z')

  3. Collapse conditions that are always true or false:

        * '(key1 LIKE 'abcde%' OR TRUE)' is always true

        * '(key1 < 'uux' AND key1 > 'z')' is always false

     Replacing these conditions with constants yields:

          (key1 < 'abc' AND TRUE) OR (key1 < 'bar' AND TRUE) OR (FALSE)

     Removing unnecessary 'TRUE' and 'FALSE' constants yields:

          (key1 < 'abc') OR (key1 < 'bar')

  4. Combining overlapping intervals into one yields the final condition
     to be used for the range scan:

          (key1 < 'bar')

In general (and as demonstrated by the preceding example), the condition
used for a range scan is less restrictive than the 'WHERE' clause.
MySQL performs an additional check to filter out rows that satisfy the
range condition but not the full 'WHERE' clause.

The range condition extraction algorithm can handle nested 'AND'/'OR'
constructs of arbitrary depth, and its output does not depend on the
order in which conditions appear in 'WHERE' clause.

MySQL does not support merging multiple ranges for the 'range' access
method for spatial indexes.  To work around this limitation, you can use
a *note 'UNION': union. with identical *note 'SELECT': select.
statements, except that you put each spatial predicate in a different
*note 'SELECT': select.

*Range Access Method for Multiple-Part Indexes*

Range conditions on a multiple-part index are an extension of range
conditions for a single-part index.  A range condition on a
multiple-part index restricts index rows to lie within one or several
key tuple intervals.  Key tuple intervals are defined over a set of key
tuples, using ordering from the index.

For example, consider a multiple-part index defined as 'key1(KEY_PART1,
KEY_PART2, KEY_PART3)', and the following set of key tuples listed in
key order:

     KEY_PART1  KEY_PART2  KEY_PART3
       NULL       1          'abc'
       NULL       1          'xyz'
       NULL       2          'foo'
        1         1          'abc'
        1         1          'xyz'
        1         2          'abc'
        2         1          'aaa'

The condition 'KEY_PART1 = 1' defines this interval:

     (1,-inf,-inf) <= (KEY_PART1,KEY_PART2,KEY_PART3) < (1,+inf,+inf)

The interval covers the 4th, 5th, and 6th tuples in the preceding data
set and can be used by the range access method.

By contrast, the condition 'KEY_PART3 = 'abc'' does not define a single
interval and cannot be used by the range access method.

The following descriptions indicate how range conditions work for
multiple-part indexes in greater detail.

   * For 'HASH' indexes, each interval containing identical values can
     be used.  This means that the interval can be produced only for
     conditions in the following form:

              KEY_PART1 CMP CONST1
          AND KEY_PART2 CMP CONST2
          AND ...
          AND KEY_PARTN CMP CONSTN;

     Here, CONST1, CONST2, ... are constants, CMP is one of the '=',
     '<=>', or 'IS NULL' comparison operators, and the conditions cover
     all index parts.  (That is, there are N conditions, one for each
     part of an N-part index.)  For example, the following is a range
     condition for a three-part 'HASH' index:

          KEY_PART1 = 1 AND KEY_PART2 IS NULL AND KEY_PART3 = 'foo'

     For the definition of what is considered to be a constant, see
     *note range-access-single-part::.

   * For a 'BTREE' index, an interval might be usable for conditions
     combined with 'AND', where each condition compares a key part with
     a constant value using '=', '<=>', 'IS NULL', '>', '<', '>=', '<=',
     '!=', '<>', 'BETWEEN', or 'LIKE 'PATTERN'' (where ''PATTERN'' does
     not start with a wildcard).  An interval can be used as long as it
     is possible to determine a single key tuple containing all rows
     that match the condition (or two intervals if '<>' or '!=' is
     used).

     The optimizer attempts to use additional key parts to determine the
     interval as long as the comparison operator is '=', '<=>', or 'IS
     NULL'.  If the operator is '>', '<', '>=', '<=', '!=', '<>',
     'BETWEEN', or 'LIKE', the optimizer uses it but considers no more
     key parts.  For the following expression, the optimizer uses '='
     from the first comparison.  It also uses '>=' from the second
     comparison but considers no further key parts and does not use the
     third comparison for interval construction:

          KEY_PART1 = 'foo' AND KEY_PART2 >= 10 AND KEY_PART3 > 10

     The single interval is:

          ('foo',10,-inf) < (KEY_PART1,KEY_PART2,KEY_PART3) < ('foo',+inf,+inf)

     It is possible that the created interval contains more rows than
     the initial condition.  For example, the preceding interval
     includes the value '('foo', 11, 0)', which does not satisfy the
     original condition.

   * If conditions that cover sets of rows contained within intervals
     are combined with 'OR', they form a condition that covers a set of
     rows contained within the union of their intervals.  If the
     conditions are combined with 'AND', they form a condition that
     covers a set of rows contained within the intersection of their
     intervals.  For example, for this condition on a two-part index:

          (KEY_PART1 = 1 AND KEY_PART2 < 2) OR (KEY_PART1 > 5)

     The intervals are:

          (1,-inf) < (KEY_PART1,KEY_PART2) < (1,2)
          (5,-inf) < (KEY_PART1,KEY_PART2)

     In this example, the interval on the first line uses one key part
     for the left bound and two key parts for the right bound.  The
     interval on the second line uses only one key part.  The 'key_len'
     column in the *note 'EXPLAIN': explain. output indicates the
     maximum length of the key prefix used.

     In some cases, 'key_len' may indicate that a key part was used, but
     that might be not what you would expect.  Suppose that KEY_PART1
     and KEY_PART2 can be 'NULL'.  Then the 'key_len' column displays
     two key part lengths for the following condition:

          KEY_PART1 >= 1 AND KEY_PART2 < 2

     But, in fact, the condition is converted to this:

          KEY_PART1 >= 1 AND KEY_PART2 IS NOT NULL

For a description of how optimizations are performed to combine or
eliminate intervals for range conditions on a single-part index, see
*note range-access-single-part::.  Analogous steps are performed for
range conditions on multiple-part indexes.


File: manual.info.tmp,  Node: index-merge-optimization,  Next: condition-pushdown-optimization,  Prev: range-optimization,  Up: select-optimization

8.2.1.3 Index Merge Optimization
................................

The _Index Merge_ access method retrieves rows with multiple 'range'
scans and merges their results into one.  This access method merges
index scans from a single table only, not scans across multiple tables.
The merge can produce unions, intersections, or unions-of-intersections
of its underlying scans.

Example queries for which Index Merge may be used:

     SELECT * FROM TBL_NAME WHERE KEY1 = 10 OR KEY2 = 20;

     SELECT * FROM TBL_NAME
       WHERE (KEY1 = 10 OR KEY2 = 20) AND NON_KEY = 30;

     SELECT * FROM t1, t2
       WHERE (t1.KEY1 IN (1,2) OR t1.KEY2 LIKE 'VALUE%')
       AND t2.KEY1 = t1.SOME_COL;

     SELECT * FROM t1, t2
       WHERE t1.KEY1 = 1
       AND (t2.KEY1 = t1.SOME_COL OR t2.KEY2 = t1.SOME_COL2);

*Note*:

The Index Merge optimization algorithm has the following known
limitations:

   * If your query has a complex 'WHERE' clause with deep 'AND'/'OR'
     nesting and MySQL does not choose the optimal plan, try
     distributing terms using the following identity transformations:

          (X AND Y) OR Z => (X OR Z) AND (Y OR Z)
          (X OR Y) AND Z => (X AND Z) OR (Y AND Z)

   * Index Merge is not applicable to full-text indexes.

   * If a range scan is possible on some key, the optimizer will not
     consider using Index Merge Union or Index Merge Sort-Union
     algorithms.  For example, consider this query:

          SELECT * FROM t1 WHERE (goodkey1 < 10 OR goodkey2 < 20) AND badkey < 30;

     For this query, two plans are possible:

        * An Index Merge scan using the '(goodkey1 < 10 OR goodkey2 <
          20)' condition.

        * A range scan using the 'badkey < 30' condition.

     However, the optimizer considers only the second plan.

In *note 'EXPLAIN': explain. output, the Index Merge method appears as
'index_merge' in the 'type' column.  In this case, the 'key' column
contains a list of indexes used, and 'key_len' contains a list of the
longest key parts for those indexes.

The Index Merge access method has several algorithms, which are
displayed in the 'Extra' field of *note 'EXPLAIN': explain. output:

   * 'Using intersect(...)'

   * 'Using union(...)'

   * 'Using sort_union(...)'

The following sections describe these algorithms in greater detail.  The
optimizer chooses between different possible Index Merge algorithms and
other access methods based on cost estimates of the various available
options.

Use of Index Merge is subject to the value of the 'index_merge',
'index_merge_intersection', 'index_merge_union', and
'index_merge_sort_union' flags of the 'optimizer_switch' system
variable.  See *note switchable-optimizations::.  By default, all those
flags are 'on'.  To enable only certain algorithms, set 'index_merge' to
'off', and enable only such of the others as should be permitted.

   * *note index-merge-intersection::

   * *note index-merge-union::

   * *note index-merge-sort-union::

*Index Merge Intersection Access Algorithm*

This access algorithm is applicable when a 'WHERE' clause is converted
to several range conditions on different keys combined with 'AND', and
each condition is one of the following:

   * An N-part expression of this form, where the index has exactly N
     parts (that is, all index parts are covered):

          KEY_PART1 = CONST1 AND KEY_PART2 = CONST2 ... AND KEY_PARTN = CONSTN

   * Any range condition over the primary key of an 'InnoDB' table.

Examples:

     SELECT * FROM INNODB_TABLE
       WHERE PRIMARY_KEY < 10 AND KEY_COL1 = 20;

     SELECT * FROM TBL_NAME
       WHERE KEY1_PART1 = 1 AND KEY1_PART2 = 2 AND KEY2 = 2;

The Index Merge intersection algorithm performs simultaneous scans on
all used indexes and produces the intersection of row sequences that it
receives from the merged index scans.

If all columns used in the query are covered by the used indexes, full
table rows are not retrieved (*note 'EXPLAIN': explain. output contains
'Using index' in 'Extra' field in this case).  Here is an example of
such a query:

     SELECT COUNT(*) FROM t1 WHERE key1 = 1 AND key2 = 1;

If the used indexes do not cover all columns used in the query, full
rows are retrieved only when the range conditions for all used keys are
satisfied.

If one of the merged conditions is a condition over the primary key of
an 'InnoDB' table, it is not used for row retrieval, but is used to
filter out rows retrieved using other conditions.

*Index Merge Union Access Algorithm*

The criteria for this algorithm are similar to those for the Index Merge
intersection algorithm.  The algorithm is applicable when the table's
'WHERE' clause is converted to several range conditions on different
keys combined with 'OR', and each condition is one of the following:

   * An N-part expression of this form, where the index has exactly N
     parts (that is, all index parts are covered):

          KEY_PART1 = CONST1 AND KEY_PART2 = CONST2 ... AND KEY_PARTN = CONSTN

   * Any range condition over a primary key of an 'InnoDB' table.

   * A condition for which the Index Merge intersection algorithm is
     applicable.

Examples:

     SELECT * FROM t1
       WHERE KEY1 = 1 OR KEY2 = 2 OR KEY3 = 3;

     SELECT * FROM INNODB_TABLE
       WHERE (KEY1 = 1 AND KEY2 = 2)
          OR (KEY3 = 'foo' AND KEY4 = 'bar') AND KEY5 = 5;

*Index Merge Sort-Union Access Algorithm*

This access algorithm is applicable when the 'WHERE' clause is converted
to several range conditions combined by 'OR', but the Index Merge union
algorithm is not applicable.

Examples:

     SELECT * FROM TBL_NAME
       WHERE KEY_COL1 < 10 OR KEY_COL2 < 20;

     SELECT * FROM TBL_NAME
       WHERE (KEY_COL1 > 10 OR KEY_COL2 = 20) AND NONKEY_COL = 30;

The difference between the sort-union algorithm and the union algorithm
is that the sort-union algorithm must first fetch row IDs for all rows
and sort them before returning any rows.


File: manual.info.tmp,  Node: condition-pushdown-optimization,  Next: nested-loop-joins,  Prev: index-merge-optimization,  Up: select-optimization

8.2.1.4 Engine Condition Pushdown Optimization
..............................................

This optimization improves the efficiency of direct comparisons between
a nonindexed column and a constant.  In such cases, the condition is
'pushed down' to the storage engine for evaluation.  This optimization
can be used only by the *note 'NDB': mysql-cluster. storage engine.

For NDB Cluster, this optimization can eliminate the need to send
nonmatching rows over the network between the cluster's data nodes and
the MySQL server that issued the query, and can speed up queries where
it is used by a factor of 5 to 10 times over cases where condition
pushdown could be but is not used.

Suppose that an NDB Cluster table is defined as follows:

     CREATE TABLE t1 (
         a INT,
         b INT,
         KEY(a)
     ) ENGINE=NDB;

Condition pushdown can be used with queries such as the one shown here,
which includes a comparison between a nonindexed column and a constant:

     SELECT a, b FROM t1 WHERE b = 10;

The use of condition pushdown can be seen in the output of *note
'EXPLAIN': explain.:

     mysql> EXPLAIN SELECT a,b FROM t1 WHERE b = 10\G
     *************************** 1. row ***************************
                id: 1
       select_type: SIMPLE
             table: t1
              type: ALL
     possible_keys: NULL
               key: NULL
           key_len: NULL
               ref: NULL
              rows: 10
             Extra: Using where with pushed condition

However, condition pushdown _cannot_ be used with either of these two
queries:

     SELECT a,b FROM t1 WHERE a = 10;
     SELECT a,b FROM t1 WHERE b + 1 = 10;

Condition pushdown is not applicable to the first query because an index
exists on column 'a'.  (An index access method would be more efficient
and so would be chosen in preference to condition pushdown.)  Condition
pushdown cannot be employed for the second query because the comparison
involving the nonindexed column 'b' is indirect.  (However, condition
pushdown could be applied if you were to reduce 'b + 1 = 10' to 'b = 9'
in the 'WHERE' clause.)

Condition pushdown may also be employed when an indexed column is
compared with a constant using a '>' or '<' operator:

     mysql> EXPLAIN SELECT a, b FROM t1 WHERE a < 2\G
     *************************** 1. row ***************************
                id: 1
       select_type: SIMPLE
             table: t1
              type: range
     possible_keys: a
               key: a
           key_len: 5
               ref: NULL
              rows: 2
             Extra: Using where with pushed condition

Other supported comparisons for condition pushdown include the
following:

   * 'COLUMN [NOT] LIKE PATTERN'

     PATTERN must be a string literal containing the pattern to be
     matched; for syntax, see *note string-comparison-functions::.

   * 'COLUMN IS [NOT] NULL'

   * 'COLUMN IN (VALUE_LIST)'

     Each item in the VALUE_LIST must be a constant, literal value.

   * 'COLUMN BETWEEN CONSTANT1 AND CONSTANT2'

     CONSTANT1 and CONSTANT2 must each be a constant, literal value.

In all of the cases in the preceding list, it is possible for the
condition to be converted into the form of one or more direct
comparisons between a column and a constant.

Engine condition pushdown is enabled by default.  To disable it at
server startup, set the 'optimizer_switch' system variable.  For
example, in a 'my.cnf' file, use these lines:

     [mysqld]
     optimizer_switch=engine_condition_pushdown=off

At runtime, disable condition pushdown like this:

     SET optimizer_switch='engine_condition_pushdown=off';

Limitations

Condition pushdown is subject to the following limitations:

   * Condition pushdown is supported only by the *note 'NDB':
     mysql-cluster. storage engine.

   * Columns may be compared with constants only; however, this includes
     expressions which evaluate to constant values.

   * Columns used in comparisons cannot be of any of the *note 'BLOB':
     blob. or *note 'TEXT': blob. types.  This exclusion extends to
     *note 'BIT': bit-type. and *note 'ENUM': enum. columns as well.

   * A string value to be compared with a column must use the same
     collation as the column.

   * Joins are not directly supported; conditions involving multiple
     tables are pushed separately where possible.  Use extended *note
     'EXPLAIN': explain. output to determine which conditions are
     actually pushed down.  See *note explain-extended::.


File: manual.info.tmp,  Node: nested-loop-joins,  Next: nested-join-optimization,  Prev: condition-pushdown-optimization,  Up: select-optimization

8.2.1.5 Nested-Loop Join Algorithms
...................................

MySQL executes joins between tables using a nested-loop algorithm or
variations on it.

   * *note nested-loop-join-algorithm::

   * *note block-nested-loop-join-algorithm::

*Nested-Loop Join Algorithm*

A simple nested-loop join (NLJ) algorithm reads rows from the first
table in a loop one at a time, passing each row to a nested loop that
processes the next table in the join.  This process is repeated as many
times as there remain tables to be joined.

Assume that a join between three tables 't1', 't2', and 't3' is to be
executed using the following join types:

     Table   Join Type
     t1      range
     t2      ref
     t3      ALL

If a simple NLJ algorithm is used, the join is processed like this:

     for each row in t1 matching range {
       for each row in t2 matching reference key {
         for each row in t3 {
           if row satisfies join conditions, send to client
         }
       }
     }

Because the NLJ algorithm passes rows one at a time from outer loops to
inner loops, it typically reads tables processed in the inner loops many
times.

*Block Nested-Loop Join Algorithm*

A Block Nested-Loop (BNL) join algorithm uses buffering of rows read in
outer loops to reduce the number of times that tables in inner loops
must be read.  For example, if 10 rows are read into a buffer and the
buffer is passed to the next inner loop, each row read in the inner loop
can be compared against all 10 rows in the buffer.  This reduces by an
order of magnitude the number of times the inner table must be read.

MySQL join buffering has these characteristics:

   * Join buffering can be used when the join is of type 'ALL' or
     'index' (in other words, when no possible keys can be used, and a
     full scan is done, of either the data or index rows, respectively),
     or 'range'.

   * A join buffer is never allocated for the first nonconstant table,
     even if it would be of type 'ALL' or 'index'.

   * Only columns of interest to a join are stored in its join buffer,
     not whole rows.

   * The 'join_buffer_size' system variable determines the size of each
     join buffer used to process a query.

   * One buffer is allocated for each join that can be buffered, so a
     given query might be processed using multiple join buffers.

   * A join buffer is allocated prior to executing the join and freed
     after the query is done.

For the example join described previously for the NLJ algorithm (without
buffering), the join is done as follows using join buffering:

     for each row in t1 matching range {
       for each row in t2 matching reference key {
         store used columns from t1, t2 in join buffer
         if buffer is full {
           for each row in t3 {
             for each t1, t2 combination in join buffer {
               if row satisfies join conditions, send to client
             }
           }
           empty join buffer
         }
       }
     }

     if buffer is not empty {
       for each row in t3 {
         for each t1, t2 combination in join buffer {
           if row satisfies join conditions, send to client
         }
       }
     }

If S is the size of each stored 't1', 't2' combination in the join
buffer and C is the number of combinations in the buffer, the number of
times table 't3' is scanned is:

     (S * C)/join_buffer_size + 1

The number of 't3' scans decreases as the value of 'join_buffer_size'
increases, up to the point when 'join_buffer_size' is large enough to
hold all previous row combinations.  At that point, no speed is gained
by making it larger.


File: manual.info.tmp,  Node: nested-join-optimization,  Next: outer-join-optimization,  Prev: nested-loop-joins,  Up: select-optimization

8.2.1.6 Nested Join Optimization
................................

The syntax for expressing joins permits nested joins.  The following
discussion refers to the join syntax described in *note join::.

The syntax of TABLE_FACTOR is extended in comparison with the SQL
Standard.  The latter accepts only TABLE_REFERENCE, not a list of them
inside a pair of parentheses.  This is a conservative extension if we
consider each comma in a list of TABLE_REFERENCE items as equivalent to
an inner join.  For example:

     SELECT * FROM t1 LEFT JOIN (t2, t3, t4)
                      ON (t2.a=t1.a AND t3.b=t1.b AND t4.c=t1.c)

Is equivalent to:

     SELECT * FROM t1 LEFT JOIN (t2 CROSS JOIN t3 CROSS JOIN t4)
                      ON (t2.a=t1.a AND t3.b=t1.b AND t4.c=t1.c)

In MySQL, 'CROSS JOIN' is syntactically equivalent to 'INNER JOIN'; they
can replace each other.  In standard SQL, they are not equivalent.
'INNER JOIN' is used with an 'ON' clause; 'CROSS JOIN' is used
otherwise.

In general, parentheses can be ignored in join expressions containing
only inner join operations.  Consider this join expression:

     t1 LEFT JOIN (t2 LEFT JOIN t3 ON t2.b=t3.b OR t2.b IS NULL)
        ON t1.a=t2.a

After removing parentheses and grouping operations to the left, that
join expression transforms into this expression:

     (t1 LEFT JOIN t2 ON t1.a=t2.a) LEFT JOIN t3
         ON t2.b=t3.b OR t2.b IS NULL

Yet, the two expressions are not equivalent.  To see this, suppose that
the tables 't1', 't2', and 't3' have the following state:

   * Table 't1' contains rows '(1)', '(2)'

   * Table 't2' contains row '(1,101)'

   * Table 't3' contains row '(101)'

In this case, the first expression returns a result set including the
rows '(1,1,101,101)', '(2,NULL,NULL,NULL)', whereas the second
expression returns the rows '(1,1,101,101)', '(2,NULL,NULL,101)':

     mysql> SELECT *
            FROM t1
                 LEFT JOIN
                 (t2 LEFT JOIN t3 ON t2.b=t3.b OR t2.b IS NULL)
                 ON t1.a=t2.a;
     +------+------+------+------+
     | a    | a    | b    | b    |
     +------+------+------+------+
     |    1 |    1 |  101 |  101 |
     |    2 | NULL | NULL | NULL |
     +------+------+------+------+

     mysql> SELECT *
            FROM (t1 LEFT JOIN t2 ON t1.a=t2.a)
                 LEFT JOIN t3
                 ON t2.b=t3.b OR t2.b IS NULL;
     +------+------+------+------+
     | a    | a    | b    | b    |
     +------+------+------+------+
     |    1 |    1 |  101 |  101 |
     |    2 | NULL | NULL |  101 |
     +------+------+------+------+

In the following example, an outer join operation is used together with
an inner join operation:

     t1 LEFT JOIN (t2, t3) ON t1.a=t2.a

That expression cannot be transformed into the following expression:

     t1 LEFT JOIN t2 ON t1.a=t2.a, t3

For the given table states, the two expressions return different sets of
rows:

     mysql> SELECT *
            FROM t1 LEFT JOIN (t2, t3) ON t1.a=t2.a;
     +------+------+------+------+
     | a    | a    | b    | b    |
     +------+------+------+------+
     |    1 |    1 |  101 |  101 |
     |    2 | NULL | NULL | NULL |
     +------+------+------+------+

     mysql> SELECT *
            FROM t1 LEFT JOIN t2 ON t1.a=t2.a, t3;
     +------+------+------+------+
     | a    | a    | b    | b    |
     +------+------+------+------+
     |    1 |    1 |  101 |  101 |
     |    2 | NULL | NULL |  101 |
     +------+------+------+------+

Therefore, if we omit parentheses in a join expression with outer join
operators, we might change the result set for the original expression.

More exactly, we cannot ignore parentheses in the right operand of the
left outer join operation and in the left operand of a right join
operation.  In other words, we cannot ignore parentheses for the inner
table expressions of outer join operations.  Parentheses for the other
operand (operand for the outer table) can be ignored.

The following expression:

     (t1,t2) LEFT JOIN t3 ON P(t2.b,t3.b)

Is equivalent to this expression for any tables 't1,t2,t3' and any
condition 'P' over attributes 't2.b' and 't3.b':

     t1, t2 LEFT JOIN t3 ON P(t2.b,t3.b)

Whenever the order of execution of join operations in a join expression
(JOINED_TABLE) is not from left to right, we talk about nested joins.
Consider the following queries:

     SELECT * FROM t1 LEFT JOIN (t2 LEFT JOIN t3 ON t2.b=t3.b) ON t1.a=t2.a
       WHERE t1.a > 1

     SELECT * FROM t1 LEFT JOIN (t2, t3) ON t1.a=t2.a
       WHERE (t2.b=t3.b OR t2.b IS NULL) AND t1.a > 1

Those queries are considered to contain these nested joins:

     t2 LEFT JOIN t3 ON t2.b=t3.b
     t2, t3

In the first query, the nested join is formed with a left join
operation.  In the second query, it is formed with an inner join
operation.

In the first query, the parentheses can be omitted: The grammatical
structure of the join expression will dictate the same order of
execution for join operations.  For the second query, the parentheses
cannot be omitted, although the join expression here can be interpreted
unambiguously without them.  In our extended syntax, the parentheses in
'(t2, t3)' of the second query are required, although theoretically the
query could be parsed without them: We still would have unambiguous
syntactical structure for the query because 'LEFT JOIN' and 'ON' play
the role of the left and right delimiters for the expression '(t2,t3)'.

The preceding examples demonstrate these points:

   * For join expressions involving only inner joins (and not outer
     joins), parentheses can be removed and joins evaluated left to
     right.  In fact, tables can be evaluated in any order.

   * The same is not true, in general, for outer joins or for outer
     joins mixed with inner joins.  Removal of parentheses may change
     the result.

Queries with nested outer joins are executed in the same pipeline manner
as queries with inner joins.  More exactly, a variation of the
nested-loop join algorithm is exploited.  Recall the algorithm by which
the nested-loop join executes a query (see *note nested-loop-joins::).
Suppose that a join query over 3 tables 'T1,T2,T3' has this form:

     SELECT * FROM T1 INNER JOIN T2 ON P1(T1,T2)
                      INNER JOIN T3 ON P2(T2,T3)
       WHERE P(T1,T2,T3)

Here, 'P1(T1,T2)' and 'P2(T3,T3)' are some join conditions (on
expressions), whereas 'P(T1,T2,T3)' is a condition over columns of
tables 'T1,T2,T3'.

The nested-loop join algorithm would execute this query in the following
manner:

     FOR each row t1 in T1 {
       FOR each row t2 in T2 such that P1(t1,t2) {
         FOR each row t3 in T3 such that P2(t2,t3) {
           IF P(t1,t2,t3) {
              t:=t1||t2||t3; OUTPUT t;
           }
         }
       }
     }

The notation 't1||t2||t3' indicates a row constructed by concatenating
the columns of rows 't1', 't2', and 't3'.  In some of the following
examples, 'NULL' where a table name appears means a row in which 'NULL'
is used for each column of that table.  For example, 't1||t2||NULL'
indicates a row constructed by concatenating the columns of rows 't1'
and 't2', and 'NULL' for each column of 't3'.  Such a row is said to be
'NULL'-complemented.

Now consider a query with nested outer joins:

     SELECT * FROM T1 LEFT JOIN
                   (T2 LEFT JOIN T3 ON P2(T2,T3))
                   ON P1(T1,T2)
       WHERE P(T1,T2,T3)

For this query, modify the nested-loop pattern to obtain:

     FOR each row t1 in T1 {
       BOOL f1:=FALSE;
       FOR each row t2 in T2 such that P1(t1,t2) {
         BOOL f2:=FALSE;
         FOR each row t3 in T3 such that P2(t2,t3) {
           IF P(t1,t2,t3) {
             t:=t1||t2||t3; OUTPUT t;
           }
           f2=TRUE;
           f1=TRUE;
         }
         IF (!f2) {
           IF P(t1,t2,NULL) {
             t:=t1||t2||NULL; OUTPUT t;
           }
           f1=TRUE;
         }
       }
       IF (!f1) {
         IF P(t1,NULL,NULL) {
           t:=t1||NULL||NULL; OUTPUT t;
         }
       }
     }

In general, for any nested loop for the first inner table in an outer
join operation, a flag is introduced that is turned off before the loop
and is checked after the loop.  The flag is turned on when for the
current row from the outer table a match from the table representing the
inner operand is found.  If at the end of the loop cycle the flag is
still off, no match has been found for the current row of the outer
table.  In this case, the row is complemented by 'NULL' values for the
columns of the inner tables.  The result row is passed to the final
check for the output or into the next nested loop, but only if the row
satisfies the join condition of all embedded outer joins.

In the example, the outer join table expressed by the following
expression is embedded:

     (T2 LEFT JOIN T3 ON P2(T2,T3))

For the query with inner joins, the optimizer could choose a different
order of nested loops, such as this one:

     FOR each row t3 in T3 {
       FOR each row t2 in T2 such that P2(t2,t3) {
         FOR each row t1 in T1 such that P1(t1,t2) {
           IF P(t1,t2,t3) {
              t:=t1||t2||t3; OUTPUT t;
           }
         }
       }
     }

For queries with outer joins, the optimizer can choose only such an
order where loops for outer tables precede loops for inner tables.
Thus, for our query with outer joins, only one nesting order is
possible.  For the following query, the optimizer evaluates two
different nestings.  In both nestings, 'T1' must be processed in the
outer loop because it is used in an outer join.  'T2' and 'T3' are used
in an inner join, so that join must be processed in the inner loop.
However, because the join is an inner join, 'T2' and 'T3' can be
processed in either order.

     SELECT * T1 LEFT JOIN (T2,T3) ON P1(T1,T2) AND P2(T1,T3)
       WHERE P(T1,T2,T3)

One nesting evaluates 'T2', then 'T3':

     FOR each row t1 in T1 {
       BOOL f1:=FALSE;
       FOR each row t2 in T2 such that P1(t1,t2) {
         FOR each row t3 in T3 such that P2(t1,t3) {
           IF P(t1,t2,t3) {
             t:=t1||t2||t3; OUTPUT t;
           }
           f1:=TRUE
         }
       }
       IF (!f1) {
         IF P(t1,NULL,NULL) {
           t:=t1||NULL||NULL; OUTPUT t;
         }
       }
     }

The other nesting evaluates 'T3', then 'T2':

     FOR each row t1 in T1 {
       BOOL f1:=FALSE;
       FOR each row t3 in T3 such that P2(t1,t3) {
         FOR each row t2 in T2 such that P1(t1,t2) {
           IF P(t1,t2,t3) {
             t:=t1||t2||t3; OUTPUT t;
           }
           f1:=TRUE
         }
       }
       IF (!f1) {
         IF P(t1,NULL,NULL) {
           t:=t1||NULL||NULL; OUTPUT t;
         }
       }
     }

When discussing the nested-loop algorithm for inner joins, we omitted
some details whose impact on the performance of query execution may be
huge.  We did not mention so-called 'pushed-down' conditions.  Suppose
that our 'WHERE' condition 'P(T1,T2,T3)' can be represented by a
conjunctive formula:

     P(T1,T2,T2) = C1(T1) AND C2(T2) AND C3(T3).

In this case, MySQL actually uses the following nested-loop algorithm
for the execution of the query with inner joins:

     FOR each row t1 in T1 such that C1(t1) {
       FOR each row t2 in T2 such that P1(t1,t2) AND C2(t2)  {
         FOR each row t3 in T3 such that P2(t2,t3) AND C3(t3) {
           IF P(t1,t2,t3) {
              t:=t1||t2||t3; OUTPUT t;
           }
         }
       }
     }

You see that each of the conjuncts 'C1(T1)', 'C2(T2)', 'C3(T3)' are
pushed out of the most inner loop to the most outer loop where it can be
evaluated.  If 'C1(T1)' is a very restrictive condition, this condition
pushdown may greatly reduce the number of rows from table 'T1' passed to
the inner loops.  As a result, the execution time for the query may
improve immensely.

For a query with outer joins, the 'WHERE' condition is to be checked
only after it has been found that the current row from the outer table
has a match in the inner tables.  Thus, the optimization of pushing
conditions out of the inner nested loops cannot be applied directly to
queries with outer joins.  Here we must introduce conditional
pushed-down predicates guarded by the flags that are turned on when a
match has been encountered.

Recall this example with outer joins:

     P(T1,T2,T3)=C1(T1) AND C(T2) AND C3(T3)

For that example, the nested-loop algorithm using guarded pushed-down
conditions looks like this:

     FOR each row t1 in T1 such that C1(t1) {
       BOOL f1:=FALSE;
       FOR each row t2 in T2
           such that P1(t1,t2) AND (f1?C2(t2):TRUE) {
         BOOL f2:=FALSE;
         FOR each row t3 in T3
             such that P2(t2,t3) AND (f1&&f2?C3(t3):TRUE) {
           IF (f1&&f2?TRUE:(C2(t2) AND C3(t3))) {
             t:=t1||t2||t3; OUTPUT t;
           }
           f2=TRUE;
           f1=TRUE;
         }
         IF (!f2) {
           IF (f1?TRUE:C2(t2) && P(t1,t2,NULL)) {
             t:=t1||t2||NULL; OUTPUT t;
           }
           f1=TRUE;
         }
       }
       IF (!f1 && P(t1,NULL,NULL)) {
           t:=t1||NULL||NULL; OUTPUT t;
       }
     }

In general, pushed-down predicates can be extracted from join conditions
such as 'P1(T1,T2)' and 'P(T2,T3)'.  In this case, a pushed-down
predicate is guarded also by a flag that prevents checking the predicate
for the 'NULL'-complemented row generated by the corresponding outer
join operation.

Access by key from one inner table to another in the same nested join is
prohibited if it is induced by a predicate from the 'WHERE' condition.


File: manual.info.tmp,  Node: outer-join-optimization,  Next: outer-join-simplification,  Prev: nested-join-optimization,  Up: select-optimization

8.2.1.7 Outer Join Optimization
...............................

Outer joins include 'LEFT JOIN' and 'RIGHT JOIN'.

MySQL implements an 'A LEFT JOIN B JOIN_SPECIFICATION' as follows:

   * Table B is set to depend on table A and all tables on which A
     depends.

   * Table A is set to depend on all tables (except B) that are used in
     the 'LEFT JOIN' condition.

   * The 'LEFT JOIN' condition is used to decide how to retrieve rows
     from table B.  (In other words, any condition in the 'WHERE' clause
     is not used.)

   * All standard join optimizations are performed, with the exception
     that a table is always read after all tables on which it depends.
     If there is a circular dependency, an error occurs.

   * All standard 'WHERE' optimizations are performed.

   * If there is a row in A that matches the 'WHERE' clause, but there
     is no row in B that matches the 'ON' condition, an extra B row is
     generated with all columns set to 'NULL'.

   * If you use 'LEFT JOIN' to find rows that do not exist in some table
     and you have the following test: 'COL_NAME IS NULL' in the 'WHERE'
     part, where COL_NAME is a column that is declared as 'NOT NULL',
     MySQL stops searching for more rows (for a particular key
     combination) after it has found one row that matches the 'LEFT
     JOIN' condition.

The 'RIGHT JOIN' implementation is analogous to that of 'LEFT JOIN' with
the table roles reversed.  Right joins are converted to equivalent left
joins, as described in *note outer-join-simplification::.

For a 'LEFT JOIN', if the 'WHERE' condition is always false for the
generated 'NULL' row, the 'LEFT JOIN' is changed to an inner join.  For
example, the 'WHERE' clause would be false in the following query if
't2.column1' were 'NULL':

     SELECT * FROM t1 LEFT JOIN t2 ON (column1) WHERE t2.column2=5;

Therefore, it is safe to convert the query to an inner join:

     SELECT * FROM t1, t2 WHERE t2.column2=5 AND t1.column1=t2.column1;

Now the optimizer can use table 't2' before table 't1' if doing so would
result in a better query plan.  To provide a hint about the table join
order, use 'STRAIGHT_JOIN'; see *note select::.


File: manual.info.tmp,  Node: outer-join-simplification,  Next: is-null-optimization,  Prev: outer-join-optimization,  Up: select-optimization

8.2.1.8 Outer Join Simplification
.................................

Table expressions in the 'FROM' clause of a query are simplified in many
cases.

At the parser stage, queries with right outer join operations are
converted to equivalent queries containing only left join operations.
In the general case, the conversion is performed such that this right
join:

     (T1, ...) RIGHT JOIN (T2, ...) ON P(T1, ..., T2, ...)

Becomes this equivalent left join:

     (T2, ...) LEFT JOIN (T1, ...) ON P(T1, ..., T2, ...)

All inner join expressions of the form 'T1 INNER JOIN T2 ON P(T1,T2)'
are replaced by the list 'T1,T2', 'P(T1,T2)' being joined as a conjunct
to the 'WHERE' condition (or to the join condition of the embedding
join, if there is any).

When the optimizer evaluates plans for outer join operations, it takes
into consideration only plans where, for each such operation, the outer
tables are accessed before the inner tables.  The optimizer choices are
limited because only such plans enable outer joins to be executed using
the nested-loop algorithm.

Consider a query of this form, where 'R(T2)' greatly narrows the number
of matching rows from table 'T2':

     SELECT * T1 LEFT JOIN T2 ON P1(T1,T2)
       WHERE P(T1,T2) AND R(T2)

If the query is executed as written, the optimizer has no choice but to
access the less-restricted table 'T1' before the more-restricted table
'T2', which may produce a very inefficient execution plan.

Instead, MySQL converts the query to a query with no outer join
operation if the 'WHERE' condition is null-rejected.  (That is, it
converts the outer join to an inner join.)  A condition is said to be
null-rejected for an outer join operation if it evaluates to 'FALSE' or
'UNKNOWN' for any 'NULL'-complemented row generated for the operation.

Thus, for this outer join:

     T1 LEFT JOIN T2 ON T1.A=T2.A

Conditions such as these are null-rejected because they cannot be true
for any 'NULL'-complemented row (with 'T2' columns set to 'NULL'):

     T2.B IS NOT NULL
     T2.B > 3
     T2.C <= T1.C
     T2.B < 2 OR T2.C > 1

Conditions such as these are not null-rejected because they might be
true for a 'NULL'-complemented row:

     T2.B IS NULL
     T1.B < 3 OR T2.B IS NOT NULL
     T1.B < 3 OR T2.B > 3

The general rules for checking whether a condition is null-rejected for
an outer join operation are simple:

   * It is of the form 'A IS NOT NULL', where 'A' is an attribute of any
     of the inner tables

   * It is a predicate containing a reference to an inner table that
     evaluates to 'UNKNOWN' when one of its arguments is 'NULL'

   * It is a conjunction containing a null-rejected condition as a
     conjunct

   * It is a disjunction of null-rejected conditions

A condition can be null-rejected for one outer join operation in a query
and not null-rejected for another.  In this query, the 'WHERE' condition
is null-rejected for the second outer join operation but is not
null-rejected for the first one:

     SELECT * FROM T1 LEFT JOIN T2 ON T2.A=T1.A
                      LEFT JOIN T3 ON T3.B=T1.B
       WHERE T3.C > 0

If the 'WHERE' condition is null-rejected for an outer join operation in
a query, the outer join operation is replaced by an inner join
operation.

For example, in the preceding query, the second outer join is
null-rejected and can be replaced by an inner join:

     SELECT * FROM T1 LEFT JOIN T2 ON T2.A=T1.A
                      INNER JOIN T3 ON T3.B=T1.B
       WHERE T3.C > 0

For the original query, the optimizer evaluates only plans compatible
with the single table-access order 'T1,T2,T3'.  For the rewritten query,
it additionally considers the access order 'T3,T1,T2'.

A conversion of one outer join operation may trigger a conversion of
another.  Thus, the query:

     SELECT * FROM T1 LEFT JOIN T2 ON T2.A=T1.A
                      LEFT JOIN T3 ON T3.B=T2.B
       WHERE T3.C > 0

Is first converted to the query:

     SELECT * FROM T1 LEFT JOIN T2 ON T2.A=T1.A
                      INNER JOIN T3 ON T3.B=T2.B
       WHERE T3.C > 0

Which is equivalent to the query:

     SELECT * FROM (T1 LEFT JOIN T2 ON T2.A=T1.A), T3
       WHERE T3.C > 0 AND T3.B=T2.B

The remaining outer join operation can also be replaced by an inner join
because the condition 'T3.B=T2.B' is null-rejected.  This results in a
query with no outer joins at all:

     SELECT * FROM (T1 INNER JOIN T2 ON T2.A=T1.A), T3
       WHERE T3.C > 0 AND T3.B=T2.B

Sometimes the optimizer succeeds in replacing an embedded outer join
operation, but cannot convert the embedding outer join.  The following
query:

     SELECT * FROM T1 LEFT JOIN
                   (T2 LEFT JOIN T3 ON T3.B=T2.B)
                   ON T2.A=T1.A
       WHERE T3.C > 0

Is converted to:

     SELECT * FROM T1 LEFT JOIN
                   (T2 INNER JOIN T3 ON T3.B=T2.B)
                   ON T2.A=T1.A
       WHERE T3.C > 0

That can be rewritten only to the form still containing the embedding
outer join operation:

     SELECT * FROM T1 LEFT JOIN
                   (T2,T3)
                   ON (T2.A=T1.A AND T3.B=T2.B)
       WHERE T3.C > 0

Any attempt to convert an embedded outer join operation in a query must
take into account the join condition for the embedding outer join
together with the 'WHERE' condition.  In this query, the 'WHERE'
condition is not null-rejected for the embedded outer join, but the join
condition of the embedding outer join 'T2.A=T1.A AND T3.C=T1.C' is
null-rejected:

     SELECT * FROM T1 LEFT JOIN
                   (T2 LEFT JOIN T3 ON T3.B=T2.B)
                   ON T2.A=T1.A AND T3.C=T1.C
       WHERE T3.D > 0 OR T1.D > 0

Consequently, the query can be converted to:

     SELECT * FROM T1 LEFT JOIN
                   (T2, T3)
                   ON T2.A=T1.A AND T3.C=T1.C AND T3.B=T2.B
       WHERE T3.D > 0 OR T1.D > 0


File: manual.info.tmp,  Node: is-null-optimization,  Next: order-by-optimization,  Prev: outer-join-simplification,  Up: select-optimization

8.2.1.9 IS NULL Optimization
............................

MySQL can perform the same optimization on COL_NAME 'IS NULL' that it
can use for COL_NAME '=' CONSTANT_VALUE.  For example, MySQL can use
indexes and ranges to search for 'NULL' with 'IS NULL'.

Examples:

     SELECT * FROM TBL_NAME WHERE KEY_COL IS NULL;

     SELECT * FROM TBL_NAME WHERE KEY_COL <=> NULL;

     SELECT * FROM TBL_NAME
       WHERE KEY_COL=CONST1 OR KEY_COL=CONST2 OR KEY_COL IS NULL;

If a 'WHERE' clause includes a COL_NAME 'IS NULL' condition for a column
that is declared as 'NOT NULL', that expression is optimized away.  This
optimization does not occur in cases when the column might produce
'NULL' anyway (for example, if it comes from a table on the right side
of a 'LEFT JOIN').

MySQL can also optimize the combination 'COL_NAME = EXPR OR COL_NAME IS
NULL', a form that is common in resolved subqueries.  *note 'EXPLAIN':
explain. shows 'ref_or_null' when this optimization is used.

This optimization can handle one 'IS NULL' for any key part.

Some examples of queries that are optimized, assuming that there is an
index on columns 'a' and 'b' of table 't2':

     SELECT * FROM t1 WHERE t1.a=EXPR OR t1.a IS NULL;

     SELECT * FROM t1, t2 WHERE t1.a=t2.a OR t2.a IS NULL;

     SELECT * FROM t1, t2
       WHERE (t1.a=t2.a OR t2.a IS NULL) AND t2.b=t1.b;

     SELECT * FROM t1, t2
       WHERE t1.a=t2.a AND (t2.b=t1.b OR t2.b IS NULL);

     SELECT * FROM t1, t2
       WHERE (t1.a=t2.a AND t2.a IS NULL AND ...)
       OR (t1.a=t2.a AND t2.a IS NULL AND ...);

'ref_or_null' works by first doing a read on the reference key, and then
a separate search for rows with a 'NULL' key value.

The optimization can handle only one 'IS NULL' level.  In the following
query, MySQL uses key lookups only on the expression '(t1.a=t2.a AND
t2.a IS NULL)' and is not able to use the key part on 'b':

     SELECT * FROM t1, t2
       WHERE (t1.a=t2.a AND t2.a IS NULL)
       OR (t1.b=t2.b AND t2.b IS NULL);


File: manual.info.tmp,  Node: order-by-optimization,  Next: group-by-optimization,  Prev: is-null-optimization,  Up: select-optimization

8.2.1.10 ORDER BY Optimization
..............................

This section describes when MySQL can use an index to satisfy an 'ORDER
BY' clause, the 'filesort' operation used when an index cannot be used,
and execution plan information available from the optimizer about 'ORDER
BY'.

An 'ORDER BY' with and without 'LIMIT' may return rows in different
orders, as discussed in *note limit-optimization::.

   * *note order-by-index-use::

   * *note order-by-filesort::

   * *note order-by-optimizer-control::

   * *note order-by-diagnostic-information::

*Use of Indexes to Satisfy ORDER BY*

In some cases, MySQL may use an index to satisfy an 'ORDER BY' clause
and avoid the extra sorting involved in performing a 'filesort'
operation.

The index may also be used even if the 'ORDER BY' does not match the
index exactly, as long as all unused portions of the index and all extra
'ORDER BY' columns are constants in the 'WHERE' clause.  If the index
does not contain all columns accessed by the query, the index is used
only if index access is cheaper than other access methods.

Assuming that there is an index on '(KEY_PART1, KEY_PART2)', the
following queries may use the index to resolve the 'ORDER BY' part.
Whether the optimizer actually does so depends on whether reading the
index is more efficient than a table scan if columns not in the index
must also be read.

   * In this query, the index on '(KEY_PART1, KEY_PART2)' enables the
     optimizer to avoid sorting:

          SELECT * FROM t1
            ORDER BY KEY_PART1, KEY_PART2;

     However, the query uses 'SELECT *', which may select more columns
     than KEY_PART1 and KEY_PART2.  In that case, scanning an entire
     index and looking up table rows to find columns not in the index
     may be more expensive than scanning the table and sorting the
     results.  If so, the optimizer probably will not use the index.  If
     'SELECT *' selects only the index columns, the index will be used
     and sorting avoided.

     If 't1' is an 'InnoDB' table, the table primary key is implicitly
     part of the index, and the index can be used to resolve the 'ORDER
     BY' for this query:

          SELECT PK, KEY_PART1, KEY_PART2 FROM t1
            ORDER BY KEY_PART1, KEY_PART2;

   * In this query, KEY_PART1 is constant, so all rows accessed through
     the index are in KEY_PART2 order, and an index on '(KEY_PART1,
     KEY_PART2)' avoids sorting if the 'WHERE' clause is selective
     enough to make an index range scan cheaper than a table scan:

          SELECT * FROM t1
            WHERE KEY_PART1 = CONSTANT
            ORDER BY KEY_PART2;

   * In the next two queries, whether the index is used is similar to
     the same queries without 'DESC' shown previously:

          SELECT * FROM t1
            ORDER BY KEY_PART1 DESC, KEY_PART2 DESC;

          SELECT * FROM t1
            WHERE KEY_PART1 = CONSTANT
            ORDER BY KEY_PART2 DESC;

   * In the next two queries, KEY_PART1 is compared to a constant.  The
     index will be used if the 'WHERE' clause is selective enough to
     make an index range scan cheaper than a table scan:

          SELECT * FROM t1
            WHERE KEY_PART1 > CONSTANT
            ORDER BY KEY_PART1 ASC;

          SELECT * FROM t1
            WHERE KEY_PART1 < CONSTANT
            ORDER BY KEY_PART1 DESC;

   * In the next query, the 'ORDER BY' does not name KEY_PART1, but all
     rows selected have a constant KEY_PART1 value, so the index can
     still be used:

          SELECT * FROM t1
            WHERE KEY_PART1 = CONSTANT1 AND KEY_PART2 > CONSTANT2
            ORDER BY KEY_PART2;

In some cases, MySQL _cannot_ use indexes to resolve the 'ORDER BY',
although it may still use indexes to find the rows that match the
'WHERE' clause.  Examples:

   * The query uses 'ORDER BY' on different indexes:

          SELECT * FROM t1 ORDER BY KEY1, KEY2;

   * The query uses 'ORDER BY' on nonconsecutive parts of an index:

          SELECT * FROM t1 WHERE KEY2=CONSTANT ORDER BY KEY1_PART1, KEY1_PART3;

   * The query mixes 'ASC' and 'DESC':

          SELECT * FROM t1 ORDER BY KEY_PART1 DESC, KEY_PART2 ASC;

   * The index used to fetch the rows differs from the one used in the
     'ORDER BY':

          SELECT * FROM t1 WHERE KEY2=CONSTANT ORDER BY KEY1;

   * The query uses 'ORDER BY' with an expression that includes terms
     other than the index column name:

          SELECT * FROM t1 ORDER BY ABS(KEY);
          SELECT * FROM t1 ORDER BY -KEY;

   * The query joins many tables, and the columns in the 'ORDER BY' are
     not all from the first nonconstant table that is used to retrieve
     rows.  (This is the first table in the *note 'EXPLAIN': explain.
     output that does not have a 'const' join type.)

   * The query has different 'ORDER BY' and 'GROUP BY' expressions.

   * There is an index on only a prefix of a column named in the 'ORDER
     BY' clause.  In this case, the index cannot be used to fully
     resolve the sort order.  For example, if only the first 10 bytes of
     a *note 'CHAR(20)': char. column are indexed, the index cannot
     distinguish values past the 10th byte and a 'filesort' is needed.

   * The index does not store rows in order.  For example, this is true
     for a 'HASH' index in a 'MEMORY' table.

Availability of an index for sorting may be affected by the use of
column aliases.  Suppose that the column 't1.a' is indexed.  In this
statement, the name of the column in the select list is 'a'.  It refers
to 't1.a', as does the reference to 'a' in the 'ORDER BY', so the index
on 't1.a' can be used:

     SELECT a FROM t1 ORDER BY a;

In this statement, the name of the column in the select list is also
'a', but it is the alias name.  It refers to 'ABS(a)', as does the
reference to 'a' in the 'ORDER BY', so the index on 't1.a' cannot be
used:

     SELECT ABS(a) AS a FROM t1 ORDER BY a;

In the following statement, the 'ORDER BY' refers to a name that is not
the name of a column in the select list.  But there is a column in 't1'
named 'a', so the 'ORDER BY' refers to 't1.a' and the index on 't1.a'
can be used.  (The resulting sort order may be completely different from
the order for 'ABS(a)', of course.)

     SELECT ABS(a) AS b FROM t1 ORDER BY a;

By default, MySQL sorts 'GROUP BY COL1, COL2, ...' queries as if you
also included 'ORDER BY COL1, COL2, ...' in the query.  If you include
an explicit 'ORDER BY' clause that contains the same column list, MySQL
optimizes it away without any speed penalty, although the sorting still
occurs.

If a query includes 'GROUP BY' but you want to avoid the overhead of
sorting the result, you can suppress sorting by specifying 'ORDER BY
NULL'.  For example:

     INSERT INTO foo
     SELECT a, COUNT(*) FROM bar GROUP BY a ORDER BY NULL;

The optimizer may still choose to use sorting to implement grouping
operations.  'ORDER BY NULL' suppresses sorting of the result, not prior
sorting done by grouping operations to determine the result.

*Note*:

'GROUP BY' implicitly sorts by default (that is, in the absence of 'ASC'
or 'DESC' designators for 'GROUP BY' columns), but relying on implicit
'GROUP BY' sorting is deprecated.  To produce a given sort order, use
explicit 'ASC' or 'DESC' designators for 'GROUP BY' columns or provide
an 'ORDER BY' clause.  'GROUP BY' sorting is a MySQL extension that may
change in a future release; for example, to make it possible for the
optimizer to order groupings in whatever manner it deems most efficient
and to avoid the sorting overhead.

*Use of filesort to Satisfy ORDER BY*

If an index cannot be used to satisfy an 'ORDER BY' clause, MySQL
performs a 'filesort' operation that reads table rows and sorts them.  A
'filesort' constitutes an extra sorting phase in query execution.

To obtain memory for 'filesort' operations, the optimizer allocates a
fixed amount of 'sort_buffer_size' bytes up front.  Individual sessions
can change the session value of this variable as desired to avoid
excessive memory use, or to allocate more memory as necessary.

A 'filesort' operation uses temporary disk files as necessary if the
result set is too large to fit in memory.  Some types of queries are
particularly suited to completely in-memory 'filesort' operations.  For
example, the optimizer can use 'filesort' to efficiently handle in
memory, without temporary files, the 'ORDER BY' operation for queries
(and subqueries) of the following form:

     SELECT ... FROM SINGLE_TABLE ... ORDER BY NON_INDEX_COLUMN [DESC] LIMIT [M,]N;

Such queries are common in web applications that display only a few rows
from a larger result set.  Examples:

     SELECT col1, ... FROM t1 ... ORDER BY name LIMIT 10;
     SELECT col1, ... FROM t1 ... ORDER BY RAND() LIMIT 15;

*Influencing ORDER BY Optimization*

For slow 'ORDER BY' queries for which 'filesort' is not used, try
lowering the 'max_length_for_sort_data' system variable to a value that
is appropriate to trigger a 'filesort'.  (A symptom of setting the value
of this variable too high is a combination of high disk activity and low
CPU activity.)

To increase 'ORDER BY' speed, check whether you can get MySQL to use
indexes rather than an extra sorting phase.  If this is not possible,
try the following strategies:

   * Increase the 'sort_buffer_size' variable value.  Ideally, the value
     should be large enough for the entire result set to fit in the sort
     buffer (to avoid writes to disk and merge passes), but at minimum
     the value must be large enough to accommodate 15 tuples.  (Up to 15
     temporary disk files are merged and there must be room in memory
     for at least one tuple per file.)

     Take into account that the size of column values stored in the sort
     buffer is affected by the 'max_sort_length' system variable value.
     For example, if tuples store values of long string columns and you
     increase the value of 'max_sort_length', the size of sort buffer
     tuples increases as well and may require you to increase
     'sort_buffer_size'.  For column values calculated as a result of
     string expressions (such as those that invoke a string-valued
     function), the 'filesort' algorithm cannot tell the maximum length
     of expression values, so it must allocate 'max_sort_length' bytes
     for each tuple.

     To monitor the number of merge passes (to merge temporary files),
     check the 'Sort_merge_passes' status variable.

   * Increase the 'read_rnd_buffer_size' variable value so that more
     rows are read at a time.

   * Use less RAM per row by declaring columns only as large as they
     need to be to hold the values stored in them.  For example,
     'CHAR(16)' is better than 'CHAR(200)' if values never exceed 16
     characters.

   * Change the 'tmpdir' system variable to point to a dedicated file
     system with large amounts of free space.  The variable value can
     list several paths that are used in round-robin fashion; you can
     use this feature to spread the load across several directories.
     Separate the paths by colon characters (':') on Unix and semicolon
     characters (';') on Windows.  The paths should name directories in
     file systems located on different _physical_ disks, not different
     partitions on the same disk.

*ORDER BY Execution Plan Information Available*

With *note 'EXPLAIN': explain. (see *note using-explain::), you can
check whether MySQL can use indexes to resolve an 'ORDER BY' clause:

   * If the 'Extra' column of *note 'EXPLAIN': explain. output does not
     contain 'Using filesort', the index is used and a 'filesort' is not
     performed.

   * If the 'Extra' column of *note 'EXPLAIN': explain. output contains
     'Using filesort', the index is not used and a 'filesort' is
     performed.


File: manual.info.tmp,  Node: group-by-optimization,  Next: distinct-optimization,  Prev: order-by-optimization,  Up: select-optimization

8.2.1.11 GROUP BY Optimization
..............................

The most general way to satisfy a 'GROUP BY' clause is to scan the whole
table and create a new temporary table where all rows from each group
are consecutive, and then use this temporary table to discover groups
and apply aggregate functions (if any).  In some cases, MySQL is able to
do much better than that and avoid creation of temporary tables by using
index access.

The most important preconditions for using indexes for 'GROUP BY' are
that all 'GROUP BY' columns reference attributes from the same index,
and that the index stores its keys in order (as is true, for example,
for a 'BTREE' index, but not for a 'HASH' index).  Whether use of
temporary tables can be replaced by index access also depends on which
parts of an index are used in a query, the conditions specified for
these parts, and the selected aggregate functions.

There are two ways to execute a 'GROUP BY' query through index access,
as detailed in the following sections.  The first method applies the
grouping operation together with all range predicates (if any).  The
second method first performs a range scan, and then groups the resulting
tuples.

In MySQL, 'GROUP BY' is used for sorting, so the server may also apply
'ORDER BY' optimizations to grouping.  However, relying on implicit
'GROUP BY' sorting is deprecated.  See *note order-by-optimization::.

   * *note loose-index-scan::

   * *note tight-index-scan::

*Loose Index Scan*

The most efficient way to process 'GROUP BY' is when an index is used to
directly retrieve the grouping columns.  With this access method, MySQL
uses the property of some index types that the keys are ordered (for
example, 'BTREE').  This property enables use of lookup groups in an
index without having to consider all keys in the index that satisfy all
'WHERE' conditions.  This access method considers only a fraction of the
keys in an index, so it is called a _Loose Index Scan_.  When there is
no 'WHERE' clause, a Loose Index Scan reads as many keys as the number
of groups, which may be a much smaller number than that of all keys.  If
the 'WHERE' clause contains range predicates (see the discussion of the
'range' join type in *note using-explain::), a Loose Index Scan looks up
the first key of each group that satisfies the range conditions, and
again reads the smallest possible number of keys.  This is possible
under the following conditions:

   * The query is over a single table.

   * The 'GROUP BY' names only columns that form a leftmost prefix of
     the index and no other columns.  (If, instead of 'GROUP BY', the
     query has a 'DISTINCT' clause, all distinct attributes refer to
     columns that form a leftmost prefix of the index.)  For example, if
     a table 't1' has an index on '(c1,c2,c3)', Loose Index Scan is
     applicable if the query has 'GROUP BY c1, c2'.  It is not
     applicable if the query has 'GROUP BY c2, c3' (the columns are not
     a leftmost prefix) or 'GROUP BY c1, c2, c4' ('c4' is not in the
     index).

   * The only aggregate functions used in the select list (if any) are
     'MIN()' and 'MAX()', and all of them refer to the same column.  The
     column must be in the index and must immediately follow the columns
     in the 'GROUP BY'.

   * Any other parts of the index than those from the 'GROUP BY'
     referenced in the query must be constants (that is, they must be
     referenced in equalities with constants), except for the argument
     of 'MIN()' or 'MAX()' functions.

   * For columns in the index, full column values must be indexed, not
     just a prefix.  For example, with 'c1 VARCHAR(20), INDEX (c1(10))',
     the index uses only a prefix of 'c1' values and cannot be used for
     Loose Index Scan.

If Loose Index Scan is applicable to a query, the *note 'EXPLAIN':
explain. output shows 'Using index for group-by' in the 'Extra' column.

Assume that there is an index 'idx(c1,c2,c3)' on table
't1(c1,c2,c3,c4)'.  The Loose Index Scan access method can be used for
the following queries:

     SELECT c1, c2 FROM t1 GROUP BY c1, c2;
     SELECT DISTINCT c1, c2 FROM t1;
     SELECT c1, MIN(c2) FROM t1 GROUP BY c1;
     SELECT c1, c2 FROM t1 WHERE c1 < CONST GROUP BY c1, c2;
     SELECT MAX(c3), MIN(c3), c1, c2 FROM t1 WHERE c2 > CONST GROUP BY c1, c2;
     SELECT c2 FROM t1 WHERE c1 < CONST GROUP BY c1, c2;
     SELECT c1, c2 FROM t1 WHERE c3 = CONST GROUP BY c1, c2;

The following queries cannot be executed with this quick select method,
for the reasons given:

   * There are aggregate functions other than 'MIN()' or 'MAX()':

          SELECT c1, SUM(c2) FROM t1 GROUP BY c1;

   * The columns in the 'GROUP BY' clause do not form a leftmost prefix
     of the index:

          SELECT c1, c2 FROM t1 GROUP BY c2, c3;

   * The query refers to a part of a key that comes after the 'GROUP BY'
     part, and for which there is no equality with a constant:

          SELECT c1, c3 FROM t1 GROUP BY c1, c2;

     Were the query to include 'WHERE c3 = CONST', Loose Index Scan
     could be used.

The Loose Index Scan access method can be applied to other forms of
aggregate function references in the select list, in addition to the
'MIN()' and 'MAX()' references already supported:

   * 'AVG(DISTINCT)', 'SUM(DISTINCT)', and 'COUNT(DISTINCT)' are
     supported.  'AVG(DISTINCT)' and 'SUM(DISTINCT)' take a single
     argument.  'COUNT(DISTINCT)' can have more than one column
     argument.

   * There must be no 'GROUP BY' or 'DISTINCT' clause in the query.

   * The Loose Index Scan limitations described previously still apply.

Assume that there is an index 'idx(c1,c2,c3)' on table
't1(c1,c2,c3,c4)'.  The Loose Index Scan access method can be used for
the following queries:

     SELECT COUNT(DISTINCT c1), SUM(DISTINCT c1) FROM t1;

     SELECT COUNT(DISTINCT c1, c2), COUNT(DISTINCT c2, c1) FROM t1;

*Tight Index Scan*

A Tight Index Scan may be either a full index scan or a range index
scan, depending on the query conditions.

When the conditions for a Loose Index Scan are not met, it still may be
possible to avoid creation of temporary tables for 'GROUP BY' queries.
If there are range conditions in the 'WHERE' clause, this method reads
only the keys that satisfy these conditions.  Otherwise, it performs an
index scan.  Because this method reads all keys in each range defined by
the 'WHERE' clause, or scans the whole index if there are no range
conditions, it is called a _Tight Index Scan_.  With a Tight Index Scan,
the grouping operation is performed only after all keys that satisfy the
range conditions have been found.

For this method to work, it is sufficient that there be a constant
equality condition for all columns in a query referring to parts of the
key coming before or in between parts of the 'GROUP BY' key.  The
constants from the equality conditions fill in any 'gaps' in the search
keys so that it is possible to form complete prefixes of the index.
These index prefixes then can be used for index lookups.  If the 'GROUP
BY' result requires sorting, and it is possible to form search keys that
are prefixes of the index, MySQL also avoids extra sorting operations
because searching with prefixes in an ordered index already retrieves
all the keys in order.

Assume that there is an index 'idx(c1,c2,c3)' on table
't1(c1,c2,c3,c4)'.  The following queries do not work with the Loose
Index Scan access method described previously, but still work with the
Tight Index Scan access method.

   * There is a gap in the 'GROUP BY', but it is covered by the
     condition 'c2 = 'a'':

          SELECT c1, c2, c3 FROM t1 WHERE c2 = 'a' GROUP BY c1, c3;

   * The 'GROUP BY' does not begin with the first part of the key, but
     there is a condition that provides a constant for that part:

          SELECT c1, c2, c3 FROM t1 WHERE c1 = 'a' GROUP BY c2, c3;


File: manual.info.tmp,  Node: distinct-optimization,  Next: limit-optimization,  Prev: group-by-optimization,  Up: select-optimization

8.2.1.12 DISTINCT Optimization
..............................

'DISTINCT' combined with 'ORDER BY' needs a temporary table in many
cases.

Because 'DISTINCT' may use 'GROUP BY', learn how MySQL works with
columns in 'ORDER BY' or 'HAVING' clauses that are not part of the
selected columns.  See *note group-by-handling::.

In most cases, a 'DISTINCT' clause can be considered as a special case
of 'GROUP BY'.  For example, the following two queries are equivalent:

     SELECT DISTINCT c1, c2, c3 FROM t1
     WHERE c1 > CONST;

     SELECT c1, c2, c3 FROM t1
     WHERE c1 > CONST GROUP BY c1, c2, c3;

Due to this equivalence, the optimizations applicable to 'GROUP BY'
queries can be also applied to queries with a 'DISTINCT' clause.  Thus,
for more details on the optimization possibilities for 'DISTINCT'
queries, see *note group-by-optimization::.

When combining 'LIMIT ROW_COUNT' with 'DISTINCT', MySQL stops as soon as
it finds ROW_COUNT unique rows.

If you do not use columns from all tables named in a query, MySQL stops
scanning any unused tables as soon as it finds the first match.  In the
following case, assuming that 't1' is used before 't2' (which you can
check with *note 'EXPLAIN': explain.), MySQL stops reading from 't2'
(for any particular row in 't1') when it finds the first row in 't2':

     SELECT DISTINCT t1.a FROM t1, t2 where t1.a=t2.a;


File: manual.info.tmp,  Node: limit-optimization,  Next: function-optimization,  Prev: distinct-optimization,  Up: select-optimization

8.2.1.13 LIMIT Query Optimization
.................................

If you need only a specified number of rows from a result set, use a
'LIMIT' clause in the query, rather than fetching the whole result set
and throwing away the extra data.

MySQL sometimes optimizes a query that has a 'LIMIT ROW_COUNT' clause
and no 'HAVING' clause:

   * If you select only a few rows with 'LIMIT', MySQL uses indexes in
     some cases when normally it would prefer to do a full table scan.

   * If you combine 'LIMIT ROW_COUNT' with 'ORDER BY', MySQL stops
     sorting as soon as it has found the first ROW_COUNT rows of the
     sorted result, rather than sorting the entire result.  If ordering
     is done by using an index, this is very fast.  If a filesort must
     be done, all rows that match the query without the 'LIMIT' clause
     are selected, and most or all of them are sorted, before the first
     ROW_COUNT are found.  After the initial rows have been found, MySQL
     does not sort any remainder of the result set.

     One manifestation of this behavior is that an 'ORDER BY' query with
     and without 'LIMIT' may return rows in different order, as
     described later in this section.

   * If you combine 'LIMIT ROW_COUNT' with 'DISTINCT', MySQL stops as
     soon as it finds ROW_COUNT unique rows.

   * In some cases, a 'GROUP BY' can be resolved by reading the index in
     order (or doing a sort on the index), then calculating summaries
     until the index value changes.  In this case, 'LIMIT ROW_COUNT'
     does not calculate any unnecessary 'GROUP BY' values.

   * As soon as MySQL has sent the required number of rows to the
     client, it aborts the query unless you are using
     'SQL_CALC_FOUND_ROWS'.  In that case, the number of rows can be
     retrieved with 'SELECT FOUND_ROWS()'.  See *note
     information-functions::.

   * 'LIMIT 0' quickly returns an empty set.  This can be useful for
     checking the validity of a query.  It can also be employed to
     obtain the types of the result columns within applications that use
     a MySQL API that makes result set metadata available.  With the
     *note 'mysql': mysql. client program, you can use the
     '--column-type-info' option to display result column types.

   * If the server uses temporary tables to resolve a query, it uses the
     'LIMIT ROW_COUNT' clause to calculate how much space is required.

If multiple rows have identical values in the 'ORDER BY' columns, the
server is free to return those rows in any order, and may do so
differently depending on the overall execution plan.  In other words,
the sort order of those rows is nondeterministic with respect to the
nonordered columns.

One factor that affects the execution plan is 'LIMIT', so an 'ORDER BY'
query with and without 'LIMIT' may return rows in different orders.
Consider this query, which is sorted by the 'category' column but
nondeterministic with respect to the 'id' and 'rating' columns:

     mysql> SELECT * FROM ratings ORDER BY category;
     +----+----------+--------+
     | id | category | rating |
     +----+----------+--------+
     |  1 |        1 |    4.5 |
     |  5 |        1 |    3.2 |
     |  3 |        2 |    3.7 |
     |  4 |        2 |    3.5 |
     |  6 |        2 |    3.5 |
     |  2 |        3 |    5.0 |
     |  7 |        3 |    2.7 |
     +----+----------+--------+

Including 'LIMIT' may affect order of rows within each 'category' value.
For example, this is a valid query result:

     mysql> SELECT * FROM ratings ORDER BY category LIMIT 5;
     +----+----------+--------+
     | id | category | rating |
     +----+----------+--------+
     |  1 |        1 |    4.5 |
     |  5 |        1 |    3.2 |
     |  4 |        2 |    3.5 |
     |  3 |        2 |    3.7 |
     |  6 |        2 |    3.5 |
     +----+----------+--------+

In each case, the rows are sorted by the 'ORDER BY' column, which is all
that is required by the SQL standard.

If it is important to ensure the same row order with and without
'LIMIT', include additional columns in the 'ORDER BY' clause to make the
order deterministic.  For example, if 'id' values are unique, you can
make rows for a given 'category' value appear in 'id' order by sorting
like this:

     mysql> SELECT * FROM ratings ORDER BY category, id;
     +----+----------+--------+
     | id | category | rating |
     +----+----------+--------+
     |  1 |        1 |    4.5 |
     |  5 |        1 |    3.2 |
     |  3 |        2 |    3.7 |
     |  4 |        2 |    3.5 |
     |  6 |        2 |    3.5 |
     |  2 |        3 |    5.0 |
     |  7 |        3 |    2.7 |
     +----+----------+--------+

     mysql> SELECT * FROM ratings ORDER BY category, id LIMIT 5;
     +----+----------+--------+
     | id | category | rating |
     +----+----------+--------+
     |  1 |        1 |    4.5 |
     |  5 |        1 |    3.2 |
     |  3 |        2 |    3.7 |
     |  4 |        2 |    3.5 |
     |  6 |        2 |    3.5 |
     +----+----------+--------+


File: manual.info.tmp,  Node: function-optimization,  Next: row-constructor-optimization,  Prev: limit-optimization,  Up: select-optimization

8.2.1.14 Function Call Optimization
...................................

MySQL functions are tagged internally as deterministic or
nondeterministic.  A function is nondeterministic if, given fixed values
for its arguments, it can return different results for different
invocations.  Examples of nondeterministic functions: 'RAND()',
'UUID()'.

If a function is tagged nondeterministic, a reference to it in a 'WHERE'
clause is evaluated for every row (when selecting from one table) or
combination of rows (when selecting from a multiple-table join).

MySQL also determines when to evaluate functions based on types of
arguments, whether the arguments are table columns or constant values.
A deterministic function that takes a table column as argument must be
evaluated whenever that column changes value.

Nondeterministic functions may affect query performance.  For example,
some optimizations may not be available, or more locking might be
required.  The following discussion uses 'RAND()' but applies to other
nondeterministic functions as well.

Suppose that a table 't' has this definition:

     CREATE TABLE t (id INT NOT NULL PRIMARY KEY, col_a VARCHAR(100));

Consider these two queries:

     SELECT * FROM t WHERE id = POW(1,2);
     SELECT * FROM t WHERE id = FLOOR(1 + RAND() * 49);

Both queries appear to use a primary key lookup because of the equality
comparison against the primary key, but that is true only for the first
of them:

   * The first query always produces a maximum of one row because
     'POW()' with constant arguments is a constant value and is used for
     index lookup.

   * The second query contains an expression that uses the
     nondeterministic function 'RAND()', which is not constant in the
     query but in fact has a new value for every row of table 't'.
     Consequently, the query reads every row of the table, evaluates the
     predicate for each row, and outputs all rows for which the primary
     key matches the random value.  This might be zero, one, or multiple
     rows, depending on the 'id' column values and the values in the
     'RAND()' sequence.

The effects of nondeterminism are not limited to *note 'SELECT': select.
statements.  This *note 'UPDATE': update. statement uses a
nondeterministic function to select rows to be modified:

     UPDATE t SET col_a = SOME_EXPR WHERE id = FLOOR(1 + RAND() * 49);

Presumably the intent is to update at most a single row for which the
primary key matches the expression.  However, it might update zero, one,
or multiple rows, depending on the 'id' column values and the values in
the 'RAND()' sequence.

The behavior just described has implications for performance and
replication:

   * Because a nondeterministic function does not produce a constant
     value, the optimizer cannot use strategies that might otherwise be
     applicable, such as index lookups.  The result may be a table scan.

   * 'InnoDB' might escalate to a range-key lock rather than taking a
     single row lock for one matching row.

   * Updates that do not execute deterministically are unsafe for
     replication.

The difficulties stem from the fact that the 'RAND()' function is
evaluated once for every row of the table.  To avoid multiple function
evaluations, use one of these techniques:

   * Move the expression containing the nondeterministic function to a
     separate statement, saving the value in a variable.  In the
     original statement, replace the expression with a reference to the
     variable, which the optimizer can treat as a constant value:

          SET @keyval = FLOOR(1 + RAND() * 49);
          UPDATE t SET col_a = SOME_EXPR WHERE id = @keyval;

   * Assign the random value to a variable in a derived table.  This
     technique causes the variable to be assigned a value, once, prior
     to its use in the comparison in the 'WHERE' clause:

          UPDATE t, (SELECT @keyval := FLOOR(1 + RAND() * 49)) AS dt
          SET col_a = SOME_EXPR WHERE id = @keyval;

As mentioned previously, a nondeterministic expression in the 'WHERE'
clause might prevent optimizations and result in a table scan.  However,
it may be possible to partially optimize the 'WHERE' clause if other
expressions are deterministic.  For example:

     SELECT * FROM t WHERE partial_key=5 AND some_column=RAND();

If the optimizer can use 'partial_key' to reduce the set of rows
selected, 'RAND()' is executed fewer times, which diminishes the effect
of nondeterminism on optimization.


File: manual.info.tmp,  Node: row-constructor-optimization,  Next: table-scan-avoidance,  Prev: function-optimization,  Up: select-optimization

8.2.1.15 Row Constructor Expression Optimization
................................................

Row constructors permit simultaneous comparisons of multiple values.
For example, these two statements are semantically equivalent:

     SELECT * FROM t1 WHERE (column1,column2) = (1,1);
     SELECT * FROM t1 WHERE column1 = 1 AND column2 = 1;

In addition, the optimizer handles both expressions the same way.

The optimizer is less likely to use available indexes if the row
constructor columns do not cover the prefix of an index.  Consider the
following table, which has a primary key on '(c1, c2, c3)':

     CREATE TABLE t1 (
       c1 INT, c2 INT, c3 INT, c4 CHAR(100),
       PRIMARY KEY(c1,c2,c3)
     );

In this query, the 'WHERE' clause uses all columns in the index.
However, the row constructor itself does not cover an index prefix, with
the result that the optimizer uses only 'c1' ('key_len=4', the size of
'c1'):

     mysql> EXPLAIN SELECT * FROM t1
            WHERE c1=1 AND (c2,c3) > (1,1)\G
     *************************** 1. row ***************************
                id: 1
       select_type: SIMPLE
             table: t1
        partitions: NULL
              type: ref
     possible_keys: PRIMARY
               key: PRIMARY
           key_len: 4
               ref: const
              rows: 3
             Extra: Using where

In such cases, rewriting the row constructor expression using an
equivalent nonconstructor expression may result in more complete index
use.  For the given query, the row constructor and equivalent
nonconstructor expressions are:

     (c2,c3) > (1,1)
     c2 > 1 OR ((c2 = 1) AND (c3 > 1))

Rewriting the query to use the nonconstructor expression results in the
optimizer using all three columns in the index ('key_len=12'):

     mysql> EXPLAIN SELECT * FROM t1
            WHERE c1 = 1 AND (c2 > 1 OR ((c2 = 1) AND (c3 > 1)))\G
     *************************** 1. row ***************************
                id: 1
       select_type: SIMPLE
             table: t1
        partitions: NULL
              type: range
     possible_keys: PRIMARY
               key: PRIMARY
           key_len: 12
               ref: NULL
              rows: 3
             Extra: Using where

Thus, for better results, avoid mixing row constructors with 'AND'/'OR'
expressions.  Use one or the other.


File: manual.info.tmp,  Node: table-scan-avoidance,  Prev: row-constructor-optimization,  Up: select-optimization

8.2.1.16 Avoiding Full Table Scans
..................................

Frequently, a full table scan is a danger sign that a query can be
speeded up significantly.  For tables with more than a few rows,
consider redesigning the query by adding an index for one or more of the
columns tested in the 'WHERE' clause.  Put extra effort into avoiding
table scans for queries that perform joins or reference foreign keys.
If the nature of the data means there is no way to avoid reading all the
rows, then it might not be practical to make the query faster, or making
it faster might involve extensive restructuring of your tables that is
beyond the scope of this section.

The output from *note 'EXPLAIN': explain. shows 'ALL' in the 'type'
column when MySQL uses a table scan to resolve a query.  This usually
happens under the following conditions:

   * The 'ON' or 'WHERE' clauses do not reference any indexed columns
     that the query can use.  Consider adding an index, or refining
     those clauses to refer to an indexed column.

   * The table is so small that it is faster to perform a table scan
     than to bother with a key lookup.  This is common for tables with
     fewer than 10 rows and a short row length.

   * You are comparing indexed columns with constant values and MySQL
     has calculated (based on the index tree) that the constants cover
     too large a part of the table and that a table scan would be
     faster.  See *note where-optimization::.  For example, to query
     census data only for males or only for females, MySQL must read
     most of the data blocks in the table, so locating the rows through
     the index would add unnecessary overhead.  If these reports are
     frequent or truly time-critical, and the table is huge, you might
     partition, shard, or create dimension tables using the relevant
     column.

   * You are using a key with low cardinality (many rows match the key
     value) through another column.  In this case, MySQL assumes that by
     using the key it probably will do many key lookups and that a table
     scan would be faster.

For small tables, a table scan often is appropriate and the performance
impact is negligible.  For large tables, try the following techniques to
avoid having the optimizer incorrectly choose a table scan:

   * Minimize the 'OR' keywords in your 'WHERE' clauses.  If there is no
     index that helps to locate the values on _both_ sides of the 'OR',
     any row could potentially be part of the result set, so all rows
     must be tested, and that requires a full table scan.  If you have
     one index that helps to optimize one side of an 'OR' query, and a
     different index that helps to optimize the other side, use a
     'UNION' operator to run separate fast queries and merge the results
     afterward.

   * With tables that use the 'MEMORY' storage engine, if you run
     queries that examine ranges of values (using operators such as '>',
     '<=', or 'BETWEEN' on the indexed columns), create the index with
     the 'USING BTREE' clause.  The default ('USING HASH') is fast for
     retrieving individual rows with an equality operator ('=' or
     '<=>'), but is much slower (requiring a full table scan) to examine
     a range of column values.  A 'MEMORY' table created with the 'USING
     BTREE' clause is still fast for equality comparisons, so use that
     clause for your 'MEMORY' tables that handle a variety of queries.

   * Use 'ANALYZE TABLE TBL_NAME' to update the key distributions for
     the scanned table.  See *note analyze-table::.

   * Use 'FORCE INDEX' for the scanned table to tell MySQL that table
     scans are very expensive compared to using the given index:

          SELECT * FROM t1, t2 FORCE INDEX (INDEX_FOR_COLUMN)
            WHERE t1.COL_NAME=t2.COL_NAME;

     See *note index-hints::.

   * Start *note 'mysqld': mysqld. with the '--max-seeks-for-key=1000'
     option or use 'SET max_seeks_for_key=1000' to tell the optimizer to
     assume that no key scan causes more than 1,000 key seeks.  See
     *note server-system-variables::.


File: manual.info.tmp,  Node: subquery-optimization,  Next: information-schema-optimization,  Prev: select-optimization,  Up: statement-optimization

8.2.2 Subquery Optimization
---------------------------

Certain optimizations are applicable to comparisons that use the 'IN'
operator to test subquery results (or that use '=ANY', which is
equivalent).  This section discusses these optimizations, particularly
with regard to the challenges that 'NULL' values present.  The last part
of the discussion suggests how you can help the optimizer.

Consider the following subquery comparison:

     OUTER_EXPR IN (SELECT INNER_EXPR FROM ... WHERE SUBQUERY_WHERE)

MySQL evaluates queries 'from outside to inside.' That is, it first
obtains the value of the outer expression OUTER_EXPR, and then runs the
subquery and captures the rows that it produces.

A very useful optimization is to 'inform' the subquery that the only
rows of interest are those where the inner expression INNER_EXPR is
equal to OUTER_EXPR.  This is done by pushing down an appropriate
equality into the subquery's 'WHERE' clause to make it more restrictive.
The converted comparison looks like this:

     EXISTS (SELECT 1 FROM ... WHERE SUBQUERY_WHERE AND OUTER_EXPR=INNER_EXPR)

After the conversion, MySQL can use the pushed-down equality to limit
the number of rows it must examine to evaluate the subquery.

More generally, a comparison of N values to a subquery that returns
N-value rows is subject to the same conversion.  If OE_I and IE_I
represent corresponding outer and inner expression values, this subquery
comparison:

     (OE_1, ..., OE_N) IN
       (SELECT IE_1, ..., IE_N FROM ... WHERE SUBQUERY_WHERE)

Becomes:

     EXISTS (SELECT 1 FROM ... WHERE SUBQUERY_WHERE
                               AND OE_1 = IE_1
                               AND ...
                               AND OE_N = IE_N)

For simplicity, the following discussion assumes a single pair of outer
and inner expression values.

The conversion just described has its limitations.  It is valid only if
we ignore possible 'NULL' values.  That is, the 'pushdown' strategy
works as long as both of these conditions are true:

   * OUTER_EXPR and INNER_EXPR cannot be 'NULL'.

   * You need not distinguish 'NULL' from 'FALSE' subquery results.  If
     the subquery is a part of an 'OR' or 'AND' expression in the
     'WHERE' clause, MySQL assumes that you do not care.

When either or both of those conditions do not hold, optimization is
more complex.

Suppose that OUTER_EXPR is known to be a non-'NULL' value but the
subquery does not produce a row such that OUTER_EXPR = INNER_EXPR.  Then
'OUTER_EXPR IN (SELECT ...)' evaluates as follows:

   * 'NULL', if the *note 'SELECT': select. produces any row where
     INNER_EXPR is 'NULL'

   * 'FALSE', if the *note 'SELECT': select. produces only non-'NULL'
     values or produces nothing

In this situation, the approach of looking for rows with 'OUTER_EXPR =
INNER_EXPR' is no longer valid.  It is necessary to look for such rows,
but if none are found, also look for rows where INNER_EXPR is 'NULL'.
Roughly speaking, the subquery can be converted to something like this:

     EXISTS (SELECT 1 FROM ... WHERE SUBQUERY_WHERE AND
             (OUTER_EXPR=INNER_EXPR OR INNER_EXPR IS NULL))

The need to evaluate the extra 'IS NULL' condition is why MySQL has the
'ref_or_null' access method:

     mysql> EXPLAIN
            SELECT OUTER_EXPR IN (SELECT t2.maybe_null_key
                                  FROM t2, t3 WHERE ...)
            FROM t1;
     *************************** 1. row ***************************
                id: 1
       select_type: PRIMARY
             table: t1
     ...
     *************************** 2. row ***************************
                id: 2
       select_type: DEPENDENT SUBQUERY
             table: t2
              type: ref_or_null
     possible_keys: maybe_null_key
               key: maybe_null_key
           key_len: 5
               ref: func
              rows: 2
             Extra: Using where; Using index
     ...

The 'unique_subquery' and 'index_subquery' subquery-specific access
methods also have 'or 'NULL'' variants.  However, they are not visible
in *note 'EXPLAIN': explain. output, so you must use *note 'EXPLAIN
EXTENDED': explain-extended. followed by *note 'SHOW WARNINGS':
show-warnings. (note the 'checking NULL' in the warning message):

     mysql> EXPLAIN EXTENDED
            SELECT OUTER_EXPR IN (SELECT maybe_null_key FROM t2) FROM t1\G
     *************************** 1. row ***************************
                id: 1
       select_type: PRIMARY
             table: t1
     ...
     *************************** 2. row ***************************
                id: 2
       select_type: DEPENDENT SUBQUERY
             table: t2
              type: index_subquery
     possible_keys: maybe_null_key
               key: maybe_null_key
           key_len: 5
               ref: func
              rows: 2
             Extra: Using index

     mysql> SHOW WARNINGS\G
     *************************** 1. row ***************************
       Level: Note
        Code: 1003
     Message: select (`test`.`t1`.`outer_expr`,
              (((`test`.`t1`.`outer_expr`) in t2 on
              maybe_null_key checking NULL))) AS `outer_expr IN (SELECT
              maybe_null_key FROM t2)` from `test`.`t1`

The additional 'OR ... IS NULL' condition makes query execution slightly
more complicated (and some optimizations within the subquery become
inapplicable), but generally this is tolerable.

The situation is much worse when OUTER_EXPR can be 'NULL'.  According to
the SQL interpretation of 'NULL' as 'unknown value,' 'NULL IN (SELECT
INNER_EXPR ...)' should evaluate to:

   * 'NULL', if the *note 'SELECT': select. produces any rows

   * 'FALSE', if the *note 'SELECT': select. produces no rows

For proper evaluation, it is necessary to be able to check whether the
*note 'SELECT': select. has produced any rows at all, so 'OUTER_EXPR =
INNER_EXPR' cannot be pushed down into the subquery.  This is a problem
because many real world subqueries become very slow unless the equality
can be pushed down.

Essentially, there must be different ways to execute the subquery
depending on the value of OUTER_EXPR.

The optimizer chooses SQL compliance over speed, so it accounts for the
possibility that OUTER_EXPR might be 'NULL':

   * If OUTER_EXPR is 'NULL', to evaluate the following expression, it
     is necessary to execute the *note 'SELECT': select. to determine
     whether it produces any rows:

          NULL IN (SELECT INNER_EXPR FROM ... WHERE SUBQUERY_WHERE)

     It is necessary to execute the original *note 'SELECT': select.
     here, without any pushed-down equalities of the kind mentioned
     previously.

   * On the other hand, when OUTER_EXPR is not 'NULL', it is absolutely
     essential that this comparison:

          OUTER_EXPR IN (SELECT INNER_EXPR FROM ... WHERE SUBQUERY_WHERE)

     be converted to this expression that uses a pushed-down condition:

          EXISTS (SELECT 1 FROM ... WHERE SUBQUERY_WHERE AND OUTER_EXPR=INNER_EXPR)

     Without this conversion, subqueries will be slow.

To solve the dilemma of whether or not to push down conditions into the
subquery, the conditions are wrapped within 'trigger' functions.  Thus,
an expression of the following form:

     OUTER_EXPR IN (SELECT INNER_EXPR FROM ... WHERE SUBQUERY_WHERE)

is converted into:

     EXISTS (SELECT 1 FROM ... WHERE SUBQUERY_WHERE
                               AND trigcond(OUTER_EXPR=INNER_EXPR))

More generally, if the subquery comparison is based on several pairs of
outer and inner expressions, the conversion takes this comparison:

     (OE_1, ..., OE_N) IN (SELECT IE_1, ..., IE_N FROM ... WHERE SUBQUERY_WHERE)

and converts it to this expression:

     EXISTS (SELECT 1 FROM ... WHERE SUBQUERY_WHERE
                               AND trigcond(OE_1=IE_1)
                               AND ...
                               AND trigcond(OE_N=IE_N)
            )

Each 'trigcond(X)' is a special function that evaluates to the following
values:

   * X when the 'linked' outer expression OE_I is not 'NULL'

   * 'TRUE' when the 'linked' outer expression OE_I is 'NULL'

*Note*:

Trigger functions are _not_ triggers of the kind that you create with
*note 'CREATE TRIGGER': create-trigger.

Equalities that are wrapped within 'trigcond()' functions are not first
class predicates for the query optimizer.  Most optimizations cannot
deal with predicates that may be turned on and off at query execution
time, so they assume any 'trigcond(X)' to be an unknown function and
ignore it.  Triggered equalities can be used by those optimizations:

   * Reference optimizations: 'trigcond(X=Y [OR Y IS NULL])' can be used
     to construct 'ref', 'eq_ref', or 'ref_or_null' table accesses.

   * Index lookup-based subquery execution engines: 'trigcond(X=Y)' can
     be used to construct 'unique_subquery' or 'index_subquery'
     accesses.

   * Table-condition generator: If the subquery is a join of several
     tables, the triggered condition is checked as soon as possible.

When the optimizer uses a triggered condition to create some kind of
index lookup-based access (as for the first two items of the preceding
list), it must have a fallback strategy for the case when the condition
is turned off.  This fallback strategy is always the same: Do a full
table scan.  In *note 'EXPLAIN': explain. output, the fallback shows up
as 'Full scan on NULL key' in the 'Extra' column:

     mysql> EXPLAIN SELECT t1.col1,
            t1.col1 IN (SELECT t2.key1 FROM t2 WHERE t2.col2=t1.col2) FROM t1\G
     *************************** 1. row ***************************
                id: 1
       select_type: PRIMARY
             table: t1
             ...
     *************************** 2. row ***************************
                id: 2
       select_type: DEPENDENT SUBQUERY
             table: t2
              type: index_subquery
     possible_keys: key1
               key: key1
           key_len: 5
               ref: func
              rows: 2
             Extra: Using where; Full scan on NULL key

If you run *note 'EXPLAIN EXTENDED': explain-extended. followed by *note
'SHOW WARNINGS': show-warnings, you can see the triggered condition:

     *************************** 1. row ***************************
       Level: Note
        Code: 1003
     Message: select `test`.`t1`.`col1` AS `col1`,
              <in_optimizer>(`test`.`t1`.`col1`,
              <exists>(<index_lookup>(<cache>(`test`.`t1`.`col1`) in t2
              on key1 checking NULL
              where (`test`.`t2`.`col2` = `test`.`t1`.`col2`) having
              trigcond(<is_not_null_test>(`test`.`t2`.`key1`))))) AS
              `t1.col1 IN (select t2.key1 from t2 where t2.col2=t1.col2)`
              from `test`.`t1`

The use of triggered conditions has some performance implications.  A
'NULL IN (SELECT ...)' expression now may cause a full table scan (which
is slow) when it previously did not.  This is the price paid for correct
results (the goal of the trigger-condition strategy is to improve
compliance, not speed).

For multiple-table subqueries, execution of 'NULL IN (SELECT ...)' is
particularly slow because the join optimizer does not optimize for the
case where the outer expression is 'NULL'.  It assumes that subquery
evaluations with 'NULL' on the left side are very rare, even if there
are statistics that indicate otherwise.  On the other hand, if the outer
expression might be 'NULL' but never actually is, there is no
performance penalty.

To help the query optimizer better execute your queries, use these
suggestions:

   * Declare a column as 'NOT NULL' if it really is.  This also helps
     other aspects of the optimizer by simplifying condition testing for
     the column.

   * If you need not distinguish a 'NULL' from 'FALSE' subquery result,
     you can easily avoid the slow execution path.  Replace a comparison
     that looks like this:

          OUTER_EXPR IN (SELECT INNER_EXPR FROM ...)

     with this expression:

          (OUTER_EXPR IS NOT NULL) AND (OUTER_EXPR IN (SELECT INNER_EXPR FROM ...))

     Then 'NULL IN (SELECT ...)' is never evaluated because MySQL stops
     evaluating 'AND' parts as soon as the expression result is clear.


File: manual.info.tmp,  Node: information-schema-optimization,  Next: data-change-optimization,  Prev: subquery-optimization,  Up: statement-optimization

8.2.3 Optimizing INFORMATION_SCHEMA Queries
-------------------------------------------

Applications that monitor databases may make frequent use of
'INFORMATION_SCHEMA' tables.  Certain types of queries for
'INFORMATION_SCHEMA' tables can be optimized to execute more quickly.
The goal is to minimize file operations (for example, scanning a
directory or opening a table file) to collect the information that makes
up these dynamic tables.

*Note*:

Comparison behavior for database and table names in 'INFORMATION_SCHEMA'
queries might differ from what you expect.  For details, see *note
charset-collation-information-schema::.

*1) Try to use constant lookup values for database and table names in
the 'WHERE' clause*

You can take advantage of this principle as follows:

   * To look up databases or tables, use expressions that evaluate to a
     constant, such as literal values, functions that return a constant,
     or scalar subqueries.

   * Avoid queries that use a nonconstant database name lookup value (or
     no lookup value) because they require a scan of the data directory
     to find matching database directory names.

   * Within a database, avoid queries that use a nonconstant table name
     lookup value (or no lookup value) because they require a scan of
     the database directory to find matching table files.

This principle applies to the 'INFORMATION_SCHEMA' tables shown in the
following table, which shows the columns for which a constant lookup
value enables the server to avoid a directory scan.  For example, if you
are selecting from *note 'TABLES': tables-table, using a constant lookup
value for 'TABLE_SCHEMA' in the 'WHERE' clause enables a data directory
scan to be avoided.

Table                    Column to specify to     Column to specify to
                         avoid data directory     avoid database
                         scan                     directory scan
                                                  
*note 'COLUMNS': columns-table.'TABLE_SCHEMA'     'TABLE_NAME'
                                                  
*note 'KEY_COLUMN_USAGE': key-column-usage-table.'TABLE_SCHEMA''TABLE_NAME'
                                                  
*note 'PARTITIONS': partitions-table.'TABLE_SCHEMA''TABLE_NAME'
                                                  
*note 'REFERENTIAL_CONSTRAINTS': referential-constraints-table.'CONSTRAINT_SCHEMA''TABLE_NAME'
                                                  
*note 'STATISTICS': statistics-table.'TABLE_SCHEMA''TABLE_NAME'
                                                  
*note 'TABLES': tables-table.'TABLE_SCHEMA'       'TABLE_NAME'
                                                  
*note 'TABLE_CONSTRAINTS': table-constraints-table.'TABLE_SCHEMA''TABLE_NAME'
                                                  
*note 'TRIGGERS': triggers-table.'EVENT_OBJECT_SCHEMA''EVENT_OBJECT_TABLE'
                                                  
*note 'VIEWS': views-table.'TABLE_SCHEMA'         'TABLE_NAME'
                         

The benefit of a query that is limited to a specific constant database
name is that checks need be made only for the named database directory.
Example:

     SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES
     WHERE TABLE_SCHEMA = 'test';

Use of the literal database name 'test' enables the server to check only
the 'test' database directory, regardless of how many databases there
might be.  By contrast, the following query is less efficient because it
requires a scan of the data directory to determine which database names
match the pattern ''test%'':

     SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES
     WHERE TABLE_SCHEMA LIKE 'test%';

For a query that is limited to a specific constant table name, checks
need be made only for the named table within the corresponding database
directory.  Example:

     SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES
     WHERE TABLE_SCHEMA = 'test' AND TABLE_NAME = 't1';

Use of the literal table name 't1' enables the server to check only the
files for the 't1' table, regardless of how many tables there might be
in the 'test' database.  By contrast, the following query requires a
scan of the 'test' database directory to determine which table names
match the pattern ''t%'':

     SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES
     WHERE TABLE_SCHEMA = 'test' AND TABLE_NAME LIKE 't%';

The following query requires a scan of the database directory to
determine matching database names for the pattern ''test%'', and for
each matching database, it requires a scan of the database directory to
determine matching table names for the pattern ''t%'':

     SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES
     WHERE TABLE_SCHEMA = 'test%' AND TABLE_NAME LIKE 't%';

*2) Write queries that minimize the number of table files that must be
opened*

For queries that refer to certain 'INFORMATION_SCHEMA' table columns,
several optimizations are available that minimize the number of table
files that must be opened.  Example:

     SELECT TABLE_NAME, ENGINE FROM INFORMATION_SCHEMA.TABLES
     WHERE TABLE_SCHEMA = 'test';

In this case, after the server has scanned the database directory to
determine the names of the tables in the database, those names become
available with no further file system lookups.  Thus, 'TABLE_NAME'
requires no files to be opened.  The 'ENGINE' (storage engine) value can
be determined by opening the table's '.frm' file, without touching other
table files such as the '.MYD' or '.MYI' file.

Some values, such as 'INDEX_LENGTH' for 'MyISAM' tables, require opening
the '.MYD' or '.MYI' file as well.

The file-opening optimization types are denoted thus:

   * 'SKIP_OPEN_TABLE': Table files do not need to be opened.  The
     information has already become available within the query by
     scanning the database directory.

   * 'OPEN_FRM_ONLY': Only the table's '.frm' file need be opened.

   * 'OPEN_TRIGGER_ONLY': Only the table's '.TRG' file need be opened.

   * 'OPEN_FULL_TABLE': The unoptimized information lookup.  The '.frm',
     '.MYD', and '.MYI' files must be opened.

The following list indicates how the preceding optimization types apply
to 'INFORMATION_SCHEMA' table columns.  For tables and columns not
named, none of the optimizations apply.

   * *note 'COLUMNS': columns-table.: 'OPEN_FRM_ONLY' applies to all
     columns

   * *note 'KEY_COLUMN_USAGE': key-column-usage-table.:
     'OPEN_FULL_TABLE' applies to all columns

   * *note 'PARTITIONS': partitions-table.: 'OPEN_FULL_TABLE' applies to
     all columns

   * *note 'REFERENTIAL_CONSTRAINTS': referential-constraints-table.:
     'OPEN_FULL_TABLE' applies to all columns

   * *note 'STATISTICS': statistics-table.:

     Column                               Optimization type
                                          
     'TABLE_CATALOG'                      'OPEN_FRM_ONLY'
                                          
     'TABLE_SCHEMA'                       'OPEN_FRM_ONLY'
                                          
     'TABLE_NAME'                         'OPEN_FRM_ONLY'
                                          
     'NON_UNIQUE'                         'OPEN_FRM_ONLY'
                                          
     'INDEX_SCHEMA'                       'OPEN_FRM_ONLY'
                                          
     'INDEX_NAME'                         'OPEN_FRM_ONLY'
                                          
     'SEQ_IN_INDEX'                       'OPEN_FRM_ONLY'
                                          
     'COLUMN_NAME'                        'OPEN_FRM_ONLY'
                                          
     'COLLATION'                          'OPEN_FRM_ONLY'
                                          
     'CARDINALITY'                        'OPEN_FULL_TABLE'
                                          
     'SUB_PART'                           'OPEN_FRM_ONLY'
                                          
     'PACKED'                             'OPEN_FRM_ONLY'
                                          
     'NULLABLE'                           'OPEN_FRM_ONLY'
                                          
     'INDEX_TYPE'                         'OPEN_FULL_TABLE'
                                          
     'COMMENT'                            'OPEN_FRM_ONLY'

   * *note 'TABLES': tables-table.:

     Column                               Optimization type
                                          
     'TABLE_CATALOG'                      'SKIP_OPEN_TABLE'
                                          
     'TABLE_SCHEMA'                       'SKIP_OPEN_TABLE'
                                          
     'TABLE_NAME'                         'SKIP_OPEN_TABLE'
                                          
     'TABLE_TYPE'                         'OPEN_FRM_ONLY'
                                          
     'ENGINE'                             'OPEN_FRM_ONLY'
                                          
     'VERSION'                            'OPEN_FRM_ONLY'
                                          
     'ROW_FORMAT'                         'OPEN_FULL_TABLE'
                                          
     'TABLE_ROWS'                         'OPEN_FULL_TABLE'
                                          
     'AVG_ROW_LENGTH'                     'OPEN_FULL_TABLE'
                                          
     'DATA_LENGTH'                        'OPEN_FULL_TABLE'
                                          
     'MAX_DATA_LENGTH'                    'OPEN_FULL_TABLE'
                                          
     'INDEX_LENGTH'                       'OPEN_FULL_TABLE'
                                          
     'DATA_FREE'                          'OPEN_FULL_TABLE'
                                          
     'AUTO_INCREMENT'                     'OPEN_FULL_TABLE'
                                          
     'CREATE_TIME'                        'OPEN_FULL_TABLE'
                                          
     'UPDATE_TIME'                        'OPEN_FULL_TABLE'
                                          
     'CHECK_TIME'                         'OPEN_FULL_TABLE'
                                          
     'TABLE_COLLATION'                    'OPEN_FRM_ONLY'
                                          
     'CHECKSUM'                           'OPEN_FULL_TABLE'
                                          
     'CREATE_OPTIONS'                     'OPEN_FRM_ONLY'
                                          
     'TABLE_COMMENT'                      'OPEN_FRM_ONLY'

   * *note 'TABLE_CONSTRAINTS': table-constraints-table.:
     'OPEN_FULL_TABLE' applies to all columns

   * *note 'TRIGGERS': triggers-table.: 'OPEN_TRIGGER_ONLY' applies to
     all columns

   * *note 'VIEWS': views-table.:

     Column                               Optimization type
                                          
     'TABLE_CATALOG'                      'OPEN_FRM_ONLY'
                                          
     'TABLE_SCHEMA'                       'OPEN_FRM_ONLY'
                                          
     'TABLE_NAME'                         'OPEN_FRM_ONLY'
                                          
     'VIEW_DEFINITION'                    'OPEN_FRM_ONLY'
                                          
     'CHECK_OPTION'                       'OPEN_FRM_ONLY'
                                          
     'IS_UPDATABLE'                       'OPEN_FULL_TABLE'
                                          
     'DEFINER'                            'OPEN_FRM_ONLY'
                                          
     'SECURITY_TYPE'                      'OPEN_FRM_ONLY'
                                          
     'CHARACTER_SET_CLIENT'               'OPEN_FRM_ONLY'
                                          
     'COLLATION_CONNECTION'               'OPEN_FRM_ONLY'

*3) Use *note 'EXPLAIN': explain. to determine whether the server can
use 'INFORMATION_SCHEMA' optimizations for a query*

This applies particularly for 'INFORMATION_SCHEMA' queries that search
for information from more than one database, which might take a long
time and impact performance.  The 'Extra' value in *note 'EXPLAIN':
explain. output indicates which, if any, of the optimizations described
earlier the server can use to evaluate 'INFORMATION_SCHEMA' queries.
The following examples demonstrate the kinds of information you can
expect to see in the 'Extra' value.

     mysql> EXPLAIN SELECT TABLE_NAME FROM INFORMATION_SCHEMA.VIEWS WHERE
            TABLE_SCHEMA = 'test' AND TABLE_NAME = 'v1'\G
     *************************** 1. row ***************************
                id: 1
       select_type: SIMPLE
             table: VIEWS
              type: ALL
     possible_keys: NULL
               key: TABLE_SCHEMA,TABLE_NAME
           key_len: NULL
               ref: NULL
              rows: NULL
             Extra: Using where; Open_frm_only; Scanned 0 databases

Use of constant database and table lookup values enables the server to
avoid directory scans.  For references to 'VIEWS.TABLE_NAME', only the
'.frm' file need be opened.

     mysql> EXPLAIN SELECT TABLE_NAME, ROW_FORMAT FROM INFORMATION_SCHEMA.TABLES\G
     *************************** 1. row ***************************
                id: 1
       select_type: SIMPLE
             table: TABLES
              type: ALL
     possible_keys: NULL
               key: NULL
           key_len: NULL
               ref: NULL
              rows: NULL
             Extra: Open_full_table; Scanned all databases

No lookup values are provided (there is no 'WHERE' clause), so the
server must scan the data directory and each database directory.  For
each table thus identified, the table name and row format are selected.
'TABLE_NAME' requires no further table files to be opened (the
'SKIP_OPEN_TABLE' optimization applies).  'ROW_FORMAT' requires all
table files to be opened ('OPEN_FULL_TABLE' applies).  *note 'EXPLAIN':
explain. reports 'OPEN_FULL_TABLE' because it is more expensive than
'SKIP_OPEN_TABLE'.

     mysql> EXPLAIN SELECT TABLE_NAME, TABLE_TYPE FROM INFORMATION_SCHEMA.TABLES
            WHERE TABLE_SCHEMA = 'test'\G
     *************************** 1. row ***************************
                id: 1
       select_type: SIMPLE
             table: TABLES
              type: ALL
     possible_keys: NULL
               key: TABLE_SCHEMA
           key_len: NULL
               ref: NULL
              rows: NULL
             Extra: Using where; Open_frm_only; Scanned 1 database

No table name lookup value is provided, so the server must scan the
'test' database directory.  For the 'TABLE_NAME' and 'TABLE_TYPE'
columns, the 'SKIP_OPEN_TABLE' and 'OPEN_FRM_ONLY' optimizations apply,
respectively.  *note 'EXPLAIN': explain. reports 'OPEN_FRM_ONLY' because
it is more expensive.

     mysql> EXPLAIN SELECT B.TABLE_NAME
            FROM INFORMATION_SCHEMA.TABLES AS A, INFORMATION_SCHEMA.COLUMNS AS B
            WHERE A.TABLE_SCHEMA = 'test'
            AND A.TABLE_NAME = 't1'
            AND B.TABLE_NAME = A.TABLE_NAME\G
     *************************** 1. row ***************************
                id: 1
       select_type: SIMPLE
             table: A
              type: ALL
     possible_keys: NULL
               key: TABLE_SCHEMA,TABLE_NAME
           key_len: NULL
               ref: NULL
              rows: NULL
             Extra: Using where; Skip_open_table; Scanned 0 databases
     *************************** 2. row ***************************
                id: 1
       select_type: SIMPLE
             table: B
              type: ALL
     possible_keys: NULL
               key: NULL
           key_len: NULL
               ref: NULL
              rows: NULL
             Extra: Using where; Open_frm_only; Scanned all databases;
                    Using join buffer

For the first *note 'EXPLAIN': explain. output row: Constant database
and table lookup values enable the server to avoid directory scans for
'TABLES' values.  References to 'TABLES.TABLE_NAME' require no further
table files.

For the second *note 'EXPLAIN': explain. output row: All *note
'COLUMNS': columns-table. table values are 'OPEN_FRM_ONLY' lookups, so
'COLUMNS.TABLE_NAME' requires the '.frm' file to be opened.

     mysql> EXPLAIN SELECT * FROM INFORMATION_SCHEMA.COLLATIONS\G
     *************************** 1. row ***************************
                id: 1
       select_type: SIMPLE
             table: COLLATIONS
              type: ALL
     possible_keys: NULL
               key: NULL
           key_len: NULL
               ref: NULL
              rows: NULL
             Extra:

In this case, no optimizations apply because *note 'COLLATIONS':
collations-table. is not one of the 'INFORMATION_SCHEMA' tables for
which optimizations are available.


File: manual.info.tmp,  Node: data-change-optimization,  Next: permission-optimization,  Prev: information-schema-optimization,  Up: statement-optimization

8.2.4 Optimizing Data Change Statements
---------------------------------------

* Menu:

* insert-optimization::          Optimizing INSERT Statements
* update-optimization::          Optimizing UPDATE Statements
* delete-optimization::          Optimizing DELETE Statements

This section explains how to speed up data change statements: *note
'INSERT': insert, *note 'UPDATE': update, and *note 'DELETE': delete.
Traditional OLTP applications and modern web applications typically do
many small data change operations, where concurrency is vital.  Data
analysis and reporting applications typically run data change operations
that affect many rows at once, where the main considerations is the I/O
to write large amounts of data and keep indexes up-to-date.  For
inserting and updating large volumes of data (known in the industry as
ETL, for 'extract-transform-load'), sometimes you use other SQL
statements or external commands, that mimic the effects of *note
'INSERT': insert, *note 'UPDATE': update, and *note 'DELETE': delete.
statements.


File: manual.info.tmp,  Node: insert-optimization,  Next: update-optimization,  Prev: data-change-optimization,  Up: data-change-optimization

8.2.4.1 Optimizing INSERT Statements
....................................

To optimize insert speed, combine many small operations into a single
large operation.  Ideally, you make a single connection, send the data
for many new rows at once, and delay all index updates and consistency
checking until the very end.

The time required for inserting a row is determined by the following
factors, where the numbers indicate approximate proportions:

   * Connecting: (3)

   * Sending query to server: (2)

   * Parsing query: (2)

   * Inserting row: (1 x size of row)

   * Inserting indexes: (1 x number of indexes)

   * Closing: (1)

This does not take into consideration the initial overhead to open
tables, which is done once for each concurrently running query.

The size of the table slows down the insertion of indexes by log N,
assuming B-tree indexes.

You can use the following methods to speed up inserts:

   * If you are inserting many rows from the same client at the same
     time, use *note 'INSERT': insert. statements with multiple 'VALUES'
     lists to insert several rows at a time.  This is considerably
     faster (many times faster in some cases) than using separate
     single-row *note 'INSERT': insert. statements.  If you are adding
     data to a nonempty table, you can tune the
     'bulk_insert_buffer_size' variable to make data insertion even
     faster.  See *note server-system-variables::.

   * When loading a table from a text file, use *note 'LOAD DATA':
     load-data.  This is usually 20 times faster than using *note
     'INSERT': insert. statements.  See *note load-data::.

   * Take advantage of the fact that columns have default values.
     Insert values explicitly only when the value to be inserted differs
     from the default.  This reduces the parsing that MySQL must do and
     improves the insert speed.

   * See *note optimizing-innodb-bulk-data-loading:: for tips specific
     to 'InnoDB' tables.

   * See *note optimizing-myisam-bulk-data-loading:: for tips specific
     to 'MyISAM' tables.


File: manual.info.tmp,  Node: update-optimization,  Next: delete-optimization,  Prev: insert-optimization,  Up: data-change-optimization

8.2.4.2 Optimizing UPDATE Statements
....................................

An update statement is optimized like a *note 'SELECT': select. query
with the additional overhead of a write.  The speed of the write depends
on the amount of data being updated and the number of indexes that are
updated.  Indexes that are not changed do not get updated.

Another way to get fast updates is to delay updates and then do many
updates in a row later.  Performing multiple updates together is much
quicker than doing one at a time if you lock the table.

For a 'MyISAM' table that uses dynamic row format, updating a row to a
longer total length may split the row.  If you do this often, it is very
important to use *note 'OPTIMIZE TABLE': optimize-table. occasionally.
See *note optimize-table::.


File: manual.info.tmp,  Node: delete-optimization,  Prev: update-optimization,  Up: data-change-optimization

8.2.4.3 Optimizing DELETE Statements
....................................

The time required to delete individual rows in a 'MyISAM' table is
exactly proportional to the number of indexes.  To delete rows more
quickly, you can increase the size of the key cache by increasing the
'key_buffer_size' system variable.  See *note server-configuration::.

To delete all rows from a 'MyISAM' table, 'TRUNCATE TABLE TBL_NAME' is
faster than 'DELETE FROM TBL_NAME'.  Truncate operations are not
transaction-safe; an error occurs when attempting one in the course of
an active transaction or active table lock.  See *note truncate-table::.


File: manual.info.tmp,  Node: permission-optimization,  Next: miscellaneous-optimization-tips,  Prev: data-change-optimization,  Up: statement-optimization

8.2.5 Optimizing Database Privileges
------------------------------------

The more complex your privilege setup, the more overhead applies to all
SQL statements.  Simplifying the privileges established by *note
'GRANT': grant. statements enables MySQL to reduce permission-checking
overhead when clients execute statements.  For example, if you do not
grant any table-level or column-level privileges, the server need not
ever check the contents of the 'tables_priv' and 'columns_priv' tables.
Similarly, if you place no resource limits on any accounts, the server
does not have to perform resource counting.  If you have a very high
statement-processing load, consider using a simplified grant structure
to reduce permission-checking overhead.


File: manual.info.tmp,  Node: miscellaneous-optimization-tips,  Prev: permission-optimization,  Up: statement-optimization

8.2.6 Other Optimization Tips
-----------------------------

This section lists a number of miscellaneous tips for improving query
processing speed:

   * If your application makes several database requests to perform
     related updates, combining the statements into a stored routine can
     help performance.  Similarly, if your application computes a single
     result based on several column values or large volumes of data,
     combining the computation into a UDF (user-defined function) can
     help performance.  The resulting fast database operations are then
     available to be reused by other queries, applications, and even
     code written in different programming languages.  See *note
     stored-routines:: and *note adding-functions:: for more
     information.

   * To fix any compression issues that occur with 'ARCHIVE' tables, use
     *note 'OPTIMIZE TABLE': optimize-table.  See *note
     archive-storage-engine::.

   * If possible, classify reports as 'live' or as 'statistical', where
     data needed for statistical reports is created only from summary
     tables that are generated periodically from the live data.

   * If you have data that does not conform well to a rows-and-columns
     table structure, you can pack and store data into a *note 'BLOB':
     blob. column.  In this case, you must provide code in your
     application to pack and unpack information, but this might save I/O
     operations to read and write the sets of related values.

   * With Web servers, store images and other binary assets as files,
     with the path name stored in the database rather than the file
     itself.  Most Web servers are better at caching files than database
     contents, so using files is generally faster.  (Although you must
     handle backups and storage issues yourself in this case.)

   * If you need really high speed, look at the low-level MySQL
     interfaces.  For example, by accessing the MySQL 'InnoDB' or
     'MyISAM' storage engine directly, you could get a substantial speed
     increase compared to using the SQL interface.

   * Replication can provide a performance benefit for some operations.
     You can distribute client retrievals among replication servers to
     split up the load.  To avoid slowing down the master while making
     backups, you can make backups using a slave server.  See *note
     replication::.


File: manual.info.tmp,  Node: optimization-indexes,  Next: optimizing-database-structure,  Prev: statement-optimization,  Up: optimization

8.3 Optimization and Indexes
============================

* Menu:

* mysql-indexes::                How MySQL Uses Indexes
* primary-key-optimization::     Primary Key Optimization
* foreign-key-optimization::     Foreign Key Optimization
* column-indexes::               Column Indexes
* multiple-column-indexes::      Multiple-Column Indexes
* verifying-index-usage::        Verifying Index Usage
* index-statistics::             InnoDB and MyISAM Index Statistics Collection
* index-btree-hash::             Comparison of B-Tree and Hash Indexes
* timestamp-lookups::            Indexed Lookups from TIMESTAMP Columns

The best way to improve the performance of *note 'SELECT': select.
operations is to create indexes on one or more of the columns that are
tested in the query.  The index entries act like pointers to the table
rows, allowing the query to quickly determine which rows match a
condition in the 'WHERE' clause, and retrieve the other column values
for those rows.  All MySQL data types can be indexed.

Although it can be tempting to create an indexes for every possible
column used in a query, unnecessary indexes waste space and waste time
for MySQL to determine which indexes to use.  Indexes also add to the
cost of inserts, updates, and deletes because each index must be
updated.  You must find the right balance to achieve fast queries using
the optimal set of indexes.


File: manual.info.tmp,  Node: mysql-indexes,  Next: primary-key-optimization,  Prev: optimization-indexes,  Up: optimization-indexes

8.3.1 How MySQL Uses Indexes
----------------------------

Indexes are used to find rows with specific column values quickly.
Without an index, MySQL must begin with the first row and then read
through the entire table to find the relevant rows.  The larger the
table, the more this costs.  If the table has an index for the columns
in question, MySQL can quickly determine the position to seek to in the
middle of the data file without having to look at all the data.  This is
much faster than reading every row sequentially.

Most MySQL indexes ('PRIMARY KEY', 'UNIQUE', 'INDEX', and 'FULLTEXT')
are stored in B-trees.  Exceptions are that indexes on spatial data
types use R-trees, and that 'MEMORY' tables also support hash indexes.

In general, indexes are used as described in the following discussion.
Characteristics specific to hash indexes (as used in 'MEMORY' tables)
are described in *note index-btree-hash::.

MySQL uses indexes for these operations:

   * To find the rows matching a 'WHERE' clause quickly.

   * To eliminate rows from consideration.  If there is a choice between
     multiple indexes, MySQL normally uses the index that finds the
     smallest number of rows (the most selective index).

   * 
     If the table has a multiple-column index, any leftmost prefix of
     the index can be used by the optimizer to look up rows.  For
     example, if you have a three-column index on '(col1, col2, col3)',
     you have indexed search capabilities on '(col1)', '(col1, col2)',
     and '(col1, col2, col3)'.  For more information, see *note
     multiple-column-indexes::.

   * To retrieve rows from other tables when performing joins.  MySQL
     can use indexes on columns more efficiently if they are declared as
     the same type and size.  In this context, *note 'VARCHAR': char.
     and *note 'CHAR': char. are considered the same if they are
     declared as the same size.  For example, 'VARCHAR(10)' and
     'CHAR(10)' are the same size, but 'VARCHAR(10)' and 'CHAR(15)' are
     not.

     For comparisons between nonbinary string columns, both columns
     should use the same character set.  For example, comparing a 'utf8'
     column with a 'latin1' column precludes use of an index.

     Comparison of dissimilar columns (comparing a string column to a
     temporal or numeric column, for example) may prevent use of indexes
     if values cannot be compared directly without conversion.  For a
     given value such as '1' in the numeric column, it might compare
     equal to any number of values in the string column such as ''1'',
     '' 1'', ''00001'', or ''01.e1''.  This rules out use of any indexes
     for the string column.

   * To find the 'MIN()' or 'MAX()' value for a specific indexed column
     KEY_COL.  This is optimized by a preprocessor that checks whether
     you are using 'WHERE KEY_PART_N = CONSTANT' on all key parts that
     occur before KEY_COL in the index.  In this case, MySQL does a
     single key lookup for each 'MIN()' or 'MAX()' expression and
     replaces it with a constant.  If all expressions are replaced with
     constants, the query returns at once.  For example:

          SELECT MIN(KEY_PART2),MAX(KEY_PART2)
            FROM TBL_NAME WHERE KEY_PART1=10;

   * To sort or group a table if the sorting or grouping is done on a
     leftmost prefix of a usable index (for example, 'ORDER BY
     KEY_PART1, KEY_PART2').  If all key parts are followed by 'DESC',
     the key is read in reverse order.  See *note
     order-by-optimization::, and *note group-by-optimization::.

   * In some cases, a query can be optimized to retrieve values without
     consulting the data rows.  (An index that provides all the
     necessary results for a query is called a covering index.)  If a
     query uses from a table only columns that are included in some
     index, the selected values can be retrieved from the index tree for
     greater speed:

          SELECT KEY_PART3 FROM TBL_NAME
            WHERE KEY_PART1=1

Indexes are less important for queries on small tables, or big tables
where report queries process most or all of the rows.  When a query
needs to access most of the rows, reading sequentially is faster than
working through an index.  Sequential reads minimize disk seeks, even if
not all the rows are needed for the query.  See *note
table-scan-avoidance:: for details.


File: manual.info.tmp,  Node: primary-key-optimization,  Next: foreign-key-optimization,  Prev: mysql-indexes,  Up: optimization-indexes

8.3.2 Primary Key Optimization
------------------------------

The primary key for a table represents the column or set of columns that
you use in your most vital queries.  It has an associated index, for
fast query performance.  Query performance benefits from the 'NOT NULL'
optimization, because it cannot include any 'NULL' values.  With the
'InnoDB' storage engine, the table data is physically organized to do
ultra-fast lookups and sorts based on the primary key column or columns.

If your table is big and important, but does not have an obvious column
or set of columns to use as a primary key, you might create a separate
column with auto-increment values to use as the primary key.  These
unique IDs can serve as pointers to corresponding rows in other tables
when you join tables using foreign keys.


File: manual.info.tmp,  Node: foreign-key-optimization,  Next: column-indexes,  Prev: primary-key-optimization,  Up: optimization-indexes

8.3.3 Foreign Key Optimization
------------------------------

If a table has many columns, and you query many different combinations
of columns, it might be efficient to split the less-frequently used data
into separate tables with a few columns each, and relate them back to
the main table by duplicating the numeric ID column from the main table.
That way, each small table can have a primary key for fast lookups of
its data, and you can query just the set of columns that you need using
a join operation.  Depending on how the data is distributed, the queries
might perform less I/O and take up less cache memory because the
relevant columns are packed together on disk.  (To maximize performance,
queries try to read as few data blocks as possible from disk; tables
with only a few columns can fit more rows in each data block.)


File: manual.info.tmp,  Node: column-indexes,  Next: multiple-column-indexes,  Prev: foreign-key-optimization,  Up: optimization-indexes

8.3.4 Column Indexes
--------------------

The most common type of index involves a single column, storing copies
of the values from that column in a data structure, allowing fast
lookups for the rows with the corresponding column values.  The B-tree
data structure lets the index quickly find a specific value, a set of
values, or a range of values, corresponding to operators such as '=',
'>', '<=', 'BETWEEN', 'IN', and so on, in a 'WHERE' clause.

The maximum number of indexes per table and the maximum index length is
defined per storage engine.  See *note innodb-storage-engine::, and
*note storage-engines::.  All storage engines support at least 16
indexes per table and a total index length of at least 256 bytes.  Most
storage engines have higher limits.

For additional information about column indexes, see *note
create-index::.

   * *note column-indexes-prefix::

   * *note column-indexes-fulltext::

   * *note column-indexes-spatial::

   * *note column-indexes-memory-storage-engine::

*Index Prefixes*

With 'COL_NAME(N)' syntax in an index specification for a string column,
you can create an index that uses only the first N characters of the
column.  Indexing only a prefix of column values in this way can make
the index file much smaller.  When you index a *note 'BLOB': blob. or
*note 'TEXT': blob. column, you _must_ specify a prefix length for the
index.  For example:

     CREATE TABLE test (blob_col BLOB, INDEX(blob_col(10)));

Prefixes can be up to 1000 bytes long (767 bytes for 'InnoDB' tables,
unless you have 'innodb_large_prefix' set).

*Note*:

Prefix limits are measured in bytes, whereas the prefix length in *note
'CREATE TABLE': create-table, *note 'ALTER TABLE': alter-table, and
*note 'CREATE INDEX': create-index. statements is interpreted as number
of characters for nonbinary string types (*note 'CHAR': char, *note
'VARCHAR': char, *note 'TEXT': blob.) and number of bytes for binary
string types (*note 'BINARY': binary-varbinary, *note 'VARBINARY':
binary-varbinary, *note 'BLOB': blob.).  Take this into account when
specifying a prefix length for a nonbinary string column that uses a
multibyte character set.

If a search term exceeds the index prefix length, the index is used to
exclude non-matching rows, and the remaining rows are examined for
possible matches.

For additional information about index prefixes, see *note
create-index::.

*FULLTEXT Indexes*

'FULLTEXT' indexes are used for full-text searches.  Only the 'MyISAM'
storage engine supports 'FULLTEXT' indexes and only for *note 'CHAR':
char, *note 'VARCHAR': char, and *note 'TEXT': blob. columns.  Indexing
always takes place over the entire column and column prefix indexing is
not supported.  For details, see *note fulltext-search::.

For queries that contain full-text expressions, MySQL evaluates those
expressions during the optimization phase of query execution.  The
optimizer does not just look at full-text expressions and make
estimates, it actually evaluates them in the process of developing an
execution plan.

An implication of this behavior is that *note 'EXPLAIN': explain. for
full-text queries is typically slower than for non-full-text queries for
which no expression evaluation occurs during the optimization phase.

*note 'EXPLAIN': explain. for full-text queries may show 'Select tables
optimized away' in the 'Extra' column due to matching occurring during
optimization; in this case, no table access need occur during later
execution.

*Spatial Indexes*

You can create indexes on spatial data types.  Only 'MyISAM' supports
R-tree indexes on spatial types.  Other storage engines use B-trees for
indexing spatial types (except for 'ARCHIVE', which does not support
spatial type indexing).

*Indexes in the MEMORY Storage Engine*

The 'MEMORY' storage engine uses 'HASH' indexes by default, but also
supports 'BTREE' indexes.


File: manual.info.tmp,  Node: multiple-column-indexes,  Next: verifying-index-usage,  Prev: column-indexes,  Up: optimization-indexes

8.3.5 Multiple-Column Indexes
-----------------------------

MySQL can create composite indexes (that is, indexes on multiple
columns).  An index may consist of up to 16 columns.  For certain data
types, you can index a prefix of the column (see *note
column-indexes::).

MySQL can use multiple-column indexes for queries that test all the
columns in the index, or queries that test just the first column, the
first two columns, the first three columns, and so on.  If you specify
the columns in the right order in the index definition, a single
composite index can speed up several kinds of queries on the same table.

A multiple-column index can be considered a sorted array, the rows of
which contain values that are created by concatenating the values of the
indexed columns.

*Note*:

As an alternative to a composite index, you can introduce a column that
is 'hashed' based on information from other columns.  If this column is
short, reasonably unique, and indexed, it might be faster than a 'wide'
index on many columns.  In MySQL, it is very easy to use this extra
column:

     SELECT * FROM TBL_NAME
       WHERE HASH_COL=MD5(CONCAT(VAL1,VAL2))
       AND COL1=VAL1 AND COL2=VAL2;

Suppose that a table has the following specification:

     CREATE TABLE test (
         id         INT NOT NULL,
         last_name  CHAR(30) NOT NULL,
         first_name CHAR(30) NOT NULL,
         PRIMARY KEY (id),
         INDEX name (last_name,first_name)
     );

The 'name' index is an index over the 'last_name' and 'first_name'
columns.  The index can be used for lookups in queries that specify
values in a known range for combinations of 'last_name' and 'first_name'
values.  It can also be used for queries that specify just a 'last_name'
value because that column is a leftmost prefix of the index (as
described later in this section).  Therefore, the 'name' index is used
for lookups in the following queries:

     SELECT * FROM test WHERE last_name='Smith';

     SELECT * FROM test
       WHERE last_name='Smith' AND first_name='John';

     SELECT * FROM test
       WHERE last_name='Smith'
       AND (first_name='John' OR first_name='Jon');

     SELECT * FROM test
       WHERE last_name='Smith'
       AND first_name >='M' AND first_name < 'N';

However, the 'name' index is _not_ used for lookups in the following
queries:

     SELECT * FROM test WHERE first_name='John';

     SELECT * FROM test
       WHERE last_name='Smith' OR first_name='John';

Suppose that you issue the following *note 'SELECT': select. statement:

     SELECT * FROM TBL_NAME
       WHERE col1=VAL1 AND col2=VAL2;

If a multiple-column index exists on 'col1' and 'col2', the appropriate
rows can be fetched directly.  If separate single-column indexes exist
on 'col1' and 'col2', the optimizer attempts to use the Index Merge
optimization (see *note index-merge-optimization::), or attempts to find
the most restrictive index by deciding which index excludes more rows
and using that index to fetch the rows.

If the table has a multiple-column index, any leftmost prefix of the
index can be used by the optimizer to look up rows.  For example, if you
have a three-column index on '(col1, col2, col3)', you have indexed
search capabilities on '(col1)', '(col1, col2)', and '(col1, col2,
col3)'.

MySQL cannot use the index to perform lookups if the columns do not form
a leftmost prefix of the index.  Suppose that you have the *note
'SELECT': select. statements shown here:

     SELECT * FROM TBL_NAME WHERE col1=VAL1;
     SELECT * FROM TBL_NAME WHERE col1=VAL1 AND col2=VAL2;

     SELECT * FROM TBL_NAME WHERE col2=VAL2;
     SELECT * FROM TBL_NAME WHERE col2=VAL2 AND col3=VAL3;

If an index exists on '(col1, col2, col3)', only the first two queries
use the index.  The third and fourth queries do involve indexed columns,
but do not use an index to perform lookups because '(col2)' and '(col2,
col3)' are not leftmost prefixes of '(col1, col2, col3)'.


File: manual.info.tmp,  Node: verifying-index-usage,  Next: index-statistics,  Prev: multiple-column-indexes,  Up: optimization-indexes

8.3.6 Verifying Index Usage
---------------------------

Always check whether all your queries really use the indexes that you
have created in the tables.  Use the *note 'EXPLAIN': explain.
statement, as described in *note using-explain::.


File: manual.info.tmp,  Node: index-statistics,  Next: index-btree-hash,  Prev: verifying-index-usage,  Up: optimization-indexes

8.3.7 InnoDB and MyISAM Index Statistics Collection
---------------------------------------------------

Storage engines collect statistics about tables for use by the
optimizer.  Table statistics are based on value groups, where a value
group is a set of rows with the same key prefix value.  For optimizer
purposes, an important statistic is the average value group size.

MySQL uses the average value group size in the following ways:

   * To estimate how many rows must be read for each 'ref' access

   * To estimate how many rows a partial join will produce; that is, the
     number of rows that an operation of this form will produce:

          (...) JOIN TBL_NAME ON TBL_NAME.KEY = EXPR

As the average value group size for an index increases, the index is
less useful for those two purposes because the average number of rows
per lookup increases: For the index to be good for optimization
purposes, it is best that each index value target a small number of rows
in the table.  When a given index value yields a large number of rows,
the index is less useful and MySQL is less likely to use it.

The average value group size is related to table cardinality, which is
the number of value groups.  The *note 'SHOW INDEX': show-index.
statement displays a cardinality value based on N/S, where N is the
number of rows in the table and S is the average value group size.  That
ratio yields an approximate number of value groups in the table.

For a join based on the '<=>' comparison operator, 'NULL' is not treated
differently from any other value: 'NULL <=> NULL', just as 'N <=> N' for
any other N.

However, for a join based on the '=' operator, 'NULL' is different from
non-'NULL' values: 'EXPR1 = EXPR2' is not true when EXPR1 or EXPR2 (or
both) are 'NULL'.  This affects 'ref' accesses for comparisons of the
form 'TBL_NAME.KEY = EXPR': MySQL will not access the table if the
current value of EXPR is 'NULL', because the comparison cannot be true.

For '=' comparisons, it does not matter how many 'NULL' values are in
the table.  For optimization purposes, the relevant value is the average
size of the non-'NULL' value groups.  However, MySQL does not currently
enable that average size to be collected or used.

For 'InnoDB' and 'MyISAM' tables, you have some control over collection
of table statistics by means of the 'innodb_stats_method' and
'myisam_stats_method' system variables, respectively.  These variables
have three possible values, which differ as follows:

   * When the variable is set to 'nulls_equal', all 'NULL' values are
     treated as identical (that is, they all form a single value group).

     If the 'NULL' value group size is much higher than the average
     non-'NULL' value group size, this method skews the average value
     group size upward.  This makes index appear to the optimizer to be
     less useful than it really is for joins that look for non-'NULL'
     values.  Consequently, the 'nulls_equal' method may cause the
     optimizer not to use the index for 'ref' accesses when it should.

   * When the variable is set to 'nulls_unequal', 'NULL' values are not
     considered the same.  Instead, each 'NULL' value forms a separate
     value group of size 1.

     If you have many 'NULL' values, this method skews the average value
     group size downward.  If the average non-'NULL' value group size is
     large, counting 'NULL' values each as a group of size 1 causes the
     optimizer to overestimate the value of the index for joins that
     look for non-'NULL' values.  Consequently, the 'nulls_unequal'
     method may cause the optimizer to use this index for 'ref' lookups
     when other methods may be better.

   * When the variable is set to 'nulls_ignored', 'NULL' values are
     ignored.

If you tend to use many joins that use '<=>' rather than '=', 'NULL'
values are not special in comparisons and one 'NULL' is equal to
another.  In this case, 'nulls_equal' is the appropriate statistics
method.

The 'innodb_stats_method' system variable has a global value; the
'myisam_stats_method' system variable has both global and session
values.  Setting the global value affects statistics collection for
tables from the corresponding storage engine.  Setting the session value
affects statistics collection only for the current client connection.
This means that you can force a table's statistics to be regenerated
with a given method without affecting other clients by setting the
session value of 'myisam_stats_method'.

To regenerate 'MyISAM' table statistics, you can use any of the
following methods:

   * Execute *note 'myisamchk --stats_method=METHOD_NAME --analyze':
     myisamchk.

   * Change the table to cause its statistics to go out of date (for
     example, insert a row and then delete it), and then set
     'myisam_stats_method' and issue an *note 'ANALYZE TABLE':
     analyze-table. statement

Some caveats regarding the use of 'innodb_stats_method' and
'myisam_stats_method':

   * You can force table statistics to be collected explicitly, as just
     described.  However, MySQL may also collect statistics
     automatically.  For example, if during the course of executing
     statements for a table, some of those statements modify the table,
     MySQL may collect statistics.  (This may occur for bulk inserts or
     deletes, or some *note 'ALTER TABLE': alter-table. statements, for
     example.)  If this happens, the statistics are collected using
     whatever value 'innodb_stats_method' or 'myisam_stats_method' has
     at the time.  Thus, if you collect statistics using one method, but
     the system variable is set to the other method when a table's
     statistics are collected automatically later, the other method will
     be used.

   * There is no way to tell which method was used to generate
     statistics for a given table.

   * These variables apply only to 'InnoDB' and 'MyISAM' tables.  Other
     storage engines have only one method for collecting table
     statistics.  Usually it is closer to the 'nulls_equal' method.


File: manual.info.tmp,  Node: index-btree-hash,  Next: timestamp-lookups,  Prev: index-statistics,  Up: optimization-indexes

8.3.8 Comparison of B-Tree and Hash Indexes
-------------------------------------------

Understanding the B-tree and hash data structures can help predict how
different queries perform on different storage engines that use these
data structures in their indexes, particularly for the 'MEMORY' storage
engine that lets you choose B-tree or hash indexes.

   * *note btree-index-characteristics::

   * *note hash-index-characteristics::

*B-Tree Index Characteristics*

A B-tree index can be used for column comparisons in expressions that
use the '=', '>', '>=', '<', '<=', or 'BETWEEN' operators.  The index
also can be used for 'LIKE' comparisons if the argument to 'LIKE' is a
constant string that does not start with a wildcard character.  For
example, the following *note 'SELECT': select. statements use indexes:

     SELECT * FROM TBL_NAME WHERE KEY_COL LIKE 'Patrick%';
     SELECT * FROM TBL_NAME WHERE KEY_COL LIKE 'Pat%_ck%';

In the first statement, only rows with ''Patrick' <= KEY_COL <
'Patricl'' are considered.  In the second statement, only rows with
''Pat' <= KEY_COL < 'Pau'' are considered.

The following *note 'SELECT': select. statements do not use indexes:

     SELECT * FROM TBL_NAME WHERE KEY_COL LIKE '%Patrick%';
     SELECT * FROM TBL_NAME WHERE KEY_COL LIKE OTHER_COL;

In the first statement, the 'LIKE' value begins with a wildcard
character.  In the second statement, the 'LIKE' value is not a constant.

If you use '... LIKE '%STRING%'' and STRING is longer than three
characters, MySQL uses the _Turbo Boyer-Moore algorithm_ to initialize
the pattern for the string and then uses this pattern to perform the
search more quickly.

A search using 'COL_NAME IS NULL' employs indexes if COL_NAME is
indexed.

Any index that does not span all 'AND' levels in the 'WHERE' clause is
not used to optimize the query.  In other words, to be able to use an
index, a prefix of the index must be used in every 'AND' group.

The following 'WHERE' clauses use indexes:

     ... WHERE INDEX_PART1=1 AND INDEX_PART2=2 AND OTHER_COLUMN=3

         /* INDEX = 1 OR INDEX = 2 */
     ... WHERE INDEX=1 OR A=10 AND INDEX=2

         /* optimized like "INDEX_PART1='hello'" */
     ... WHERE INDEX_PART1='hello' AND INDEX_PART3=5

         /* Can use index on INDEX1 but not on INDEX2 or INDEX3 */
     ... WHERE INDEX1=1 AND INDEX2=2 OR INDEX1=3 AND INDEX3=3;

These 'WHERE' clauses do _not_ use indexes:

         /* INDEX_PART1 is not used */
     ... WHERE INDEX_PART2=1 AND INDEX_PART3=2

         /*  Index is not used in both parts of the WHERE clause  */
     ... WHERE INDEX=1 OR A=10

         /* No index spans all rows  */
     ... WHERE INDEX_PART1=1 OR INDEX_PART2=10

Sometimes MySQL does not use an index, even if one is available.  One
circumstance under which this occurs is when the optimizer estimates
that using the index would require MySQL to access a very large
percentage of the rows in the table.  (In this case, a table scan is
likely to be much faster because it requires fewer seeks.)  However, if
such a query uses 'LIMIT' to retrieve only some of the rows, MySQL uses
an index anyway, because it can much more quickly find the few rows to
return in the result.

*Hash Index Characteristics*

Hash indexes have somewhat different characteristics from those just
discussed:

   * They are used only for equality comparisons that use the '=' or
     '<=>' operators (but are _very_ fast).  They are not used for
     comparison operators such as '<' that find a range of values.
     Systems that rely on this type of single-value lookup are known as
     'key-value stores'; to use MySQL for such applications, use hash
     indexes wherever possible.

   * The optimizer cannot use a hash index to speed up 'ORDER BY'
     operations.  (This type of index cannot be used to search for the
     next entry in order.)

   * MySQL cannot determine approximately how many rows there are
     between two values (this is used by the range optimizer to decide
     which index to use).  This may affect some queries if you change a
     'MyISAM' or 'InnoDB' table to a hash-indexed 'MEMORY' table.

   * Only whole keys can be used to search for a row.  (With a B-tree
     index, any leftmost prefix of the key can be used to find rows.)


File: manual.info.tmp,  Node: timestamp-lookups,  Prev: index-btree-hash,  Up: optimization-indexes

8.3.9 Indexed Lookups from TIMESTAMP Columns
--------------------------------------------

Temporal values are stored in *note 'TIMESTAMP': datetime. columns as
UTC values, and values inserted into and retrieved from *note
'TIMESTAMP': datetime. columns are converted between the session time
zone and UTC. (This is the same type of conversion performed by the
'CONVERT_TZ()' function.  If the session time zone is UTC, there is
effectively no time zone conversion.)

Due to conventions for local time zone changes such as Daylight Saving
Time (DST), conversions between UTC and non-UTC time zones are not
one-to-one in both directions.  UTC values that are distinct may not be
distinct in another time zone.  The following example shows distinct UTC
values that become identical in a non-UTC time zone:

     mysql> CREATE TABLE tstable (ts TIMESTAMP);
     mysql> SET time_zone = 'UTC'; -- insert UTC values
     mysql> INSERT INTO tstable VALUES
            ('2018-10-28 00:30:00'),
            ('2018-10-28 01:30:00');
     mysql> SELECT ts FROM tstable;
     +---------------------+
     | ts                  |
     +---------------------+
     | 2018-10-28 00:30:00 |
     | 2018-10-28 01:30:00 |
     +---------------------+
     mysql> SET time_zone = 'MET'; -- retrieve non-UTC values
     mysql> SELECT ts FROM tstable;
     +---------------------+
     | ts                  |
     +---------------------+
     | 2018-10-28 02:30:00 |
     | 2018-10-28 02:30:00 |
     +---------------------+

*Note*:

To use named time zones such as ''MET'' or ''Europe/Amsterdam'', the
time zone tables must be properly set up.  For instructions, see *note
time-zone-support::.

You can see that the two distinct UTC values are the same when converted
to the ''MET'' time zone.  This phenomenon can lead to different results
for a given *note 'TIMESTAMP': datetime. column query, depending on
whether the optimizer uses an index to execute the query.

Suppose that a query selects values from the table shown earlier using a
'WHERE' clause to search the 'ts' column for a single specific value
such as a user-provided timestamp literal:

     SELECT ts FROM tstable
     WHERE ts = 'LITERAL';

Suppose further that the query executes under these conditions:

   * The session time zone is not UTC and has a DST shift.  For example:

          SET time_zone = 'MET';

   * Unique UTC values stored in the *note 'TIMESTAMP': datetime. column
     are not unique in the session time zone due to DST shifts.  (The
     example shown earlier illustrates how this can occur.)

   * The query specifies a search value that is within the hour of entry
     into DST in the session time zone.

Under those conditions, the comparison in the 'WHERE' clause occurs in
different ways for nonindexed and indexed lookups and leads to different
results:

   * If there is no index or the optimizer cannot use it, comparisons
     occur in the session time zone.  The optimizer performs a table
     scan in which it retrieves each 'ts' column value, converts it from
     UTC to the session time zone, and compares it to the search value
     (also interpreted in the session time zone):

          mysql> SELECT ts FROM tstable
                 WHERE ts = '2018-10-28 02:30:00';
          +---------------------+
          | ts                  |
          +---------------------+
          | 2018-10-28 02:30:00 |
          | 2018-10-28 02:30:00 |
          +---------------------+

     Because the stored 'ts' values are converted to the session time
     zone, it is possible for the query to return two timestamp values
     that are distinct as UTC values but equal in the session time zone:
     One value that occurs before the DST shift when clocks are changed,
     and one value that was occurs after the DST shift.

   * If there is a usable index, comparisons occur in UTC. The optimizer
     performs an index scan, first converting the search value from the
     session time zone to UTC, then comparing the result to the UTC
     index entries:

          mysql> ALTER TABLE tstable ADD INDEX (ts);
          mysql> SELECT ts FROM tstable
                 WHERE ts = '2018-10-28 02:30:00';
          +---------------------+
          | ts                  |
          +---------------------+
          | 2018-10-28 02:30:00 |
          +---------------------+

     In this case, the (converted) search value is matched only to index
     entries, and because the index entries for the distinct stored UTC
     values are also distinct, the search value can match only one of
     them.

Due to different optimizer operation for nonindexed and indexed lookups,
the query produces different results in each case.  The result from the
nonindexed lookup returns all values that match in the session time
zone.  The indexed lookup cannot do so:

   * It is performed within the storage engine, which knows only about
     UTC values.

   * For the two distinct session time zone values that map to the same
     UTC value, the indexed lookup matches only the corresponding UTC
     index entry and returns only a single row.

In the preceding discussion, the data set stored in 'tstable' happens to
consist of distinct UTC values.  In such cases, all index-using queries
of the form shown match at most one index entry.

If the index is not 'UNIQUE', it is possible for the table (and the
index) to store multiple instances of a given UTC value.  For example,
the 'ts' column might contain multiple instances of the UTC value
''2018-10-28 00:30:00''.  In this case, the index-using query would
return each of them (converted to the MET value ''2018-10-28 02:30:00''
in the result set).  It remains true that index-using queries match the
converted search value to a single value in the UTC index entries,
rather than matching multiple UTC values that convert to the search
value in the session time zone.

If it is important to return all 'ts' values that match in the session
time zone, the workaround is to suppress use of the index with an
'IGNORE INDEX' hint:

     mysql> SELECT ts FROM tstable
            IGNORE INDEX (ts)
            WHERE ts = '2018-10-28 02:30:00';
     +---------------------+
     | ts                  |
     +---------------------+
     | 2018-10-28 02:30:00 |
     | 2018-10-28 02:30:00 |
     +---------------------+

The same lack of one-to-one mapping for time zone conversions in both
directions occurs in other contexts as well, such as conversions
performed with the 'FROM_UNIXTIME()' and 'UNIX_TIMESTAMP()' functions.
See *note date-and-time-functions::.


File: manual.info.tmp,  Node: optimizing-database-structure,  Next: optimizing-innodb,  Prev: optimization-indexes,  Up: optimization

8.4 Optimizing Database Structure
=================================

* Menu:

* data-size::                    Optimizing Data Size
* optimize-data-types::          Optimizing MySQL Data Types
* optimize-multi-tables::        Optimizing for Many Tables
* internal-temporary-tables::    Internal Temporary Table Use in MySQL
* database-count-limit::         Limits on Number of Databases and Tables
* table-size-limit::             Limits on Table Size
* column-count-limit::           Limits on Table Column Count and Row Size

In your role as a database designer, look for the most efficient way to
organize your schemas, tables, and columns.  As when tuning application
code, you minimize I/O, keep related items together, and plan ahead so
that performance stays high as the data volume increases.  Starting with
an efficient database design makes it easier for team members to write
high-performing application code, and makes the database likely to
endure as applications evolve and are rewritten.


File: manual.info.tmp,  Node: data-size,  Next: optimize-data-types,  Prev: optimizing-database-structure,  Up: optimizing-database-structure

8.4.1 Optimizing Data Size
--------------------------

Design your tables to minimize their space on the disk.  This can result
in huge improvements by reducing the amount of data written to and read
from disk.  Smaller tables normally require less main memory while their
contents are being actively processed during query execution.  Any space
reduction for table data also results in smaller indexes that can be
processed faster.

MySQL supports many different storage engines (table types) and row
formats.  For each table, you can decide which storage and indexing
method to use.  Choosing the proper table format for your application
can give you a big performance gain.  See *note innodb-storage-engine::,
and *note storage-engines::.

You can get better performance for a table and minimize storage space by
using the techniques listed here:

   * *note data-size-table-columns::

   * *note data-size-row-format::

   * *note data-size-indexes::

   * *note data-size-joins::

   * *note data-size-normalization::

*Table Columns*

   * Use the most efficient (smallest) data types possible.  MySQL has
     many specialized types that save disk space and memory.  For
     example, use the smaller integer types if possible to get smaller
     tables.  *note 'MEDIUMINT': integer-types. is often a better choice
     than *note 'INT': integer-types. because a *note 'MEDIUMINT':
     integer-types. column uses 25% less space.

   * Declare columns to be 'NOT NULL' if possible.  It makes SQL
     operations faster, by enabling better use of indexes and
     eliminating overhead for testing whether each value is 'NULL'.  You
     also save some storage space, one bit per column.  If you really
     need 'NULL' values in your tables, use them.  Just avoid the
     default setting that allows 'NULL' values in every column.

*Row Format*

   * In MySQL 5.5, 'InnoDB' tables use the 'COMPACT' row storage format
     ('ROW_FORMAT=COMPACT') by default.  Older versions of MySQL use the
     'REDUNDANT' row format ('ROW_FORMAT=REDUNDANT').

     The compact family of row formats, which includes 'COMPACT',
     'DYNAMIC', and 'COMPRESSED', decreases row storage space at the
     cost of increasing CPU use for some operations.  If your workload
     is a typical one that is limited by cache hit rates and disk speed
     it is likely to be faster.  If it is a rare case that is limited by
     CPU speed, it might be slower.

     The compact family of row formats also optimizes *note 'CHAR':
     char. column storage when using a variable-length character set
     such as 'utf8mb3' or 'utf8mb4'.  With 'ROW_FORMAT=REDUNDANT',
     'CHAR(N)' occupies N x the maximum byte length of the character
     set.  Many languages can be written primarily using single-byte
     'utf8' characters, so a fixed storage length often wastes space.
     With the compact family of rows formats, 'InnoDB' allocates a
     variable amount of storage in the range of N to N x the maximum
     byte length of the character set for these columns by stripping
     trailing spaces.  The minimum storage length is N bytes to
     facilitate in-place updates in typical cases.  For more
     information, see *note innodb-row-format::.

   * To minimize space even further by storing table data in compressed
     form, specify 'ROW_FORMAT=COMPRESSED' when creating 'InnoDB'
     tables, or run the *note 'myisampack': myisampack. command on an
     existing 'MyISAM' table.  ('InnoDB' compressed tables are readable
     and writable, while 'MyISAM' compressed tables are read-only.)

   * For 'MyISAM' tables, if you do not have any variable-length columns
     (*note 'VARCHAR': char, *note 'TEXT': blob, or *note 'BLOB': blob.
     columns), a fixed-size row format is used.  This is faster but may
     waste some space.  See *note myisam-table-formats::.  You can hint
     that you want to have fixed length rows even if you have *note
     'VARCHAR': char. columns with the *note 'CREATE TABLE':
     create-table. option 'ROW_FORMAT=FIXED'.

*Indexes*

   * The primary index of a table should be as short as possible.  This
     makes identification of each row easy and efficient.  For 'InnoDB'
     tables, the primary key columns are duplicated in each secondary
     index entry, so a short primary key saves considerable space if you
     have many secondary indexes.

   * Create only the indexes that you need to improve query performance.
     Indexes are good for retrieval, but slow down insert and update
     operations.  If you access a table mostly by searching on a
     combination of columns, create a single composite index on them
     rather than a separate index for each column.  The first part of
     the index should be the column most used.  If you _always_ use many
     columns when selecting from the table, the first column in the
     index should be the one with the most duplicates, to obtain better
     compression of the index.

   * If it is very likely that a long string column has a unique prefix
     on the first number of characters, it is better to index only this
     prefix, using MySQL's support for creating an index on the leftmost
     part of the column (see *note create-index::).  Shorter indexes are
     faster, not only because they require less disk space, but because
     they also give you more hits in the index cache, and thus fewer
     disk seeks.  See *note server-configuration::.

*Joins*

   * In some circumstances, it can be beneficial to split into two a
     table that is scanned very often.  This is especially true if it is
     a dynamic-format table and it is possible to use a smaller static
     format table that can be used to find the relevant rows when
     scanning the table.

   * Declare columns with identical information in different tables with
     identical data types, to speed up joins based on the corresponding
     columns.

   * Keep column names simple, so that you can use the same name across
     different tables and simplify join queries.  For example, in a
     table named 'customer', use a column name of 'name' instead of
     'customer_name'.  To make your names portable to other SQL servers,
     consider keeping them shorter than 18 characters.

*Normalization*

   * Normally, try to keep all data nonredundant (observing what is
     referred to in database theory as _third normal form_).  Instead of
     repeating lengthy values such as names and addresses, assign them
     unique IDs, repeat these IDs as needed across multiple smaller
     tables, and join the tables in queries by referencing the IDs in
     the join clause.

   * If speed is more important than disk space and the maintenance
     costs of keeping multiple copies of data, for example in a business
     intelligence scenario where you analyze all the data from large
     tables, you can relax the normalization rules, duplicating
     information or creating summary tables to gain more speed.


File: manual.info.tmp,  Node: optimize-data-types,  Next: optimize-multi-tables,  Prev: data-size,  Up: optimizing-database-structure

8.4.2 Optimizing MySQL Data Types
---------------------------------

* Menu:

* optimize-numeric::             Optimizing for Numeric Data
* optimize-character::           Optimizing for Character and String Types
* optimize-blob::                Optimizing for BLOB Types
* procedure-analyse::            Using PROCEDURE ANALYSE


File: manual.info.tmp,  Node: optimize-numeric,  Next: optimize-character,  Prev: optimize-data-types,  Up: optimize-data-types

8.4.2.1 Optimizing for Numeric Data
...................................

   * For unique IDs or other values that can be represented as either
     strings or numbers, prefer numeric columns to string columns.
     Since large numeric values can be stored in fewer bytes than the
     corresponding strings, it is faster and takes less memory to
     transfer and compare them.

   * If you are using numeric data, it is faster in many cases to access
     information from a database (using a live connection) than to
     access a text file.  Information in the database is likely to be
     stored in a more compact format than in the text file, so accessing
     it involves fewer disk accesses.  You also save code in your
     application because you can avoid parsing the text file to find
     line and column boundaries.


File: manual.info.tmp,  Node: optimize-character,  Next: optimize-blob,  Prev: optimize-numeric,  Up: optimize-data-types

8.4.2.2 Optimizing for Character and String Types
.................................................

For character and string columns, follow these guidelines:

   * Use binary collation order for fast comparison and sort operations,
     when you do not need language-specific collation features.  You can
     use the 'BINARY' operator to use binary collation within a
     particular query.

   * When comparing values from different columns, declare those columns
     with the same character set and collation wherever possible, to
     avoid string conversions while running the query.

   * For column values less than 8KB in size, use binary 'VARCHAR'
     instead of 'BLOB'.  The 'GROUP BY' and 'ORDER BY' clauses can
     generate temporary tables, and these temporary tables can use the
     'MEMORY' storage engine if the original table does not contain any
     'BLOB' columns.

   * If a table contains string columns such as name and address, but
     many queries do not retrieve those columns, consider splitting the
     string columns into a separate table and using join queries with a
     foreign key when necessary.  When MySQL retrieves any value from a
     row, it reads a data block containing all the columns of that row
     (and possibly other adjacent rows).  Keeping each row small, with
     only the most frequently used columns, allows more rows to fit in
     each data block.  Such compact tables reduce disk I/O and memory
     usage for common queries.

   * When you use a randomly generated value as a primary key in an
     'InnoDB' table, prefix it with an ascending value such as the
     current date and time if possible.  When consecutive primary values
     are physically stored near each other, 'InnoDB' can insert and
     retrieve them faster.

   * See *note optimize-numeric:: for reasons why a numeric column is
     usually preferable to an equivalent string column.


File: manual.info.tmp,  Node: optimize-blob,  Next: procedure-analyse,  Prev: optimize-character,  Up: optimize-data-types

8.4.2.3 Optimizing for BLOB Types
.................................

   * When storing a large blob containing textual data, consider
     compressing it first.  Do not use this technique when the entire
     table is compressed by 'InnoDB' or 'MyISAM'.

   * For a table with several columns, to reduce memory requirements for
     queries that do not use the BLOB column, consider splitting the
     BLOB column into a separate table and referencing it with a join
     query when needed.

   * Since the performance requirements to retrieve and display a BLOB
     value might be very different from other data types, you could put
     the BLOB-specific table on a different storage device or even a
     separate database instance.  For example, to retrieve a BLOB might
     require a large sequential disk read that is better suited to a
     traditional hard drive than to an SSD device.

   * See *note optimize-character:: for reasons why a binary 'VARCHAR'
     column is sometimes preferable to an equivalent BLOB column.

   * Rather than testing for equality against a very long text string,
     you can store a hash of the column value in a separate column,
     index that column, and test the hashed value in queries.  (Use the
     'MD5()' or 'CRC32()' function to produce the hash value.)  Since
     hash functions can produce duplicate results for different inputs,
     you still include a clause 'AND BLOB_COLUMN = LONG_STRING_VALUE' in
     the query to guard against false matches; the performance benefit
     comes from the smaller, easily scanned index for the hashed values.


File: manual.info.tmp,  Node: procedure-analyse,  Prev: optimize-blob,  Up: optimize-data-types

8.4.2.4 Using PROCEDURE ANALYSE
...............................

'ANALYSE([MAX_ELEMENTS[,MAX_MEMORY]])'

'ANALYSE()' examines the result from a query and returns an analysis of
the results that suggests optimal data types for each column that may
help reduce table sizes.  To obtain this analysis, append 'PROCEDURE
ANALYSE' to the end of a *note 'SELECT': select. statement:

     SELECT ... FROM ... WHERE ... PROCEDURE ANALYSE([MAX_ELEMENTS,[MAX_MEMORY]])

For example:

     SELECT col1, col2 FROM table1 PROCEDURE ANALYSE(10, 2000);

The results show some statistics for the values returned by the query,
and propose an optimal data type for the columns.  This can be helpful
for checking your existing tables, or after importing new data.  You may
need to try different settings for the arguments so that 'PROCEDURE
ANALYSE()' does not suggest the *note 'ENUM': enum. data type when it is
not appropriate.

The arguments are optional and are used as follows:

   * MAX_ELEMENTS (default 256) is the maximum number of distinct values
     that 'ANALYSE()' notices per column.  This is used by 'ANALYSE()'
     to check whether the optimal data type should be of type *note
     'ENUM': enum.; if there are more than MAX_ELEMENTS distinct values,
     then *note 'ENUM': enum. is not a suggested type.

   * MAX_MEMORY (default 8192) is the maximum amount of memory that
     'ANALYSE()' should allocate per column while trying to find all
     distinct values.

A 'PROCEDURE' clause is not permitted in a *note 'UNION': union.
statement.


File: manual.info.tmp,  Node: optimize-multi-tables,  Next: internal-temporary-tables,  Prev: optimize-data-types,  Up: optimizing-database-structure

8.4.3 Optimizing for Many Tables
--------------------------------

* Menu:

* table-cache::                  How MySQL Opens and Closes Tables
* creating-many-tables::         Disadvantages of Creating Many Tables in the Same Database

Some techniques for keeping individual queries fast involve splitting
data across many tables.  When the number of tables runs into the
thousands or even millions, the overhead of dealing with all these
tables becomes a new performance consideration.


File: manual.info.tmp,  Node: table-cache,  Next: creating-many-tables,  Prev: optimize-multi-tables,  Up: optimize-multi-tables

8.4.3.1 How MySQL Opens and Closes Tables
.........................................

When you execute a *note 'mysqladmin status': mysqladmin. command, you
should see something like this:

     Uptime: 426 Running threads: 1 Questions: 11082
     Reloads: 1 Open tables: 12

The 'Open tables' value of 12 can be somewhat puzzling if you have fewer
than 12 tables.

MySQL is multithreaded, so there may be many clients issuing queries for
a given table simultaneously.  To minimize the problem with multiple
client sessions having different states on the same table, the table is
opened independently by each concurrent session.  This uses additional
memory but normally increases performance.  With 'MyISAM' tables, one
extra file descriptor is required for the data file for each client that
has the table open.  (By contrast, the index file descriptor is shared
between all sessions.)

The 'table_open_cache' and 'max_connections' system variables affect the
maximum number of files the server keeps open.  If you increase one or
both of these values, you may run up against a limit imposed by your
operating system on the per-process number of open file descriptors.
Many operating systems permit you to increase the open-files limit,
although the method varies widely from system to system.  Consult your
operating system documentation to determine whether it is possible to
increase the limit and how to do so.

'table_open_cache' is related to 'max_connections'.  For example, for
200 concurrent running connections, specify a table cache size of at
least '200 * N', where N is the maximum number of tables per join in any
of the queries which you execute.  You must also reserve some extra file
descriptors for temporary tables and files.

Make sure that your operating system can handle the number of open file
descriptors implied by the 'table_open_cache' setting.  If
'table_open_cache' is set too high, MySQL may run out of file
descriptors and exhibit symptoms such as refusing connections or failing
to perform queries.

Also take into account that the 'MyISAM' storage engine needs two file
descriptors for each unique open table.  For a partitioned 'MyISAM'
table, two file descriptors are required for each partition of the
opened table.  (When 'MyISAM' opens a partitioned table, it opens every
partition of this table, whether or not a given partition is actually
used.  See *note partitioning-limitations-myisam-file-descriptors::.)
To increase the number of file descriptors available to MySQL, set the
'open_files_limit' system variable.  See *note
not-enough-file-handles::.

The cache of open tables is kept at a level of 'table_open_cache'
entries.  The default value is 400.  To set the size explicitly, set the
'table_open_cache' system variable at startup.  MySQL may temporarily
open more tables than this to execute queries, as described later in
this section.

MySQL closes an unused table and removes it from the table cache under
the following circumstances:

   * When the cache is full and a thread tries to open a table that is
     not in the cache.

   * When the cache contains more than 'table_open_cache' entries and a
     table in the cache is no longer being used by any threads.

   * When a table-flushing operation occurs.  This happens when someone
     issues a 'FLUSH TABLES' statement or executes a *note 'mysqladmin
     flush-tables': mysqladmin. or *note 'mysqladmin refresh':
     mysqladmin. command.

When the table cache fills up, the server uses the following procedure
to locate a cache entry to use:

   * Tables not currently in use are released, beginning with the table
     least recently used.

   * If a new table must be opened, but the cache is full and no tables
     can be released, the cache is temporarily extended as necessary.
     When the cache is in a temporarily extended state and a table goes
     from a used to unused state, the table is closed and released from
     the cache.

A 'MyISAM' table is opened for each concurrent access.  This means the
table needs to be opened twice if two threads access the same table or
if a thread accesses the table twice in the same query (for example, by
joining the table to itself).  Each concurrent open requires an entry in
the table cache.  The first open of any 'MyISAM' table takes two file
descriptors: one for the data file and one for the index file.  Each
additional use of the table takes only one file descriptor for the data
file.  The index file descriptor is shared among all threads.

If you are opening a table with the 'HANDLER TBL_NAME OPEN' statement, a
dedicated table object is allocated for the thread.  This table object
is not shared by other threads and is not closed until the thread calls
'HANDLER TBL_NAME CLOSE' or the thread terminates.  When this happens,
the table is put back in the table cache (if the cache is not full).
See *note handler::.

To determine whether your table cache is too small, check the
'Opened_tables' status variable, which indicates the number of
table-opening operations since the server started:

     mysql> SHOW GLOBAL STATUS LIKE 'Opened_tables';
     +---------------+-------+
     | Variable_name | Value |
     +---------------+-------+
     | Opened_tables | 2741  |
     +---------------+-------+

If the value is very large or increases rapidly, even when you have not
issued many 'FLUSH TABLES' statements, increase the 'table_open_cache'
value at server startup.


File: manual.info.tmp,  Node: creating-many-tables,  Prev: table-cache,  Up: optimize-multi-tables

8.4.3.2 Disadvantages of Creating Many Tables in the Same Database
..................................................................

If you have many 'MyISAM' tables in the same database directory, open,
close, and create operations are slow.  If you execute *note 'SELECT':
select. statements on many different tables, there is a little overhead
when the table cache is full, because for every table that has to be
opened, another must be closed.  You can reduce this overhead by
increasing the number of entries permitted in the table cache.


File: manual.info.tmp,  Node: internal-temporary-tables,  Next: database-count-limit,  Prev: optimize-multi-tables,  Up: optimizing-database-structure

8.4.4 Internal Temporary Table Use in MySQL
-------------------------------------------

In some cases, the server creates internal temporary tables while
processing statements.  Users have no direct control over when this
occurs.

The server creates temporary tables under conditions such as these:

   * Evaluation of *note 'UNION': union. statements.

   * Evaluation of some views, such those that use the 'TEMPTABLE'
     algorithm, *note 'UNION': union, or aggregation.

   * Evaluation of statements that contain an 'ORDER BY' clause and a
     different 'GROUP BY' clause, or for which the 'ORDER BY' or 'GROUP
     BY' contains columns from tables other than the first table in the
     join queue.

   * Evaluation of 'DISTINCT' combined with 'ORDER BY' may require a
     temporary table.

   * For queries that use the 'SQL_SMALL_RESULT' modifier, MySQL uses an
     in-memory temporary table, unless the query also contains elements
     (described later) that require on-disk storage.

   * To evaluate *note 'INSERT ... SELECT': insert-select. statements
     that select from and insert into the same table, MySQL creates an
     internal temporary table to hold the rows from the *note 'SELECT':
     select, then inserts those rows into the target table.  See *note
     insert-select::.

   * Evaluation of multiple-table *note 'UPDATE': update. statements.

   * Evaluation of 'GROUP_CONCAT()' or 'COUNT(DISTINCT)' expressions.

To determine whether a statement requires a temporary table, use *note
'EXPLAIN': explain. and check the 'Extra' column to see whether it says
'Using temporary' (see *note using-explain::).

When the server creates an internal temporary table (either in memory or
on disk), it increments the 'Created_tmp_tables' status variable.  If
the server creates the table on disk (either initially or by converting
an in-memory table) it increments the 'Created_tmp_disk_tables' status
variable.

Some query conditions prevent the use of an in-memory temporary table,
in which case the server uses an on-disk table instead:

   * Presence of a *note 'BLOB': blob. or *note 'TEXT': blob. column in
     the table.  This includes user-defined variables having a string
     value because they are treated as *note 'BLOB': blob. or *note
     'TEXT': blob. columns, depending on whether their value is a binary
     or nonbinary string, respectively.

   * Presence of any string column in a 'GROUP BY' or 'DISTINCT' clause
     larger than 512 bytes.

   * Presence of any string column with a maximum length larger than 512
     (bytes for binary strings, characters for nonbinary strings) in the
     *note 'SELECT': select. list, if *note 'UNION': union. or *note
     'UNION ALL': union. is used.

   * The *note 'SHOW COLUMNS': show-columns. and *note 'DESCRIBE':
     describe. statements use 'BLOB' as the type for some columns, thus
     the temporary table used for the results is an on-disk table.

   * *note internal-temporary-tables-engines::

   * *note internal-temporary-tables-storage::

*Internal Temporary Table Storage Engine*

An internal temporary table can be held in memory and processed by the
'MEMORY' storage engine, or stored on disk and processed by the 'MyISAM'
storage engine.

If an internal temporary table is created as an in-memory table but
becomes too large, MySQL automatically converts it to an on-disk table.
The maximum size for in-memory temporary tables is defined by the
'tmp_table_size' or 'max_heap_table_size' value, whichever is smaller.
This differs from 'MEMORY' tables explicitly created with *note 'CREATE
TABLE': create-table.  For such tables, only the 'max_heap_table_size'
variable determines how large a table can grow, and there is no
conversion to on-disk format.

*Internal Temporary Table Storage Format*

In-memory temporary tables are managed by the 'MEMORY' storage engine,
which uses fixed-length row format.  'VARCHAR' and 'VARBINARY' column
values are padded to the maximum column length, in effect storing them
as 'CHAR' and 'BINARY' columns.

On-disk temporary tables are managed by the 'MyISAM' storage engine
using dynamic-width row format.  Columns take only as much storage as
needed, which reduces disk I/O and space requirements, and processing
time compared to on-disk tables that use fixed-length rows.  The
exception to dynamic-length row format is that on-disk temporary tables
for derived tables are stored using fixed-length rows.

Prior to MySQL 5.5.47, on-disk temporary table storage uses fixed-length
rows.

For statements that initially create an internal temporary table in
memory, then convert it to an on-disk table, better performance might be
achieved by skipping the conversion step and creating the table on disk
to begin with.  The 'big_tables' variable can be used to force disk
storage of internal temporary tables.


File: manual.info.tmp,  Node: database-count-limit,  Next: table-size-limit,  Prev: internal-temporary-tables,  Up: optimizing-database-structure

8.4.5 Limits on Number of Databases and Tables
----------------------------------------------

MySQL has no limit on the number of databases.  The underlying file
system may have a limit on the number of directories.

MySQL has no limit on the number of tables.  The underlying file system
may have a limit on the number of files that represent tables.
Individual storage engines may impose engine-specific constraints.
'InnoDB' permits up to 4 billion tables.


File: manual.info.tmp,  Node: table-size-limit,  Next: column-count-limit,  Prev: database-count-limit,  Up: optimizing-database-structure

8.4.6 Limits on Table Size
--------------------------

The effective maximum table size for MySQL databases is usually
determined by operating system constraints on file sizes, not by MySQL
internal limits.  For up-to-date information operating system file size
limits, refer to the documentation specific to your operating system.

Windows users, please note that FAT and VFAT (FAT32) are _not_
considered suitable for production use with MySQL. Use NTFS instead.

If you encounter a full-table error, there are several reasons why it
might have occurred:

   * The disk might be full.

   * You are using 'InnoDB' tables and have run out of room in an
     'InnoDB' tablespace file.  The maximum tablespace size is four
     billion pages (64TB), which is also the maximum size for a table.
     See *note innodb-limits::.

     Generally, partitioning of tables into multiple tablespace files is
     recommended for tables larger than 1TB in size.

   * You have hit an operating system file size limit.  For example, you
     are using 'MyISAM' tables on an operating system that supports
     files only up to 2GB in size and you have hit this limit for the
     data file or index file.

   * You are using a 'MyISAM' table and the space required for the table
     exceeds what is permitted by the internal pointer size.  'MyISAM'
     permits data and index files to grow up to 256TB by default, but
     this limit can be changed up to the maximum permissible size of
     65,536TB (256^7 − 1 bytes).

     If you need a 'MyISAM' table that is larger than the default limit
     and your operating system supports large files, the *note 'CREATE
     TABLE': create-table. statement supports 'AVG_ROW_LENGTH' and
     'MAX_ROWS' options.  See *note create-table::.  The server uses
     these options to determine how large a table to permit.

     If the pointer size is too small for an existing table, you can
     change the options with *note 'ALTER TABLE': alter-table. to
     increase a table's maximum permissible size.  See *note
     alter-table::.

          ALTER TABLE TBL_NAME MAX_ROWS=1000000000 AVG_ROW_LENGTH=NNN;

     You have to specify 'AVG_ROW_LENGTH' only for tables with *note
     'BLOB': blob. or *note 'TEXT': blob. columns; in this case, MySQL
     cannot optimize the space required based only on the number of
     rows.

     To change the default size limit for 'MyISAM' tables, set the
     'myisam_data_pointer_size', which sets the number of bytes used for
     internal row pointers.  The value is used to set the pointer size
     for new tables if you do not specify the 'MAX_ROWS' option.  The
     value of 'myisam_data_pointer_size' can be from 2 to 7.  A value of
     4 permits tables up to 4GB; a value of 6 permits tables up to
     256TB.

     You can check the maximum data and index sizes by using this
     statement:

          SHOW TABLE STATUS FROM DB_NAME LIKE 'TBL_NAME';

     You also can use *note 'myisamchk -dv /path/to/table-index-file':
     myisamchk.  See *note show::, or *note myisamchk::.

     Other ways to work around file-size limits for 'MyISAM' tables are
     as follows:

        * If your large table is read only, you can use *note
          'myisampack': myisampack. to compress it.  *note 'myisampack':
          myisampack. usually compresses a table by at least 50%, so you
          can have, in effect, much bigger tables.  *note 'myisampack':
          myisampack. also can merge multiple tables into a single
          table.  See *note myisampack::.

        * MySQL includes a 'MERGE' library that enables you to handle a
          collection of 'MyISAM' tables that have identical structure as
          a single 'MERGE' table.  See *note merge-storage-engine::.

   * You are using the *note 'NDB': mysql-cluster. storage engine, in
     which case you need to increase the values for the 'DataMemory' and
     'IndexMemory' configuration parameters in your 'config.ini' file.
     See *note mysql-cluster-params-ndbd::.

   * You are using the 'MEMORY' ('HEAP') storage engine; in this case
     you need to increase the value of the 'max_heap_table_size' system
     variable.  See *note server-system-variables::.


File: manual.info.tmp,  Node: column-count-limit,  Prev: table-size-limit,  Up: optimizing-database-structure

8.4.7 Limits on Table Column Count and Row Size
-----------------------------------------------

This section describes limits on the number of columns in tables and the
size of individual rows.

   * *note column-count-limits::

   * *note row-size-limits::

*Column Count Limits*

MySQL has hard limit of 4096 columns per table, but the effective
maximum may be less for a given table.  The exact column limit depends
on several factors:

   * The maximum row size for a table constrains the number (and
     possibly size) of columns because the total length of all columns
     cannot exceed this size.  See *note row-size-limits::.

   * The storage requirements of individual columns constrain the number
     of columns that fit within a given maximum row size.  Storage
     requirements for some data types depend on factors such as storage
     engine, storage format, and character set.  See *note
     storage-requirements::.

   * Storage engines may impose additional restrictions that limit table
     column count.  For example, *note 'InnoDB': innodb-storage-engine.
     has a limit of 1000 columns per table.  See *note innodb-limits::.
     For information about other storage engines, see *note
     storage-engines::.

   * Each table has an '.frm' file that contains the table definition.
     The definition affects the content of this file in ways that may
     affect the number of columns permitted in the table.  See *note
     limits-frm-file::.

*Row Size Limits*

The maximum row size for a given table is determined by several factors:

   * The internal representation of a MySQL table has a maximum row size
     limit of 65,535 bytes, even if the storage engine is capable of
     supporting larger rows.  *note 'BLOB': blob. and *note 'TEXT':
     blob. columns only contribute 9 to 12 bytes toward the row size
     limit because their contents are stored separately from the rest of
     the row.

   * The maximum row size for an 'InnoDB' table, which applies to data
     stored locally within a database page, is slightly less than half a
     page.  For example, the maximum row size is slightly less than 8KB
     for the default 16KB 'InnoDB' page size.  See *note
     innodb-limits::.

     If a row containing variable-length columns exceeds the 'InnoDB'
     maximum row size, 'InnoDB' selects variable-length columns for
     external off-page storage until the row fits within the 'InnoDB'
     row size limit.  The amount of data stored locally for
     variable-length columns that are stored off-page differs by row
     format.  For more information, see *note innodb-row-format::.

   * Different storage formats use different amounts of page header and
     trailer data, which affects the amount of storage available for
     rows.

        * For information about 'InnoDB' row formats, see *note
          innodb-row-format::.

        * For information about 'MyISAM' storage formats, see *note
          myisam-table-formats::.

*Row Size Limit Examples*

   * The MySQL maximum row size limit of 65,535 bytes is demonstrated in
     the following 'InnoDB' and 'MyISAM' examples.  The limit is
     enforced regardless of storage engine, even though the storage
     engine may be capable of supporting larger rows.

          mysql> CREATE TABLE t (a VARCHAR(10000), b VARCHAR(10000),
                 c VARCHAR(10000), d VARCHAR(10000), e VARCHAR(10000),
                 f VARCHAR(10000), g VARCHAR(6000)) ENGINE=InnoDB CHARACTER SET latin1;
          ERROR 1118 (42000): Row size too large. The maximum row size for the used
          table type, not counting BLOBs, is 65535. You have to change some columns
          to TEXT or BLOBs

          mysql> CREATE TABLE t (a VARCHAR(10000), b VARCHAR(10000),
                 c VARCHAR(10000), d VARCHAR(10000), e VARCHAR(10000),
                 f VARCHAR(10000), g VARCHAR(6000)) ENGINE=MyISAM CHARACTER SET latin1;
          ERROR 1118 (42000): Row size too large. The maximum row size for the used
          table type, not counting BLOBs, is 65535. You have to change some columns
          to TEXT or BLOBs

     In the following 'MyISAM' example, changing a column to *note
     'TEXT': blob. avoids the 65,535-byte row size limit and permits the
     operation to succeed because *note 'BLOB': blob. and *note 'TEXT':
     blob. columns only contribute 9 to 12 bytes toward the row size.

          mysql> CREATE TABLE t (a VARCHAR(10000), b VARCHAR(10000),
                 c VARCHAR(10000), d VARCHAR(10000), e VARCHAR(10000),
                 f VARCHAR(10000), g TEXT(6000)) ENGINE=MyISAM CHARACTER SET latin1;
          Query OK, 0 rows affected (0.02 sec)

     The operation succeeds for an 'InnoDB' table because changing a
     column to *note 'TEXT': blob. avoids the MySQL 65,535-byte row size
     limit, and 'InnoDB' off-page storage of variable-length columns
     avoids the 'InnoDB' row size limit.

          mysql> CREATE TABLE t (a VARCHAR(10000), b VARCHAR(10000),
                 c VARCHAR(10000), d VARCHAR(10000), e VARCHAR(10000),
                 f VARCHAR(10000), g TEXT(6000)) ENGINE=InnoDB CHARACTER SET latin1;
          Query OK, 0 rows affected (0.02 sec)

   * Storage for variable-length columns includes length bytes, which
     are counted toward the row size.  For example, a *note
     'VARCHAR(255) CHARACTER SET utf8mb3': char. column takes two bytes
     to store the length of the value, so each value can take up to 767
     bytes.

     The statement to create table 't1' succeeds because the columns
     require 32,765 + 2 bytes and 32,766 + 2 bytes, which falls within
     the maximum row size of 65,535 bytes:

          mysql> CREATE TABLE t1
                 (c1 VARCHAR(32765) NOT NULL, c2 VARCHAR(32766) NOT NULL)
                 ENGINE = InnoDB CHARACTER SET latin1;
          Query OK, 0 rows affected (0.02 sec)

     The statement to create table 't2' fails because, although the
     column length is within the maximum length of 65,535 bytes, two
     additional bytes are required to record the length, which causes
     the row size to exceed 65,535 bytes:

          mysql> CREATE TABLE t2
                 (c1 VARCHAR(65535) NOT NULL)
                 ENGINE = InnoDB CHARACTER SET latin1;
          ERROR 1118 (42000): Row size too large. The maximum row size for the
          used table type, not counting BLOBs, is 65535. You have to change some
          columns to TEXT or BLOBs

     Reducing the column length to 65,533 or less permits the statement
     to succeed.

          mysql> CREATE TABLE t2
                 (c1 VARCHAR(65533) NOT NULL)
                 ENGINE = InnoDB CHARACTER SET latin1;
          Query OK, 0 rows affected (0.01 sec)

   * For *note 'MyISAM': myisam-storage-engine. tables, 'NULL' columns
     require additional space in the row to record whether their values
     are 'NULL'.  Each 'NULL' column takes one bit extra, rounded up to
     the nearest byte.

     The statement to create table 't3' fails because *note 'MyISAM':
     myisam-storage-engine. requires space for 'NULL' columns in
     addition to the space required for variable-length column length
     bytes, causing the row size to exceed 65,535 bytes:

          mysql> CREATE TABLE t3
                 (c1 VARCHAR(32765) NULL, c2 VARCHAR(32766) NULL)
                 ENGINE = MyISAM CHARACTER SET latin1;
          ERROR 1118 (42000): Row size too large. The maximum row size for the
          used table type, not counting BLOBs, is 65535. You have to change some
          columns to TEXT or BLOBs

     For information about *note 'InnoDB': innodb-storage-engine. 'NULL'
     column storage, see *note innodb-row-format::.

   * 'InnoDB' restricts row size (for data stored locally within the
     database page) to slightly less than half a database page.  For
     example, the maximum row size is slightly less than 8KB for the
     16KB 'InnoDB' page size.

     The statement to create table 't4' fails because the defined
     columns exceed the row size limit for a 16KB 'InnoDB' page.

     *Note*:

     'innodb_strict_mode' is enabled in the following example to ensure
     that 'InnoDB' returns an error if the defined columns exceed the
     'InnoDB' row size limit.  When 'innodb_strict_mode' is disabled
     (the default), creating a table that uses 'REDUNDANT' or 'COMPACT'
     row format succeeds with a warning if the 'InnoDB' row size limit
     is exceeded.

     'DYNAMIC' and 'COMPRESSED' row formats are more restrictive in this
     regard.  Creating a table that uses 'DYNAMIC' or 'COMRESSED' row
     format fails with an error if the 'InnoDB' row size limit is
     exceeded, regardless of the 'innodb_strict_mode' setting.

          mysql> SET SESSION innodb_strict_mode=1;
          mysql> CREATE TABLE t4 (
                 c1 CHAR(255),c2 CHAR(255),c3 CHAR(255),
                 c4 CHAR(255),c5 CHAR(255),c6 CHAR(255),
                 c7 CHAR(255),c8 CHAR(255),c9 CHAR(255),
                 c10 CHAR(255),c11 CHAR(255),c12 CHAR(255),
                 c13 CHAR(255),c14 CHAR(255),c15 CHAR(255),
                 c16 CHAR(255),c17 CHAR(255),c18 CHAR(255),
                 c19 CHAR(255),c20 CHAR(255),c21 CHAR(255),
                 c22 CHAR(255),c23 CHAR(255),c24 CHAR(255),
                 c25 CHAR(255),c26 CHAR(255),c27 CHAR(255),
                 c28 CHAR(255),c29 CHAR(255),c30 CHAR(255),
                 c31 CHAR(255),c32 CHAR(255),c33 CHAR(255)
                 ) ENGINE=InnoDB ROW_FORMAT=COMPACT DEFAULT CHARSET latin1;
          ERROR 1118 (42000): Row size too large (> 8126). Changing some columns to TEXT or BLOB or using
          ROW_FORMAT=DYNAMIC or ROW_FORMAT=COMPRESSED may help. In current row format, BLOB prefix of 768
          bytes is stored inline.


File: manual.info.tmp,  Node: optimizing-innodb,  Next: optimizing-myisam,  Prev: optimizing-database-structure,  Up: optimization

8.5 Optimizing for InnoDB Tables
================================

* Menu:

* optimizing-innodb-storage-layout::  Optimizing Storage Layout for InnoDB Tables
* optimizing-innodb-transaction-management::  Optimizing InnoDB Transaction Management
* optimizing-innodb-logging::    Optimizing InnoDB Redo Logging
* optimizing-innodb-bulk-data-loading::  Bulk Data Loading for InnoDB Tables
* optimizing-innodb-queries::    Optimizing InnoDB Queries
* optimizing-innodb-ddl-operations::  Optimizing InnoDB DDL Operations
* optimizing-innodb-diskio::     Optimizing InnoDB Disk I/O
* optimizing-innodb-configuration-variables::  Optimizing InnoDB Configuration Variables
* optimizing-innodb-many-tables::  Optimizing InnoDB for Systems with Many Tables

*note 'InnoDB': innodb-storage-engine. is the storage engine that MySQL
customers typically use in production databases where reliability and
concurrency are important.  Because 'InnoDB' is the default storage
engine in MySQL 5.5 and higher, you can expect to see 'InnoDB' tables
more often than before.  This section explains how to optimize database
operations for 'InnoDB' tables.


File: manual.info.tmp,  Node: optimizing-innodb-storage-layout,  Next: optimizing-innodb-transaction-management,  Prev: optimizing-innodb,  Up: optimizing-innodb

8.5.1 Optimizing Storage Layout for InnoDB Tables
-------------------------------------------------

   * Once your data reaches a stable size, or a growing table has
     increased by tens or some hundreds of megabytes, consider using the
     'OPTIMIZE TABLE' statement to reorganize the table and compact any
     wasted space.  The reorganized tables require less disk I/O to
     perform full table scans.  This is a straightforward technique that
     can improve performance when other techniques such as improving
     index usage or tuning application code are not practical.

     'OPTIMIZE TABLE' copies the data part of the table and rebuilds the
     indexes.  The benefits come from improved packing of data within
     indexes, and reduced fragmentation within the tablespaces and on
     disk.  The benefits vary depending on the data in each table.  You
     may find that there are significant gains for some and not for
     others, or that the gains decrease over time until you next
     optimize the table.  This operation can be slow if the table is
     large or if the indexes being rebuilt do not fit into the buffer
     pool.  The first run after adding a lot of data to a table is often
     much slower than later runs.

   * In 'InnoDB', having a long 'PRIMARY KEY' (either a single column
     with a lengthy value, or several columns that form a long composite
     value) wastes a lot of disk space.  The primary key value for a row
     is duplicated in all the secondary index records that point to the
     same row.  (See *note innodb-index-types::.)  Create an
     'AUTO_INCREMENT' column as the primary key if your primary key is
     long, or index a prefix of a long 'VARCHAR' column instead of the
     entire column.

   * Use the *note 'VARCHAR': char. data type instead of *note 'CHAR':
     char. to store variable-length strings or for columns with many
     'NULL' values.  A *note 'CHAR(N)': char. column always takes N
     characters to store data, even if the string is shorter or its
     value is 'NULL'.  Smaller tables fit better in the buffer pool and
     reduce disk I/O.

     When using 'COMPACT' row format (the default 'InnoDB' format) and
     variable-length character sets, such as 'utf8' or 'sjis', *note
     'CHAR(N)': char. columns occupy a variable amount of space, but
     still at least N bytes.

   * For tables that are big, or contain lots of repetitive text or
     numeric data, consider using 'COMPRESSED' row format.  Less disk
     I/O is required to bring data into the buffer pool, or to perform
     full table scans.  Before making a permanent decision, measure the
     amount of compression you can achieve by using 'COMPRESSED' versus
     'COMPACT' row format.


File: manual.info.tmp,  Node: optimizing-innodb-transaction-management,  Next: optimizing-innodb-logging,  Prev: optimizing-innodb-storage-layout,  Up: optimizing-innodb

8.5.2 Optimizing InnoDB Transaction Management
----------------------------------------------

To optimize 'InnoDB' transaction processing, find the ideal balance
between the performance overhead of transactional features and the
workload of your server.  For example, an application might encounter
performance issues if it commits thousands of times per second, and
different performance issues if it commits only every 2-3 hours.

   * The default MySQL setting 'AUTOCOMMIT=1' can impose performance
     limitations on a busy database server.  Where practical, wrap
     several related data change operations into a single transaction,
     by issuing 'SET AUTOCOMMIT=0' or a 'START TRANSACTION' statement,
     followed by a 'COMMIT' statement after making all the changes.

     'InnoDB' must flush the log to disk at each transaction commit if
     that transaction made modifications to the database.  When each
     change is followed by a commit (as with the default autocommit
     setting), the I/O throughput of the storage device puts a cap on
     the number of potential operations per second.

   * Avoid performing rollbacks after inserting, updating, or deleting
     huge numbers of rows.  If a big transaction is slowing down server
     performance, rolling it back can make the problem worse,
     potentially taking several times as long to perform as the original
     data change operations.  Killing the database process does not
     help, because the rollback starts again on server startup.

     To minimize the chance of this issue occurring:

        * Increase the size of the buffer pool so that all the data
          change changes can be cached rather than immediately written
          to disk.

        * Set 'innodb_change_buffering=all' so that update and delete
          operations are buffered in addition to inserts.

        * Consider issuing 'COMMIT' statements periodically during the
          big data change operation, possibly breaking a single delete
          or update into multiple statements that operate on smaller
          numbers of rows.

     To get rid of a runaway rollback once it occurs, increase the
     buffer pool so that the rollback becomes CPU-bound and runs fast,
     or kill the server and restart with 'innodb_force_recovery=3', as
     explained in *note innodb-recovery::.

     This issue is expected to be infrequent with the default setting
     'innodb_change_buffering=all', which allows update and delete
     operations to be cached in memory, making them faster to perform in
     the first place, and also faster to roll back if needed.  Make sure
     to use this parameter setting on servers that process long-running
     transactions with many inserts, updates, or deletes.

   * If you can afford the loss of some of the latest committed
     transactions if a crash occurs, you can set the
     'innodb_flush_log_at_trx_commit' parameter to 0.  'InnoDB' tries to
     flush the log once per second anyway, although the flush is not
     guaranteed.  Also, set the value of 'innodb_support_xa' to 0, which
     will reduce the number of disk flushes due to synchronizing on disk
     data and the binary log.

   * When rows are modified or deleted, the rows and associated undo
     logs are not physically removed immediately, or even immediately
     after the transaction commits.  The old data is preserved until
     transactions that started earlier or concurrently are finished, so
     that those transactions can access the previous state of modified
     or deleted rows.  Thus, a long-running transaction can prevent
     'InnoDB' from purging data that was changed by a different
     transaction.

   * When rows are modified or deleted within a long-running
     transaction, other transactions using the 'READ COMMITTED' and
     'REPEATABLE READ' isolation levels have to do more work to
     reconstruct the older data if they read those same rows.

   * When a long-running transaction modifies a table, queries against
     that table from other transactions do not make use of the covering
     index technique.  Queries that normally could retrieve all the
     result columns from a secondary index, instead look up the
     appropriate values from the table data.

     If secondary index pages are found to have a 'PAGE_MAX_TRX_ID' that
     is too new, or if records in the secondary index are delete-marked,
     'InnoDB' may need to look up records using a clustered index.


File: manual.info.tmp,  Node: optimizing-innodb-logging,  Next: optimizing-innodb-bulk-data-loading,  Prev: optimizing-innodb-transaction-management,  Up: optimizing-innodb

8.5.3 Optimizing InnoDB Redo Logging
------------------------------------

Consider the following guidelines for optimizing redo logging:

   * Make your redo log files big, even as big as the buffer pool.  When
     'InnoDB' has written the redo log files full, it must write the
     modified contents of the buffer pool to disk in a checkpoint.
     Small redo log files cause many unnecessary disk writes.  Although
     historically big redo log files caused lengthy recovery times,
     recovery is now much faster and you can confidently use large redo
     log files.

     The size and number of redo log files are configured using the
     'innodb_log_file_size' and 'innodb_log_files_in_group'
     configuration options.  For information about modifying an existing
     redo log file configuration, see Changing the Number or Size of
     InnoDB Redo Log Files
     (https://dev.mysql.com/doc/refman/5.7/en/innodb-redo-log.html#innodb-redo-log-file-reconfigure).

   * Consider increasing the size of the log buffer.  A large log buffer
     enables large transactions to run without a need to write the log
     to disk before the transactions commit.  Thus, if you have
     transactions that update, insert, or delete many rows, making the
     log buffer larger saves disk I/O. Log buffer size is configured
     using the 'innodb_log_buffer_size' configuration option.


File: manual.info.tmp,  Node: optimizing-innodb-bulk-data-loading,  Next: optimizing-innodb-queries,  Prev: optimizing-innodb-logging,  Up: optimizing-innodb

8.5.4 Bulk Data Loading for InnoDB Tables
-----------------------------------------

These performance tips supplement the general guidelines for fast
inserts in *note insert-optimization::.

   * When importing data into 'InnoDB', turn off autocommit mode,
     because it performs a log flush to disk for every insert.  To
     disable autocommit during your import operation, surround it with
     *note 'SET autocommit': commit. and *note 'COMMIT': commit.
     statements:

          SET autocommit=0;... SQL IMPORT STATEMENTS ...
          COMMIT;

     The *note 'mysqldump': mysqldump. option '--opt' creates dump files
     that are fast to import into an 'InnoDB' table, even without
     wrapping them with the *note 'SET autocommit': commit. and *note
     'COMMIT': commit. statements.

   * If you have 'UNIQUE' constraints on secondary keys, you can speed
     up table imports by temporarily turning off the uniqueness checks
     during the import session:

          SET unique_checks=0;... SQL IMPORT STATEMENTS ...
          SET unique_checks=1;

     For big tables, this saves a lot of disk I/O because 'InnoDB' can
     use its change buffer to write secondary index records in a batch.
     Be certain that the data contains no duplicate keys.

   * If you have 'FOREIGN KEY' constraints in your tables, you can speed
     up table imports by turning off the foreign key checks for the
     duration of the import session:

          SET foreign_key_checks=0;... SQL IMPORT STATEMENTS ...
          SET foreign_key_checks=1;

     For big tables, this can save a lot of disk I/O.

   * Use the multiple-row *note 'INSERT': insert. syntax to reduce
     communication overhead between the client and the server if you
     need to insert many rows:

          INSERT INTO yourtable VALUES (1,2), (5,5), ...;

     This tip is valid for inserts into any table, not just 'InnoDB'
     tables.

   * When doing bulk inserts into tables with auto-increment columns,
     set 'innodb_autoinc_lock_mode' to 2 instead of the default value 1.
     See *note innodb-auto-increment-handling:: for details.

   * When performing bulk inserts, it is faster to insert rows in
     'PRIMARY KEY' order.  'InnoDB' tables use a clustered index, which
     makes it relatively fast to use data in the order of the 'PRIMARY
     KEY'.  Performing bulk inserts in 'PRIMARY KEY' order is
     particularly important for tables that do not fit entirely within
     the buffer pool.


File: manual.info.tmp,  Node: optimizing-innodb-queries,  Next: optimizing-innodb-ddl-operations,  Prev: optimizing-innodb-bulk-data-loading,  Up: optimizing-innodb

8.5.5 Optimizing InnoDB Queries
-------------------------------

To tune queries for 'InnoDB' tables, create an appropriate set of
indexes on each table.  See *note mysql-indexes:: for details.  Follow
these guidelines for 'InnoDB' indexes:

   * Because each 'InnoDB' table has a primary key (whether you request
     one or not), specify a set of primary key columns for each table,
     columns that are used in the most important and time-critical
     queries.

   * Do not specify too many or too long columns in the primary key,
     because these column values are duplicated in each secondary index.
     When an index contains unnecessary data, the I/O to read this data
     and memory to cache it reduce the performance and scalability of
     the server.

   * Do not create a separate secondary index for each column, because
     each query can only make use of one index.  Indexes on rarely
     tested columns or columns with only a few different values might
     not be helpful for any queries.  If you have many queries for the
     same table, testing different combinations of columns, try to
     create a small number of concatenated indexes rather than a large
     number of single-column indexes.  If an index contains all the
     columns needed for the result set (known as a covering index), the
     query might be able to avoid reading the table data at all.

   * If an indexed column cannot contain any 'NULL' values, declare it
     as 'NOT NULL' when you create the table.  The optimizer can better
     determine which index is most effective to use for a query, when it
     knows whether each column contains 'NULL' values.


File: manual.info.tmp,  Node: optimizing-innodb-ddl-operations,  Next: optimizing-innodb-diskio,  Prev: optimizing-innodb-queries,  Up: optimizing-innodb

8.5.6 Optimizing InnoDB DDL Operations
--------------------------------------

   * For DDL operations on tables and indexes ('CREATE', 'ALTER', and
     'DROP' statements), the most significant aspect for 'InnoDB' tables
     is that creating and dropping secondary indexes is much faster in
     MySQL 5.5 and higher, than in earlier versions.  See *note
     innodb-create-index:: for details.

   * 'Fast index creation' makes it faster in some cases to drop an
     index before loading data into a table, then re-create the index
     after loading the data.

   * Use *note 'TRUNCATE TABLE': truncate-table. to empty a table, not
     'DELETE FROM TBL_NAME'.  Foreign key constraints can make a
     'TRUNCATE' statement work like a regular 'DELETE' statement, in
     which case a sequence of commands like *note 'DROP TABLE':
     drop-table. and *note 'CREATE TABLE': create-table. might be
     fastest.

   * Because the primary key is integral to the storage layout of each
     'InnoDB' table, and changing the definition of the primary key
     involves reorganizing the whole table, always set up the primary
     key as part of the *note 'CREATE TABLE': create-table. statement,
     and plan ahead so that you do not need to 'ALTER' or 'DROP' the
     primary key afterward.


File: manual.info.tmp,  Node: optimizing-innodb-diskio,  Next: optimizing-innodb-configuration-variables,  Prev: optimizing-innodb-ddl-operations,  Up: optimizing-innodb

8.5.7 Optimizing InnoDB Disk I/O
--------------------------------

If you follow best practices for database design and tuning techniques
for SQL operations, but your database is still slow due to heavy disk
I/O activity, consider these disk I/O optimizations.  If the Unix 'top'
tool or the Windows Task Manager shows that the CPU usage percentage
with your workload is less than 70%, your workload is probably
disk-bound.

   * Increase buffer pool size

     When table data is cached in the 'InnoDB' buffer pool, it can be
     accessed repeatedly by queries without requiring any disk I/O.
     Specify the size of the buffer pool with the
     'innodb_buffer_pool_size' option.  This memory area is important
     enough that it is typically recommended that
     'innodb_buffer_pool_size' is configured to 50 to 75 percent of
     system memory.  For more information see, *note memory-use::.

   * Adjust the flush method

     In some versions of GNU/Linux and Unix, flushing files to disk with
     the Unix 'fsync()' call (which 'InnoDB' uses by default) and
     similar methods is surprisingly slow.  If database write
     performance is an issue, conduct benchmarks with the
     'innodb_flush_method' parameter set to 'O_DSYNC'.

   * Use a noop or deadline I/O scheduler with native AIO on Linux

     'InnoDB' uses the asynchronous I/O subsystem (native AIO) on Linux
     to perform read-ahead and write requests for data file pages.  This
     behavior is controlled by the 'innodb_use_native_aio' configuration
     option, which is enabled by default.  With native AIO, the type of
     I/O scheduler has greater influence on I/O performance.  Generally,
     noop and deadline I/O schedulers are recommended.  Conduct
     benchmarks to determine which I/O scheduler provides the best
     results for your workload and environment.  For more information,
     see *note innodb-linux-native-aio::.

   * Use direct I/O on Solaris 10 for x86_64 architecture

     When using the 'InnoDB' storage engine on Solaris 10 for x86_64
     architecture (AMD Opteron), use direct I/O for 'InnoDB'-related
     files to avoid degradation of 'InnoDB' performance.  To use direct
     I/O for an entire UFS file system used for storing 'InnoDB'-related
     files, mount it with the 'forcedirectio' option; see
     'mount_ufs(1M)'.  (The default on Solaris 10/x86_64 is _not_ to use
     this option.)  To apply direct I/O only to 'InnoDB' file operations
     rather than the whole file system, set 'innodb_flush_method =
     O_DIRECT'.  With this setting, 'InnoDB' calls 'directio()' instead
     of 'fcntl()' for I/O to data files (not for I/O to log files).

   * Use raw storage for data and log files with Solaris 2.6 or later

     When using the 'InnoDB' storage engine with a large
     'innodb_buffer_pool_size' value on any release of Solaris 2.6 and
     up and any platform (sparc/x86/x64/amd64), conduct benchmarks with
     'InnoDB' data files and log files on raw devices or on a separate
     direct I/O UFS file system, using the 'forcedirectio' mount option
     as described previously.  (It is necessary to use the mount option
     rather than setting 'innodb_flush_method' if you want direct I/O
     for the log files.)  Users of the Veritas file system VxFS should
     use the 'convosync=direct' mount option.

     Do not place other MySQL data files, such as those for 'MyISAM'
     tables, on a direct I/O file system.  Executables or libraries
     _must not_ be placed on a direct I/O file system.

   * Use additional storage devices

     Additional storage devices could be used to set up a RAID
     configuration.  For related information, see *note disk-issues::.

     Alternatively, 'InnoDB' tablespace data files and log files can be
     placed on different physical disks.  For more information, refer to
     the following sections:

        * *note innodb-init-startup-configuration::

        * *note innodb-migration::

   * Non-rotational storage generally provides better performance for
     random I/O operations; and rotational storage for sequential I/O
     operations.  When distributing data and log files across rotational
     and non-rotational storage devices, consider the type of I/O
     operations that are predominantly performed on each file.

     Random I/O-oriented files are typically file-per-table data files.
     Sequential I/O-oriented files include 'InnoDB' system tablespace
     files (due to doublewrite buffering and change buffering) and log
     files such as binary log files and redo log files.

     Review settings for the following configuration options when using
     non-rotational storage:

        * 'innodb_io_capacity'

          The default setting of 200 is generally sufficient for a
          lower-end non-rotational storage device.  For higher-end,
          bus-attached devices, consider a higher setting such as 1000.

        * 'innodb_log_file_size'

          If redo logs are on non-rotational storage, configure this
          option to maximize caching and write combining.

     Early-generation SSD devices often have a 4KB sector size, but some
     newer devices have a 16KB sector size.  The default 'InnoDB' page
     size is 16KB. Keeping the page size close to the storage device
     block size minimizes the amount of unchanged data that is rewritten
     to disk.

     Ensure that TRIM support is enabled for your operating system.  It
     is typically enabled by default.

   * Increase I/O capacity to avoid backlogs

     If throughput drops periodically because of 'InnoDB' checkpoint
     operations, consider increasing the value of the
     'innodb_io_capacity' configuration option.  Higher values cause
     more frequent flushing, avoiding the backlog of work that can cause
     dips in throughput.

   * Lower I/O capacity if flushing does not fall behind

     If the system is not falling behind with 'InnoDB' flushing
     operations, consider lowering the value of the 'innodb_io_capacity'
     configuration option.  Typically, you keep this option value as low
     as practical, but not so low that it causes periodic drops in
     throughput as mentioned in the preceding bullet.  In a typical
     scenario where you could lower the option value, you might see a
     combination like this in the output from *note 'SHOW ENGINE INNODB
     STATUS': show-engine.:

        * History list length low, below a few thousand.

        * Insert buffer merges close to rows inserted.

        * Modified pages in buffer pool consistently well below
          'innodb_max_dirty_pages_pct' of the buffer pool.  (Measure at
          a time when the server is not doing bulk inserts; it is normal
          during bulk inserts for the modified pages percentage to rise
          significantly.)

        * 'Log sequence number - Last checkpoint' is at less than 7/8 or
          ideally less than 6/8 of the total size of the 'InnoDB' log
          files.


File: manual.info.tmp,  Node: optimizing-innodb-configuration-variables,  Next: optimizing-innodb-many-tables,  Prev: optimizing-innodb-diskio,  Up: optimizing-innodb

8.5.8 Optimizing InnoDB Configuration Variables
-----------------------------------------------

Different settings work best for servers with light, predictable loads,
versus servers that are running near full capacity all the time, or that
experience spikes of high activity.

Because the 'InnoDB' storage engine performs many of its optimizations
automatically, many performance-tuning tasks involve monitoring to
ensure that the database is performing well, and changing configuration
options when performance drops.  See *note innodb-performance-schema::
for information about detailed 'InnoDB' performance monitoring.

For a list of the most important and most recent 'InnoDB' performance
features, see *note mysql-nutshell::.  Even if you have used 'InnoDB'
tables in prior versions, these features might be new to you, because
they are from the 'InnoDB Plugin'.  The Plugin co-existed alongside the
built-in 'InnoDB' in MySQL 5.1, and becomes the default storage engine
in MySQL 5.5 and higher.

The main configuration steps you can perform include:

   * Enabling 'InnoDB' to use high-performance memory allocators on
     systems that include them.  See *note
     innodb-performance-use_sys_malloc::.

   * Controlling the types of data change operations for which 'InnoDB'
     buffers the changed data, to avoid frequent small disk writes.  See
     *note innodb-change-buffer-configuration::.  Because the default is
     to buffer all types of data change operations, only change this
     setting if you need to reduce the amount of buffering.

   * Turning the adaptive hash indexing feature on and off using the
     'innodb_adaptive_hash_index' option.  See *note
     innodb-adaptive-hash:: for more information.  You might change this
     setting during periods of unusual activity, then restore it to its
     original setting.

   * Setting a limit on the number of concurrent threads that 'InnoDB'
     processes, if context switching is a bottleneck.  See *note
     innodb-performance-thread_concurrency::.

   * Controlling the amount of prefetching that 'InnoDB' does with its
     read-ahead operations.  When the system has unused I/O capacity,
     more read-ahead can improve the performance of queries.  Too much
     read-ahead can cause periodic drops in performance on a heavily
     loaded system.  See *note innodb-performance-read_ahead::.

   * Increasing the number of background threads for read or write
     operations, if you have a high-end I/O subsystem that is not fully
     utilized by the default values.  See *note
     innodb-performance-multiple_io_threads::.

   * Controlling how much I/O 'InnoDB' performs in the background.  See
     *note innodb-configuring-io-capacity::.  You might scale back this
     setting if you observe periodic drops in performance.

   * Controlling the algorithm that determines when 'InnoDB' performs
     certain types of background writes.  See *note
     innodb-buffer-pool-flushing::.  The algorithm works for some types
     of workloads but not others, so might turn off this setting if you
     observe periodic drops in performance.

   * Taking advantage of multicore processors and their cache memory
     configuration, to minimize delays in context switching.  See *note
     innodb-performance-spin_lock_polling::.

   * Preventing one-time operations such as table scans from interfering
     with the frequently accessed data stored in the 'InnoDB' buffer
     cache.  See *note innodb-performance-midpoint_insertion::.

   * Adjusting log files to a size that makes sense for reliability and
     crash recovery.  'InnoDB' log files have often been kept small to
     avoid long startup times after a crash.  Optimizations introduced
     in MySQL 5.5 speed up certain steps of the crash recovery process.
     In particular, scanning the redo log and applying the redo log are
     faster due to improved algorithms for memory management.  If you
     have kept your log files artificially small to avoid long startup
     times, you can now consider increasing log file size to reduce the
     I/O that occurs due recycling of redo log records.

   * Configuring the size and number of instances for the 'InnoDB'
     buffer pool, especially important for systems with multi-gigabyte
     buffer pools.  See *note innodb-multiple-buffer-pools::.

   * Increasing the maximum number of concurrent transactions, which
     dramatically improves scalability for the busiest databases.  See
     *note innodb-undo-logs::.  Although this feature does not require
     any action during day-to-day operation, you must perform a slow
     shutdown during or after upgrading the database to MySQL 5.5 to
     enable the higher limit.

   * Moving purge operations (a type of garbage collection) into a
     background thread.  See *note innodb-purge-configuration::.  To
     effectively measure the results of this setting, tune the other
     I/O-related and thread-related configuration settings first.

   * Reducing the amount of switching that 'InnoDB' does between
     concurrent threads, so that SQL operations on a busy server do not
     queue up and form a 'traffic jam'.  Set a value for the
     'innodb_thread_concurrency' option, up to approximately 32 for a
     high-powered modern system.  Increase the value for the
     'innodb_concurrency_tickets' option, typically to 5000 or so.  This
     combination of options sets a cap on the number of threads that
     'InnoDB' processes at any one time, and allows each thread to do
     substantial work before being swapped out, so that the number of
     waiting threads stays low and operations can complete without
     excessive context switching.


File: manual.info.tmp,  Node: optimizing-innodb-many-tables,  Prev: optimizing-innodb-configuration-variables,  Up: optimizing-innodb

8.5.9 Optimizing InnoDB for Systems with Many Tables
----------------------------------------------------

   * 'InnoDB' computes index cardinality values for a table the first
     time that table is accessed after startup, instead of storing such
     values in the table.  This step can take significant time on
     systems that partition the data into many tables.  Since this
     overhead only applies to the initial table open operation, to 'warm
     up' a table for later use, access it immediately after startup by
     issuing a statement such as 'SELECT 1 FROM TBL_NAME LIMIT 1'.


File: manual.info.tmp,  Node: optimizing-myisam,  Next: optimizing-memory-tables,  Prev: optimizing-innodb,  Up: optimization

8.6 Optimizing for MyISAM Tables
================================

* Menu:

* optimizing-queries-myisam::    Optimizing MyISAM Queries
* optimizing-myisam-bulk-data-loading::  Bulk Data Loading for MyISAM Tables
* repair-table-optimization::    Optimizing REPAIR TABLE Statements

The *note 'MyISAM': myisam-storage-engine. storage engine performs best
with read-mostly data or with low-concurrency operations, because table
locks limit the ability to perform simultaneous updates.  In MySQL 5.5,
*note 'InnoDB': innodb-storage-engine. is the default storage engine
rather than 'MyISAM'.


File: manual.info.tmp,  Node: optimizing-queries-myisam,  Next: optimizing-myisam-bulk-data-loading,  Prev: optimizing-myisam,  Up: optimizing-myisam

8.6.1 Optimizing MyISAM Queries
-------------------------------

Some general tips for speeding up queries on 'MyISAM' tables:

   * To help MySQL better optimize queries, use *note 'ANALYZE TABLE':
     analyze-table. or run *note 'myisamchk --analyze': myisamchk. on a
     table after it has been loaded with data.  This updates a value for
     each index part that indicates the average number of rows that have
     the same value.  (For unique indexes, this is always 1.)  MySQL
     uses this to decide which index to choose when you join two tables
     based on a nonconstant expression.  You can check the result from
     the table analysis by using 'SHOW INDEX FROM TBL_NAME' and
     examining the 'Cardinality' value.  *note 'myisamchk --description
     --verbose': myisamchk. shows index distribution information.

   * To sort an index and data according to an index, use *note
     'myisamchk --sort-index --sort-records=1': myisamchk. (assuming
     that you want to sort on index 1).  This is a good way to make
     queries faster if you have a unique index from which you want to
     read all rows in order according to the index.  The first time you
     sort a large table this way, it may take a long time.

   * Try to avoid complex *note 'SELECT': select. queries on 'MyISAM'
     tables that are updated frequently, to avoid problems with table
     locking that occur due to contention between readers and writers.

   * 'MyISAM' supports concurrent inserts: If a table has no free blocks
     in the middle of the data file, you can *note 'INSERT': insert. new
     rows into it at the same time that other threads are reading from
     the table.  If it is important to be able to do this, consider
     using the table in ways that avoid deleting rows.  Another
     possibility is to run *note 'OPTIMIZE TABLE': optimize-table. to
     defragment the table after you have deleted a lot of rows from it.
     This behavior is altered by setting the 'concurrent_insert'
     variable.  You can force new rows to be appended (and therefore
     permit concurrent inserts), even in tables that have deleted rows.
     See *note concurrent-inserts::.

   * For 'MyISAM' tables that change frequently, try to avoid all
     variable-length columns (*note 'VARCHAR': char, *note 'BLOB': blob,
     and *note 'TEXT': blob.).  The table uses dynamic row format if it
     includes even a single variable-length column.  See *note
     storage-engines::.

   * It is normally not useful to split a table into different tables
     just because the rows become large.  In accessing a row, the
     biggest performance hit is the disk seek needed to find the first
     byte of the row.  After finding the data, most modern disks can
     read the entire row fast enough for most applications.  The only
     cases where splitting up a table makes an appreciable difference is
     if it is a 'MyISAM' table using dynamic row format that you can
     change to a fixed row size, or if you very often need to scan the
     table but do not need most of the columns.  See *note
     storage-engines::.

   * Use 'ALTER TABLE ... ORDER BY EXPR1, EXPR2, ...' if you usually
     retrieve rows in 'EXPR1, EXPR2, ...' order.  By using this option
     after extensive changes to the table, you may be able to get higher
     performance.

   * If you often need to calculate results such as counts based on
     information from a lot of rows, it may be preferable to introduce a
     new table and update the counter in real time.  An update of the
     following form is very fast:

          UPDATE TBL_NAME SET COUNT_COL=COUNT_COL+1 WHERE KEY_COL=CONSTANT;

     This is very important when you use a MySQL storage engine such as
     'MyISAM' that has only table-level locking (multiple readers with
     single writers).  This also gives better performance with most
     database systems, because the row locking manager in this case has
     less to do.

   * Use *note 'INSERT DELAYED': insert-delayed. when you do not need to
     know when your data is written.  This reduces the overall insertion
     impact because many rows can be written with a single disk write.

   * Use 'INSERT LOW_PRIORITY' when you want to give *note 'SELECT':
     select. statements higher priority than your inserts.

     Use 'SELECT HIGH_PRIORITY' to get retrievals that jump the queue.
     That is, the *note 'SELECT': select. is executed even if there is
     another client waiting to do a write.

     'LOW_PRIORITY' and 'HIGH_PRIORITY' have an effect only for storage
     engines that use only table-level locking (such as 'MyISAM',
     'MEMORY', and 'MERGE').

   * Use *note 'OPTIMIZE TABLE': optimize-table. periodically to avoid
     fragmentation with dynamic-format 'MyISAM' tables.  See *note
     myisam-table-formats::.

   * Declaring a 'MyISAM' table with the 'DELAY_KEY_WRITE=1' table
     option makes index updates faster because they are not flushed to
     disk until the table is closed.  The downside is that if something
     kills the server while such a table is open, you must ensure that
     the table is okay by running the server with the
     'myisam_recover_options' system variable set, or by running *note
     'myisamchk': myisamchk. before restarting the server.  (However,
     even in this case, you should not lose anything by using
     'DELAY_KEY_WRITE', because the key information can always be
     generated from the data rows.)

   * Strings are automatically prefix- and end-space compressed in
     'MyISAM' indexes.  See *note create-index::.

   * You can increase performance by caching queries or answers in your
     application and then executing many inserts or updates together.
     Locking the table during this operation ensures that the index
     cache is only flushed once after all updates.  You can also take
     advantage of MySQL's query cache to achieve similar results; see
     *note query-cache::.


File: manual.info.tmp,  Node: optimizing-myisam-bulk-data-loading,  Next: repair-table-optimization,  Prev: optimizing-queries-myisam,  Up: optimizing-myisam

8.6.2 Bulk Data Loading for MyISAM Tables
-----------------------------------------

These performance tips supplement the general guidelines for fast
inserts in *note insert-optimization::.

   * To improve performance when multiple clients insert a lot of rows,
     use the *note 'INSERT DELAYED': insert-delayed. statement.  See
     *note insert-delayed::.  This technique works for 'MyISAM' and some
     other storage engines, but not 'InnoDB'.

   * For a 'MyISAM' table, you can use concurrent inserts to add rows at
     the same time that *note 'SELECT': select. statements are running,
     if there are no deleted rows in middle of the data file.  See *note
     concurrent-inserts::.

   * With some extra work, it is possible to make *note 'LOAD DATA':
     load-data. run even faster for a 'MyISAM' table when the table has
     many indexes.  Use the following procedure:

       1. Execute a 'FLUSH TABLES' statement or a *note 'mysqladmin
          flush-tables': mysqladmin. command.

       2. Use *note 'myisamchk --keys-used=0 -rq /PATH/TO/DB/TBL_NAME':
          myisamchk. to remove all use of indexes for the table.

       3. Insert data into the table with *note 'LOAD DATA': load-data.
          This does not update any indexes and therefore is very fast.

       4. If you intend only to read from the table in the future, use
          *note 'myisampack': myisampack. to compress it.  See *note
          compressed-format::.

       5. Re-create the indexes with *note 'myisamchk -rq
          /PATH/TO/DB/TBL_NAME': myisamchk.  This creates the index tree
          in memory before writing it to disk, which is much faster than
          updating the index during *note 'LOAD DATA': load-data.
          because it avoids lots of disk seeks.  The resulting index
          tree is also perfectly balanced.

       6. Execute a 'FLUSH TABLES' statement or a *note 'mysqladmin
          flush-tables': mysqladmin. command.

     *note 'LOAD DATA': load-data. performs the preceding optimization
     automatically if the 'MyISAM' table into which you insert data is
     empty.  The main difference between automatic optimization and
     using the procedure explicitly is that you can let *note
     'myisamchk': myisamchk. allocate much more temporary memory for the
     index creation than you might want the server to allocate for index
     re-creation when it executes the *note 'LOAD DATA': load-data.
     statement.

     You can also disable or enable the nonunique indexes for a 'MyISAM'
     table by using the following statements rather than *note
     'myisamchk': myisamchk.  If you use these statements, you can skip
     the 'FLUSH TABLES' operations:

          ALTER TABLE TBL_NAME DISABLE KEYS;
          ALTER TABLE TBL_NAME ENABLE KEYS;

   * To speed up *note 'INSERT': insert. operations that are performed
     with multiple statements for nontransactional tables, lock your
     tables:

          LOCK TABLES a WRITE;
          INSERT INTO a VALUES (1,23),(2,34),(4,33);
          INSERT INTO a VALUES (8,26),(6,29);
          ...
          UNLOCK TABLES;

     This benefits performance because the index buffer is flushed to
     disk only once, after all *note 'INSERT': insert. statements have
     completed.  Normally, there would be as many index buffer flushes
     as there are *note 'INSERT': insert. statements.  Explicit locking
     statements are not needed if you can insert all rows with a single
     *note 'INSERT': insert.

     Locking also lowers the total time for multiple-connection tests,
     although the maximum wait time for individual connections might go
     up because they wait for locks.  Suppose that five clients attempt
     to perform inserts simultaneously as follows:

        * Connection 1 does 1000 inserts

        * Connections 2, 3, and 4 do 1 insert

        * Connection 5 does 1000 inserts

     If you do not use locking, connections 2, 3, and 4 finish before 1
     and 5.  If you use locking, connections 2, 3, and 4 probably do not
     finish before 1 or 5, but the total time should be about 40%
     faster.

     *note 'INSERT': insert, *note 'UPDATE': update, and *note 'DELETE':
     delete. operations are very fast in MySQL, but you can obtain
     better overall performance by adding locks around everything that
     does more than about five successive inserts or updates.  If you do
     very many successive inserts, you could do a *note 'LOCK TABLES':
     lock-tables. followed by an *note 'UNLOCK TABLES': lock-tables.
     once in a while (each 1,000 rows or so) to permit other threads to
     access table.  This would still result in a nice performance gain.

     *note 'INSERT': insert. is still much slower for loading data than
     *note 'LOAD DATA': load-data, even when using the strategies just
     outlined.

   * To increase performance for 'MyISAM' tables, for both *note 'LOAD
     DATA': load-data. and *note 'INSERT': insert, enlarge the key cache
     by increasing the 'key_buffer_size' system variable.  See *note
     server-configuration::.


File: manual.info.tmp,  Node: repair-table-optimization,  Prev: optimizing-myisam-bulk-data-loading,  Up: optimizing-myisam

8.6.3 Optimizing REPAIR TABLE Statements
----------------------------------------

*note 'REPAIR TABLE': repair-table. for 'MyISAM' tables is similar to
using *note 'myisamchk': myisamchk. for repair operations, and some of
the same performance optimizations apply:

   * *note 'myisamchk': myisamchk. has variables that control memory
     allocation.  You may be able to its improve performance by setting
     these variables, as described in *note myisamchk-memory::.

   * For *note 'REPAIR TABLE': repair-table, the same principle applies,
     but because the repair is done by the server, you set server system
     variables instead of *note 'myisamchk': myisamchk. variables.
     Also, in addition to setting memory-allocation variables,
     increasing the 'myisam_max_sort_file_size' system variable
     increases the likelihood that the repair will use the faster
     filesort method and avoid the slower repair by key cache method.
     Set the variable to the maximum file size for your system, after
     checking to be sure that there is enough free space to hold a copy
     of the table files.  The free space must be available in the file
     system containing the original table files.

Suppose that a *note 'myisamchk': myisamchk. table-repair operation is
done using the following options to set its memory-allocation variables:

     --key_buffer_size=128M --myisam_sort_buffer_size=256M
     --read_buffer_size=64M --write_buffer_size=64M

Some of those *note 'myisamchk': myisamchk. variables correspond to
server system variables:

*note 'myisamchk': myisamchk.        System Variable
Variable                             

'key_buffer_size'                    'key_buffer_size'
                                     
'myisam_sort_buffer_size'            'myisam_sort_buffer_size'
                                     
'read_buffer_size'                   'read_buffer_size'
                                     
'write_buffer_size'                  none

Each of the server system variables can be set at runtime, and some of
them ('myisam_sort_buffer_size', 'read_buffer_size') have a session
value in addition to a global value.  Setting a session value limits the
effect of the change to your current session and does not affect other
users.  Changing a global-only variable ('key_buffer_size',
'myisam_max_sort_file_size') affects other users as well.  For
'key_buffer_size', you must take into account that the buffer is shared
with those users.  For example, if you set the *note 'myisamchk':
myisamchk. 'key_buffer_size' variable to 128MB, you could set the
corresponding 'key_buffer_size' system variable larger than that (if it
is not already set larger), to permit key buffer use by activity in
other sessions.  However, changing the global key buffer size
invalidates the buffer, causing increased disk I/O and slowdown for
other sessions.  An alternative that avoids this problem is to use a
separate key cache, assign to it the indexes from the table to be
repaired, and deallocate it when the repair is complete.  See *note
multiple-key-caches::.

Based on the preceding remarks, a *note 'REPAIR TABLE': repair-table.
operation can be done as follows to use settings similar to the *note
'myisamchk': myisamchk. command.  Here a separate 128MB key buffer is
allocated and the file system is assumed to permit a file size of at
least 100GB.

     SET SESSION myisam_sort_buffer_size = 256*1024*1024;
     SET SESSION read_buffer_size = 64*1024*1024;
     SET GLOBAL myisam_max_sort_file_size = 100*1024*1024*1024;
     SET GLOBAL repair_cache.key_buffer_size = 128*1024*1024;
     CACHE INDEX TBL_NAME IN repair_cache;
     LOAD INDEX INTO CACHE TBL_NAME;
     REPAIR TABLE TBL_NAME ;
     SET GLOBAL repair_cache.key_buffer_size = 0;

If you intend to change a global variable but want to do so only for the
duration of a *note 'REPAIR TABLE': repair-table. operation to minimally
affect other users, save its value in a user variable and restore it
afterward.  For example:

     SET @old_myisam_sort_buffer_size = @@GLOBAL.myisam_max_sort_file_size;
     SET GLOBAL myisam_max_sort_file_size = 100*1024*1024*1024;
     REPAIR TABLE tbl_name ;
     SET GLOBAL myisam_max_sort_file_size = @old_myisam_max_sort_file_size;

The system variables that affect *note 'REPAIR TABLE': repair-table. can
be set globally at server startup if you want the values to be in effect
by default.  For example, add these lines to the server 'my.cnf' file:

     [mysqld]
     myisam_sort_buffer_size=256M
     key_buffer_size=1G
     myisam_max_sort_file_size=100G

These settings do not include 'read_buffer_size'.  Setting
'read_buffer_size' globally to a large value does so for all sessions
and can cause performance to suffer due to excessive memory allocation
for a server with many simultaneous sessions.


File: manual.info.tmp,  Node: optimizing-memory-tables,  Next: execution-plan-information,  Prev: optimizing-myisam,  Up: optimization

8.7 Optimizing for MEMORY Tables
================================

Consider using 'MEMORY' tables for noncritical data that is accessed
often, and is read-only or rarely updated.  Benchmark your application
against equivalent 'InnoDB' or 'MyISAM' tables under a realistic
workload, to confirm that any additional performance is worth the risk
of losing data, or the overhead of copying data from a disk-based table
at application start.

For best performance with 'MEMORY' tables, examine the kinds of queries
against each table, and specify the type to use for each associated
index, either a B-tree index or a hash index.  On the *note 'CREATE
INDEX': create-index. statement, use the clause 'USING BTREE' or 'USING
HASH'.  B-tree indexes are fast for queries that do greater-than or
less-than comparisons through operators such as '>' or 'BETWEEN'.  Hash
indexes are only fast for queries that look up single values through the
'=' operator, or a restricted set of values through the 'IN' operator.
For why 'USING BTREE' is often a better choice than the default 'USING
HASH', see *note table-scan-avoidance::.  For implementation details of
the different types of 'MEMORY' indexes, see *note index-btree-hash::.


File: manual.info.tmp,  Node: execution-plan-information,  Next: controlling-optimizer,  Prev: optimizing-memory-tables,  Up: optimization

8.8 Understanding the Query Execution Plan
==========================================

* Menu:

* using-explain::                Optimizing Queries with EXPLAIN
* explain-output::               EXPLAIN Output Format
* explain-extended::             Extended EXPLAIN Output Format
* estimating-performance::       Estimating Query Performance

Depending on the details of your tables, columns, indexes, and the
conditions in your 'WHERE' clause, the MySQL optimizer considers many
techniques to efficiently perform the lookups involved in an SQL query.
A query on a huge table can be performed without reading all the rows; a
join involving several tables can be performed without comparing every
combination of rows.  The set of operations that the optimizer chooses
to perform the most efficient query is called the 'query execution
plan', also known as the *note 'EXPLAIN': explain. plan.  Your goals are
to recognize the aspects of the *note 'EXPLAIN': explain. plan that
indicate a query is optimized well, and to learn the SQL syntax and
indexing techniques to improve the plan if you see some inefficient
operations.


File: manual.info.tmp,  Node: using-explain,  Next: explain-output,  Prev: execution-plan-information,  Up: execution-plan-information

8.8.1 Optimizing Queries with EXPLAIN
-------------------------------------

The *note 'EXPLAIN': explain. statement provides information about how
MySQL executes statements:

   * When you precede a *note 'SELECT': select. statement with the
     keyword *note 'EXPLAIN': explain, MySQL displays information from
     the optimizer about the statement execution plan.  That is, MySQL
     explains how it would process the statement, including information
     about how tables are joined and in which order.  For information
     about using *note 'EXPLAIN': explain. to obtain execution plan
     information, see *note explain-output::.

   * *note 'EXPLAIN EXTENDED': explain-extended. produces additional
     execution plan information that can be displayed using *note 'SHOW
     WARNINGS': show-warnings.  See *note explain-extended::.

   * *note 'EXPLAIN PARTITIONS': explain. is useful for examining
     queries involving partitioned tables.  See *note
     partitioning-info::.

With the help of *note 'EXPLAIN': explain, you can see where you should
add indexes to tables so that the statement executes faster by using
indexes to find rows.  You can also use *note 'EXPLAIN': explain. to
check whether the optimizer joins the tables in an optimal order.  To
give a hint to the optimizer to use a join order corresponding to the
order in which the tables are named in a *note 'SELECT': select.
statement, begin the statement with 'SELECT STRAIGHT_JOIN' rather than
just *note 'SELECT': select.  (See *note select::.)

If you have a problem with indexes not being used when you believe that
they should be, run *note 'ANALYZE TABLE': analyze-table. to update
table statistics, such as cardinality of keys, that can affect the
choices the optimizer makes.  See *note analyze-table::.

*Note*:

*note 'EXPLAIN': explain. can also be used to obtain information about
the columns in a table.  *note 'EXPLAIN TBL_NAME': explain. is
synonymous with 'DESCRIBE TBL_NAME' and 'SHOW COLUMNS FROM TBL_NAME'.
For more information, see *note describe::, and *note show-columns::.


File: manual.info.tmp,  Node: explain-output,  Next: explain-extended,  Prev: using-explain,  Up: execution-plan-information

8.8.2 EXPLAIN Output Format
---------------------------

The *note 'EXPLAIN': explain. statement provides information about the
execution plan for a *note 'SELECT': select. statement.

*note 'EXPLAIN': explain. returns a row of information for each table
used in the *note 'SELECT': select. statement.  It lists the tables in
the output in the order that MySQL would read them while processing the
statement.  MySQL resolves all joins using a nested-loop join method.
This means that MySQL reads a row from the first table, and then finds a
matching row in the second table, the third table, and so on.  When all
tables are processed, MySQL outputs the selected columns and backtracks
through the table list until a table is found for which there are more
matching rows.  The next row is read from this table and the process
continues with the next table.

When the 'EXTENDED' keyword is used, *note 'EXPLAIN': explain. produces
extra information that can be viewed by issuing a *note 'SHOW WARNINGS':
show-warnings. statement following the *note 'EXPLAIN': explain.
statement.  *note 'EXPLAIN EXTENDED': explain-extended. also displays
the 'filtered' column.  See *note explain-extended::.

*Note*:

You cannot use the 'EXTENDED' and 'PARTITIONS' keywords together in the
same *note 'EXPLAIN': explain. statement.

*Note*:

MySQL Workbench has a Visual Explain capability that provides a visual
representation of *note 'EXPLAIN': explain. output.  See Tutorial: Using
Explain to Improve Query Performance
(https://dev.mysql.com/doc/workbench/en/wb-tutorial-visual-explain-dbt3.html).

   * *note explain-output-columns::

   * *note explain-join-types::

   * *note explain-extra-information::

   * *note explain-output-interpretation::

*EXPLAIN Output Columns*

This section describes the output columns produced by *note 'EXPLAIN':
explain.  Later sections provide additional information about the 'type'
and 'Extra' columns.

Each output row from *note 'EXPLAIN': explain. provides information
about one table.  Each row contains the values summarized in *note
explain-output-column-table::, and described in more detail following
the table.

*EXPLAIN Output Columns*

Column             Meaning
                   
'id'               The 'SELECT' identifier
                   
'select_type'      The 'SELECT' type
                   
'table'            The table for the output row
                   
'partitions'       The matching partitions
                   
'type'             The join type
                   
'possible_keys'    The possible indexes to choose
                   
'key'              The index actually chosen
                   
'key_len'          The length of the chosen key
                   
'ref'              The columns compared to the index
                   
'rows'             Estimate of rows to be examined
                   
'filtered'         Percentage of rows filtered by table condition
                   
'Extra'            Additional information

   * 'id'

     The *note 'SELECT': select. identifier.  This is the sequential
     number of the *note 'SELECT': select. within the query.  The value
     can be 'NULL' if the row refers to the union result of other rows.
     In this case, the 'table' column shows a value like '<unionM,N>' to
     indicate that the row refers to the union of the rows with 'id'
     values of M and N.

   * 'select_type'

     The type of *note 'SELECT': select, which can be any of those shown
     in the following table.

     'select_type'      Meaning
     Value              

     'SIMPLE'           Simple *note 'SELECT': select. (not using
                        *note 'UNION': union. or subqueries)
                        
     'PRIMARY'          Outermost *note 'SELECT': select.
                        
     *note 'UNION': union.Second or later *note 'SELECT': select. statement in
                        a *note 'UNION': union.
                        
     'DEPENDENT         Second or later *note 'SELECT': select. statement in
     UNION'             a *note 'UNION': union, dependent on outer query
                        
     'UNION RESULT'     Result of a *note 'UNION': union.
                        
     'SUBQUERY'         First *note 'SELECT': select. in subquery
                        
     'DEPENDENT         First *note 'SELECT': select. in subquery, dependent
     SUBQUERY'          on outer query
                        
     'DERIVED'          Derived table
                        
     'UNCACHEABLE       A subquery for which the result cannot be cached and
     SUBQUERY'          must be re-evaluated for each row of the outer query
                        
     'UNCACHEABLE       The second or later select in a
     UNION'             *note 'UNION': union. that belongs to an uncacheable
                        subquery (see 'UNCACHEABLE SUBQUERY')

     'DEPENDENT' typically signifies the use of a correlated subquery.
     See *note correlated-subqueries::.

     'DEPENDENT SUBQUERY' evaluation differs from 'UNCACHEABLE SUBQUERY'
     evaluation.  For 'DEPENDENT SUBQUERY', the subquery is re-evaluated
     only once for each set of different values of the variables from
     its outer context.  For 'UNCACHEABLE SUBQUERY', the subquery is
     re-evaluated for each row of the outer context.

     Cacheability of subqueries differs from caching of query results in
     the query cache (which is described in *note
     query-cache-operation::).  Subquery caching occurs during query
     execution, whereas the query cache is used to store results only
     after query execution finishes.

   * 'table'

     The name of the table to which the row of output refers.  This can
     also be one of the following values:

        * '<unionM,N>': The row refers to the union of the rows with
          'id' values of M and N.

        * '<derivedN>': The row refers to the derived table result for
          the row with an 'id' value of N.  A derived table may result,
          for example, from a subquery in the 'FROM' clause.

   * 'partitions'

     The partitions from which records would be matched by the query.
     This column is displayed only if the 'PARTITIONS' keyword is used.
     The value is 'NULL' for nonpartitioned tables.  See *note
     partitioning-info::.

   * 'type'

     The join type.  For descriptions of the different types, see
     'EXPLAIN' Join Types.

   * 'possible_keys'

     The 'possible_keys' column indicates the indexes from which MySQL
     can choose to find the rows in this table.  Note that this column
     is totally independent of the order of the tables as displayed in
     the output from *note 'EXPLAIN': explain.  That means that some of
     the keys in 'possible_keys' might not be usable in practice with
     the generated table order.

     If this column is 'NULL', there are no relevant indexes.  In this
     case, you may be able to improve the performance of your query by
     examining the 'WHERE' clause to check whether it refers to some
     column or columns that would be suitable for indexing.  If so,
     create an appropriate index and check the query with *note
     'EXPLAIN': explain. again.  See *note alter-table::.

     To see what indexes a table has, use 'SHOW INDEX FROM TBL_NAME'.

   * 'key'

     The 'key' column indicates the key (index) that MySQL actually
     decided to use.  If MySQL decides to use one of the 'possible_keys'
     indexes to look up rows, that index is listed as the key value.

     It is possible that 'key' will name an index that is not present in
     the 'possible_keys' value.  This can happen if none of the
     'possible_keys' indexes are suitable for looking up rows, but all
     the columns selected by the query are columns of some other index.
     That is, the named index covers the selected columns, so although
     it is not used to determine which rows to retrieve, an index scan
     is more efficient than a data row scan.

     For 'InnoDB', a secondary index might cover the selected columns
     even if the query also selects the primary key because 'InnoDB'
     stores the primary key value with each secondary index.  If 'key'
     is 'NULL', MySQL found no index to use for executing the query more
     efficiently.

     To force MySQL to use or ignore an index listed in the
     'possible_keys' column, use 'FORCE INDEX', 'USE INDEX', or 'IGNORE
     INDEX' in your query.  See *note index-hints::.

     For 'MyISAM' and 'NDB' tables, running *note 'ANALYZE TABLE':
     analyze-table. helps the optimizer choose better indexes.  For
     'NDB' tables, this also improves performance of distributed
     pushed-down joins.  For 'MyISAM' tables, *note 'myisamchk
     --analyze': myisamchk. does the same as *note 'ANALYZE TABLE':
     analyze-table.  See *note myisam-table-maintenance::.

   * 'key_len'

     The 'key_len' column indicates the length of the key that MySQL
     decided to use.  The value of 'key_len' enables you to determine
     how many parts of a multiple-part key MySQL actually uses.  If the
     'key' column says 'NULL', the 'len_len' column also says 'NULL'.

     Due to the key storage format, the key length is one greater for a
     column that can be 'NULL' than for a 'NOT NULL' column.

   * 'ref'

     The 'ref' column shows which columns or constants are compared to
     the index named in the 'key' column to select rows from the table.

   * 'rows'

     The 'rows' column indicates the number of rows MySQL believes it
     must examine to execute the query.

     For *note 'InnoDB': innodb-storage-engine. tables, this number is
     an estimate, and may not always be exact.

   * 'filtered'

     The 'filtered' column indicates an estimated percentage of table
     rows that will be filtered by the table condition.  The maximum
     value is 100, which means no filtering of rows occurred.  Values
     decreasing from 100 indicate increasing amounts of filtering.
     'rows' shows the estimated number of rows examined and 'rows' x
     'filtered' shows the number of rows that will be joined with the
     following table.  For example, if 'rows' is 1000 and 'filtered' is
     50.00 (50%), the number of rows to be joined with the following
     table is 1000 x 50% = 500.  This column is displayed if you use
     *note 'EXPLAIN EXTENDED': explain-extended.

   * 'Extra'

     This column contains additional information about how MySQL
     resolves the query.  For descriptions of the different values, see
     'EXPLAIN' Extra Information.

*EXPLAIN Join Types*

The 'type' column of *note 'EXPLAIN': explain. output describes how
tables are joined.  The following list describes the join types, ordered
from the best type to the worst:

   * 
     'system'

     The table has only one row (= system table).  This is a special
     case of the 'const' join type.

   * 
     'const'

     The table has at most one matching row, which is read at the start
     of the query.  Because there is only one row, values from the
     column in this row can be regarded as constants by the rest of the
     optimizer.  'const' tables are very fast because they are read only
     once.

     'const' is used when you compare all parts of a 'PRIMARY KEY' or
     'UNIQUE' index to constant values.  In the following queries,
     TBL_NAME can be used as a 'const' table:

          SELECT * FROM TBL_NAME WHERE PRIMARY_KEY=1;

          SELECT * FROM TBL_NAME
            WHERE PRIMARY_KEY_PART1=1 AND PRIMARY_KEY_PART2=2;

   * 
     'eq_ref'

     One row is read from this table for each combination of rows from
     the previous tables.  Other than the 'system' and 'const' types,
     this is the best possible join type.  It is used when all parts of
     an index are used by the join and the index is a 'PRIMARY KEY' or
     'UNIQUE NOT NULL' index.

     'eq_ref' can be used for indexed columns that are compared using
     the '=' operator.  The comparison value can be a constant or an
     expression that uses columns from tables that are read before this
     table.  In the following examples, MySQL can use an 'eq_ref' join
     to process REF_TABLE:

          SELECT * FROM REF_TABLE,OTHER_TABLE
            WHERE REF_TABLE.KEY_COLUMN=OTHER_TABLE.COLUMN;

          SELECT * FROM REF_TABLE,OTHER_TABLE
            WHERE REF_TABLE.KEY_COLUMN_PART1=OTHER_TABLE.COLUMN
            AND REF_TABLE.KEY_COLUMN_PART2=1;

   * 
     'ref'

     All rows with matching index values are read from this table for
     each combination of rows from the previous tables.  'ref' is used
     if the join uses only a leftmost prefix of the key or if the key is
     not a 'PRIMARY KEY' or 'UNIQUE' index (in other words, if the join
     cannot select a single row based on the key value).  If the key
     that is used matches only a few rows, this is a good join type.

     'ref' can be used for indexed columns that are compared using the
     '=' or '<=>' operator.  In the following examples, MySQL can use a
     'ref' join to process REF_TABLE:

          SELECT * FROM REF_TABLE WHERE KEY_COLUMN=EXPR;

          SELECT * FROM REF_TABLE,OTHER_TABLE
            WHERE REF_TABLE.KEY_COLUMN=OTHER_TABLE.COLUMN;

          SELECT * FROM REF_TABLE,OTHER_TABLE
            WHERE REF_TABLE.KEY_COLUMN_PART1=OTHER_TABLE.COLUMN
            AND REF_TABLE.KEY_COLUMN_PART2=1;

   * 
     'fulltext'

     The join is performed using a 'FULLTEXT' index.

   * 
     'ref_or_null'

     This join type is like 'ref', but with the addition that MySQL does
     an extra search for rows that contain 'NULL' values.  This join
     type optimization is used most often in resolving subqueries.  In
     the following examples, MySQL can use a 'ref_or_null' join to
     process REF_TABLE:

          SELECT * FROM REF_TABLE
            WHERE KEY_COLUMN=EXPR OR KEY_COLUMN IS NULL;

     See *note is-null-optimization::.

   * 
     'index_merge'

     This join type indicates that the Index Merge optimization is used.
     In this case, the 'key' column in the output row contains a list of
     indexes used, and 'key_len' contains a list of the longest key
     parts for the indexes used.  For more information, see *note
     index-merge-optimization::.

   * 
     'unique_subquery'

     This type replaces 'eq_ref' for some 'IN' subqueries of the
     following form:

          VALUE IN (SELECT PRIMARY_KEY FROM SINGLE_TABLE WHERE SOME_EXPR)

     'unique_subquery' is just an index lookup function that replaces
     the subquery completely for better efficiency.

   * 
     'index_subquery'

     This join type is similar to 'unique_subquery'.  It replaces 'IN'
     subqueries, but it works for nonunique indexes in subqueries of the
     following form:

          VALUE IN (SELECT KEY_COLUMN FROM SINGLE_TABLE WHERE SOME_EXPR)

   * 
     'range'

     Only rows that are in a given range are retrieved, using an index
     to select the rows.  The 'key' column in the output row indicates
     which index is used.  The 'key_len' contains the longest key part
     that was used.  The 'ref' column is 'NULL' for this type.

     'range' can be used when a key column is compared to a constant
     using any of the '=', '<>', '>', '>=', '<', '<=', 'IS NULL', '<=>',
     'BETWEEN', 'LIKE', or 'IN()' operators:

          SELECT * FROM TBL_NAME
            WHERE KEY_COLUMN = 10;

          SELECT * FROM TBL_NAME
            WHERE KEY_COLUMN BETWEEN 10 and 20;

          SELECT * FROM TBL_NAME
            WHERE KEY_COLUMN IN (10,20,30);

          SELECT * FROM TBL_NAME
            WHERE KEY_PART1 = 10 AND KEY_PART2 IN (10,20,30);

   * 
     'index'

     The 'index' join type is the same as 'ALL', except that the index
     tree is scanned.  This occurs two ways:

        * If the index is a covering index for the queries and can be
          used to satisfy all data required from the table, only the
          index tree is scanned.  In this case, the 'Extra' column says
          'Using index'.  An index-only scan usually is faster than
          'ALL' because the size of the index usually is smaller than
          the table data.

        * A full table scan is performed using reads from the index to
          look up data rows in index order.  'Uses index' does not
          appear in the 'Extra' column.

     MySQL can use this join type when the query uses only columns that
     are part of a single index.

   * 
     'ALL'

     A full table scan is done for each combination of rows from the
     previous tables.  This is normally not good if the table is the
     first table not marked 'const', and usually _very_ bad in all other
     cases.  Normally, you can avoid 'ALL' by adding indexes that enable
     row retrieval from the table based on constant values or column
     values from earlier tables.

*EXPLAIN Extra Information*

The 'Extra' column of *note 'EXPLAIN': explain. output contains
additional information about how MySQL resolves the query.  The
following list explains the values that can appear in this column.  If
you want to make your queries as fast as possible, look out for 'Extra'
values of 'Using filesort' and 'Using temporary'.

   * 'Child of 'TABLE' pushed join@1'

     This table is referenced as the child of TABLE in a join that can
     be pushed down to the NDB kernel.  Applies only in MySQL NDB
     Cluster 7.2 and later, when pushed-down joins are enabled.  See the
     description of the 'ndb_join_pushdown' server system variable for
     more information and examples.

   * 'const row not found'

     For a query such as 'SELECT ... FROM TBL_NAME', the table was
     empty.

   * 'Distinct'

     MySQL is looking for distinct values, so it stops searching for
     more rows for the current row combination after it has found the
     first matching row.

   * 'Full scan on NULL key'

     This occurs for subquery optimization as a fallback strategy when
     the optimizer cannot use an index-lookup access method.

   * 'Impossible HAVING'

     The 'HAVING' clause is always false and cannot select any rows.

   * 'Impossible WHERE'

     The 'WHERE' clause is always false and cannot select any rows.

   * 'Impossible WHERE noticed after reading const tables'

     MySQL has read all 'const' (and 'system') tables and notice that
     the 'WHERE' clause is always false.

   * 'No matching min/max row'

     No row satisfies the condition for a query such as 'SELECT MIN(...)
     FROM ... WHERE CONDITION'.

   * 'no matching row in const table'

     For a query with a join, there was an empty table or a table with
     no rows satisfying a unique index condition.

   * 'No tables used'

     The query has no 'FROM' clause, or has a 'FROM DUAL' clause.

   * 'Not exists'

     MySQL was able to do a 'LEFT JOIN' optimization on the query and
     does not examine more rows in this table for the previous row
     combination after it finds one row that matches the 'LEFT JOIN'
     criteria.  Here is an example of the type of query that can be
     optimized this way:

          SELECT * FROM t1 LEFT JOIN t2 ON t1.id=t2.id
            WHERE t2.id IS NULL;

     Assume that 't2.id' is defined as 'NOT NULL'.  In this case, MySQL
     scans 't1' and looks up the rows in 't2' using the values of
     't1.id'.  If MySQL finds a matching row in 't2', it knows that
     't2.id' can never be 'NULL', and does not scan through the rest of
     the rows in 't2' that have the same 'id' value.  In other words,
     for each row in 't1', MySQL needs to do only a single lookup in
     't2', regardless of how many rows actually match in 't2'.

   * 'Range checked for each record (index map: N)'

     MySQL found no good index to use, but found that some of indexes
     might be used after column values from preceding tables are known.
     For each row combination in the preceding tables, MySQL checks
     whether it is possible to use a 'range' or 'index_merge' access
     method to retrieve rows.  This is not very fast, but is faster than
     performing a join with no index at all.  The applicability criteria
     are as described in *note range-optimization::, and *note
     index-merge-optimization::, with the exception that all column
     values for the preceding table are known and considered to be
     constants.

     Indexes are numbered beginning with 1, in the same order as shown
     by *note 'SHOW INDEX': show-index. for the table.  The index map
     value N is a bitmask value that indicates which indexes are
     candidates.  For example, a value of '0x19' (binary 11001) means
     that indexes 1, 4, and 5 will be considered.

   * 'Scanned N databases'

     This indicates how many directory scans the server performs when
     processing a query for 'INFORMATION_SCHEMA' tables, as described in
     *note information-schema-optimization::.  The value of N can be 0,
     1, or 'all'.

   * 'Select tables optimized away'

     The optimizer determined 1) that at most one row should be
     returned, and 2) that to produce this row, a deterministic set of
     rows must be read.  When the rows to be read can be read during the
     optimization phase (for example, by reading index rows), there is
     no need to read any tables during query execution.

     The first condition is fulfilled when the query is implicitly
     grouped (contains an aggregate function but no 'GROUP BY' clause).
     The second condition is fulfilled when one row lookup is performed
     per index used.  The number of indexes read determines the number
     of rows to read.

     Consider the following implicitly grouped query:

          SELECT MIN(c1), MIN(c2) FROM t1;

     Suppose that 'MIN(c1)' can be retrieved by reading one index row
     and 'MIN(c2)' can be retrieved by reading one row from a different
     index.  That is, for each column 'c1' and 'c2', there exists an
     index where the column is the first column of the index.  In this
     case, one row is returned, produced by reading two deterministic
     rows.

     This 'Extra' value does not occur if the rows to read are not
     deterministic.  Consider this query:

          SELECT MIN(c2) FROM t1 WHERE c1 <= 10;

     Suppose that '(c1, c2)' is a covering index.  Using this index, all
     rows with 'c1 <= 10' must be scanned to find the minimum 'c2'
     value.  By contrast, consider this query:

          SELECT MIN(c2) FROM t1 WHERE c1 = 10;

     In this case, the first index row with 'c1 = 10' contains the
     minimum 'c2' value.  Only one row must be read to produce the
     returned row.

     For storage engines that maintain an exact row count per table
     (such as 'MyISAM', but not 'InnoDB'), this 'Extra' value can occur
     for 'COUNT(*)' queries for which the 'WHERE' clause is missing or
     always true and there is no 'GROUP BY' clause.  (This is an
     instance of an implicitly grouped query where the storage engine
     influences whether a deterministic number of rows can be read.)

   * 'Skip_open_table', 'Open_frm_only', 'Open_full_table'

     These values indicate file-opening optimizations that apply to
     queries for 'INFORMATION_SCHEMA' tables, as described in *note
     information-schema-optimization::.

        * 'Skip_open_table': Table files do not need to be opened.  The
          information has already become available within the query by
          scanning the database directory.

        * 'Open_frm_only': Only the table's '.frm' file need be opened.

        * 'Open_full_table': The unoptimized information lookup.  The
          '.frm', '.MYD', and '.MYI' files must be opened.

   * 'unique row not found'

     For a query such as 'SELECT ... FROM TBL_NAME', no rows satisfy the
     condition for a 'UNIQUE' index or 'PRIMARY KEY' on the table.

   * 'Using filesort'

     MySQL must do an extra pass to find out how to retrieve the rows in
     sorted order.  The sort is done by going through all rows according
     to the join type and storing the sort key and pointer to the row
     for all rows that match the 'WHERE' clause.  The keys then are
     sorted and the rows are retrieved in sorted order.  See *note
     order-by-optimization::.

   * 'Using index'

     The column information is retrieved from the table using only
     information in the index tree without having to do an additional
     seek to read the actual row.  This strategy can be used when the
     query uses only columns that are part of a single index.

     For 'InnoDB' tables that have a user-defined clustered index, that
     index can be used even when 'Using index' is absent from the
     'Extra' column.  This is the case if 'type' is 'index' and 'key' is
     'PRIMARY'.

   * 'Using index for group-by'

     Similar to the 'Using index' table access method, 'Using index for
     group-by' indicates that MySQL found an index that can be used to
     retrieve all columns of a 'GROUP BY' or 'DISTINCT' query without
     any extra disk access to the actual table.  Additionally, the index
     is used in the most efficient way so that for each group, only a
     few index entries are read.  For details, see *note
     group-by-optimization::.

   * 'Using join buffer'

     Tables from earlier joins are read in portions into the join
     buffer, and then their rows are used from the buffer to perform the
     join with the current table.

   * 'Using sort_union(...)', 'Using union(...)', 'Using intersect(...)'

     These indicate the particular algorithm showing how index scans are
     merged for the 'index_merge' join type.  See *note
     index-merge-optimization::.

   * 'Using temporary'

     To resolve the query, MySQL needs to create a temporary table to
     hold the result.  This typically happens if the query contains
     'GROUP BY' and 'ORDER BY' clauses that list columns differently.

   * 'Using where'

     A 'WHERE' clause is used to restrict which rows to match against
     the next table or send to the client.  Unless you specifically
     intend to fetch or examine all rows from the table, you may have
     something wrong in your query if the 'Extra' value is not 'Using
     where' and the table join type is 'ALL' or 'index'.  Even if you
     are using an index for all parts of a 'WHERE' clause, you may see
     'Using where' if the column can be 'NULL'.

   * 'Using where with pushed condition'

     This item applies to *note 'NDB': mysql-cluster. tables _only_.  It
     means that NDB Cluster is using the Condition Pushdown optimization
     to improve the efficiency of a direct comparison between a
     nonindexed column and a constant.  In such cases, the condition is
     'pushed down' to the cluster's data nodes and is evaluated on all
     data nodes simultaneously.  This eliminates the need to send
     nonmatching rows over the network, and can speed up such queries by
     a factor of 5 to 10 times over cases where Condition Pushdown could
     be but is not used.  For more information, see *note
     condition-pushdown-optimization::.

*EXPLAIN Output Interpretation*

You can get a good indication of how good a join is by taking the
product of the values in the 'rows' column of the *note 'EXPLAIN':
explain. output.  This should tell you roughly how many rows MySQL must
examine to execute the query.  If you restrict queries with the
'max_join_size' system variable, this row product also is used to
determine which multiple-table *note 'SELECT': select. statements to
execute and which to abort.  See *note server-configuration::.

The following example shows how a multiple-table join can be optimized
progressively based on the information provided by *note 'EXPLAIN':
explain.

Suppose that you have the *note 'SELECT': select. statement shown here
and that you plan to examine it using *note 'EXPLAIN': explain.:

     EXPLAIN SELECT tt.TicketNumber, tt.TimeIn,
                    tt.ProjectReference, tt.EstimatedShipDate,
                    tt.ActualShipDate, tt.ClientID,
                    tt.ServiceCodes, tt.RepetitiveID,
                    tt.CurrentProcess, tt.CurrentDPPerson,
                    tt.RecordVolume, tt.DPPrinted, et.COUNTRY,
                    et_1.COUNTRY, do.CUSTNAME
             FROM tt, et, et AS et_1, do
             WHERE tt.SubmitTime IS NULL
               AND tt.ActualPC = et.EMPLOYID
               AND tt.AssignedPC = et_1.EMPLOYID
               AND tt.ClientID = do.CUSTNMBR;

For this example, make the following assumptions:

   * The columns being compared have been declared as follows.

     Table   Column             Data Type
                                
     'tt'    'ActualPC'         'CHAR(10)'
                                
     'tt'    'AssignedPC'       'CHAR(10)'
                                
     'tt'    'ClientID'         'CHAR(10)'
                                
     'et'    'EMPLOYID'         'CHAR(15)'
                                
     'do'    'CUSTNMBR'         'CHAR(15)'
             

   * The tables have the following indexes.

     Table   Index
             
     'tt'    'ActualPC'
             
     'tt'    'AssignedPC'
             
     'tt'    'ClientID'
             
     'et'    'EMPLOYID' (primary key)
             
     'do'    'CUSTNMBR' (primary key)

   * The 'tt.ActualPC' values are not evenly distributed.

Initially, before any optimizations have been performed, the *note
'EXPLAIN': explain. statement produces the following information:

     table type possible_keys key  key_len ref  rows  Extra
     et    ALL  PRIMARY       NULL NULL    NULL 74
     do    ALL  PRIMARY       NULL NULL    NULL 2135
     et_1  ALL  PRIMARY       NULL NULL    NULL 74
     tt    ALL  AssignedPC,   NULL NULL    NULL 3872
                ClientID,
                ActualPC
           Range checked for each record (index map: 0x23)

Because 'type' is 'ALL' for each table, this output indicates that MySQL
is generating a Cartesian product of all the tables; that is, every
combination of rows.  This takes quite a long time, because the product
of the number of rows in each table must be examined.  For the case at
hand, this product is 74 x 2135 x 74 x 3872 = 45,268,558,720 rows.  If
the tables were bigger, you can only imagine how long it would take.

One problem here is that MySQL can use indexes on columns more
efficiently if they are declared as the same type and size.  In this
context, *note 'VARCHAR': char. and *note 'CHAR': char. are considered
the same if they are declared as the same size.  'tt.ActualPC' is
declared as 'CHAR(10)' and 'et.EMPLOYID' is 'CHAR(15)', so there is a
length mismatch.

To fix this disparity between column lengths, use *note 'ALTER TABLE':
alter-table. to lengthen 'ActualPC' from 10 characters to 15 characters:

     mysql> ALTER TABLE tt MODIFY ActualPC VARCHAR(15);

Now 'tt.ActualPC' and 'et.EMPLOYID' are both 'VARCHAR(15)'.  Executing
the *note 'EXPLAIN': explain. statement again produces this result:

     table type   possible_keys key     key_len ref         rows    Extra
     tt    ALL    AssignedPC,   NULL    NULL    NULL        3872    Using
                  ClientID,                                         where
                  ActualPC
     do    ALL    PRIMARY       NULL    NULL    NULL        2135
           Range checked for each record (index map: 0x1)
     et_1  ALL    PRIMARY       NULL    NULL    NULL        74
           Range checked for each record (index map: 0x1)
     et    eq_ref PRIMARY       PRIMARY 15      tt.ActualPC 1

This is not perfect, but is much better: The product of the 'rows'
values is less by a factor of 74.  This version executes in a couple of
seconds.

A second alteration can be made to eliminate the column length
mismatches for the 'tt.AssignedPC = et_1.EMPLOYID' and 'tt.ClientID =
do.CUSTNMBR' comparisons:

     mysql> ALTER TABLE tt MODIFY AssignedPC VARCHAR(15),
                           MODIFY ClientID   VARCHAR(15);

After that modification, *note 'EXPLAIN': explain. produces the output
shown here:

     table type   possible_keys key      key_len ref           rows Extra
     et    ALL    PRIMARY       NULL     NULL    NULL          74
     tt    ref    AssignedPC,   ActualPC 15      et.EMPLOYID   52   Using
                  ClientID,                                         where
                  ActualPC
     et_1  eq_ref PRIMARY       PRIMARY  15      tt.AssignedPC 1
     do    eq_ref PRIMARY       PRIMARY  15      tt.ClientID   1

At this point, the query is optimized almost as well as possible.  The
remaining problem is that, by default, MySQL assumes that values in the
'tt.ActualPC' column are evenly distributed, and that is not the case
for the 'tt' table.  Fortunately, it is easy to tell MySQL to analyze
the key distribution:

     mysql> ANALYZE TABLE tt;

With the additional index information, the join is perfect and *note
'EXPLAIN': explain. produces this result:

     table type   possible_keys key     key_len ref           rows Extra
     tt    ALL    AssignedPC    NULL    NULL    NULL          3872 Using
                  ClientID,                                        where
                  ActualPC
     et    eq_ref PRIMARY       PRIMARY 15      tt.ActualPC   1
     et_1  eq_ref PRIMARY       PRIMARY 15      tt.AssignedPC 1
     do    eq_ref PRIMARY       PRIMARY 15      tt.ClientID   1

The 'rows' column in the output from *note 'EXPLAIN': explain. is an
educated guess from the MySQL join optimizer.  Check whether the numbers
are even close to the truth by comparing the 'rows' product with the
actual number of rows that the query returns.  If the numbers are quite
different, you might get better performance by using 'STRAIGHT_JOIN' in
your *note 'SELECT': select. statement and trying to list the tables in
a different order in the 'FROM' clause.

It is possible in some cases to execute statements that modify data when
*note 'EXPLAIN SELECT': explain. is used with a subquery; for more
information, see *note derived-tables::.


File: manual.info.tmp,  Node: explain-extended,  Next: estimating-performance,  Prev: explain-output,  Up: execution-plan-information

8.8.3 Extended EXPLAIN Output Format
------------------------------------

When *note 'EXPLAIN': explain. is used with the 'EXTENDED' keyword, the
output includes a 'filtered' column not otherwise displayed.  This
column indicates the estimated percentage of table rows that will be
filtered by the table condition.

In addition, the statement produces extra ('extended') information that
is not part of *note 'EXPLAIN': explain. output but can be viewed by
issuing a *note 'SHOW WARNINGS': show-warnings. statement following
*note 'EXPLAIN': explain.  The 'Message' value in *note 'SHOW WARNINGS':
show-warnings. output displays how the optimizer qualifies table and
column names in the *note 'SELECT': select. statement, what the *note
'SELECT': select. looks like after the application of rewriting and
optimization rules, and possibly other notes about the optimization
process.

Here is an example of extended *note 'EXPLAIN': explain. output:

     mysql> EXPLAIN EXTENDED
            SELECT t1.a, t1.a IN (SELECT t2.a FROM t2) FROM t1\G
     *************************** 1. row ***************************
                id: 1
       select_type: PRIMARY
             table: t1
              type: index
     possible_keys: NULL
               key: PRIMARY
           key_len: 4
               ref: NULL
              rows: 4
          filtered: 100.00
             Extra: Using index
     *************************** 2. row ***************************
                id: 2
       select_type: DEPENDENT SUBQUERY
             table: t2
              type: index_subquery
     possible_keys: a
               key: a
           key_len: 5
               ref: func
              rows: 2
          filtered: 100.00
             Extra: Using index
     2 rows in set, 1 warning (0.00 sec)

     mysql> SHOW WARNINGS\G
     *************************** 1. row ***************************
       Level: Note
        Code: 1003
     Message: select `test`.`t1`.`a` AS `a`,
              <in_optimizer>(`test`.`t1`.`a`,
              <exists>(<index_lookup>(<cache>(`test`.`t1`.`a`)
              in t2 on a checking NULL having
              <is_not_null_test>(`test`.`t2`.`a`)))) AS `t1.a
              IN (SELECT t2.a FROM t2)` from `test`.`t1`
     1 row in set (0.00 sec)

Because the statement displayed by *note 'SHOW WARNINGS': show-warnings.
may contain special markers to provide information about query rewriting
or optimizer actions, the statement is not necessarily valid SQL and is
not intended to be executed.  The output may also include rows with
'Message' values that provide additional non-SQL explanatory notes about
actions taken by the optimizer.

The following list describes special markers that can appear in the
extended output displayed by *note 'SHOW WARNINGS': show-warnings.:

   * '<cache>(EXPR)'

     The expression (such as a scalar subquery) is executed once and the
     resulting value is saved in memory for later use.

   * '<exists>(QUERY FRAGMENT)'

     The subquery predicate is converted to an 'EXISTS' predicate and
     the subquery is transformed so that it can be used together with
     the 'EXISTS' predicate.

   * '<in_optimizer>(QUERY FRAGMENT)'

     This is an internal optimizer object with no user significance.

   * '<index_lookup>(QUERY FRAGMENT)'

     The query fragment is processed using an index lookup to find
     qualifying rows.

   * '<is_not_null_test>(EXPR)'

     A test to verify that the expression does not evaluate to 'NULL'.

   * '<primary_index_lookup>(QUERY FRAGMENT)'

     The query fragment is processed using a primary key lookup to find
     qualifying rows.

   * '<ref_null_helper>(EXPR)'

     This is an internal optimizer object with no user significance.

When some tables are of 'const' or 'system' type, expressions involving
columns from these tables are evaluated early by the optimizer and are
not part of the displayed statement.  However, with 'FORMAT=JSON', some
'const' table accesses are displayed as a 'ref' access that uses a const
value.


File: manual.info.tmp,  Node: estimating-performance,  Prev: explain-extended,  Up: execution-plan-information

8.8.4 Estimating Query Performance
----------------------------------

In most cases, you can estimate query performance by counting disk
seeks.  For small tables, you can usually find a row in one disk seek
(because the index is probably cached).  For bigger tables, you can
estimate that, using B-tree indexes, you need this many seeks to find a
row: 'log(ROW_COUNT) / log(INDEX_BLOCK_LENGTH / 3 * 2 / (INDEX_LENGTH +
DATA_POINTER_LENGTH)) + 1'.

In MySQL, an index block is usually 1,024 bytes and the data pointer is
usually four bytes.  For a 500,000-row table with a key value length of
three bytes (the size of *note 'MEDIUMINT': integer-types.), the formula
indicates 'log(500,000)/log(1024/3*2/(3+4)) + 1' = '4' seeks.

This index would require storage of about 500,000 * 7 * 3/2 = 5.2MB
(assuming a typical index buffer fill ratio of 2/3), so you probably
have much of the index in memory and so need only one or two calls to
read data to find the row.

For writes, however, you need four seek requests to find where to place
a new index value and normally two seeks to update the index and write
the row.

The preceding discussion does not mean that your application performance
slowly degenerates by log N.  As long as everything is cached by the OS
or the MySQL server, things become only marginally slower as the table
gets bigger.  After the data gets too big to be cached, things start to
go much slower until your applications are bound only by disk seeks
(which increase by log N).  To avoid this, increase the key cache size
as the data grows.  For 'MyISAM' tables, the key cache size is
controlled by the 'key_buffer_size' system variable.  See *note
server-configuration::.


File: manual.info.tmp,  Node: controlling-optimizer,  Next: buffering-caching,  Prev: execution-plan-information,  Up: optimization

8.9 Controlling the Query Optimizer
===================================

* Menu:

* controlling-query-plan-evaluation::  Controlling Query Plan Evaluation
* switchable-optimizations::     Switchable Optimizations
* index-hints::                  Index Hints

MySQL provides optimizer control through system variables that affect
how query plans are evaluated, switchable optimizations, and index
hints.


File: manual.info.tmp,  Node: controlling-query-plan-evaluation,  Next: switchable-optimizations,  Prev: controlling-optimizer,  Up: controlling-optimizer

8.9.1 Controlling Query Plan Evaluation
---------------------------------------

The task of the query optimizer is to find an optimal plan for executing
an SQL query.  Because the difference in performance between 'good' and
'bad' plans can be orders of magnitude (that is, seconds versus hours or
even days), most query optimizers, including that of MySQL, perform a
more or less exhaustive search for an optimal plan among all possible
query evaluation plans.  For join queries, the number of possible plans
investigated by the MySQL optimizer grows exponentially with the number
of tables referenced in a query.  For small numbers of tables (typically
less than 7 to 10) this is not a problem.  However, when larger queries
are submitted, the time spent in query optimization may easily become
the major bottleneck in the server's performance.

A more flexible method for query optimization enables the user to
control how exhaustive the optimizer is in its search for an optimal
query evaluation plan.  The general idea is that the fewer plans that
are investigated by the optimizer, the less time it spends in compiling
a query.  On the other hand, because the optimizer skips some plans, it
may miss finding an optimal plan.

The behavior of the optimizer with respect to the number of plans it
evaluates can be controlled using two system variables:

   * The 'optimizer_prune_level' variable tells the optimizer to skip
     certain plans based on estimates of the number of rows accessed for
     each table.  Our experience shows that this kind of 'educated
     guess' rarely misses optimal plans, and may dramatically reduce
     query compilation times.  That is why this option is on
     ('optimizer_prune_level=1') by default.  However, if you believe
     that the optimizer missed a better query plan, this option can be
     switched off ('optimizer_prune_level=0') with the risk that query
     compilation may take much longer.  Note that, even with the use of
     this heuristic, the optimizer still explores a roughly exponential
     number of plans.

   * The 'optimizer_search_depth' variable tells how far into the
     'future' of each incomplete plan the optimizer should look to
     evaluate whether it should be expanded further.  Smaller values of
     'optimizer_search_depth' may result in orders of magnitude smaller
     query compilation times.  For example, queries with 12, 13, or more
     tables may easily require hours and even days to compile if
     'optimizer_search_depth' is close to the number of tables in the
     query.  At the same time, if compiled with 'optimizer_search_depth'
     equal to 3 or 4, the optimizer may compile in less than a minute
     for the same query.  If you are unsure of what a reasonable value
     is for 'optimizer_search_depth', this variable can be set to 0 to
     tell the optimizer to determine the value automatically.


File: manual.info.tmp,  Node: switchable-optimizations,  Next: index-hints,  Prev: controlling-query-plan-evaluation,  Up: controlling-optimizer

8.9.2 Switchable Optimizations
------------------------------

The 'optimizer_switch' system variable enables control over optimizer
behavior.  Its value is a set of flags, each of which has a value of
'on' or 'off' to indicate whether the corresponding optimizer behavior
is enabled or disabled.  This variable has global and session values and
can be changed at runtime.  The global default can be set at server
startup.

To see the current set of optimizer flags, select the variable value:

     mysql> SELECT @@optimizer_switch\G
     *************************** 1. row ***************************
     @@optimizer_switch: index_merge=on,index_merge_union=on,
                         index_merge_sort_union=on,
                         index_merge_intersection=on,
                         engine_condition_pushdown=on

To change the value of 'optimizer_switch', assign a value consisting of
a comma-separated list of one or more commands:

     SET [GLOBAL|SESSION] optimizer_switch='COMMAND[,COMMAND]...';

Each COMMAND value should have one of the forms shown in the following
table.

Command Syntax     Meaning
                   
'default'          Reset every optimization to its default value
                   
'OPT_NAME=default' Set the named optimization to its default value
                   
'OPT_NAME=off'     Disable the named optimization
                   
'OPT_NAME=on'      Enable the named optimization

The order of the commands in the value does not matter, although the
'default' command is executed first if present.  Setting an OPT_NAME
flag to 'default' sets it to whichever of 'on' or 'off' is its default
value.  Specifying any given OPT_NAME more than once in the value is not
permitted and causes an error.  Any errors in the value cause the
assignment to fail with an error, leaving the value of
'optimizer_switch' unchanged.

The following list describes the permissible OPT_NAME flag names,
grouped by optimization strategy:

   * Engine Condition Pushdown Flags

        * 'engine_condition_pushdown' (default 'on')

          Controls engine condition pushdown.

     For more information, see *note condition-pushdown-optimization::.

   * Index Merge Flags

        * 'index_merge' (default 'on')

          Controls all Index Merge optimizations.

        * 'index_merge_intersection' (default 'on')

          Controls the Index Merge Intersection Access optimization.

        * 'index_merge_sort_union' (default 'on')

          Controls the Index Merge Sort-Union Access optimization.

        * 'index_merge_union' (default 'on')

          Controls the Index Merge Union Access optimization.

     For more information, see *note index-merge-optimization::.

When you assign a value to 'optimizer_switch', flags that are not
mentioned keep their current values.  This makes it possible to enable
or disable specific optimizer behaviors in a single statement without
affecting other behaviors.  The statement does not depend on what other
optimizer flags exist and what their values are.  Suppose that all Index
Merge optimizations are enabled:

     mysql> SELECT @@optimizer_switch\G
     *************************** 1. row ***************************
     @@optimizer_switch: index_merge=on,index_merge_union=on,
                         index_merge_sort_union=on,
                         index_merge_intersection=on,
                         engine_condition_pushdown=on

If the server is using the Index Merge Union or Index Merge Sort-Union
access methods for certain queries and you want to check whether the
optimizer will perform better without them, set the variable value like
this:

     mysql> SET optimizer_switch='index_merge_union=off,index_merge_sort_union=off';

     mysql> SELECT @@optimizer_switch\G
     *************************** 1. row ***************************
     @@optimizer_switch: index_merge=on,index_merge_union=off,
                         index_merge_sort_union=off,
                         index_merge_intersection=on,
                         engine_condition_pushdown=on


File: manual.info.tmp,  Node: index-hints,  Prev: switchable-optimizations,  Up: controlling-optimizer

8.9.3 Index Hints
-----------------

Index hints give the optimizer information about how to choose indexes
during query processing.  Index hints apply only to *note 'SELECT':
select. and *note 'UPDATE': update. statements.

Index hints are specified following a table name.  (For the general
syntax for specifying tables in a *note 'SELECT': select. statement, see
*note join::.)  The syntax for referring to an individual table,
including index hints, looks like this:

     TBL_NAME [[AS] ALIAS] [INDEX_HINT_LIST]

     INDEX_HINT_LIST:
         INDEX_HINT [INDEX_HINT] ...

     INDEX_HINT:
         USE {INDEX|KEY}
           [FOR {JOIN|ORDER BY|GROUP BY}] ([INDEX_LIST])
       | {IGNORE|FORCE} {INDEX|KEY}
           [FOR {JOIN|ORDER BY|GROUP BY}] (INDEX_LIST)

     INDEX_LIST:
         INDEX_NAME [, INDEX_NAME] ...

The 'USE INDEX (INDEX_LIST)' hint tells MySQL to use only one of the
named indexes to find rows in the table.  The alternative syntax 'IGNORE
INDEX (INDEX_LIST)' tells MySQL to not use some particular index or
indexes.  These hints are useful if *note 'EXPLAIN': explain. shows that
MySQL is using the wrong index from the list of possible indexes.

The 'FORCE INDEX' hint acts like 'USE INDEX (INDEX_LIST)', with the
addition that a table scan is assumed to be _very_ expensive.  In other
words, a table scan is used only if there is no way to use one of the
named indexes to find rows in the table.

Each hint requires index names, not column names.  To refer to a primary
key, use the name 'PRIMARY'.  To see the index names for a table, use
the *note 'SHOW INDEX': show-index. statement or the *note
'INFORMATION_SCHEMA.STATISTICS': statistics-table. table.

An INDEX_NAME value need not be a full index name.  It can be an
unambiguous prefix of an index name.  If a prefix is ambiguous, an error
occurs.

Examples:

     SELECT * FROM table1 USE INDEX (col1_index,col2_index)
       WHERE col1=1 AND col2=2 AND col3=3;

     SELECT * FROM table1 IGNORE INDEX (col3_index)
       WHERE col1=1 AND col2=2 AND col3=3;

The syntax for index hints has the following characteristics:

   * It is syntactically valid to omit INDEX_LIST for 'USE INDEX', which
     means 'use no indexes.' Omitting INDEX_LIST for 'FORCE INDEX' or
     'IGNORE INDEX' is a syntax error.

   * You can specify the scope of an index hint by adding a 'FOR' clause
     to the hint.  This provides more fine-grained control over
     optimizer selection of an execution plan for various phases of
     query processing.  To affect only the indexes used when MySQL
     decides how to find rows in the table and how to process joins, use
     'FOR JOIN'.  To influence index usage for sorting or grouping rows,
     use 'FOR ORDER BY' or 'FOR GROUP BY'.

   * You can specify multiple index hints:

          SELECT * FROM t1 USE INDEX (i1) IGNORE INDEX FOR ORDER BY (i2) ORDER BY a;

     It is not an error to name the same index in several hints (even
     within the same hint):

          SELECT * FROM t1 USE INDEX (i1) USE INDEX (i1,i1);

     However, it is an error to mix 'USE INDEX' and 'FORCE INDEX' for
     the same table:

          SELECT * FROM t1 USE INDEX FOR JOIN (i1) FORCE INDEX FOR JOIN (i2);

If an index hint includes no 'FOR' clause, the scope of the hint is to
apply to all parts of the statement.  For example, this hint:

     IGNORE INDEX (i1)

is equivalent to this combination of hints:

     IGNORE INDEX FOR JOIN (i1)
     IGNORE INDEX FOR ORDER BY (i1)
     IGNORE INDEX FOR GROUP BY (i1)

In MySQL 5.0, hint scope with no 'FOR' clause was to apply only to row
retrieval.  To cause the server to use this older behavior when no 'FOR'
clause is present, enable the 'old' system variable at server startup.
Take care about enabling this variable in a replication setup.  With
statement-based binary logging, having different modes for the master
and slaves might lead to replication errors.

When index hints are processed, they are collected in a single list by
type ('USE', 'FORCE', 'IGNORE') and by scope ('FOR JOIN', 'FOR ORDER
BY', 'FOR GROUP BY').  For example:

     SELECT * FROM t1
       USE INDEX () IGNORE INDEX (i2) USE INDEX (i1) USE INDEX (i2);

is equivalent to:

     SELECT * FROM t1
        USE INDEX (i1,i2) IGNORE INDEX (i2);

The index hints then are applied for each scope in the following order:

  1. '{USE|FORCE} INDEX' is applied if present.  (If not, the
     optimizer-determined set of indexes is used.)

  2. 'IGNORE INDEX' is applied over the result of the previous step.
     For example, the following two queries are equivalent:

          SELECT * FROM t1 USE INDEX (i1) IGNORE INDEX (i2) USE INDEX (i2);

          SELECT * FROM t1 USE INDEX (i1);

For 'FULLTEXT' searches, index hints work as follows:

   * For natural language mode searches, index hints are silently
     ignored.  For example, 'IGNORE INDEX(i1)' is ignored with no
     warning and the index is still used.

   * For boolean mode searches, index hints with 'FOR ORDER BY' or 'FOR
     GROUP BY' are silently ignored.  Index hints with 'FOR JOIN' or no
     'FOR' modifier are honored.  In contrast to how hints apply for
     non-'FULLTEXT' searches, the hint is used for all phases of query
     execution (finding rows and retrieval, grouping, and ordering).
     This is true even if the hint is given for a non-'FULLTEXT' index.

     For example, the following two queries are equivalent:

          SELECT * FROM t
            USE INDEX (index1)
            IGNORE INDEX (index1) FOR ORDER BY
            IGNORE INDEX (index1) FOR GROUP BY
            WHERE ... IN BOOLEAN MODE ... ;

          SELECT * FROM t
            USE INDEX (index1)
            WHERE ... IN BOOLEAN MODE ... ;


File: manual.info.tmp,  Node: buffering-caching,  Next: locking-issues,  Prev: controlling-optimizer,  Up: optimization

8.10 Buffering and Caching
==========================

* Menu:

* innodb-buffer-pool-optimization::  InnoDB Buffer Pool Optimization
* myisam-key-cache::             The MyISAM Key Cache
* query-cache::                  The MySQL Query Cache

MySQL uses several strategies that cache information in memory buffers
to increase performance.  Depending on your database architecture, you
balance the size and layout of these areas, to provide the most
performance benefit without wasting memory or exceeding available
memory.  When you set up or resize these memory areas, test the
resulting performance using the techniques from *note
optimize-benchmarking::.


File: manual.info.tmp,  Node: innodb-buffer-pool-optimization,  Next: myisam-key-cache,  Prev: buffering-caching,  Up: buffering-caching

8.10.1 InnoDB Buffer Pool Optimization
--------------------------------------

*note 'InnoDB': innodb-storage-engine. maintains a storage area called
the buffer pool for caching data and indexes in memory.  Knowing how the
'InnoDB' buffer pool works, and taking advantage of it to keep
frequently accessed data in memory, is an important aspect of MySQL
tuning.

For an explanation of the inner workings of the 'InnoDB' buffer pool, an
overview of its LRU replacement algorithm, and general configuration
information, see *note innodb-buffer-pool::.

For additional 'InnoDB' buffer pool configuration and tuning
information, see these sections:

   * *note innodb-performance-read_ahead::

   * *note innodb-buffer-pool-flushing::

   * *note innodb-performance-midpoint_insertion::

   * *note innodb-multiple-buffer-pools::


File: manual.info.tmp,  Node: myisam-key-cache,  Next: query-cache,  Prev: innodb-buffer-pool-optimization,  Up: buffering-caching

8.10.2 The MyISAM Key Cache
---------------------------

* Menu:

* shared-key-cache::             Shared Key Cache Access
* multiple-key-caches::          Multiple Key Caches
* midpoint-insertion::           Midpoint Insertion Strategy
* index-preloading::             Index Preloading
* key-cache-block-size::         Key Cache Block Size
* key-cache-restructuring::      Restructuring a Key Cache

To minimize disk I/O, the 'MyISAM' storage engine exploits a strategy
that is used by many database management systems.  It employs a cache
mechanism to keep the most frequently accessed table pages in memory:

   * For index blocks, a special structure called the _key cache_ (or
     _key buffer_) is maintained.  The structure contains a number of
     block buffers where the most-used index blocks are placed.

   * For data blocks, MySQL uses no special cache.  Instead it relies on
     the native operating system file system cache.

This section first describes the basic operation of the 'MyISAM' key
cache.  Then it discusses features that improve key cache performance
and that enable you to better control cache operation:

   * Multiple sessions can access the cache concurrently.

   * You can set up multiple key caches and assign table indexes to
     specific caches.

To control the size of the key cache, use the 'key_buffer_size' system
variable.  If this variable is set equal to zero, no key cache is used.
The key cache also is not used if the 'key_buffer_size' value is too
small to allocate the minimal number of block buffers (8).

When the key cache is not operational, index files are accessed using
only the native file system buffering provided by the operating system.
(In other words, table index blocks are accessed using the same strategy
as that employed for table data blocks.)

An index block is a contiguous unit of access to the 'MyISAM' index
files.  Usually the size of an index block is equal to the size of nodes
of the index B-tree.  (Indexes are represented on disk using a B-tree
data structure.  Nodes at the bottom of the tree are leaf nodes.  Nodes
above the leaf nodes are nonleaf nodes.)

All block buffers in a key cache structure are the same size.  This size
can be equal to, greater than, or less than the size of a table index
block.  Usually one these two values is a multiple of the other.

When data from any table index block must be accessed, the server first
checks whether it is available in some block buffer of the key cache.
If it is, the server accesses data in the key cache rather than on disk.
That is, it reads from the cache or writes into it rather than reading
from or writing to disk.  Otherwise, the server chooses a cache block
buffer containing a different table index block (or blocks) and replaces
the data there by a copy of required table index block.  As soon as the
new index block is in the cache, the index data can be accessed.

If it happens that a block selected for replacement has been modified,
the block is considered 'dirty.' In this case, prior to being replaced,
its contents are flushed to the table index from which it came.

Usually the server follows an _LRU (Least Recently Used)_ strategy: When
choosing a block for replacement, it selects the least recently used
index block.  To make this choice easier, the key cache module maintains
all used blocks in a special list (_LRU chain_) ordered by time of use.
When a block is accessed, it is the most recently used and is placed at
the end of the list.  When blocks need to be replaced, blocks at the
beginning of the list are the least recently used and become the first
candidates for eviction.

The 'InnoDB' storage engine also uses an LRU algorithm, to manage its
buffer pool.  See *note innodb-buffer-pool::.


File: manual.info.tmp,  Node: shared-key-cache,  Next: multiple-key-caches,  Prev: myisam-key-cache,  Up: myisam-key-cache

8.10.2.1 Shared Key Cache Access
................................

Threads can access key cache buffers simultaneously, subject to the
following conditions:

   * A buffer that is not being updated can be accessed by multiple
     sessions.

   * A buffer that is being updated causes sessions that need to use it
     to wait until the update is complete.

   * Multiple sessions can initiate requests that result in cache block
     replacements, as long as they do not interfere with each other
     (that is, as long as they need different index blocks, and thus
     cause different cache blocks to be replaced).

Shared access to the key cache enables the server to improve throughput
significantly.


File: manual.info.tmp,  Node: multiple-key-caches,  Next: midpoint-insertion,  Prev: shared-key-cache,  Up: myisam-key-cache

8.10.2.2 Multiple Key Caches
............................

Shared access to the key cache improves performance but does not
eliminate contention among sessions entirely.  They still compete for
control structures that manage access to the key cache buffers.  To
reduce key cache access contention further, MySQL also provides multiple
key caches.  This feature enables you to assign different table indexes
to different key caches.

Where there are multiple key caches, the server must know which cache to
use when processing queries for a given 'MyISAM' table.  By default, all
'MyISAM' table indexes are cached in the default key cache.  To assign
table indexes to a specific key cache, use the *note 'CACHE INDEX':
cache-index. statement (see *note cache-index::).  For example, the
following statement assigns indexes from the tables 't1', 't2', and 't3'
to the key cache named 'hot_cache':

     mysql> CACHE INDEX t1, t2, t3 IN hot_cache;
     +---------+--------------------+----------+----------+
     | Table   | Op                 | Msg_type | Msg_text |
     +---------+--------------------+----------+----------+
     | test.t1 | assign_to_keycache | status   | OK       |
     | test.t2 | assign_to_keycache | status   | OK       |
     | test.t3 | assign_to_keycache | status   | OK       |
     +---------+--------------------+----------+----------+

The key cache referred to in a *note 'CACHE INDEX': cache-index.
statement can be created by setting its size with a *note 'SET GLOBAL':
set-variable. parameter setting statement or by using server startup
options.  For example:

     mysql> SET GLOBAL keycache1.key_buffer_size=128*1024;

To destroy a key cache, set its size to zero:

     mysql> SET GLOBAL keycache1.key_buffer_size=0;

You cannot destroy the default key cache.  Any attempt to do this is
ignored:

     mysql> SET GLOBAL key_buffer_size = 0;

     mysql> SHOW VARIABLES LIKE 'key_buffer_size';
     +-----------------+---------+
     | Variable_name   | Value   |
     +-----------------+---------+
     | key_buffer_size | 8384512 |
     +-----------------+---------+

Key cache variables are structured system variables that have a name and
components.  For 'keycache1.key_buffer_size', 'keycache1' is the cache
variable name and 'key_buffer_size' is the cache component.  See *note
structured-system-variables::, for a description of the syntax used for
referring to structured key cache system variables.

By default, table indexes are assigned to the main (default) key cache
created at the server startup.  When a key cache is destroyed, all
indexes assigned to it are reassigned to the default key cache.

For a busy server, you can use a strategy that involves three key
caches:

   * A 'hot' key cache that takes up 20% of the space allocated for all
     key caches.  Use this for tables that are heavily used for searches
     but that are not updated.

   * A 'cold' key cache that takes up 20% of the space allocated for all
     key caches.  Use this cache for medium-sized, intensively modified
     tables, such as temporary tables.

   * A 'warm' key cache that takes up 60% of the key cache space.
     Employ this as the default key cache, to be used by default for all
     other tables.

One reason the use of three key caches is beneficial is that access to
one key cache structure does not block access to the others.  Statements
that access tables assigned to one cache do not compete with statements
that access tables assigned to another cache.  Performance gains occur
for other reasons as well:

   * The hot cache is used only for retrieval queries, so its contents
     are never modified.  Consequently, whenever an index block needs to
     be pulled in from disk, the contents of the cache block chosen for
     replacement need not be flushed first.

   * For an index assigned to the hot cache, if there are no queries
     requiring an index scan, there is a high probability that the index
     blocks corresponding to nonleaf nodes of the index B-tree remain in
     the cache.

   * An update operation most frequently executed for temporary tables
     is performed much faster when the updated node is in the cache and
     need not be read from disk first.  If the size of the indexes of
     the temporary tables are comparable with the size of cold key
     cache, the probability is very high that the updated node is in the
     cache.

The *note 'CACHE INDEX': cache-index. statement sets up an association
between a table and a key cache, but the association is lost each time
the server restarts.  If you want the association to take effect each
time the server starts, one way to accomplish this is to use an option
file: Include variable settings that configure your key caches, and an
'init_file' system variable that names a file containing *note 'CACHE
INDEX': cache-index. statements to be executed.  For example:

     key_buffer_size = 4G
     hot_cache.key_buffer_size = 2G
     cold_cache.key_buffer_size = 2G
     init_file=/PATH/TO/DATA-DIRECTORY/mysqld_init.sql

The statements in 'mysqld_init.sql' are executed each time the server
starts.  The file should contain one SQL statement per line.  The
following example assigns several tables each to 'hot_cache' and
'cold_cache':

     CACHE INDEX db1.t1, db1.t2, db2.t3 IN hot_cache
     CACHE INDEX db1.t4, db2.t5, db2.t6 IN cold_cache


File: manual.info.tmp,  Node: midpoint-insertion,  Next: index-preloading,  Prev: multiple-key-caches,  Up: myisam-key-cache

8.10.2.3 Midpoint Insertion Strategy
....................................

By default, the key cache management system uses a simple LRU strategy
for choosing key cache blocks to be evicted, but it also supports a more
sophisticated method called the _midpoint insertion strategy._

When using the midpoint insertion strategy, the LRU chain is divided
into two parts: a hot sublist and a warm sublist.  The division point
between two parts is not fixed, but the key cache management system
takes care that the warm part is not 'too short,' always containing at
least 'key_cache_division_limit' percent of the key cache blocks.
'key_cache_division_limit' is a component of structured key cache
variables, so its value is a parameter that can be set per cache.

When an index block is read from a table into the key cache, it is
placed at the end of the warm sublist.  After a certain number of hits
(accesses of the block), it is promoted to the hot sublist.  At present,
the number of hits required to promote a block (3) is the same for all
index blocks.

A block promoted into the hot sublist is placed at the end of the list.
The block then circulates within this sublist.  If the block stays at
the beginning of the sublist for a long enough time, it is demoted to
the warm sublist.  This time is determined by the value of the
'key_cache_age_threshold' component of the key cache.

The threshold value prescribes that, for a key cache containing N
blocks, the block at the beginning of the hot sublist not accessed
within the last 'N * key_cache_age_threshold / 100' hits is to be moved
to the beginning of the warm sublist.  It then becomes the first
candidate for eviction, because blocks for replacement always are taken
from the beginning of the warm sublist.

The midpoint insertion strategy enables you to keep more-valued blocks
always in the cache.  If you prefer to use the plain LRU strategy, leave
the 'key_cache_division_limit' value set to its default of 100.

The midpoint insertion strategy helps to improve performance when
execution of a query that requires an index scan effectively pushes out
of the cache all the index blocks corresponding to valuable high-level
B-tree nodes.  To avoid this, you must use a midpoint insertion strategy
with the 'key_cache_division_limit' set to much less than 100.  Then
valuable frequently hit nodes are preserved in the hot sublist during an
index scan operation as well.


File: manual.info.tmp,  Node: index-preloading,  Next: key-cache-block-size,  Prev: midpoint-insertion,  Up: myisam-key-cache

8.10.2.4 Index Preloading
.........................

If there are enough blocks in a key cache to hold blocks of an entire
index, or at least the blocks corresponding to its nonleaf nodes, it
makes sense to preload the key cache with index blocks before starting
to use it.  Preloading enables you to put the table index blocks into a
key cache buffer in the most efficient way: by reading the index blocks
from disk sequentially.

Without preloading, the blocks are still placed into the key cache as
needed by queries.  Although the blocks will stay in the cache, because
there are enough buffers for all of them, they are fetched from disk in
random order, and not sequentially.

To preload an index into a cache, use the *note 'LOAD INDEX INTO CACHE':
load-index. statement.  For example, the following statement preloads
nodes (index blocks) of indexes of the tables 't1' and 't2':

     mysql> LOAD INDEX INTO CACHE t1, t2 IGNORE LEAVES;
     +---------+--------------+----------+----------+
     | Table   | Op           | Msg_type | Msg_text |
     +---------+--------------+----------+----------+
     | test.t1 | preload_keys | status   | OK       |
     | test.t2 | preload_keys | status   | OK       |
     +---------+--------------+----------+----------+

The 'IGNORE LEAVES' modifier causes only blocks for the nonleaf nodes of
the index to be preloaded.  Thus, the statement shown preloads all index
blocks from 't1', but only blocks for the nonleaf nodes from 't2'.

If an index has been assigned to a key cache using a *note 'CACHE
INDEX': cache-index. statement, preloading places index blocks into that
cache.  Otherwise, the index is loaded into the default key cache.


File: manual.info.tmp,  Node: key-cache-block-size,  Next: key-cache-restructuring,  Prev: index-preloading,  Up: myisam-key-cache

8.10.2.5 Key Cache Block Size
.............................

It is possible to specify the size of the block buffers for an
individual key cache using the 'key_cache_block_size' variable.  This
permits tuning of the performance of I/O operations for index files.

The best performance for I/O operations is achieved when the size of
read buffers is equal to the size of the native operating system I/O
buffers.  But setting the size of key nodes equal to the size of the I/O
buffer does not always ensure the best overall performance.  When
reading the big leaf nodes, the server pulls in a lot of unnecessary
data, effectively preventing reading other leaf nodes.

To control the size of blocks in the '.MYI' index file of 'MyISAM'
tables, use the '--myisam-block-size' option at server startup.


File: manual.info.tmp,  Node: key-cache-restructuring,  Prev: key-cache-block-size,  Up: myisam-key-cache

8.10.2.6 Restructuring a Key Cache
..................................

A key cache can be restructured at any time by updating its parameter
values.  For example:

     mysql> SET GLOBAL cold_cache.key_buffer_size=4*1024*1024;

If you assign to either the 'key_buffer_size' or 'key_cache_block_size'
key cache component a value that differs from the component's current
value, the server destroys the cache's old structure and creates a new
one based on the new values.  If the cache contains any dirty blocks,
the server saves them to disk before destroying and re-creating the
cache.  Restructuring does not occur if you change other key cache
parameters.

When restructuring a key cache, the server first flushes the contents of
any dirty buffers to disk.  After that, the cache contents become
unavailable.  However, restructuring does not block queries that need to
use indexes assigned to the cache.  Instead, the server directly
accesses the table indexes using native file system caching.  File
system caching is not as efficient as using a key cache, so although
queries execute, a slowdown can be anticipated.  After the cache has
been restructured, it becomes available again for caching indexes
assigned to it, and the use of file system caching for the indexes
ceases.


File: manual.info.tmp,  Node: query-cache,  Prev: myisam-key-cache,  Up: buffering-caching

8.10.3 The MySQL Query Cache
----------------------------

* Menu:

* query-cache-operation::        How the Query Cache Operates
* query-cache-in-select::        Query Cache SELECT Options
* query-cache-configuration::    Query Cache Configuration
* query-cache-status-and-maintenance::  Query Cache Status and Maintenance

The query cache stores the text of a *note 'SELECT': select. statement
together with the corresponding result that was sent to the client.  If
an identical statement is received later, the server retrieves the
results from the query cache rather than parsing and executing the
statement again.  The query cache is shared among sessions, so a result
set generated by one client can be sent in response to the same query
issued by another client.

The query cache can be useful in an environment where you have tables
that do not change very often and for which the server receives many
identical queries.  This is a typical situation for many Web servers
that generate many dynamic pages based on database content.  For
example, when an order form queries a table to display the lists of all
US states or all countries in the world, those values can be retrieved
from the query cache.  Although the values would probably be retrieved
from memory in any case (from the 'InnoDB' buffer pool or 'MyISAM' key
cache), using the query cache avoids the overhead of processing the
query, deciding whether to use a table scan, and locating the data block
for each row.

The query cache always contains current and reliable data.  Any insert,
update, delete, or other modification to a table causes any relevant
entries in the query cache to be flushed.

*Note*:

The query cache does not work in an environment where you have multiple
*note 'mysqld': mysqld. servers updating the same 'MyISAM' tables.

The query cache is used for prepared statements under the conditions
described in *note query-cache-operation::.

*Note*:

As of MySQL 5.5.23, the query cache is not supported for partitioned
tables, and is automatically disabled for queries involving partitioned
tables.  The query cache cannot be enabled for such queries.  (Bug
#53775)

Some performance data for the query cache follows.  These results were
generated by running the MySQL benchmark suite on a Linux Alpha 2x500MHz
system with 2GB RAM and a 64MB query cache.

   * If all the queries you are performing are simple (such as selecting
     a row from a table with one row), but still differ so that the
     queries cannot be cached, the overhead for having the query cache
     active is 13%.  This could be regarded as the worst case scenario.
     In real life, queries tend to be much more complicated, so the
     overhead normally is significantly lower.

   * Searches for a single row in a single-row table are 238% faster
     with the query cache than without it.  This can be regarded as
     close to the minimum speedup to be expected for a query that is
     cached.

To disable the query cache at server startup, set the 'query_cache_size'
system variable to 0.  By disabling the query cache code, there is no
noticeable overhead.

The query cache offers the potential for substantial performance
improvement, but do not assume that it will do so under all
circumstances.  With some query cache configurations or server
workloads, you might actually see a performance decrease:

   * Be cautious about sizing the query cache excessively large, which
     increases the overhead required to maintain the cache, possibly
     beyond the benefit of enabling it.  Sizes in tens of megabytes are
     usually beneficial.  Sizes in the hundreds of megabytes might not
     be.

   * Server workload has a significant effect on query cache efficiency.
     A query mix consisting almost entirely of a fixed set of *note
     'SELECT': select. statements is much more likely to benefit from
     enabling the cache than a mix in which frequent *note 'INSERT':
     insert. statements cause continual invalidation of results in the
     cache.  In some cases, a workaround is to use the 'SQL_NO_CACHE'
     option to prevent results from even entering the cache for *note
     'SELECT': select. statements that use frequently modified tables.
     (See *note query-cache-in-select::.)

To verify that enabling the query cache is beneficial, test the
operation of your MySQL server with the cache enabled and disabled.
Then retest periodically because query cache efficiency may change as
server workload changes.


File: manual.info.tmp,  Node: query-cache-operation,  Next: query-cache-in-select,  Prev: query-cache,  Up: query-cache

8.10.3.1 How the Query Cache Operates
.....................................

This section describes how the query cache works when it is operational.
*note query-cache-configuration::, describes how to control whether it
is operational.

Incoming queries are compared to those in the query cache before
parsing, so the following two queries are regarded as different by the
query cache:

     SELECT * FROM TBL_NAME
     Select * from TBL_NAME

Queries must be _exactly_ the same (byte for byte) to be seen as
identical.  In addition, query strings that are identical may be treated
as different for other reasons.  Queries that use different databases,
different protocol versions, or different default character sets are
considered different queries and are cached separately.

The cache is not used for queries of the following types:

   * Queries that are a subquery of an outer query

   * Queries executed within the body of a stored function, trigger, or
     event

Before a query result is fetched from the query cache, MySQL checks
whether the user has *note 'SELECT': select. privilege for all databases
and tables involved.  If this is not the case, the cached result is not
used.

If a query result is returned from query cache, the server increments
the 'Qcache_hits' status variable, not 'Com_select'.  See *note
query-cache-status-and-maintenance::.

If a table changes, all cached queries that use the table become invalid
and are removed from the cache.  This includes queries that use 'MERGE'
tables that map to the changed table.  A table can be changed by many
types of statements, such as *note 'INSERT': insert, *note 'UPDATE':
update, *note 'DELETE': delete, *note 'TRUNCATE TABLE': truncate-table,
*note 'ALTER TABLE': alter-table, *note 'DROP TABLE': drop-table, or
*note 'DROP DATABASE': drop-database.

The query cache also works within transactions when using 'InnoDB'
tables.

The result from a *note 'SELECT': select. query on a view is cached.

The query cache works for 'SELECT SQL_CALC_FOUND_ROWS ...' queries and
stores a value that is returned by a following 'SELECT FOUND_ROWS()'
query.  'FOUND_ROWS()' returns the correct value even if the preceding
query was fetched from the cache because the number of found rows is
also stored in the cache.  The 'SELECT FOUND_ROWS()' query itself cannot
be cached.

Prepared statements that are issued using the binary protocol using
*note 'mysql_stmt_prepare()': mysql-stmt-prepare. and *note
'mysql_stmt_execute()': mysql-stmt-execute. (see *note
c-api-prepared-statements::), are subject to limitations on caching.
Comparison with statements in the query cache is based on the text of
the statement after expansion of '?' parameter markers.  The statement
is compared only with other cached statements that were executed using
the binary protocol.  That is, for query cache purposes, prepared
statements issued using the binary protocol are distinct from prepared
statements issued using the text protocol (see *note
sql-prepared-statements::).

A query cannot be cached if it uses any of the following functions:

   * 'BENCHMARK()'

   * 'CONNECTION_ID()'

   * 'CONVERT_TZ()'

   * 'CURDATE()'

   * 'CURRENT_DATE()'

   * 'CURRENT_TIME()'

   * 'CURRENT_TIMESTAMP()'

   * 'CURRENT_USER()'

   * 'CURTIME()'

   * 'DATABASE()'

   * 'ENCRYPT()' with one parameter

   * 'FOUND_ROWS()'

   * 'GET_LOCK()'

   * 'IS_FREE_LOCK()'

   * 'IS_USED_LOCK()'

   * 'LAST_INSERT_ID()'

   * 'LOAD_FILE()'

   * 'MASTER_POS_WAIT()'

   * 'NOW()'

   * 'RAND()'

   * 'RELEASE_ALL_LOCKS()'
     (https://dev.mysql.com/doc/refman/5.7/en/locking-functions.html#function_release-all-locks)

   * 'RELEASE_LOCK()'

   * 'SLEEP()'

   * 'SYSDATE()'

   * 'UNIX_TIMESTAMP()' with no parameters

   * 'USER()'

   * 'UUID()'

   * 'UUID_SHORT()'

A query also is not cached under these conditions:

   * It refers to user-defined functions (UDFs) or stored functions.

   * It refers to user variables or local stored program variables.

   * It refers to tables in the 'mysql', 'INFORMATION_SCHEMA', or
     'performance_schema' database.

   * (_MySQL 5.5.23 and later_:) It refers to any partitioned tables.

   * It is of any of the following forms:

          SELECT ... LOCK IN SHARE MODE
          SELECT ... FOR UPDATE
          SELECT ... INTO OUTFILE ...
          SELECT ... INTO DUMPFILE ...
          SELECT * FROM ... WHERE autoincrement_col IS NULL

     The last form is not cached because it is used as the ODBC
     workaround for obtaining the last insert ID value.  See the
     Connector/ODBC section of *note connectors-apis::.

     Statements within transactions that use 'SERIALIZABLE' isolation
     level also cannot be cached because they use 'LOCK IN SHARE MODE'
     locking.

   * It uses 'TEMPORARY' tables.

   * It does not use any tables.

   * It generates warnings.

   * The user has a column-level privilege for any of the involved
     tables.


File: manual.info.tmp,  Node: query-cache-in-select,  Next: query-cache-configuration,  Prev: query-cache-operation,  Up: query-cache

8.10.3.2 Query Cache SELECT Options
...................................

Two query cache-related options may be specified in *note 'SELECT':
select. statements:

   * 
     'SQL_CACHE'

     The query result is cached if it is cacheable and the value of the
     'query_cache_type' system variable is 'ON' or 'DEMAND'.

   * 
     'SQL_NO_CACHE'

     The server does not use the query cache.  It neither checks the
     query cache to see whether the result is already cached, nor does
     it cache the query result.  (Due to a limitation in the parser, a
     space character must precede and follow the 'SQL_NO_CACHE' keyword;
     a nonspace such as a newline causes the server to check the query
     cache to see whether the result is already cached.)

Examples:

     SELECT SQL_CACHE id, name FROM customer;
     SELECT SQL_NO_CACHE id, name FROM customer;


File: manual.info.tmp,  Node: query-cache-configuration,  Next: query-cache-status-and-maintenance,  Prev: query-cache-in-select,  Up: query-cache

8.10.3.3 Query Cache Configuration
..................................

The 'have_query_cache' server system variable indicates whether the
query cache is available:

     mysql> SHOW VARIABLES LIKE 'have_query_cache';
     +------------------+-------+
     | Variable_name    | Value |
     +------------------+-------+
     | have_query_cache | YES   |
     +------------------+-------+

When using a standard MySQL binary, this value is always 'YES', even if
query caching is disabled.

Several other system variables control query cache operation.  These can
be set in an option file or on the command line when starting *note
'mysqld': mysqld.  The query cache system variables all have names that
begin with 'query_cache_'.  They are described briefly in *note
server-system-variables::, with additional configuration information
given here.

To set the size of the query cache, set the 'query_cache_size' system
variable.  Setting it to 0 disables the query cache.  The default size
is 0, so the query cache is disabled by default.

To reduce overhead significantly, start the server with
'query_cache_type=0' if you will not be using the query cache.

*Note*:

When using the Windows Configuration Wizard to install or configure
MySQL, the default value for 'query_cache_size' will be configured
automatically for you based on the different configuration types
available.  When using the Windows Configuration Wizard, the query cache
may be enabled (that is, set to a nonzero value) due to the selected
configuration.  The query cache is also controlled by the setting of the
'query_cache_type' variable.  Check the values of these variables as set
in your 'my.ini' file after configuration has taken place.

When you set 'query_cache_size' to a nonzero value, keep in mind that
the query cache needs a minimum size of about 40KB to allocate its
structures.  (The exact size depends on system architecture.)  If you
set the value too small, you'll get a warning, as in this example:

     mysql> SET GLOBAL query_cache_size = 40000;
     Query OK, 0 rows affected, 1 warning (0.00 sec)

     mysql> SHOW WARNINGS\G
     *************************** 1. row ***************************
       Level: Warning
        Code: 1282
     Message: Query cache failed to set size 39936;
              new query cache size is 0

     mysql> SET GLOBAL query_cache_size = 41984;
     Query OK, 0 rows affected (0.00 sec)

     mysql> SHOW VARIABLES LIKE 'query_cache_size';
     +------------------+-------+
     | Variable_name    | Value |
     +------------------+-------+
     | query_cache_size | 41984 |
     +------------------+-------+

For the query cache to actually be able to hold any query results, its
size must be set larger:

     mysql> SET GLOBAL query_cache_size = 1000000;
     Query OK, 0 rows affected (0.04 sec)

     mysql> SHOW VARIABLES LIKE 'query_cache_size';
     +------------------+--------+
     | Variable_name    | Value  |
     +------------------+--------+
     | query_cache_size | 999424 |
     +------------------+--------+
     1 row in set (0.00 sec)

The 'query_cache_size' value is aligned to the nearest 1024 byte block.
The value reported may therefore be different from the value that you
assign.

If the query cache size is greater than 0, the 'query_cache_type'
variable influences how it works.  This variable can be set to the
following values:

   * A value of '0' or 'OFF' prevents caching or retrieval of cached
     results.

   * A value of '1' or 'ON' enables caching except of those statements
     that begin with 'SELECT SQL_NO_CACHE'.

   * A value of '2' or 'DEMAND' causes caching of only those statements
     that begin with 'SELECT SQL_CACHE'.

If 'query_cache_size' is 0, you should also set 'query_cache_type'
variable to 0.  In this case, the server does not acquire the query
cache mutex at all, which means that the query cache cannot be enabled
at runtime and there is reduced overhead in query execution.

Setting the 'GLOBAL' 'query_cache_type' value determines query cache
behavior for all clients that connect after the change is made.
Individual clients can control cache behavior for their own connection
by setting the 'SESSION' 'query_cache_type' value.  For example, a
client can disable use of the query cache for its own queries like this:

     mysql> SET SESSION query_cache_type = OFF;

If you set 'query_cache_type' at server startup (rather than at runtime
with a *note 'SET': set-variable. statement), only the numeric values
are permitted.

To control the maximum size of individual query results that can be
cached, set the 'query_cache_limit' system variable.  The default value
is 1MB.

Be careful not to set the size of the cache too large.  Due to the need
for threads to lock the cache during updates, you may see lock
contention issues with a very large cache.

*Note*:

You can set the maximum size that can be specified for the query cache
at runtime with the *note 'SET': set-variable. statement by using the
'--maximum-query_cache_size=32M' option on the command line or in the
configuration file.

When a query is to be cached, its result (the data sent to the client)
is stored in the query cache during result retrieval.  Therefore the
data usually is not handled in one big chunk.  The query cache allocates
blocks for storing this data on demand, so when one block is filled, a
new block is allocated.  Because memory allocation operation is costly
(timewise), the query cache allocates blocks with a minimum size given
by the 'query_cache_min_res_unit' system variable.  When a query is
executed, the last result block is trimmed to the actual data size so
that unused memory is freed.  Depending on the types of queries your
server executes, you might find it helpful to tune the value of
'query_cache_min_res_unit':

   * The default value of 'query_cache_min_res_unit' is 4KB. This should
     be adequate for most cases.

   * If you have a lot of queries with small results, the default block
     size may lead to memory fragmentation, as indicated by a large
     number of free blocks.  Fragmentation can force the query cache to
     prune (delete) queries from the cache due to lack of memory.  In
     this case, decrease the value of 'query_cache_min_res_unit'.  The
     number of free blocks and queries removed due to pruning are given
     by the values of the 'Qcache_free_blocks' and
     'Qcache_lowmem_prunes' status variables.

   * If most of your queries have large results (check the
     'Qcache_total_blocks' and 'Qcache_queries_in_cache' status
     variables), you can increase performance by increasing
     'query_cache_min_res_unit'.  However, be careful to not make it too
     large (see the previous item).


File: manual.info.tmp,  Node: query-cache-status-and-maintenance,  Prev: query-cache-configuration,  Up: query-cache

8.10.3.4 Query Cache Status and Maintenance
...........................................

To check whether the query cache is present in your MySQL server, use
the following statement:

     mysql> SHOW VARIABLES LIKE 'have_query_cache';
     +------------------+-------+
     | Variable_name    | Value |
     +------------------+-------+
     | have_query_cache | YES   |
     +------------------+-------+

You can defragment the query cache to better utilize its memory with the
'FLUSH QUERY CACHE' statement.  The statement does not remove any
queries from the cache.

The 'RESET QUERY CACHE' statement removes all query results from the
query cache.  The 'FLUSH TABLES' statement also does this.

To monitor query cache performance, use *note 'SHOW STATUS':
show-status. to view the cache status variables:

     mysql> SHOW STATUS LIKE 'Qcache%';
     +-------------------------+--------+
     | Variable_name           | Value  |
     +-------------------------+--------+
     | Qcache_free_blocks      | 36     |
     | Qcache_free_memory      | 138488 |
     | Qcache_hits             | 79570  |
     | Qcache_inserts          | 27087  |
     | Qcache_lowmem_prunes    | 3114   |
     | Qcache_not_cached       | 22989  |
     | Qcache_queries_in_cache | 415    |
     | Qcache_total_blocks     | 912    |
     +-------------------------+--------+

Descriptions of each of these variables are given in *note
server-status-variables::.  Some uses for them are described here.

The total number of *note 'SELECT': select. queries is given by this
formula:

       Com_select
     + Qcache_hits
     + queries with errors found by parser

The 'Com_select' value is given by this formula:

       Qcache_inserts
     + Qcache_not_cached
     + queries with errors found during the column-privileges check

The query cache uses variable-length blocks, so 'Qcache_total_blocks'
and 'Qcache_free_blocks' may indicate query cache memory fragmentation.
After 'FLUSH QUERY CACHE', only a single free block remains.

Every cached query requires a minimum of two blocks (one for the query
text and one or more for the query results).  Also, every table that is
used by a query requires one block.  However, if two or more queries use
the same table, only one table block needs to be allocated.

The information provided by the 'Qcache_lowmem_prunes' status variable
can help you tune the query cache size.  It counts the number of queries
that have been removed from the cache to free up memory for caching new
queries.  The query cache uses a least recently used (LRU) strategy to
decide which queries to remove from the cache.  Tuning information is
given in *note query-cache-configuration::.


File: manual.info.tmp,  Node: locking-issues,  Next: optimizing-server,  Prev: buffering-caching,  Up: optimization

8.11 Optimizing Locking Operations
==================================

* Menu:

* internal-locking::             Internal Locking Methods
* table-locking::                Table Locking Issues
* concurrent-inserts::           Concurrent Inserts
* metadata-locking::             Metadata Locking
* external-locking::             External Locking

When your database is busy with multiple sessions reading and writing
data, the mechanism that controls access to data files and memory areas
can become a consideration for performance tuning.  Otherwise, sessions
can spend time waiting for access to resources when they could be
running concurrently.

MySQL manages contention for table contents using locking:

   * Internal locking is performed within the MySQL server itself to
     manage contention for table contents by multiple threads.  This
     type of locking is internal because it is performed entirely by the
     server and involves no other programs.  See *note
     internal-locking::.

   * External locking occurs when the server and other programs lock
     *note 'MyISAM': myisam-storage-engine. table files to coordinate
     among themselves which program can access the tables at which time.
     See *note external-locking::.


File: manual.info.tmp,  Node: internal-locking,  Next: table-locking,  Prev: locking-issues,  Up: locking-issues

8.11.1 Internal Locking Methods
-------------------------------

This section discusses internal locking; that is, locking performed
within the MySQL server itself to manage contention for table contents
by multiple sessions.  This type of locking is internal because it is
performed entirely by the server and involves no other programs.  For
locking performed on MySQL files by other programs, see *note
external-locking::.

   * *note internal-row-level-locking::

   * *note internal-table-level-locking::

   * *note internal-locking-choices::

*Row-Level Locking*

MySQL uses row-level locking for 'InnoDB' tables to support simultaneous
write access by multiple sessions, making them suitable for multi-user,
highly concurrent, and OLTP applications.

To avoid deadlocks when performing multiple concurrent write operations
on a single 'InnoDB' table, acquire necessary locks at the start of the
transaction by issuing a 'SELECT ... FOR UPDATE' statement for each
group of rows expected to be modified, even if the data change
statements come later in the transaction.  If transactions modify or
lock more than one table, issue the applicable statements in the same
order within each transaction.  Deadlocks affect performance rather than
representing a serious error, because 'InnoDB' automatically detects
deadlock conditions and rolls back one of the affected transactions.

Advantages of row-level locking:

   * Fewer lock conflicts when different sessions access different rows.

   * Fewer changes for rollbacks.

   * Possible to lock a single row for a long time.

*Table-Level Locking*

MySQL uses table-level locking for 'MyISAM', 'MEMORY', and 'MERGE'
tables, permitting only one session to update those tables at a time.
This locking level makes these storage engines more suitable for
read-only, read-mostly, or single-user applications.

These storage engines avoid deadlocks by always requesting all needed
locks at once at the beginning of a query and always locking the tables
in the same order.  The tradeoff is that this strategy reduces
concurrency; other sessions that want to modify the table must wait
until the current data change statement finishes.

Advantages of table-level locking:

   * Relatively little memory required (row locking requires memory per
     row or group of rows locked)

   * Fast when used on a large part of the table because only a single
     lock is involved.

   * Fast if you often do 'GROUP BY' operations on a large part of the
     data or must scan the entire table frequently.

MySQL grants table write locks as follows:

  1. If there are no locks on the table, put a write lock on it.

  2. Otherwise, put the lock request in the write lock queue.

MySQL grants table read locks as follows:

  1. If there are no write locks on the table, put a read lock on it.

  2. Otherwise, put the lock request in the read lock queue.

Table updates are given higher priority than table retrievals.
Therefore, when a lock is released, the lock is made available to the
requests in the write lock queue and then to the requests in the read
lock queue.  This ensures that updates to a table are not 'starved' even
when there is heavy *note 'SELECT': select. activity for the table.
However, if there are many updates for a table, *note 'SELECT': select.
statements wait until there are no more updates.

For information on altering the priority of reads and writes, see *note
table-locking::.

You can analyze the table lock contention on your system by checking the
'Table_locks_immediate' and 'Table_locks_waited' status variables, which
indicate the number of times that requests for table locks could be
granted immediately and the number that had to wait, respectively:

     mysql> SHOW STATUS LIKE 'Table%';
     +-----------------------+---------+
     | Variable_name         | Value   |
     +-----------------------+---------+
     | Table_locks_immediate | 1151552 |
     | Table_locks_waited    | 15324   |
     +-----------------------+---------+

The 'MyISAM' storage engine supports concurrent inserts to reduce
contention between readers and writers for a given table: If a 'MyISAM'
table has no free blocks in the middle of the data file, rows are always
inserted at the end of the data file.  In this case, you can freely mix
concurrent *note 'INSERT': insert. and *note 'SELECT': select.
statements for a 'MyISAM' table without locks.  That is, you can insert
rows into a 'MyISAM' table at the same time other clients are reading
from it.  Holes can result from rows having been deleted from or updated
in the middle of the table.  If there are holes, concurrent inserts are
disabled but are enabled again automatically when all holes have been
filled with new data.  To control this behavior, use the
'concurrent_insert' system variable.  See *note concurrent-inserts::.

If you acquire a table lock explicitly with *note 'LOCK TABLES':
lock-tables, you can request a 'READ LOCAL' lock rather than a 'READ'
lock to enable other sessions to perform concurrent inserts while you
have the table locked.

To perform many *note 'INSERT': insert. and *note 'SELECT': select.
operations on a table 't1' when concurrent inserts are not possible, you
can insert rows into a temporary table 'temp_t1' and update the real
table with the rows from the temporary table:

     mysql> LOCK TABLES t1 WRITE, temp_t1 WRITE;
     mysql> INSERT INTO t1 SELECT * FROM temp_t1;
     mysql> DELETE FROM temp_t1;
     mysql> UNLOCK TABLES;

*Choosing the Type of Locking*

Generally, table locks are superior to row-level locks in the following
cases:

   * Most statements for the table are reads.

   * Statements for the table are a mix of reads and writes, where
     writes are updates or deletes for a single row that can be fetched
     with one key read:

          UPDATE TBL_NAME SET COLUMN=VALUE WHERE UNIQUE_KEY_COL=KEY_VALUE;
          DELETE FROM TBL_NAME WHERE UNIQUE_KEY_COL=KEY_VALUE;

   * *note 'SELECT': select. combined with concurrent *note 'INSERT':
     insert. statements, and very few *note 'UPDATE': update. or *note
     'DELETE': delete. statements.

   * Many scans or 'GROUP BY' operations on the entire table without any
     writers.

With higher-level locks, you can more easily tune applications by
supporting locks of different types, because the lock overhead is less
than for row-level locks.

Options other than row-level locking:

   * Versioning (such as that used in MySQL for concurrent inserts)
     where it is possible to have one writer at the same time as many
     readers.  This means that the database or table supports different
     views for the data depending on when access begins.  Other common
     terms for this are 'time travel,' 'copy on write,' or 'copy on
     demand.'

   * Copy on demand is in many cases superior to row-level locking.
     However, in the worst case, it can use much more memory than using
     normal locks.

   * Instead of using row-level locks, you can employ application-level
     locks, such as those provided by 'GET_LOCK()' and 'RELEASE_LOCK()'
     in MySQL. These are advisory locks, so they work only with
     applications that cooperate with each other.  See *note
     locking-functions::.


File: manual.info.tmp,  Node: table-locking,  Next: concurrent-inserts,  Prev: internal-locking,  Up: locking-issues

8.11.2 Table Locking Issues
---------------------------

'InnoDB' tables use row-level locking so that multiple sessions and
applications can read from and write to the same table simultaneously,
without making each other wait or producing inconsistent results.  For
this storage engine, avoid using the *note 'LOCK TABLES': lock-tables.
statement, because it does not offer any extra protection, but instead
reduces concurrency.  The automatic row-level locking makes these tables
suitable for your busiest databases with your most important data, while
also simplifying application logic since you do not need to lock and
unlock tables.  Consequently, the 'InnoDB' storage engine is the default
in MySQL 5.5 and higher.

MySQL uses table locking (instead of page, row, or column locking) for
all storage engines except 'InnoDB' and *note 'NDB': mysql-cluster.  The
locking operations themselves do not have much overhead.  But because
only one session can write to a table at any one time, for best
performance with these other storage engines, use them primarily for
tables that are queried often and rarely inserted into or updated.

   * *note table-locking-innodb::

   * *note table-locking-workarounds::

*Performance Considerations Favoring InnoDB*

When choosing whether to create a table using 'InnoDB' or a different
storage engine, keep in mind the following disadvantages of table
locking:

   * Table locking enables many sessions to read from a table at the
     same time, but if a session wants to write to a table, it must
     first get exclusive access, meaning it might have to wait for other
     sessions to finish with the table first.  During the update, all
     other sessions that want to access this particular table must wait
     until the update is done.

   * Table locking causes problems when a session is waiting because the
     disk is full and free space needs to become available before the
     session can proceed.  In this case, all sessions that want to
     access the problem table are also put in a waiting state until more
     disk space is made available.

   * A *note 'SELECT': select. statement that takes a long time to run
     prevents other sessions from updating the table in the meantime,
     making the other sessions appear slow or unresponsive.  While a
     session is waiting to get exclusive access to the table for
     updates, other sessions that issue *note 'SELECT': select.
     statements will queue up behind it, reducing concurrency even for
     read-only sessions.

*Workarounds for Locking Performance Issues*

The following items describe some ways to avoid or reduce contention
caused by table locking:

   * Consider switching the table to the 'InnoDB' storage engine, either
     using 'CREATE TABLE ... ENGINE=INNODB' during setup, or using
     'ALTER TABLE ... ENGINE=INNODB' for an existing table.  See *note
     innodb-storage-engine:: for more details about this storage engine.

   * Optimize *note 'SELECT': select. statements to run faster so that
     they lock tables for a shorter time.  You might have to create some
     summary tables to do this.

   * Start *note 'mysqld': mysqld. with '--low-priority-updates'.  For
     storage engines that use only table-level locking (such as
     'MyISAM', 'MEMORY', and 'MERGE'), this gives all statements that
     update (modify) a table lower priority than *note 'SELECT': select.
     statements.  In this case, the second *note 'SELECT': select.
     statement in the preceding scenario would execute before the *note
     'UPDATE': update. statement, and would not wait for the first *note
     'SELECT': select. to finish.

   * To specify that all updates issued in a specific connection should
     be done with low priority, set the 'low_priority_updates' server
     system variable equal to 1.

   * To give a specific *note 'INSERT': insert, *note 'UPDATE': update,
     or *note 'DELETE': delete. statement lower priority, use the
     'LOW_PRIORITY' attribute.

   * To give a specific *note 'SELECT': select. statement higher
     priority, use the 'HIGH_PRIORITY' attribute.  See *note select::.

   * Start *note 'mysqld': mysqld. with a low value for the
     'max_write_lock_count' system variable to force MySQL to
     temporarily elevate the priority of all *note 'SELECT': select.
     statements that are waiting for a table after a specific number of
     inserts to the table occur.  This permits 'READ' locks after a
     certain number of 'WRITE' locks.

   * If you have problems with *note 'INSERT': insert. combined with
     *note 'SELECT': select, consider switching to 'MyISAM' tables,
     which support concurrent *note 'SELECT': select. and *note
     'INSERT': insert. statements.  (See *note concurrent-inserts::.)

   * If you mix inserts and deletes on the same table, *note 'INSERT
     DELAYED': insert-delayed. may be of great help.  See *note
     insert-delayed::.

   * If you have problems with mixed *note 'SELECT': select. and *note
     'DELETE': delete. statements, the 'LIMIT' option to *note 'DELETE':
     delete. may help.  See *note delete::.

   * Using 'SQL_BUFFER_RESULT' with *note 'SELECT': select. statements
     can help to make the duration of table locks shorter.  See *note
     select::.

   * Splitting table contents into separate tables may help, by allowing
     queries to run against columns in one table, while updates are
     confined to columns in a different table.

   * You could change the locking code in 'mysys/thr_lock.c' to use a
     single queue.  In this case, write locks and read locks would have
     the same priority, which might help some applications.


File: manual.info.tmp,  Node: concurrent-inserts,  Next: metadata-locking,  Prev: table-locking,  Up: locking-issues

8.11.3 Concurrent Inserts
-------------------------

The 'MyISAM' storage engine supports concurrent inserts to reduce
contention between readers and writers for a given table: If a 'MyISAM'
table has no holes in the data file (deleted rows in the middle), an
*note 'INSERT': insert. statement can be executed to add rows to the end
of the table at the same time that *note 'SELECT': select. statements
are reading rows from the table.  If there are multiple *note 'INSERT':
insert. statements, they are queued and performed in sequence,
concurrently with the *note 'SELECT': select. statements.  The results
of a concurrent *note 'INSERT': insert. may not be visible immediately.

The 'concurrent_insert' system variable can be set to modify the
concurrent-insert processing.  By default, the variable is set to 'AUTO'
(or 1) and concurrent inserts are handled as just described.  If
'concurrent_insert' is set to 'NEVER' (or 0), concurrent inserts are
disabled.  If the variable is set to 'ALWAYS' (or 2), concurrent inserts
at the end of the table are permitted even for tables that have deleted
rows.  See also the description of the 'concurrent_insert' system
variable.

Under circumstances where concurrent inserts can be used, there is
seldom any need to use the 'DELAYED' modifier for *note 'INSERT':
insert. statements.  See *note insert-delayed::.

If you are using the binary log, concurrent inserts are converted to
normal inserts for 'CREATE ... SELECT' or *note 'INSERT ... SELECT':
insert-select. statements.  This is done to ensure that you can
re-create an exact copy of your tables by applying the log during a
backup operation.  See *note binary-log::.  In addition, for those
statements a read lock is placed on the selected-from table such that
inserts into that table are blocked.  The effect is that concurrent
inserts for that table must wait as well.

With *note 'LOAD DATA': load-data, if you specify 'CONCURRENT' with a
'MyISAM' table that satisfies the condition for concurrent inserts (that
is, it contains no free blocks in the middle), other sessions can
retrieve data from the table while *note 'LOAD DATA': load-data. is
executing.  Use of the 'CONCURRENT' option affects the performance of
*note 'LOAD DATA': load-data. a bit, even if no other session is using
the table at the same time.

If you specify 'HIGH_PRIORITY', it overrides the effect of the
'--low-priority-updates' option if the server was started with that
option.  It also causes concurrent inserts not to be used.

For *note 'LOCK TABLE': lock-tables, the difference between 'READ LOCAL'
and 'READ' is that 'READ LOCAL' permits nonconflicting *note 'INSERT':
insert. statements (concurrent inserts) to execute while the lock is
held.  However, this cannot be used if you are going to manipulate the
database using processes external to the server while you hold the lock.


File: manual.info.tmp,  Node: metadata-locking,  Next: external-locking,  Prev: concurrent-inserts,  Up: locking-issues

8.11.4 Metadata Locking
-----------------------

MySQL uses metadata locking to manage concurrent access to database
objects and to ensure data consistency.  Metadata locking applies not
just to tables, but also to schemas and stored programs (procedures,
functions, triggers, and scheduled events).

Metadata locking does involve some overhead, which increases as query
volume increases.  Metadata contention increases the more that multiple
queries attempt to access the same objects.

Metadata locking is not a replacement for the table definition cache,
and its mutexes and locks differ from the 'LOCK_open' mutex.  The
following discussion provides some information about how metadata
locking works.

   * *note metadata-lock-acquisition::

   * *note metadata-lock-release::

*Metadata Lock Acquisition*

If there are multiple waiters for a given lock, the highest-priority
lock request is satisfied first, with an exception related to the
'max_write_lock_count' system variable.  Write lock requests have higher
priority than read lock requests.  However, if 'max_write_lock_count' is
set to some low value (say, 10), read lock requests may be preferred
over pending write lock requests if the read lock requests have already
been passed over in favor of 10 write lock requests.  Normally this
behavior does not occur because 'max_write_lock_count' by default has a
very large value.

Statements acquire metadata locks one by one, not simultaneously, and
perform deadlock detection in the process.

DML statements normally acquire locks in the order in which tables are
mentioned in the statement.

DDL statements, *note 'LOCK TABLES': lock-tables, and other similar
statements try to reduce the number of possible deadlocks between
concurrent DDL statements by acquiring locks on explicitly named tables
in name order.  Locks might be acquired in a different order for
implicitly used tables (such as tables in foreign key relationships that
also must be locked).

For example, *note 'RENAME TABLE': rename-table. is a DDL statement that
acquires locks in name order:

   * This *note 'RENAME TABLE': rename-table. statement renames 'tbla'
     to something else, and renames 'tblc' to 'tbla':

          RENAME TABLE tbla TO tbld, tblc TO tbla;

     The statement acquires metadata locks, in order, on 'tbla', 'tblc',
     and 'tbld' (because 'tbld' follows 'tblc' in name order):

   * This slightly different statement also renames 'tbla' to something
     else, and renames 'tblc' to 'tbla':

          RENAME TABLE tbla TO tblb, tblc TO tbla;

     In this case, the statement acquires metadata locks, in order, on
     'tbla', 'tblb', and 'tblc' (because 'tblb' precedes 'tblc' in name
     order):

Both statements acquire locks on 'tbla' and 'tblc', in that order, but
differ in whether the lock on the remaining table name is acquired
before or after 'tblc'.

Metadata lock acquisition order can make a difference in operation
outcome when multiple transactions execute concurrently, as the
following example illustrates.

Begin with two tables 'x' and 'x_new' that have identical structure.
Three clients issue statements that involve these tables:

Client 1:

     LOCK TABLE x WRITE, x_new WRITE;

The statement requests and acquires write locks in name order on 'x' and
'x_new'.

Client 2:

     INSERT INTO x VALUES(1);

The statement requests and blocks waiting for a write lock on 'x'.

Client 3:

     RENAME TABLE x TO x_old, x_new TO x;

The statement requests exclusive locks in name order on 'x', 'x_new',
and 'x_old', but blocks waiting for the lock on 'x'.

Client 1:

     UNLOCK TABLES;

The statement releases the write locks on 'x' and 'x_new'.  The
exclusive lock request for 'x' by Client 3 has higher priority than the
write lock request by Client 2, so Client 3 acquires its lock on 'x',
then also on 'x_new' and 'x_old', performs the renaming, and releases
its locks.  Client 2 then acquires its lock on 'x', performs the insert,
and releases its lock.

Lock acquisition order results in the *note 'RENAME TABLE':
rename-table. executing before the *note 'INSERT': insert.  The 'x' into
which the insert occurs is the table that was named 'x_new' when Client
2 issued the insert and was renamed to 'x' by Client 3:

     mysql> SELECT * FROM x;
     +------+
     | i    |
     +------+
     |    1 |
     +------+

     mysql> SELECT * FROM x_old;
     Empty set (0.01 sec)

Now begin instead with tables named 'x' and 'new_x' that have identical
structure.  Again, three clients issue statements that involve these
tables:

Client 1:

     LOCK TABLE x WRITE, new_x WRITE;

The statement requests and acquires write locks in name order on 'new_x'
and 'x'.

Client 2:

     INSERT INTO x VALUES(1);

The statement requests and blocks waiting for a write lock on 'x'.

Client 3:

     RENAME TABLE x TO old_x, new_x TO x;

The statement requests exclusive locks in name order on 'new_x',
'old_x', and 'x', but blocks waiting for the lock on 'new_x'.

Client 1:

     UNLOCK TABLES;

The statement releases the write locks on 'x' and 'new_x'.  For 'x', the
only pending request is by Client 2, so Client 2 acquires its lock,
performs the insert, and releases the lock.  For 'new_x', the only
pending request is by Client 3, which is permitted to acquire that lock
(and also the lock on 'old_x').  The rename operation still blocks for
the lock on 'x' until the Client 2 insert finishes and releases its
lock.  Then Client 3 acquires the lock on 'x', performs the rename, and
releases its lock.

In this case, lock acquisition order results in the *note 'INSERT':
insert. executing before the *note 'RENAME TABLE': rename-table.  The
'x' into which the insert occurs is the original 'x', now renamed to
'old_x' by the rename operation:

     mysql> SELECT * FROM x;
     Empty set (0.01 sec)

     mysql> SELECT * FROM old_x;
     +------+
     | i    |
     +------+
     |    1 |
     +------+

If order of lock acquisition in concurrent statements makes a difference
to an application in operation outcome, as in the preceding example, you
may be able to adjust the table names to affect the order of lock
acquisition.

*Metadata Lock Release*

To ensure transaction serializability, the server must not permit one
session to perform a data definition language (DDL) statement on a table
that is used in an uncompleted explicitly or implicitly started
transaction in another session.  The server achieves this by acquiring
metadata locks on tables used within a transaction and deferring release
of those locks until the transaction ends.  A metadata lock on a table
prevents changes to the table's structure.  This locking approach has
the implication that a table that is being used by a transaction within
one session cannot be used in DDL statements by other sessions until the
transaction ends.

This principle applies not only to transactional tables, but also to
nontransactional tables.  Suppose that a session begins a transaction
that uses transactional table 't' and nontransactional table 'nt' as
follows:

     START TRANSACTION;
     SELECT * FROM t;
     SELECT * FROM nt;

The server holds metadata locks on both 't' and 'nt' until the
transaction ends.  If another session attempts a DDL or write lock
operation on either table, it blocks until metadata lock release at
transaction end.  For example, a second session blocks if it attempts
any of these operations:

     DROP TABLE t;
     ALTER TABLE t ...;
     DROP TABLE nt;
     ALTER TABLE nt ...;
     LOCK TABLE t ... WRITE;

If the server acquires metadata locks for a statement that is
syntactically valid but fails during execution, it does not release the
locks early.  Lock release is still deferred to the end of the
transaction because the failed statement is written to the binary log
and the locks protect log consistency.

In autocommit mode, each statement is in effect a complete transaction,
so metadata locks acquired for the statement are held only to the end of
the statement.

Metadata locks acquired during a *note 'PREPARE': prepare. statement are
released once the statement has been prepared, even if preparation
occurs within a multiple-statement transaction.


File: manual.info.tmp,  Node: external-locking,  Prev: metadata-locking,  Up: locking-issues

8.11.5 External Locking
-----------------------

External locking is the use of file system locking to manage contention
for *note 'MyISAM': myisam-storage-engine. database tables by multiple
processes.  External locking is used in situations where a single
process such as the MySQL server cannot be assumed to be the only
process that requires access to tables.  Here are some examples:

   * If you run multiple servers that use the same database directory
     (not recommended), each server must have external locking enabled.

   * If you use *note 'myisamchk': myisamchk. to perform table
     maintenance operations on *note 'MyISAM': myisam-storage-engine.
     tables, you must either ensure that the server is not running, or
     that the server has external locking enabled so that it locks table
     files as necessary to coordinate with *note 'myisamchk': myisamchk.
     for access to the tables.  The same is true for use of *note
     'myisampack': myisampack. to pack *note 'MyISAM':
     myisam-storage-engine. tables.

     If the server is run with external locking enabled, you can use
     *note 'myisamchk': myisamchk. at any time for read operations such
     a checking tables.  In this case, if the server tries to update a
     table that *note 'myisamchk': myisamchk. is using, the server will
     wait for *note 'myisamchk': myisamchk. to finish before it
     continues.

     If you use *note 'myisamchk': myisamchk. for write operations such
     as repairing or optimizing tables, or if you use *note
     'myisampack': myisampack. to pack tables, you _must_ always ensure
     that the *note 'mysqld': mysqld. server is not using the table.  If
     you do not stop *note 'mysqld': mysqld, at least do a *note
     'mysqladmin flush-tables': mysqladmin. before you run *note
     'myisamchk': myisamchk.  Your tables _may become corrupted_ if the
     server and *note 'myisamchk': myisamchk. access the tables
     simultaneously.

With external locking in effect, each process that requires access to a
table acquires a file system lock for the table files before proceeding
to access the table.  If all necessary locks cannot be acquired, the
process is blocked from accessing the table until the locks can be
obtained (after the process that currently holds the locks releases
them).

External locking affects server performance because the server must
sometimes wait for other processes before it can access tables.

External locking is unnecessary if you run a single server to access a
given data directory (which is the usual case) and if no other programs
such as *note 'myisamchk': myisamchk. need to modify tables while the
server is running.  If you only _read_ tables with other programs,
external locking is not required, although *note 'myisamchk': myisamchk.
might report warnings if the server changes tables while *note
'myisamchk': myisamchk. is reading them.

With external locking disabled, to use *note 'myisamchk': myisamchk, you
must either stop the server while *note 'myisamchk': myisamchk. executes
or else lock and flush the tables before running *note 'myisamchk':
myisamchk.  (See *note system-optimization::.)  To avoid this
requirement, use the *note 'CHECK TABLE': check-table. and *note 'REPAIR
TABLE': repair-table. statements to check and repair *note 'MyISAM':
myisam-storage-engine. tables.

For *note 'mysqld': mysqld, external locking is controlled by the value
of the 'skip_external_locking' system variable.  When this variable is
enabled, external locking is disabled, and vice versa.  External locking
is disabled by default.

Use of external locking can be controlled at server startup by using the
'--external-locking' or '--skip-external-locking' option.

If you do use external locking option to enable updates to *note
'MyISAM': myisam-storage-engine. tables from many MySQL processes, you
must ensure that the following conditions are satisfied:

   * Do not use the query cache for queries that use tables that are
     updated by another process.

   * Do not start the server with the 'delay_key_write' system variable
     set to 'ALL' or use the 'DELAY_KEY_WRITE=1' table option for any
     shared tables.  Otherwise, index corruption can occur.

The easiest way to satisfy these conditions is to always use
'--external-locking' together with '--delay-key-write=OFF' and
'--query-cache-size=0'.  (This is not done by default because in many
setups it is useful to have a mixture of the preceding options.)


File: manual.info.tmp,  Node: optimizing-server,  Next: optimize-benchmarking,  Prev: locking-issues,  Up: optimization

8.12 Optimizing the MySQL Server
================================

* Menu:

* system-optimization::          System Factors
* disk-issues::                  Optimizing Disk I/O
* symbolic-links::               Using Symbolic Links
* optimizing-memory::            Optimizing Memory Use
* optimizing-network::           Optimizing Network Use

This section discusses optimization techniques for the database server,
primarily dealing with system configuration rather than tuning SQL
statements.  The information in this section is appropriate for DBAs who
want to ensure performance and scalability across the servers they
manage; for developers constructing installation scripts that include
setting up the database; and people running MySQL themselves for
development, testing, and so on who want to maximize their own
productivity.


File: manual.info.tmp,  Node: system-optimization,  Next: disk-issues,  Prev: optimizing-server,  Up: optimizing-server

8.12.1 System Factors
---------------------

Some system-level factors can affect performance in a major way:

   * If you have enough RAM, you could remove all swap devices.  Some
     operating systems use a swap device in some contexts even if you
     have free memory.

   * Avoid external locking for *note 'MyISAM': myisam-storage-engine.
     tables.  The default is for external locking to be disabled.  The
     '--external-locking' and '--skip-external-locking' options
     explicitly enable and disable external locking.

     Disabling external locking does not affect MySQL's functionality as
     long as you run only one server.  Just remember to take down the
     server (or lock and flush the relevant tables) before you run *note
     'myisamchk': myisamchk.  On some systems it is mandatory to disable
     external locking because it does not work, anyway.

     The only case in which you cannot disable external locking is when
     you run multiple MySQL _servers_ (not clients) on the same data, or
     if you run *note 'myisamchk': myisamchk. to check (not repair) a
     table without telling the server to flush and lock the tables
     first.  Note that using multiple MySQL servers to access the same
     data concurrently is generally _not_ recommended, except when using
     NDB Cluster.

     The *note 'LOCK TABLES': lock-tables. and *note 'UNLOCK TABLES':
     lock-tables. statements use internal locking, so you can use them
     even if external locking is disabled.


File: manual.info.tmp,  Node: disk-issues,  Next: symbolic-links,  Prev: system-optimization,  Up: optimizing-server

8.12.2 Optimizing Disk I/O
--------------------------

This section describes ways to configure storage devices when you can
devote more and faster storage hardware to the database server.  For
information about optimizing an 'InnoDB' configuration to improve I/O
performance, see *note optimizing-innodb-diskio::.

   * Disk seeks are a huge performance bottleneck.  This problem becomes
     more apparent when the amount of data starts to grow so large that
     effective caching becomes impossible.  For large databases where
     you access data more or less randomly, you can be sure that you
     need at least one disk seek to read and a couple of disk seeks to
     write things.  To minimize this problem, use disks with low seek
     times.

   * Increase the number of available disk spindles (and thereby reduce
     the seek overhead) by either symlinking files to different disks or
     striping the disks:

        * Using symbolic links

          This means that, for 'MyISAM' tables, you symlink the index
          file and data files from their usual location in the data
          directory to another disk (that may also be striped).  This
          makes both the seek and read times better, assuming that the
          disk is not used for other purposes as well.  See *note
          symbolic-links::.

          Symbolic links are not supported for use with 'InnoDB' tables.
          However, it is possible to place 'InnoDB' data and log files
          on different physical disks.  For more information, see *note
          optimizing-innodb-diskio::.

        * 
          Striping

          Striping means that you have many disks and put the first
          block on the first disk, the second block on the second disk,
          and the N-th block on the ('N MOD NUMBER_OF_DISKS') disk, and
          so on.  This means if your normal data size is less than the
          stripe size (or perfectly aligned), you get much better
          performance.  Striping is very dependent on the operating
          system and the stripe size, so benchmark your application with
          different stripe sizes.  See *note custom-benchmarks::.

          The speed difference for striping is _very_ dependent on the
          parameters.  Depending on how you set the striping parameters
          and number of disks, you may get differences measured in
          orders of magnitude.  You have to choose to optimize for
          random or sequential access.

   * For reliability, you may want to use RAID 0+1 (striping plus
     mirroring), but in this case, you need 2 x N drives to hold N
     drives of data.  This is probably the best option if you have the
     money for it.  However, you may also have to invest in some
     volume-management software to handle it efficiently.

   * A good option is to vary the RAID level according to how critical a
     type of data is.  For example, store semi-important data that can
     be regenerated on a RAID 0 disk, but store really important data
     such as host information and logs on a RAID 0+1 or RAID N disk.
     RAID N can be a problem if you have many writes, due to the time
     required to update the parity bits.

   * You can also set the parameters for the file system that the
     database uses:

     If you do not need to know when files were last accessed (which is
     not really useful on a database server), you can mount your file
     systems with the '-o noatime' option.  That skips updates to the
     last access time in inodes on the file system, which avoids some
     disk seeks.

     On many operating systems, you can set a file system to be updated
     asynchronously by mounting it with the '-o async' option.  If your
     computer is reasonably stable, this should give you better
     performance without sacrificing too much reliability.  (This flag
     is on by default on Linux.)

*Using NFS with MySQL*

Caution is advised when considering using NFS with MySQL. Potential
issues, which vary by operating system and NFS version, include:

   * MySQL data and log files placed on NFS volumes becoming locked and
     unavailable for use.  Locking issues may occur in cases where
     multiple instances of MySQL access the same data directory or where
     MySQL is shut down improperly, due to a power outage, for example.
     NFS version 4 addresses underlying locking issues with the
     introduction of advisory and lease-based locking.  However, sharing
     a data directory among MySQL instances is not recommended.

   * Data inconsistencies introduced due to messages received out of
     order or lost network traffic.  To avoid this issue, use TCP with
     'hard' and 'intr' mount options.

   * Maximum file size limitations.  NFS Version 2 clients can only
     access the lowest 2GB of a file (signed 32 bit offset).  NFS
     Version 3 clients support larger files (up to 64 bit offsets).  The
     maximum supported file size also depends on the local file system
     of the NFS server.

Using NFS within a professional SAN environment or other storage system
tends to offer greater reliability than using NFS outside of such an
environment.  However, NFS within a SAN environment may be slower than
directly attached or bus-attached non-rotational storage.

If you choose to use NFS, NFS Version 4 or later is recommended, as is
testing your NFS setup thoroughly before deploying into a production
environment.


File: manual.info.tmp,  Node: symbolic-links,  Next: optimizing-memory,  Prev: disk-issues,  Up: optimizing-server

8.12.3 Using Symbolic Links
---------------------------

* Menu:

* symbolic-links-to-databases::  Using Symbolic Links for Databases on Unix
* symbolic-links-to-tables::     Using Symbolic Links for MyISAM Tables on Unix
* windows-symbolic-links::       Using Symbolic Links for Databases on Windows

You can move databases or tables from the database directory to other
locations and replace them with symbolic links to the new locations.
You might want to do this, for example, to move a database to a file
system with more free space or increase the speed of your system by
spreading your tables to different disks.

The recommended way to do this is to symlink entire database directories
to a different disk.  Symlink 'MyISAM' tables only as a last resort.

To determine the location of your data directory, use this statement:

     SHOW VARIABLES LIKE 'datadir';


File: manual.info.tmp,  Node: symbolic-links-to-databases,  Next: symbolic-links-to-tables,  Prev: symbolic-links,  Up: symbolic-links

8.12.3.1 Using Symbolic Links for Databases on Unix
...................................................

On Unix, the way to symlink a database is first to create a directory on
some disk where you have free space and then to create a soft link to it
from the MySQL data directory.

     shell> mkdir /dr1/databases/test
     shell> ln -s /dr1/databases/test /PATH/TO/DATADIR

MySQL does not support linking one directory to multiple databases.
Replacing a database directory with a symbolic link works as long as you
do not make a symbolic link between databases.  Suppose that you have a
database 'db1' under the MySQL data directory, and then make a symlink
'db2' that points to 'db1':

     shell> cd /PATH/TO/DATADIR
     shell> ln -s db1 db2

The result is that, for any table 'tbl_a' in 'db1', there also appears
to be a table 'tbl_a' in 'db2'.  If one client updates 'db1.tbl_a' and
another client updates 'db2.tbl_a', problems are likely to occur.


File: manual.info.tmp,  Node: symbolic-links-to-tables,  Next: windows-symbolic-links,  Prev: symbolic-links-to-databases,  Up: symbolic-links

8.12.3.2 Using Symbolic Links for MyISAM Tables on Unix
.......................................................

Symlinks are fully supported only for 'MyISAM' tables.  For files used
by tables for other storage engines, you may get strange problems if you
try to use symbolic links.

Do not symlink tables on systems that do not have a fully operational
'realpath()' call.  (Linux and Solaris support 'realpath()').  To
determine whether your system supports symbolic links, check the value
of the 'have_symlink' system variable using this statement:

     SHOW VARIABLES LIKE 'have_symlink';

The handling of symbolic links for 'MyISAM' tables works as follows:

   * In the data directory, you always have the table format ('.frm')
     file, the data ('.MYD') file, and the index ('.MYI') file.  The
     data file and index file can be moved elsewhere and replaced in the
     data directory by symlinks.  The format file cannot.

   * You can symlink the data file and the index file independently to
     different directories.

   * To instruct a running MySQL server to perform the symlinking, use
     the 'DATA DIRECTORY' and 'INDEX DIRECTORY' options to *note 'CREATE
     TABLE': create-table.  See *note create-table::.  Alternatively, if
     *note 'mysqld': mysqld. is not running, symlinking can be
     accomplished manually using 'ln -s' from the command line.

     *Note*:

     The path used with either or both of the 'DATA DIRECTORY' and
     'INDEX DIRECTORY' options may not include the MySQL 'data'
     directory.  (Bug #32167)

   * *note 'myisamchk': myisamchk. does not replace a symlink with the
     data file or index file.  It works directly on the file to which
     the symlink points.  Any temporary files are created in the
     directory where the data file or index file is located.  The same
     is true for the *note 'ALTER TABLE': alter-table, *note 'OPTIMIZE
     TABLE': optimize-table, and *note 'REPAIR TABLE': repair-table.
     statements.

   * *Note*:

     When you drop a table that is using symlinks, _both the symlink and
     the file to which the symlink points are dropped_.  This is an
     extremely good reason _not_ to run *note 'mysqld': mysqld. as the
     'root' operating system user or permit operating system users to
     have write access to MySQL database directories.

   * If you rename a table with *note 'ALTER TABLE ... RENAME':
     alter-table. or *note 'RENAME TABLE': rename-table. and you do not
     move the table to another database, the symlinks in the database
     directory are renamed to the new names and the data file and index
     file are renamed accordingly.

   * If you use *note 'ALTER TABLE ... RENAME': alter-table. or *note
     'RENAME TABLE': rename-table. to move a table to another database,
     the table is moved to the other database directory.  If the table
     name changed, the symlinks in the new database directory are
     renamed to the new names and the data file and index file are
     renamed accordingly.

   * If you are not using symlinks, start *note 'mysqld': mysqld. with
     the '--skip-symbolic-links' option to ensure that no one can use
     *note 'mysqld': mysqld. to drop or rename a file outside of the
     data directory.

These table symlink operations are not supported:

   * *note 'ALTER TABLE': alter-table. ignores the 'DATA DIRECTORY' and
     'INDEX DIRECTORY' table options.

   * As indicated previously, only the data and index files can be
     symbolic links.  The '.frm' file must _never_ be a symbolic link.
     Attempting to do this (for example, to make one table name a
     synonym for another) produces incorrect results.  Suppose that you
     have a database 'db1' under the MySQL data directory, a table
     'tbl1' in this database, and in the 'db1' directory you make a
     symlink 'tbl2' that points to 'tbl1':

          shell> cd /PATH/TO/DATADIR/db1
          shell> ln -s tbl1.frm tbl2.frm
          shell> ln -s tbl1.MYD tbl2.MYD
          shell> ln -s tbl1.MYI tbl2.MYI

     Problems result if one thread reads 'db1.tbl1' and another thread
     updates 'db1.tbl2':

        * The query cache is 'fooled' (it has no way of knowing that
          'tbl1' has not been updated, so it returns outdated results).

        * 'ALTER' statements on 'tbl2' fail.


File: manual.info.tmp,  Node: windows-symbolic-links,  Prev: symbolic-links-to-tables,  Up: symbolic-links

8.12.3.3 Using Symbolic Links for Databases on Windows
......................................................

On Windows, symbolic links can be used for database directories.  This
enables you to put a database directory at a different location (for
example, on a different disk) by setting up a symbolic link to it.  Use
of database symlinks on Windows is similar to their use on Unix,
although the procedure for setting up the link differs.

Suppose that you want to place the database directory for a database
named 'mydb' at 'D:\data\mydb'.  To do this, create a symbolic link in
the MySQL data directory that points to 'D:\data\mydb'.  However, before
creating the symbolic link, make sure that the 'D:\data\mydb' directory
exists by creating it if necessary.  If you already have a database
directory named 'mydb' in the data directory, move it to 'D:\data'.
Otherwise, the symbolic link will be ineffective.  To avoid problems,
make sure that the server is not running when you move the database
directory.

The procedure for creating the database symbolic link depends on your
version of Windows.

On Windows, you can create a symlink using the 'mklink' command.  This
command requires administrative privileges.

  1. Change location into the data directory:

          C:\> cd \PATH\TO\DATADIR

  2. In the data directory, create a symlink named 'mydb' that points to
     the location of the database directory:

          C:\> mklink /d mydb D:\data\mydb

After this, all tables created in the database 'mydb' are created in
'D:\data\mydb'.

Alternatively, on any version of Windows supported by MySQL, you can
create a symbolic link to a MySQL database by creating a '.sym' file in
the data directory that contains the path to the destination directory.
The file should be named 'DB_NAME.sym', where DB_NAME is the database
name.

Support for database symbolic links on Windows using '.sym' files is
enabled by default.  If you do not need '.sym' file symbolic links, you
can disable support for them by starting *note 'mysqld': mysqld. with
the '--skip-symbolic-links' option.  To determine whether your system
supports '.sym' file symbolic links, check the value of the
'have_symlink' system variable using this statement:

     SHOW VARIABLES LIKE 'have_symlink';

To create a '.sym' file symlink, use this procedure:

  1. Change location into the data directory:

          C:\> cd \PATH\TO\DATADIR

  2. In the data directory, create a text file named 'mydb.sym' that
     contains this path name: 'D:\data\mydb\'

     *Note*:

     The path name to the new database and tables should be absolute.
     If you specify a relative path, the location will be relative to
     the 'mydb.sym' file.

After this, all tables created in the database 'mydb' are created in
'D:\data\mydb'.

The following limitations apply to the use of '.sym' files for database
symbolic linking on Windows.  These limitations do not apply for
symlinks created using 'mklink'.

   * The symbolic link is not used if a directory with the same name as
     the database exists in the MySQL data directory.

   * The '--innodb_file_per_table' option cannot be used.

   * If you run *note 'mysqld': mysqld. as a service, you cannot use a
     mapped drive to a remote server as the destination of the symbolic
     link.  As a workaround, you can use the full path
     ('\\servername\path\').


File: manual.info.tmp,  Node: optimizing-memory,  Next: optimizing-network,  Prev: symbolic-links,  Up: optimizing-server

8.12.4 Optimizing Memory Use
----------------------------

* Menu:

* memory-use::                   How MySQL Uses Memory
* large-page-support::           Enabling Large Page Support


File: manual.info.tmp,  Node: memory-use,  Next: large-page-support,  Prev: optimizing-memory,  Up: optimizing-memory

8.12.4.1 How MySQL Uses Memory
..............................

MySQL allocates buffers and caches to improve performance of database
operations.  You can improve MySQL performance by increasing the values
of certain cache and buffer-related system variables.  You can also
modify these variables to run MySQL on systems with limited memory.

The following list describes some of the ways that MySQL uses memory.
Where applicable, relevant system variables are referenced.  Some items
are storage engine or feature specific.

   * The 'InnoDB' buffer pool is a memory area that holds cached
     'InnoDB' data for tables, indexes, and other auxiliary buffers.
     For efficiency of high-volume read operations, the buffer pool is
     divided into pages that can potentially hold multiple rows.  For
     efficiency of cache management, the buffer pool is implemented as a
     linked list of pages; data that is rarely used is aged out of the
     cache, using a variation of the LRU algorithm.  For more
     information, see *note innodb-buffer-pool::.

     The size of the buffer pool is important for system performance:

        * 'InnoDB' allocates memory for the entire buffer pool at server
          startup, using 'malloc()' operations.  The
          'innodb_buffer_pool_size' system variable defines the buffer
          pool size.  Typically, a recommended 'innodb_buffer_pool_size'
          value is 50 to 75 percent of system memory.  For more
          information, see Configuring InnoDB Buffer Pool Size
          (https://dev.mysql.com/doc/refman/5.7/en/innodb-buffer-pool-resize.html).

        * On systems with a large amount of memory, you can improve
          concurrency by dividing the buffer pool into multiple buffer
          pool instances.  The 'innodb_buffer_pool_instances' system
          variable defines the number of buffer pool instances.

        * A buffer pool that is too small may cause excessive churning
          as pages are flushed from the buffer pool only to be required
          again a short time later.

        * A buffer pool that is too large may cause swapping due to
          competition for memory.

   * All threads share the *note 'MyISAM': myisam-storage-engine. key
     buffer.  The 'key_buffer_size' system variable determines its size.

     For each 'MyISAM' table the server opens, the index file is opened
     once; the data file is opened once for each concurrently running
     thread that accesses the table.  For each concurrent thread, a
     table structure, column structures for each column, and a buffer of
     size '3 * N' are allocated (where N is the maximum row length, not
     counting *note 'BLOB': blob. columns).  A *note 'BLOB': blob.
     column requires five to eight bytes plus the length of the *note
     'BLOB': blob. data.  The 'MyISAM' storage engine maintains one
     extra row buffer for internal use.

   * The 'myisam_use_mmap' system variable can be set to 1 to enable
     memory-mapping for all 'MyISAM' tables.

   * If an internal in-memory temporary table becomes too large (as
     determined using the 'tmp_table_size' and 'max_heap_table_size'
     system variables), MySQL automatically converts the table from
     in-memory to on-disk format.  On-disk temporary tables use the
     'MyISAM' storage engine.  You can increase the permissible
     temporary table size as described in *note
     internal-temporary-tables::.

     For *note 'MEMORY': memory-storage-engine. tables explicitly
     created with *note 'CREATE TABLE': create-table, only the
     'max_heap_table_size' system variable determines how large a table
     can grow, and there is no conversion to on-disk format.

   * The *note MySQL Performance Schema: performance-schema. is a
     feature for monitoring MySQL server execution at a low level.  For
     performance reasons, fixed memory buffers for Performance Schema
     are allocated at server startup and do not change in size while the
     server is running.

   * Each thread that the server uses to manage client connections
     requires some thread-specific space.  The following list indicates
     these and which system variables control their size:

        * A stack ('thread_stack')

        * A connection buffer ('net_buffer_length')

        * A result buffer ('net_buffer_length')

     The connection buffer and result buffer each begin with a size
     equal to 'net_buffer_length' bytes, but are dynamically enlarged up
     to 'max_allowed_packet' bytes as needed.  The result buffer shrinks
     to 'net_buffer_length' bytes after each SQL statement.  While a
     statement is running, a copy of the current statement string is
     also allocated.

   * All threads share the same base memory.

   * When a thread is no longer needed, the memory allocated to it is
     released and returned to the system unless the thread goes back
     into the thread cache.  In that case, the memory remains allocated.

   * Each request that performs a sequential scan of a table allocates a
     _read buffer_.  The 'read_buffer_size' system variable determines
     the buffer size.

   * When reading rows in an arbitrary sequence (for example, following
     a sort), a _random-read buffer_ may be allocated to avoid disk
     seeks.  The 'read_rnd_buffer_size' system variable determines the
     buffer size.

   * All joins are executed in a single pass, and most joins can be done
     without even using a temporary table.  Most temporary tables are
     memory-based hash tables.  Temporary tables with a large row length
     (calculated as the sum of all column lengths) or that contain *note
     'BLOB': blob. columns are stored on disk.

   * Most requests that perform a sort allocate a sort buffer and zero
     to two temporary files depending on the result set size.  See *note
     temporary-files::.

   * Almost all parsing and calculating is done in thread-local and
     reusable memory pools.  No memory overhead is needed for small
     items, thus avoiding the normal slow memory allocation and freeing.
     Memory is allocated only for unexpectedly large strings.

   * For each table having *note 'BLOB': blob. columns, a buffer is
     enlarged dynamically to read in larger *note 'BLOB': blob. values.
     If you scan a table, the buffer grows as large as the largest *note
     'BLOB': blob. value.

   * MySQL requires memory and descriptors for the table cache.  Handler
     structures for all in-use tables are saved in the table cache and
     managed as 'First In, First Out' (FIFO). The 'table_open_cache'
     system variable defines the initial table cache size; see *note
     table-cache::.

     MySQL also requires memory for the table definition cache.  The
     'table_definition_cache' system variable defines the number of
     table definitions (from '.frm' files) that can be stored in the
     table definition cache.  If you use a large number of tables, you
     can create a large table definition cache to speed up the opening
     of tables.  The table definition cache takes less space and does
     not use file descriptors, unlike the table cache.

   * A 'FLUSH TABLES' statement or *note 'mysqladmin flush-tables':
     mysqladmin. command closes all tables that are not in use at once
     and marks all in-use tables to be closed when the currently
     executing thread finishes.  This effectively frees most in-use
     memory.  'FLUSH TABLES' does not return until all tables have been
     closed.

   * The server caches information in memory as a result of *note
     'GRANT': grant, *note 'CREATE USER': create-user, *note 'CREATE
     SERVER': create-server, and *note 'INSTALL PLUGIN': install-plugin.
     statements.  This memory is not released by the corresponding *note
     'REVOKE': revoke, *note 'DROP USER': drop-user, *note 'DROP
     SERVER': drop-server, and *note 'UNINSTALL PLUGIN':
     uninstall-plugin. statements, so for a server that executes many
     instances of the statements that cause caching, there will be an
     increase in cached memory use unless it is freed with 'FLUSH
     PRIVILEGES'.

'ps' and other system status programs may report that *note 'mysqld':
mysqld. uses a lot of memory.  This may be caused by thread stacks on
different memory addresses.  For example, the Solaris version of 'ps'
counts the unused memory between stacks as used memory.  To verify this,
check available swap with 'swap -s'.  We test *note 'mysqld': mysqld.
with several memory-leakage detectors (both commercial and Open Source),
so there should be no memory leaks.


File: manual.info.tmp,  Node: large-page-support,  Prev: memory-use,  Up: optimizing-memory

8.12.4.2 Enabling Large Page Support
....................................

Some hardware/operating system architectures support memory pages
greater than the default (usually 4KB). The actual implementation of
this support depends on the underlying hardware and operating system.
Applications that perform a lot of memory accesses may obtain
performance improvements by using large pages due to reduced Translation
Lookaside Buffer (TLB) misses.

In MySQL, large pages can be used by InnoDB, to allocate memory for its
buffer pool and additional memory pool.

Standard use of large pages in MySQL attempts to use the largest size
supported, up to 4MB. Under Solaris, a 'super large pages' feature
enables uses of pages up to 256MB. This feature is available for recent
SPARC platforms.  It can be enabled or disabled by using the
'--super-large-pages' or '--skip-super-large-pages' option.

MySQL also supports the Linux implementation of large page support
(which is called HugeTLB in Linux).

Before large pages can be used on Linux, the kernel must be enabled to
support them and it is necessary to configure the HugeTLB memory pool.
For reference, the HugeTBL API is documented in the
'Documentation/vm/hugetlbpage.txt' file of your Linux sources.

The kernel for some recent systems such as Red Hat Enterprise Linux
appear to have the large pages feature enabled by default.  To check
whether this is true for your kernel, use the following command and look
for output lines containing 'huge':

     shell> cat /proc/meminfo | grep -i huge
     HugePages_Total:       0
     HugePages_Free:        0
     HugePages_Rsvd:        0
     HugePages_Surp:        0
     Hugepagesize:       4096 kB

The nonempty command output indicates that large page support is
present, but the zero values indicate that no pages are configured for
use.

If your kernel needs to be reconfigured to support large pages, consult
the 'hugetlbpage.txt' file for instructions.

Assuming that your Linux kernel has large page support enabled,
configure it for use by MySQL using the following commands.  Normally,
you put these in an 'rc' file or equivalent startup file that is
executed during the system boot sequence, so that the commands execute
each time the system starts.  The commands should execute early in the
boot sequence, before the MySQL server starts.  Be sure to change the
allocation numbers and the group number as appropriate for your system.

     # Set the number of pages to be used.
     # Each page is normally 2MB, so a value of 20 = 40MB.
     # This command actually allocates memory, so this much
     # memory must be available.
     echo 20 > /proc/sys/vm/nr_hugepages

     # Set the group number that is permitted to access this
     # memory (102 in this case). The mysql user must be a
     # member of this group.
     echo 102 > /proc/sys/vm/hugetlb_shm_group

     # Increase the amount of shmem permitted per segment
     # (12G in this case).
     echo 1560281088 > /proc/sys/kernel/shmmax

     # Increase total amount of shared memory.  The value
     # is the number of pages. At 4KB/page, 4194304 = 16GB.
     echo 4194304 > /proc/sys/kernel/shmall

For MySQL usage, you normally want the value of 'shmmax' to be close to
the value of 'shmall'.

To verify the large page configuration, check '/proc/meminfo' again as
described previously.  Now you should see some nonzero values:

     shell> cat /proc/meminfo | grep -i huge
     HugePages_Total:      20
     HugePages_Free:       20
     HugePages_Rsvd:        0
     HugePages_Surp:        0
     Hugepagesize:       4096 kB

The final step to make use of the 'hugetlb_shm_group' is to give the
'mysql' user an 'unlimited' value for the memlock limit.  This can be
done either by editing '/etc/security/limits.conf' or by adding the
following command to your *note 'mysqld_safe': mysqld-safe. script:

     ulimit -l unlimited

Adding the 'ulimit' command to *note 'mysqld_safe': mysqld-safe. causes
the 'root' user to set the memlock limit to 'unlimited' before switching
to the 'mysql' user.  (This assumes that *note 'mysqld_safe':
mysqld-safe. is started by 'root'.)

Large page support in MySQL is disabled by default.  To enable it, start
the server with the '--large-pages' option.  For example, you can use
the following lines in the server 'my.cnf' file:

     [mysqld]
     large-pages

With this option, 'InnoDB' uses large pages automatically for its buffer
pool and additional memory pool.  If 'InnoDB' cannot do this, it falls
back to use of traditional memory and writes a warning to the error log:
'Warning: Using conventional memory pool'

To verify that large pages are being used, check '/proc/meminfo' again:

     shell> cat /proc/meminfo | grep -i huge
     HugePages_Total:      20
     HugePages_Free:       20
     HugePages_Rsvd:        2
     HugePages_Surp:        0
     Hugepagesize:       4096 kB


File: manual.info.tmp,  Node: optimizing-network,  Prev: optimizing-memory,  Up: optimizing-server

8.12.5 Optimizing Network Use
-----------------------------

* Menu:

* client-connections::           How MySQL Handles Client Connections
* host-cache::                   DNS Lookup Optimization and the Host Cache


File: manual.info.tmp,  Node: client-connections,  Next: host-cache,  Prev: optimizing-network,  Up: optimizing-network

8.12.5.1 How MySQL Handles Client Connections
.............................................

This section describes aspects of how the MySQL server manages client
connections.

   * *note client-connections-interfaces::

   * *note client-connections-thread-management::

   * *note client-connections-volume-management::

*Network Interfaces and Connection Manager Threads*

The server is capable of listening for client connections on multiple
network interfaces.  Connection manager threads handle client connection
requests on the network interfaces that the server listens to:

   * On all platforms, one manager thread handles TCP/IP connection
     requests.

   * On Unix, the same manager thread also handles Unix socket file
     connection requests.

   * On Windows, a manager thread handles shared-memory connection
     requests, and another handles named-pipe connection requests.

The server does not create threads to handle interfaces that it does not
listen to.  For example, a Windows server that does not have support for
named-pipe connections enabled does not create a thread to handle them.

*Client Connection Thread Management*

Connection manager threads associate each client connection with a
thread dedicated to it that handles authentication and request
processing for that connection.  Manager threads create a new thread
when necessary but try to avoid doing so by consulting the thread cache
first to see whether it contains a thread that can be used for the
connection.  When a connection ends, its thread is returned to the
thread cache if the cache is not full.

In this connection thread model, there are as many threads as there are
clients currently connected, which has some disadvantages when server
workload must scale to handle large numbers of connections.  For
example, thread creation and disposal becomes expensive.  Also, each
thread requires server and kernel resources, such as stack space.  To
accommodate a large number of simultaneous connections, the stack size
per thread must be kept small, leading to a situation where it is either
too small or the server consumes large amounts of memory.  Exhaustion of
other resources can occur as well, and scheduling overhead can become
significant.

As of MySQL 5.5.16, MySQL Enterprise Edition includes a thread pool
plugin that provides an alternative thread-handling model designed to
reduce overhead and improve performance.  It implements a thread pool
that increases server performance by efficiently managing statement
execution threads for large numbers of client connections.  See *note
thread-pool::.

To control and monitor how the server manages threads that handle client
connections, several system and status variables are relevant.  (See
*note server-system-variables::, and *note server-status-variables::.)

The 'thread_cache_size' system variable determines the thread cache
size.  A value of 0 disables caching, which causes a thread to be set up
for each new connection and disposed of when the connection terminates.
To enable N inactive connection threads to be cached, set
'thread_cache_size' to N at server startup or at runtime.  A connection
thread becomes inactive when the client connection with which it was
associated terminates.

To monitor the number of threads in the cache and how many threads have
been created because a thread could not be taken from the cache, check
the 'Threads_cached' and 'Threads_created' status variables.

When the thread stack is too small, this limits the complexity of the
SQL statements which the server can handle, the recursion depth of
stored procedures, and other memory-consuming actions.  To set a stack
size of N bytes for each thread, start the server with 'thread_stack'.
set to N at server startup.

*Connection Volume Management*

To control the maximum number of clients the server permits to connect
simultaneously, set the 'max_connections' system variable at server
startup or at runtime.  It may be necessary to increase
'max_connections' if more clients attempt to connect simultaneously then
the server is configured to handle (see *note too-many-connections::).

*note 'mysqld': mysqld. actually permits 'max_connections' + 1 client
connections.  The extra connection is reserved for use by accounts that
have the 'SUPER' privilege.  By granting the privilege to administrators
and not to normal users (who should not need it), an administrator who
also has the 'PROCESS' privilege can connect to the server and use *note
'SHOW PROCESSLIST': show-processlist. to diagnose problems even if the
maximum number of unprivileged clients are connected.  See *note
show-processlist::.

The maximum number of connections MySQL supports (that is, the maximum
value to which 'max_connections' can be set) depends on several factors:

   * The quality of the thread library on a given platform.

   * The amount of RAM available.

   * The amount of RAM is used for each connection.

   * The workload from each connection.

   * The desired response time.

   * The number of file descriptors available.

Linux or Solaris should be able to support at least 500 to 1000
simultaneous connections routinely and as many as 10,000 connections if
you have many gigabytes of RAM available and the workload from each is
low or the response time target undemanding.

Increasing the 'max_connections' value increases the number of file
descriptors that *note 'mysqld': mysqld. requires.  If the required
number of descriptors are not available, the server reduces the value of
'max_connections'.  For comments on file descriptor limits, see *note
table-cache::.

Increasing the 'open_files_limit' system variable may be necessary,
which may also require raising the operating system limit on how many
file descriptors can be used by MySQL. Consult your operating system
documentation to determine whether it is possible to increase the limit
and how to do so.  See also *note not-enough-file-handles::.


File: manual.info.tmp,  Node: host-cache,  Prev: client-connections,  Up: optimizing-network

8.12.5.2 DNS Lookup Optimization and the Host Cache
...................................................

The MySQL server maintains a host cache in memory that contains
information about clients: IP address, host name, and error information.

*Note*:

The server uses the host cache only for nonlocal TCP connections.  It
does not use the cache for TCP connections established using a loopback
interface address (for example, '127.0.0.1' or '::1'), or for
connections established using a Unix socket file, named pipe, or shared
memory.

   * *note host-cache-operation::

   * *note host-cache-configuration::

*Host Cache Operation*

The server uses the host cache for several purposes:

   * By caching the results of IP-to-host name lookups, the server
     avoids doing a Domain Name System (DNS) lookup for each client
     connection.  Instead, for a given host, it needs to perform a
     lookup only for the first connection from that host.

   * The cache contains information about errors that occur during the
     connection process.  Some errors are considered 'blocking.' If too
     many of these occur successively from a given host without a
     successful connection, the server blocks further connections from
     that host.  The 'max_connect_errors' system variable determines the
     permitted number of successive errors before blocking occurs (see
     *note blocked-host::).

For each new client connection, the server uses the client IP address to
check whether the client host name is in the host cache.  If so, the
server refuses or continues to process the connection request depending
on whether or not the host is blocked.  If the host is not in the cache,
the server attempts to resolve the host name.  First, it resolves the IP
address to a host name and resolves that host name back to an IP
address.  Then it compares the result to the original IP address to
ensure that they are the same.  The server stores information about the
result of this operation in the host cache.  If the cache is full, the
least recently used entry is discarded.

The server performs host name resolution using the 'gethostbyaddr()' and
'gethostbyname()' system calls.

To unblock blocked hosts, flush the host cache by executing a 'FLUSH
HOSTS' statement or *note 'mysqladmin flush-hosts': mysqladmin. command.

It is possible for a blocked host to become unblocked even without
flushing the host cache if activity from other hosts has occurred since
the last connection attempt from the blocked host.  This can occur
because the server discards the least recently used cache entry to make
room for a new entry if the cache is full when a connection arrives from
a client IP not in the cache.  If the discarded entry is for a blocked
host, that host becomes unblocked.

Some connection errors are not associated with TCP connections, occur
very early in the connection process (even before an IP address is
known), or are not specific to any particular IP address (such as
out-of-memory conditions).

*Host Cache Configuration*

The host cache is enabled by default.  To disable it, start the server
with the '--skip-host-cache' option.  With the cache disabled, the
server performs a DNS lookup every time a client connects.

To disable DNS host name lookups, start the server with the
'skip_name_resolve' system variable enabled.  In this case, the server
uses only IP addresses and not host names to match connecting hosts to
rows in the MySQL grant tables.  Only accounts specified in those tables
using IP addresses can be used.  (A client may not be able to connect if
no account exists that specifies the client IP address.)

If you have a very slow DNS and many hosts, you might be able to improve
performance either by disabling DNS lookups by enabling
'skip_name_resolve' or by increasing the 'HOST_CACHE_SIZE' define
(default value: 128) and recompiling the server

To disallow TCP/IP connections entirely, start the server with the
'skip_networking' system variable enabled.


File: manual.info.tmp,  Node: optimize-benchmarking,  Next: thread-information,  Prev: optimizing-server,  Up: optimization

8.13 Measuring Performance (Benchmarking)
=========================================

* Menu:

* select-benchmarking::          Measuring the Speed of Expressions and Functions
* mysql-benchmarks::             The MySQL Benchmark Suite
* custom-benchmarks::            Using Your Own Benchmarks
* monitoring-performance-schema::  Measuring Performance with performance_schema

To measure performance, consider the following factors:

   * Whether you are measuring the speed of a single operation on a
     quiet system, or how a set of operations (a 'workload') works over
     a period of time.  With simple tests, you usually test how changing
     one aspect (a configuration setting, the set of indexes on a table,
     the SQL clauses in a query) affects performance.  Benchmarks are
     typically long-running and elaborate performance tests, where the
     results could dictate high-level choices such as hardware and
     storage configuration, or how soon to upgrade to a new MySQL
     version.

   * For benchmarking, sometimes you must simulate a heavy database
     workload to get an accurate picture.

   * Performance can vary depending on so many different factors that a
     difference of a few percentage points might not be a decisive
     victory.  The results might shift the opposite way when you test in
     a different environment.

   * Certain MySQL features help or do not help performance depending on
     the workload.  For completeness, always test performance with those
     features turned on and turned off.  The two most important features
     to try with each workload are the *note MySQL query cache:
     query-cache, and the *note adaptive hash index:
     innodb-adaptive-hash. for 'InnoDB' tables.

This section progresses from simple and direct measurement techniques
that a single developer can do, to more complicated ones that require
additional expertise to perform and interpret the results.


File: manual.info.tmp,  Node: select-benchmarking,  Next: mysql-benchmarks,  Prev: optimize-benchmarking,  Up: optimize-benchmarking

8.13.1 Measuring the Speed of Expressions and Functions
-------------------------------------------------------

To measure the speed of a specific MySQL expression or function, invoke
the 'BENCHMARK()' function using the *note 'mysql': mysql. client
program.  Its syntax is 'BENCHMARK(LOOP_COUNT,EXPR)'.  The return value
is always zero, but *note 'mysql': mysql. prints a line displaying
approximately how long the statement took to execute.  For example:

     mysql> SELECT BENCHMARK(1000000,1+1);
     +------------------------+
     | BENCHMARK(1000000,1+1) |
     +------------------------+
     |                      0 |
     +------------------------+
     1 row in set (0.32 sec)

This result was obtained on a Pentium II 400MHz system.  It shows that
MySQL can execute 1,000,000 simple addition expressions in 0.32 seconds
on that system.

The built-in MySQL functions are typically highly optimized, but there
may be some exceptions.  'BENCHMARK()' is an excellent tool for finding
out if some function is a problem for your queries.


File: manual.info.tmp,  Node: mysql-benchmarks,  Next: custom-benchmarks,  Prev: select-benchmarking,  Up: optimize-benchmarking

8.13.2 The MySQL Benchmark Suite
--------------------------------

This benchmark suite is meant to tell any user what operations a given
SQL implementation performs well or poorly.  You can get a good idea for
how the benchmarks work by looking at the code and results in the
'sql-bench' directory in any MySQL source distribution.

To use the benchmark suite, the following requirements must be
satisfied:

   * The benchmark suite is provided with MySQL source distributions.
     You can either download a released distribution from
     <https://dev.mysql.com/downloads/>, or use the current development
     source tree.  (See *note installing-development-tree::.)

   * The benchmark scripts are written in Perl and use the Perl DBI
     module to access database servers, so DBI must be installed.  You
     also need the server-specific DBD drivers for each of the servers
     you want to test.  For example, to test MySQL, PostgreSQL, and DB2,
     you must have the 'DBD::mysql', 'DBD::Pg', and 'DBD::DB2' modules
     installed.  See *note perl-support::.

After you obtain a MySQL source distribution, you can find the benchmark
suite located in its 'sql-bench' directory.  To run the benchmark tests,
build MySQL, and then change location into the 'sql-bench' directory and
execute the 'run-all-tests' script:

     shell> cd sql-bench
     shell> perl run-all-tests --server=SERVER_NAME

SERVER_NAME should be the name of one of the supported servers.  To get
a list of all options and supported servers, invoke this command:

     shell> perl run-all-tests --help

The 'crash-me' script also is located in the 'sql-bench' directory.
'crash-me' tries to determine what features a database system supports
and what its capabilities and limitations are by actually running
queries.  For example, it determines:

   * What data types are supported

   * How many indexes are supported

   * What functions are supported

   * How big a query can be

   * How big a *note 'VARCHAR': char. column can be

For more information about benchmark results, visit
<http://www.mysql.com/why-mysql/benchmarks/>.


File: manual.info.tmp,  Node: custom-benchmarks,  Next: monitoring-performance-schema,  Prev: mysql-benchmarks,  Up: optimize-benchmarking

8.13.3 Using Your Own Benchmarks
--------------------------------

Benchmark your application and database to find out where the
bottlenecks are.  After fixing one bottleneck (or by replacing it with a
'dummy' module), you can proceed to identify the next bottleneck.  Even
if the overall performance for your application currently is acceptable,
you should at least make a plan for each bottleneck and decide how to
solve it if someday you really need the extra performance.

For examples of portable benchmark programs, look at those in the MySQL
benchmark suite.  See *note mysql-benchmarks::.  You can take any
program from this suite and modify it for your own needs.  By doing
this, you can try different solutions to your problem and test which
really is fastest for you.

Another free benchmark suite is the Open Source Database Benchmark,
available at <http://osdb.sourceforge.net/>.

It is very common for a problem to occur only when the system is very
heavily loaded.  We have had many customers who contact us when they
have a (tested) system in production and have encountered load problems.
In most cases, performance problems turn out to be due to issues of
basic database design (for example, table scans are not good under high
load) or problems with the operating system or libraries.  Most of the
time, these problems would be much easier to fix if the systems were not
already in production.

To avoid problems like this, benchmark your whole application under the
worst possible load:

   * The *note 'mysqlslap': mysqlslap. program can be helpful for
     simulating a high load produced by multiple clients issuing queries
     simultaneously.  See *note mysqlslap::.

   * You can also try benchmarking packages such as SysBench and DBT2,
     available at <https://launchpad.net/sysbench>, and
     <http://osdldbt.sourceforge.net/#dbt2>.

These programs or packages can bring a system to its knees, so be sure
to use them only on your development systems.


File: manual.info.tmp,  Node: monitoring-performance-schema,  Prev: custom-benchmarks,  Up: optimize-benchmarking

8.13.4 Measuring Performance with performance_schema
----------------------------------------------------

You can query the tables in the 'performance_schema' database to see
real-time information about the performance characteristics of your
server and the applications it is running.  See *note
performance-schema:: for details.


File: manual.info.tmp,  Node: thread-information,  Prev: optimize-benchmarking,  Up: optimization

8.14 Examining Thread Information
=================================

* Menu:

* thread-commands::              Thread Command Values
* general-thread-states::        General Thread States
* delayed-insert-thread-states::  Delayed-Insert Thread States
* query-cache-thread-states::    Query Cache Thread States
* master-thread-states::         Replication Master Thread States
* slave-io-thread-states::       Replication Slave I/O Thread States
* slave-sql-thread-states::      Replication Slave SQL Thread States
* slave-connection-thread-states::  Replication Slave Connection Thread States
* mysql-cluster-thread-states::  NDB Cluster Thread States
* event-scheduler-thread-states::  Event Scheduler Thread States

As you monitor the performance of your MySQL server, examine the process
list, which is the set of threads currently executing within the server.
Process list information is available from these sources:

   * The 'SHOW [FULL] PROCESSLIST' statement: *note show-processlist::

   * The *note 'SHOW PROFILE': show-profile. statement: *note
     show-profiles::

   * The 'INFORMATION_SCHEMA' *note 'PROCESSLIST': processlist-table.
     table: *note processlist-table::

   * The *note 'mysqladmin processlist': mysqladmin. command: *note
     mysqladmin::

You can always view information about your own threads.  To view
information about threads being executed for other accounts, you must
have the 'PROCESS' privilege.

Each process list entry contains several pieces of information:

   * 'Id' is the connection identifier for the client associated with
     the thread.

   * 'User' and 'Host' indicate the account associated with the thread.

   * 'db' is the default database for the thread, or 'NULL' if none is
     selected.

   * 'Command' and 'State' indicate what the thread is doing.

     Most states correspond to very quick operations.  If a thread stays
     in a given state for many seconds, there might be a problem that
     needs to be investigated.

   * 'Time' indicates how long the thread has been in its current state.
     The thread's notion of the current time may be altered in some
     cases: The thread can change the time with *note 'SET TIMESTAMP =
     VALUE': set-variable.  For a thread running on a slave that is
     processing events from the master, the thread time is set to the
     time found in the events and thus reflects current time on the
     master and not the slave.

   * 'Info' contains the text of the statement being executed by the
     thread, or 'NULL' if it is not executing one.  By default, this
     value contains only the first 100 characters of the statement.  To
     see the complete statements, use *note 'SHOW FULL PROCESSLIST':
     show-processlist.

The following sections list the possible 'Command' values, and 'State'
values grouped by category.  The meaning for some of these values is
self-evident.  For others, additional description is provided.


File: manual.info.tmp,  Node: thread-commands,  Next: general-thread-states,  Prev: thread-information,  Up: thread-information

8.14.1 Thread Command Values
----------------------------

A thread can have any of the following 'Command' values:

   * 
     'Binlog Dump'

     This is a thread on a master server for sending binary log contents
     to a slave server.

   * 
     'Change user'

     The thread is executing a change-user operation.

   * 
     'Close stmt'

     The thread is closing a prepared statement.

   * 
     'Connect'

     A replication slave is connected to its master.

   * 
     'Connect Out'

     A replication slave is connecting to its master.

   * 
     'Create DB'

     The thread is executing a create-database operation.

   * 
     'Daemon'

     This thread is internal to the server, not a thread that services a
     client connection.

   * 
     'Debug'

     The thread is generating debugging information.

   * 
     'Delayed insert'

     The thread is a delayed-insert handler.

   * 
     'Drop DB'

     The thread is executing a drop-database operation.

   * 
     'Error'

   * 
     'Execute'

     The thread is executing a prepared statement.

   * 
     'Fetch'

     The thread is fetching the results from executing a prepared
     statement.

   * 
     'Field List'

     The thread is retrieving information for table columns.

   * 
     'Init DB'

     The thread is selecting a default database.

   * 
     'Kill'

     The thread is killing another thread.

   * 
     'Long Data'

     The thread is retrieving long data in the result of executing a
     prepared statement.

   * 
     'Ping'

     The thread is handling a server-ping request.

   * 
     'Prepare'

     The thread is preparing a prepared statement.

   * 
     'Processlist'

     The thread is producing information about server threads.

   * 
     'Query'

     The thread is executing a statement.

   * 
     'Quit'

     The thread is terminating.

   * 
     'Refresh'

     The thread is flushing table, logs, or caches, or resetting status
     variable or replication server information.

   * 
     'Register Slave'

     The thread is registering a slave server.

   * 
     'Reset stmt'

     The thread is resetting a prepared statement.

   * 
     'Set option'

     The thread is setting or resetting a client statement-execution
     option.

   * 
     'Shutdown'

     The thread is shutting down the server.

   * 
     'Sleep'

     The thread is waiting for the client to send a new statement to it.

   * 
     'Statistics'

     The thread is producing server-status information.

   * 
     'Table Dump'

     The thread is sending table contents to a slave server.

   * 
     'Time'

     Unused.


File: manual.info.tmp,  Node: general-thread-states,  Next: delayed-insert-thread-states,  Prev: thread-commands,  Up: thread-information

8.14.2 General Thread States
----------------------------

The following list describes thread 'State' values that are associated
with general query processing and not more specialized activities such
as replication.  Many of these are useful only for finding bugs in the
server.

   * 
     'After create'

     This occurs when the thread creates a table (including internal
     temporary tables), at the end of the function that creates the
     table.  This state is used even if the table could not be created
     due to some error.

   * 
     'Analyzing'

     The thread is calculating a 'MyISAM' table key distributions (for
     example, for *note 'ANALYZE TABLE': analyze-table.).

   * 
     'checking permissions'

     The thread is checking whether the server has the required
     privileges to execute the statement.

   * 
     'Checking table'

     The thread is performing a table check operation.

   * 
     'cleaning up'

     The thread has processed one command and is preparing to free
     memory and reset certain state variables.

   * 
     'closing tables'

     The thread is flushing the changed table data to disk and closing
     the used tables.  This should be a fast operation.  If not, verify
     that you do not have a full disk and that the disk is not in very
     heavy use.

   * 
     'converting HEAP to MyISAM'

     The thread is converting an internal temporary table from a
     'MEMORY' table to an on-disk 'MyISAM' table.

   * 
     'copy to tmp table'

     The thread is processing an *note 'ALTER TABLE': alter-table.
     statement.  This state occurs after the table with the new
     structure has been created but before rows are copied into it.

   * 
     'Copying to group table'

     If a statement has different 'ORDER BY' and 'GROUP BY' criteria,
     the rows are sorted by group and copied to a temporary table.

   * 
     'Copying to tmp table'

     The server is copying to a temporary table in memory.

   * 
     'Copying to tmp table on disk'

     The server is copying to a temporary table on disk.  The temporary
     result set has become too large (see *note
     internal-temporary-tables::).  Consequently, the thread is changing
     the temporary table from in-memory to disk-based format to save
     memory.

   * 
     'Creating index'

     The thread is processing 'ALTER TABLE ... ENABLE KEYS' for a
     'MyISAM' table.

   * 
     'Creating sort index'

     The thread is processing a *note 'SELECT': select. that is resolved
     using an internal temporary table.

   * 
     'creating table'

     The thread is creating a table.  This includes creation of
     temporary tables.

   * 
     'Creating tmp table'

     The thread is creating a temporary table in memory or on disk.  If
     the table is created in memory but later is converted to an on-disk
     table, the state during that operation will be 'Copying to tmp
     table on disk'.

   * 
     'deleting from main table'

     The server is executing the first part of a multiple-table delete.
     It is deleting only from the first table, and saving columns and
     offsets to be used for deleting from the other (reference) tables.

   * 
     'deleting from reference tables'

     The server is executing the second part of a multiple-table delete
     and deleting the matched rows from the other tables.

   * 
     'discard_or_import_tablespace'

     The thread is processing an 'ALTER TABLE ... DISCARD TABLESPACE' or
     'ALTER TABLE ... IMPORT TABLESPACE' statement.

   * 
     'end'

     This occurs at the end but before the cleanup of *note 'ALTER
     TABLE': alter-table, *note 'CREATE VIEW': create-view, *note
     'DELETE': delete, *note 'INSERT': insert, *note 'SELECT': select,
     or *note 'UPDATE': update. statements.

   * 
     'executing'

     The thread has begun executing a statement.

   * 
     'Execution of init_command'

     The thread is executing statements in the value of the
     'init_command' system variable.

   * 
     'freeing items'

     The thread has executed a command.  Some freeing of items done
     during this state involves the query cache.  This state is usually
     followed by 'cleaning up'.

   * 
     'FULLTEXT initialization'

     The server is preparing to perform a natural-language full-text
     search.

   * 
     'init'

     This occurs before the initialization of *note 'ALTER TABLE':
     alter-table, *note 'DELETE': delete, *note 'INSERT': insert, *note
     'SELECT': select, or *note 'UPDATE': update. statements.  Actions
     taken by the server in this state include flushing the binary log,
     the 'InnoDB' log, and some query cache cleanup operations.

     For the 'end' state, the following operations could be happening:

        * Removing query cache entries after data in a table is changed

        * Writing an event to the binary log

        * Freeing memory buffers, including for blobs

   * 
     'Killed'

     Someone has sent a *note 'KILL': kill. statement to the thread and
     it should abort next time it checks the kill flag.  The flag is
     checked in each major loop in MySQL, but in some cases it might
     still take a short time for the thread to die.  If the thread is
     locked by some other thread, the kill takes effect as soon as the
     other thread releases its lock.

   * 
     'logging slow query'

     The thread is writing a statement to the slow-query log.

   * 
     'login'

     The initial state for a connection thread until the client has been
     authenticated successfully.

   * 
     'manage keys'

     The server is enabling or disabling a table index.

   * 
     'NULL'

     This state is used for the *note 'SHOW PROCESSLIST':
     show-processlist. state.

   * 
     'Opening tables', 'Opening table'

     The thread is trying to open a table.  This is should be very fast
     procedure, unless something prevents opening.  For example, an
     *note 'ALTER TABLE': alter-table. or a *note 'LOCK TABLE':
     lock-tables. statement can prevent opening a table until the
     statement is finished.  It is also worth checking that your
     'table_open_cache' value is large enough.

   * 
     'optimizing'

     The server is performing initial optimizations for a query.

   * 
     'preparing'

     This state occurs during query optimization.

   * 
     'Purging old relay logs'

     The thread is removing unneeded relay log files.

   * 
     'query end'

     This state occurs after processing a query but before the 'freeing
     items' state.

   * 
     'Reading from net'

     The server is reading a packet from the network.

   * 
     'Removing duplicates'

     The query was using *note 'SELECT DISTINCT': select. in such a way
     that MySQL could not optimize away the distinct operation at an
     early stage.  Because of this, MySQL requires an extra stage to
     remove all duplicated rows before sending the result to the client.

   * 
     'removing tmp table'

     The thread is removing an internal temporary table after processing
     a *note 'SELECT': select. statement.  This state is not used if no
     temporary table was created.

   * 
     'rename'

     The thread is renaming a table.

   * 
     'rename result table'

     The thread is processing an *note 'ALTER TABLE': alter-table.
     statement, has created the new table, and is renaming it to replace
     the original table.

   * 
     'Reopen tables'

     The thread got a lock for the table, but noticed after getting the
     lock that the underlying table structure changed.  It has freed the
     lock, closed the table, and is trying to reopen it.

   * 
     'Repair by sorting'

     The repair code is using a sort to create indexes.

   * 
     'Repair done'

     The thread has completed a multithreaded repair for a 'MyISAM'
     table.

   * 
     'Repair with keycache'

     The repair code is using creating keys one by one through the key
     cache.  This is much slower than 'Repair by sorting'.

   * 
     'Rolling back'

     The thread is rolling back a transaction.

   * 
     'Saving state'

     For 'MyISAM' table operations such as repair or analysis, the
     thread is saving the new table state to the '.MYI' file header.
     State includes information such as number of rows, the
     'AUTO_INCREMENT' counter, and key distributions.

   * 
     'Searching rows for update'

     The thread is doing a first phase to find all matching rows before
     updating them.  This has to be done if the *note 'UPDATE': update.
     is changing the index that is used to find the involved rows.

   * 'Sending data'

     The thread is reading and processing rows for a *note 'SELECT':
     select. statement, and sending data to the client.  Because
     operations occurring during this state tend to perform large
     amounts of disk access (reads), it is often the longest-running
     state over the lifetime of a given query.

   * 
     'setup'

     The thread is beginning an *note 'ALTER TABLE': alter-table.
     operation.

   * 
     'Sorting for group'

     The thread is doing a sort to satisfy a 'GROUP BY'.

   * 
     'Sorting for order'

     The thread is doing a sort to satisfy an 'ORDER BY'.

   * 
     'Sorting index'

     The thread is sorting index pages for more efficient access during
     a 'MyISAM' table optimization operation.

   * 
     'Sorting result'

     For a *note 'SELECT': select. statement, this is similar to
     'Creating sort index', but for nontemporary tables.

   * 
     'statistics'

     The server is calculating statistics to develop a query execution
     plan.  If a thread is in this state for a long time, the server is
     probably disk-bound performing other work.

   * 
     'System lock'

     The thread has called 'mysql_lock_tables()' and the thread state
     has not been updated since.  This is a very general state that can
     occur for many reasons.

     For example, the thread is going to request or is waiting for an
     internal or external system lock for the table.  This can occur
     when *note 'InnoDB': innodb-storage-engine. waits for a table-level
     lock during execution of *note 'LOCK TABLES': lock-tables.  If this
     state is being caused by requests for external locks and you are
     not using multiple *note 'mysqld': mysqld. servers that are
     accessing the same *note 'MyISAM': myisam-storage-engine. tables,
     you can disable external system locks with the
     '--skip-external-locking' option.  However, external locking is
     disabled by default, so it is likely that this option will have no
     effect.  For *note 'SHOW PROFILE': show-profile, this state means
     the thread is requesting the lock (not waiting for it).

   * 
     'update'

     The thread is getting ready to start updating the table.

   * 
     'Updating'

     The thread is searching for rows to update and is updating them.

   * 
     'updating main table'

     The server is executing the first part of a multiple-table update.
     It is updating only the first table, and saving columns and offsets
     to be used for updating the other (reference) tables.

   * 
     'updating reference tables'

     The server is executing the second part of a multiple-table update
     and updating the matched rows from the other tables.

   * 
     'User lock'

     The thread is going to request or is waiting for an advisory lock
     requested with a 'GET_LOCK()' call.  For *note 'SHOW PROFILE':
     show-profile, this state means the thread is requesting the lock
     (not waiting for it).

   * 
     'User sleep'

     The thread has invoked a 'SLEEP()' call.

   * 
     'Waiting for commit lock'

     A statement that causes an explicit or implicit commit is waiting
     for release of a read lock or 'FLUSH TABLES WITH READ LOCK' is
     waiting for a commit lock.

   * 
     'Waiting for global read lock'

     'FLUSH TABLES WITH READ LOCK' is waiting for a global read lock or
     the global 'read_only' system variable is being set.

   * 
     'Waiting for tables'

     The thread got a notification that the underlying structure for a
     table has changed and it needs to reopen the table to get the new
     structure.  However, to reopen the table, it must wait until all
     other threads have closed the table in question.

     This notification takes place if another thread has used 'FLUSH
     TABLES' or one of the following statements on the table in
     question: 'FLUSH TABLES TBL_NAME', *note 'ALTER TABLE':
     alter-table, *note 'RENAME TABLE': rename-table, *note 'REPAIR
     TABLE': repair-table, *note 'ANALYZE TABLE': analyze-table, or
     *note 'OPTIMIZE TABLE': optimize-table.

   * 
     'Waiting for table flush'

     The thread is executing 'FLUSH TABLES' and is waiting for all
     threads to close their tables, or the thread got a notification
     that the underlying structure for a table has changed and it needs
     to reopen the table to get the new structure.  However, to reopen
     the table, it must wait until all other threads have closed the
     table in question.

     This notification takes place if another thread has used 'FLUSH
     TABLES' or one of the following statements on the table in
     question: 'FLUSH TABLES TBL_NAME', *note 'ALTER TABLE':
     alter-table, *note 'RENAME TABLE': rename-table, *note 'REPAIR
     TABLE': repair-table, *note 'ANALYZE TABLE': analyze-table, or
     *note 'OPTIMIZE TABLE': optimize-table.

   * 
     'Waiting for LOCK_TYPE lock'

     The server is waiting to acquire a 'THR_LOCK' lock or a lock from
     the metadata locking subsystem, where LOCK_TYPE indicates the type
     of lock.

     This state indicates a wait for a 'THR_LOCK':

        * 'Waiting for table level lock'

     These states indicate a wait for a metadata lock:

        * 'Waiting for event metadata lock'

        * 'Waiting for global metadata lock'

        * 'Waiting for global read lock'

        * 'Waiting for schema metadata lock'

        * 'Waiting for stored function metadata lock'

        * 'Waiting for stored procedure metadata lock'

        * 'Waiting for table metadata lock'

        * 'Waiting for trigger metadata lock'

     For information about table lock indicators, see *note
     internal-locking::.  For information about metadata locking, see
     *note metadata-locking::.

   * 
     'Waiting on cond'

     A generic state in which the thread is waiting for a condition to
     become true.  No specific state information is available.

   * 
     'Writing to net'

     The server is writing a packet to the network.


File: manual.info.tmp,  Node: delayed-insert-thread-states,  Next: query-cache-thread-states,  Prev: general-thread-states,  Up: thread-information

8.14.3 Delayed-Insert Thread States
-----------------------------------

These thread states are associated with processing for 'DELAYED' inserts
(see *note insert-delayed::).  Some states are associated with
connection threads that process *note 'INSERT DELAYED': insert-delayed.
statements from clients.  Other states are associated with
delayed-insert handler threads that insert the rows.  There is a
delayed-insert handler thread for each table for which *note 'INSERT
DELAYED': insert-delayed. statements are issued.

States associated with a connection thread that processes an *note
'INSERT DELAYED': insert-delayed. statement from the client:

   * 
     'allocating local table'

     The thread is preparing to feed rows to the delayed-insert handler
     thread.

   * 
     'Creating delayed handler'

     The thread is creating a handler for 'DELAYED' inserts.

   * 
     'got handler lock'

     This occurs before the 'allocating local table' state and after the
     'waiting for handler lock' state, when the connection thread gets
     access to the delayed-insert handler thread.

   * 
     'got old table'

     This occurs after the 'waiting for handler open' state.  The
     delayed-insert handler thread has signaled that it has ended its
     initialization phase, which includes opening the table for delayed
     inserts.

   * 
     'storing row into queue'

     The thread is adding a new row to the list of rows that the
     delayed-insert handler thread must insert.

   * 
     'waiting for delay_list'

     This occurs during the initialization phase when the thread is
     trying to find the delayed-insert handler thread for the table, and
     before attempting to gain access to the list of delayed-insert
     threads.

   * 
     'waiting for handler insert'

     An *note 'INSERT DELAYED': insert-delayed. handler has processed
     all pending inserts and is waiting for new ones.

   * 
     'waiting for handler lock'

     This occurs before the 'allocating local table' state when the
     connection thread waits for access to the delayed-insert handler
     thread.

   * 
     'waiting for handler open'

     This occurs after the 'Creating delayed handler' state and before
     the 'got old table' state.  The delayed-insert handler thread has
     just been started, and the connection thread is waiting for it to
     initialize.

States associated with a delayed-insert handler thread that inserts the
rows:

   * 
     'insert'

     The state that occurs just before inserting rows into the table.

   * 
     'reschedule'

     After inserting a number of rows, the delayed-insert thread sleeps
     to let other threads do work.

   * 
     'upgrading lock'

     A delayed-insert handler is trying to get a lock for the table to
     insert rows.

   * 
     'Waiting for INSERT'

     A delayed-insert handler is waiting for a connection thread to add
     rows to the queue (see 'storing row into queue').


File: manual.info.tmp,  Node: query-cache-thread-states,  Next: master-thread-states,  Prev: delayed-insert-thread-states,  Up: thread-information

8.14.4 Query Cache Thread States
--------------------------------

These thread states are associated with the query cache (see *note
query-cache::).

   * 
     'checking privileges on cached query'

     The server is checking whether the user has privileges to access a
     cached query result.

   * 
     'checking query cache for query'

     The server is checking whether the current query is present in the
     query cache.

   * 
     'invalidating query cache entries'

     Query cache entries are being marked invalid because the underlying
     tables have changed.

   * 
     'sending cached result to client'

     The server is taking the result of a query from the query cache and
     sending it to the client.

   * 
     'storing result in query cache'

     The server is storing the result of a query in the query cache.

   * 
     'Waiting for query cache lock'

     This state occurs while a session is waiting to take the query
     cache lock.  This can happen for any statement that needs to
     perform some query cache operation, such as an *note 'INSERT':
     insert. or *note 'DELETE': delete. that invalidates the query
     cache, a *note 'SELECT': select. that looks for a cached entry,
     *note 'RESET QUERY CACHE': reset, and so forth.


File: manual.info.tmp,  Node: master-thread-states,  Next: slave-io-thread-states,  Prev: query-cache-thread-states,  Up: thread-information

8.14.5 Replication Master Thread States
---------------------------------------

The following list shows the most common states you may see in the
'State' column for the master's 'Binlog Dump' thread.  If you see no
'Binlog Dump' threads on a master server, this means that replication is
not running--that is, that no slaves are currently connected.

   * 
     'Finished reading one binlog; switching to next binlog'

     The thread has finished reading a binary log file and is opening
     the next one to send to the slave.

   * 
     'Master has sent all binlog to slave; waiting for binlog to be
     updated'

     The thread has read all outstanding updates from the binary logs
     and sent them to the slave.  The thread is now idle, waiting for
     new events to appear in the binary log resulting from new updates
     occurring on the master.

   * 
     'Sending binlog event to slave'

     Binary logs consist of _events_, where an event is usually an
     update plus some other information.  The thread has read an event
     from the binary log and is now sending it to the slave.

   * 
     'Waiting to finalize termination'

     A very brief state that occurs as the thread is stopping.


File: manual.info.tmp,  Node: slave-io-thread-states,  Next: slave-sql-thread-states,  Prev: master-thread-states,  Up: thread-information

8.14.6 Replication Slave I/O Thread States
------------------------------------------

The following list shows the most common states you see in the 'State'
column for a slave server I/O thread.  This state also appears in the
'Slave_IO_State' column displayed by *note 'SHOW SLAVE STATUS':
show-slave-status, so you can get a good view of what is happening by
using that statement.

   * 
     'Checking master version'

     A state that occurs very briefly, after the connection to the
     master is established.

   * 
     'Connecting to master'

     The thread is attempting to connect to the master.

   * 
     'Queueing master event to the relay log'

     The thread has read an event and is copying it to the relay log so
     that the SQL thread can process it.

   * 
     'Reconnecting after a failed binlog dump request'

     The thread is trying to reconnect to the master.

   * 
     'Reconnecting after a failed master event read'

     The thread is trying to reconnect to the master.  When connection
     is established again, the state becomes 'Waiting for master to send
     event'.

   * 
     'Registering slave on master'

     A state that occurs very briefly after the connection to the master
     is established.

   * 
     'Requesting binlog dump'

     A state that occurs very briefly, after the connection to the
     master is established.  The thread sends to the master a request
     for the contents of its binary logs, starting from the requested
     binary log file name and position.

   * 
     'Waiting for master to send event'

     The thread has connected to the master and is waiting for binary
     log events to arrive.  This can last for a long time if the master
     is idle.  If the wait lasts for 'slave_net_timeout' seconds, a
     timeout occurs.  At that point, the thread considers the connection
     to be broken and makes an attempt to reconnect.

   * 
     'Waiting for master update'

     The initial state before 'Connecting to master'.

   * 
     'Waiting for slave mutex on exit'

     A state that occurs briefly as the thread is stopping.

   * 
     'Waiting for the slave SQL thread to free enough relay log space'

     You are using a nonzero 'relay_log_space_limit' value, and the
     relay logs have grown large enough that their combined size exceeds
     this value.  The I/O thread is waiting until the SQL thread frees
     enough space by processing relay log contents so that it can delete
     some relay log files.

   * 
     'Waiting to reconnect after a failed binlog dump request'

     If the binary log dump request failed (due to disconnection), the
     thread goes into this state while it sleeps, then tries to
     reconnect periodically.  The interval between retries can be
     specified using the *note 'CHANGE MASTER TO': change-master-to.
     statement.

   * 
     'Waiting to reconnect after a failed master event read'

     An error occurred while reading (due to disconnection).  The thread
     is sleeping for the number of seconds set by the *note 'CHANGE
     MASTER TO': change-master-to. statement (default 60) before
     attempting to reconnect.


File: manual.info.tmp,  Node: slave-sql-thread-states,  Next: slave-connection-thread-states,  Prev: slave-io-thread-states,  Up: thread-information

8.14.7 Replication Slave SQL Thread States
------------------------------------------

The following list shows the most common states you may see in the
'State' column for a slave server SQL thread:

   * 
     'Waiting for the next event in relay log'

     The initial state before 'Reading event from the relay log'.

   * 
     'Reading event from the relay log'

     The thread has read an event from the relay log so that the event
     can be processed.

   * 
     'Making temp file'

     The thread is executing a *note 'LOAD DATA': load-data. statement
     and is creating a temporary file containing the data from which the
     slave will read rows.

   * 
     'Slave has read all relay log; waiting for the slave I/O thread to
     update it'

     The thread has processed all events in the relay log files, and is
     now waiting for the I/O thread to write new events to the relay
     log.

   * 
     'Waiting for slave mutex on exit'

     A very brief state that occurs as the thread is stopping.

The 'State' column for the I/O thread may also show the text of a
statement.  This indicates that the thread has read an event from the
relay log, extracted the statement from it, and is executing it.


File: manual.info.tmp,  Node: slave-connection-thread-states,  Next: mysql-cluster-thread-states,  Prev: slave-sql-thread-states,  Up: thread-information

8.14.8 Replication Slave Connection Thread States
-------------------------------------------------

These thread states occur on a replication slave but are associated with
connection threads, not with the I/O or SQL threads.

   * 
     'Changing master'

     The thread is processing a *note 'CHANGE MASTER TO':
     change-master-to. statement.

   * 
     'Killing slave'

     The thread is processing a 'STOP SLAVE' statement.

   * 
     'Opening master dump table'

     This state occurs after 'Creating table from master dump'.

   * 
     'Reading master dump table data'

     This state occurs after 'Opening master dump table'.

   * 
     'Rebuilding the index on master dump table'

     This state occurs after 'Reading master dump table data'.


File: manual.info.tmp,  Node: mysql-cluster-thread-states,  Next: event-scheduler-thread-states,  Prev: slave-connection-thread-states,  Up: thread-information

8.14.9 NDB Cluster Thread States
--------------------------------

   * 
     'Committing events to binlog'

   * 
     'Opening mysql.ndb_apply_status'

   * 
     'Processing events'

     The thread is processing events for binary logging.

   * 
     'Processing events from schema table'

     The thread is doing the work of schema replication.

   * 
     'Shutting down'

   * 
     'Syncing ndb table schema operation and binlog'

     This is used to have a correct binary log of schema operations for
     NDB.

   * 
     'Waiting for allowed to take ndbcluster global schema lock'

     The thread is waiting for permission to take a global schema lock.
     Added in MySQL NDB Cluster 7.2.

   * 
     'Waiting for event from ndbcluster'

     The server is acting as an SQL node in an NDB Cluster, and is
     connected to a cluster management node.

   * 
     'Waiting for first event from ndbcluster'

   * 
     'Waiting for ndbcluster binlog update to reach current position'

   * 
     'Waiting for ndbcluster global schema lock'

     The thread is waiting for a global schema lock held by another
     thread to be released.  Added in MySQL NDB Cluster 7.2.

   * 
     'Waiting for ndbcluster to start'

   * 
     'Waiting for schema epoch'

     The thread is waiting for a schema epoch (that is, a global
     checkpoint).


File: manual.info.tmp,  Node: event-scheduler-thread-states,  Prev: mysql-cluster-thread-states,  Up: thread-information

8.14.10 Event Scheduler Thread States
-------------------------------------

These states occur for the Event Scheduler thread, threads that are
created to execute scheduled events, or threads that terminate the
scheduler.

   * 
     'Clearing'

     The scheduler thread or a thread that was executing an event is
     terminating and is about to end.

   * 
     'Initialized'

     The scheduler thread or a thread that will execute an event has
     been initialized.

   * 
     'Waiting for next activation'

     The scheduler has a nonempty event queue but the next activation is
     in the future.

   * 
     'Waiting for scheduler to stop'

     The thread issued 'SET GLOBAL event_scheduler=OFF' and is waiting
     for the scheduler to stop.

   * 
     'Waiting on empty queue'

     The scheduler's event queue is empty and it is sleeping.


File: manual.info.tmp,  Node: language-structure,  Next: charset,  Prev: optimization,  Up: Top

9 Language Structure
********************

* Menu:

* literals::                     Literal Values
* identifiers::                  Schema Object Names
* keywords::                     Keywords and Reserved Words
* user-variables::               User-Defined Variables
* expressions::                  Expressions
* comments::                     Comment Syntax

This chapter discusses the rules for writing the following elements of
SQL statements when using MySQL:

   * Literal values such as strings and numbers

   * Identifiers such as database, table, and column names

   * Keywords and reserved words

   * User-defined and system variables

   * Comments


File: manual.info.tmp,  Node: literals,  Next: identifiers,  Prev: language-structure,  Up: language-structure

9.1 Literal Values
==================

* Menu:

* string-literals::              String Literals
* number-literals::              Numeric Literals
* date-and-time-literals::       Date and Time Literals
* hexadecimal-literals::         Hexadecimal Literals
* bit-value-literals::           Bit-Value Literals
* boolean-literals::             Boolean Literals
* null-values::                  NULL Values

This section describes how to write literal values in MySQL. These
include strings, numbers, hexadecimal and bit values, boolean values,
and 'NULL'.  The section also covers various nuances that you may
encounter when dealing with these basic types in MySQL.


File: manual.info.tmp,  Node: string-literals,  Next: number-literals,  Prev: literals,  Up: literals

9.1.1 String Literals
---------------------

A string is a sequence of bytes or characters, enclosed within either
single quote (''') or double quote ('"') characters.  Examples:

     'a string'
     "another string"

Quoted strings placed next to each other are concatenated to a single
string.  The following lines are equivalent:

     'a string'
     'a' ' ' 'string'

If the 'ANSI_QUOTES' SQL mode is enabled, string literals can be quoted
only within single quotation marks because a string quoted within double
quotation marks is interpreted as an identifier.

A _binary string_ is a string of bytes.  Every binary string has a
character set and collation named 'binary'.  A _nonbinary string_ is a
string of characters.  It has a character set other than 'binary' and a
collation that is compatible with the character set.

For both types of strings, comparisons are based on the numeric values
of the string unit.  For binary strings, the unit is the byte;
comparisons use numeric byte values.  For nonbinary strings, the unit is
the character and some character sets support multibyte characters;
comparisons use numeric character code values.  Character code ordering
is a function of the string collation.  (For more information, see *note
charset-binary-collations::.)

A character string literal may have an optional character set introducer
and 'COLLATE' clause, to designate it as a string that uses a particular
character set and collation:

     [_CHARSET_NAME]'STRING' [COLLATE COLLATION_NAME]

Examples:

     SELECT _latin1'STRING';
     SELECT _binary'STRING';
     SELECT _utf8'STRING' COLLATE utf8_danish_ci;

You can use 'N'LITERAL'' (or 'n'LITERAL'') to create a string in the
national character set.  These statements are equivalent:

     SELECT N'some text';
     SELECT n'some text';
     SELECT _utf8'some text';

For information about these forms of string syntax, see *note
charset-national::, and *note charset-introducer::.

Within a string, certain sequences have special meaning unless the
'NO_BACKSLASH_ESCAPES' SQL mode is enabled.  Each of these sequences
begins with a backslash ('\'), known as the _escape character_.  MySQL
recognizes the escape sequences shown in *note
character-escape-sequences::.  For all other escape sequences, backslash
is ignored.  That is, the escaped character is interpreted as if it was
not escaped.  For example, '\x' is just 'x'.  These sequences are
case-sensitive.  For example, '\b' is interpreted as a backspace, but
'\B' is interpreted as 'B'.  Escape processing is done according to the
character set indicated by the 'character_set_connection' system
variable.  This is true even for strings that are preceded by an
introducer that indicates a different character set, as discussed in
*note charset-literal::.

*Special Character Escape Sequences*

Escape      Character Represented by Sequence
Sequence    

'\0'        An ASCII NUL ('X'00'') character
            
'\''        A single quote (''') character
            
'\"'        A double quote ('"') character
            
'\b'        A backspace character
            
'\n'        A newline (linefeed) character
            
'\r'        A carriage return character
            
'\t'        A tab character
            
'\Z'        ASCII 26 (Control+Z); see note following the table
            
'\\'        A backslash ('\') character
            
'\%'        A '%' character; see note following the table
            
'\_'        A '_' character; see note following the table

The ASCII 26 character can be encoded as '\Z' to enable you to work
around the problem that ASCII 26 stands for END-OF-FILE on Windows.
ASCII 26 within a file causes problems if you try to use 'mysql DB_NAME
< FILE_NAME'.

The '\%' and '\_' sequences are used to search for literal instances of
'%' and '_' in pattern-matching contexts where they would otherwise be
interpreted as wildcard characters.  See the description of the 'LIKE'
operator in *note string-comparison-functions::.  If you use '\%' or
'\_' outside of pattern-matching contexts, they evaluate to the strings
'\%' and '\_', not to '%' and '_'.

There are several ways to include quote characters within a string:

   * A ''' inside a string quoted with ''' may be written as ''''.

   * A '"' inside a string quoted with '"' may be written as '""'.

   * Precede the quote character by an escape character ('\').

   * A ''' inside a string quoted with '"' needs no special treatment
     and need not be doubled or escaped.  In the same way, '"' inside a
     string quoted with ''' needs no special treatment.

The following *note 'SELECT': select. statements demonstrate how quoting
and escaping work:

     mysql> SELECT 'hello', '"hello"', '""hello""', 'hel''lo', '\'hello';
     +-------+---------+-----------+--------+--------+
     | hello | "hello" | ""hello"" | hel'lo | 'hello |
     +-------+---------+-----------+--------+--------+

     mysql> SELECT "hello", "'hello'", "''hello''", "hel""lo", "\"hello";
     +-------+---------+-----------+--------+--------+
     | hello | 'hello' | ''hello'' | hel"lo | "hello |
     +-------+---------+-----------+--------+--------+

     mysql> SELECT 'This\nIs\nFour\nLines';
     +--------------------+
     | This
     Is
     Four
     Lines |
     +--------------------+

     mysql> SELECT 'disappearing\ backslash';
     +------------------------+
     | disappearing backslash |
     +------------------------+

To insert binary data into a string column (such as a *note 'BLOB':
blob. column), you should represent certain characters by escape
sequences.  Backslash ('\') and the quote character used to quote the
string must be escaped.  In certain client environments, it may also be
necessary to escape 'NUL' or Control+Z. The *note 'mysql': mysql. client
truncates quoted strings containing 'NUL' characters if they are not
escaped, and Control+Z may be taken for END-OF-FILE on Windows if not
escaped.  For the escape sequences that represent each of these
characters, see *note character-escape-sequences::.

When writing application programs, any string that might contain any of
these special characters must be properly escaped before the string is
used as a data value in an SQL statement that is sent to the MySQL
server.  You can do this in two ways:

   * Process the string with a function that escapes the special
     characters.  In a C program, you can use the *note
     'mysql_real_escape_string()': mysql-real-escape-string. C API
     function to escape characters.  See *note
     mysql-real-escape-string::.  Within SQL statements that construct
     other SQL statements, you can use the 'QUOTE()' function.  The Perl
     DBI interface provides a 'quote' method to convert special
     characters to the proper escape sequences.  See *note apis-perl::.
     Other language interfaces may provide a similar capability.

   * As an alternative to explicitly escaping special characters, many
     MySQL APIs provide a placeholder capability that enables you to
     insert special markers into a statement string, and then bind data
     values to them when you issue the statement.  In this case, the API
     takes care of escaping special characters in the values for you.


File: manual.info.tmp,  Node: number-literals,  Next: date-and-time-literals,  Prev: string-literals,  Up: literals

9.1.2 Numeric Literals
----------------------

Number literals include exact-value (integer and *note 'DECIMAL':
fixed-point-types.) literals and approximate-value (floating-point)
literals.

Integers are represented as a sequence of digits.  Numbers may include
'.' as a decimal separator.  Numbers may be preceded by '-' or '+' to
indicate a negative or positive value, respectively.  Numbers
represented in scientific notation with a mantissa and exponent are
approximate-value numbers.

Exact-value numeric literals have an integer part or fractional part, or
both.  They may be signed.  Examples: '1', '.2', '3.4', '-5', '-6.78',
'+9.10'.

Approximate-value numeric literals are represented in scientific
notation with a mantissa and exponent.  Either or both parts may be
signed.  Examples: '1.2E3', '1.2E-3', '-1.2E3', '-1.2E-3'.

Two numbers that look similar may be treated differently.  For example,
'2.34' is an exact-value (fixed-point) number, whereas '2.34E0' is an
approximate-value (floating-point) number.

The *note 'DECIMAL': fixed-point-types. data type is a fixed-point type
and calculations are exact.  In MySQL, the *note 'DECIMAL':
fixed-point-types. type has several synonyms: *note 'NUMERIC':
fixed-point-types, *note 'DEC': fixed-point-types, *note 'FIXED':
fixed-point-types.  The integer types also are exact-value types.  For
more information about exact-value calculations, see *note
precision-math::.

The *note 'FLOAT': floating-point-types. and *note 'DOUBLE':
floating-point-types. data types are floating-point types and
calculations are approximate.  In MySQL, types that are synonymous with
*note 'FLOAT': floating-point-types. or *note 'DOUBLE':
floating-point-types. are *note 'DOUBLE PRECISION':
floating-point-types. and *note 'REAL': floating-point-types.

An integer may be used in floating-point context; it is interpreted as
the equivalent floating-point number.


File: manual.info.tmp,  Node: date-and-time-literals,  Next: hexadecimal-literals,  Prev: number-literals,  Up: literals

9.1.3 Date and Time Literals
----------------------------

Date and time values can be represented in several formats, such as
quoted strings or as numbers, depending on the exact type of the value
and other factors.  For example, in contexts where MySQL expects a date,
it interprets any of ''2015-07-21'', ''20150721'', and '20150721' as a
date.

This section describes the acceptable formats for date and time
literals.  For more information about the temporal data types, such as
the range of permitted values, see *note date-and-time-types::.

Standard SQL and ODBC Date and Time Literals

Standard SQL permits temporal literals to be specified using a type
keyword and a string.  The space between the keyword and string is
optional.

     DATE 'STR'
     TIME 'STR'
     TIMESTAMP 'STR'

MySQL recognizes those constructions and also the corresponding ODBC
syntax:

     { d 'STR' }
     { t 'STR' }
     { ts 'STR' }

However, MySQL ignores the type keyword and each of the preceding
constructions produces the string value ''STR'', with a type of *note
'VARCHAR': char.

String and Numeric Literals in Date and Time Context

MySQL recognizes *note 'DATE': datetime. values in these formats:

   * As a string in either ''YYYY-MM-DD'' or ''YY-MM-DD'' format.  A
     'relaxed' syntax is permitted: Any punctuation character may be
     used as the delimiter between date parts.  For example,
     ''2012-12-31'', ''2012/12/31'', ''2012^12^31'', and ''2012@12@31''
     are equivalent.

   * As a string with no delimiters in either ''YYYYMMDD'' or ''YYMMDD''
     format, provided that the string makes sense as a date.  For
     example, ''20070523'' and ''070523'' are interpreted as
     ''2007-05-23'', but ''071332'' is illegal (it has nonsensical month
     and day parts) and becomes ''0000-00-00''.

   * As a number in either YYYYMMDD or YYMMDD format, provided that the
     number makes sense as a date.  For example, '19830905' and '830905'
     are interpreted as ''1983-09-05''.

MySQL recognizes *note 'DATETIME': datetime. and *note 'TIMESTAMP':
datetime. values in these formats:

   * As a string in either ''YYYY-MM-DD HH:MM:SS'' or ''YY-MM-DD
     HH:MM:SS'' format.  A 'relaxed' syntax is permitted here, too: Any
     punctuation character may be used as the delimiter between date
     parts or time parts.  For example, ''2012-12-31 11:30:45'',
     ''2012^12^31 11+30+45'', ''2012/12/31 11*30*45'', and ''2012@12@31
     11^30^45'' are equivalent.

     The date and time parts can be separated by 'T' rather than a
     space.  For example, ''2012-12-31 11:30:45''
     ''2012-12-31T11:30:45'' are equivalent.

   * As a string with no delimiters in either ''YYYYMMDDHHMMSS'' or
     ''YYMMDDHHMMSS'' format, provided that the string makes sense as a
     date.  For example, ''20070523091528'' and ''070523091528'' are
     interpreted as ''2007-05-23 09:15:28'', but ''071122129015'' is
     illegal (it has a nonsensical minute part) and becomes ''0000-00-00
     00:00:00''.

   * As a number in either YYYYMMDDHHMMSS or YYMMDDHHMMSS format,
     provided that the number makes sense as a date.  For example,
     '19830905132800' and '830905132800' are interpreted as ''1983-09-05
     13:28:00''.

A *note 'DATETIME': datetime. or *note 'TIMESTAMP': datetime. value can
include a trailing fractional seconds part in up to microseconds (6
digits) precision.  Although this fractional part is recognized, it is
discarded from values stored into *note 'DATETIME': datetime. or *note
'TIMESTAMP': datetime. columns.  For information about fractional
seconds support in MySQL, see *note fractional-seconds::.

Dates containing two-digit year values are ambiguous because the century
is unknown.  MySQL interprets two-digit year values using these rules:

   * Year values in the range '70-99' become '1970-1999'.

   * Year values in the range '00-69' become '2000-2069'.

See also *note two-digit-years::.

For values specified as strings that include date part delimiters, it is
unnecessary to specify two digits for month or day values that are less
than '10'.  ''2015-6-9'' is the same as ''2015-06-09''.  Similarly, for
values specified as strings that include time part delimiters, it is
unnecessary to specify two digits for hour, minute, or second values
that are less than '10'.  ''2015-10-30 1:2:3'' is the same as
''2015-10-30 01:02:03''.

Values specified as numbers should be 6, 8, 12, or 14 digits long.  If a
number is 8 or 14 digits long, it is assumed to be in YYYYMMDD or
YYYYMMDDHHMMSS format and that the year is given by the first 4 digits.
If the number is 6 or 12 digits long, it is assumed to be in YYMMDD or
YYMMDDHHMMSS format and that the year is given by the first 2 digits.
Numbers that are not one of these lengths are interpreted as though
padded with leading zeros to the closest length.

Values specified as nondelimited strings are interpreted according their
length.  For a string 8 or 14 characters long, the year is assumed to be
given by the first 4 characters.  Otherwise, the year is assumed to be
given by the first 2 characters.  The string is interpreted from left to
right to find year, month, day, hour, minute, and second values, for as
many parts as are present in the string.  This means you should not use
strings that have fewer than 6 characters.  For example, if you specify
''9903'', thinking that represents March, 1999, MySQL converts it to the
'zero' date value.  This occurs because the year and month values are
'99' and '03', but the day part is completely missing.  However, you can
explicitly specify a value of zero to represent missing month or day
parts.  For example, to insert the value ''1999-03-00'', use ''990300''.

MySQL recognizes *note 'TIME': time. values in these formats:

   * As a string in 'D HH:MM:SS' format.  You can also use one of the
     following 'relaxed' syntaxes: 'HH:MM:SS', 'HH:MM', 'D HH:MM', 'D
     HH', or 'SS'.  Here D represents days and can have a value from 0
     to 34.

   * As a string with no delimiters in 'HHMMSS' format, provided that it
     makes sense as a time.  For example, ''101112'' is understood as
     ''10:11:12'', but ''109712'' is illegal (it has a nonsensical
     minute part) and becomes ''00:00:00''.

   * As a number in HHMMSS format, provided that it makes sense as a
     time.  For example, '101112' is understood as ''10:11:12''.  The
     following alternative formats are also understood: SS, MMSS, or
     HHMMSS.

A trailing fractional seconds part is recognized in the 'D
HH:MM:SS.FRACTION', 'HH:MM:SS.FRACTION', 'HHMMSS.FRACTION', and
HHMMSS.FRACTION time formats, where 'fraction' is the fractional part in
up to microseconds (6 digits) precision.  Although this fractional part
is recognized, it is discarded from values stored into *note 'TIME':
time. columns.  For information about fractional seconds support in
MySQL, see *note fractional-seconds::.

For *note 'TIME': time. values specified as strings that include a time
part delimiter, it is unnecessary to specify two digits for hours,
minutes, or seconds values that are less than '10'.  ''8:3:2'' is the
same as ''08:03:02''.


File: manual.info.tmp,  Node: hexadecimal-literals,  Next: bit-value-literals,  Prev: date-and-time-literals,  Up: literals

9.1.4 Hexadecimal Literals
--------------------------

Hexadecimal literal values are written using 'X'VAL'' or '0xVAL'
notation, where VAL contains hexadecimal digits ('0..9', 'A..F').
Lettercase of the digits and of any leading 'X' does not matter.  A
leading '0x' is case-sensitive and cannot be written as '0X'.

Legal hexadecimal literals:

     X'01AF'
     X'01af'
     x'01AF'
     x'01af'
     0x01AF
     0x01af

Illegal hexadecimal literals:

     X'0G'   (G is not a hexadecimal digit)
     0X01AF  (0X must be written as 0x)

Values written using 'X'VAL'' notation must contain an even number of
digits or a syntax error occurs.  To correct the problem, pad the value
with a leading zero:

     mysql> SET @s = X'FFF';
     ERROR 1064 (42000): You have an error in your SQL syntax;
     check the manual that corresponds to your MySQL server
     version for the right syntax to use near 'X'FFF''

     mysql> SET @s = X'0FFF';
     Query OK, 0 rows affected (0.00 sec)

Values written using '0xVAL' notation that contain an odd number of
digits are treated as having an extra leading '0'.  For example, '0xaaa'
is interpreted as '0x0aaa'.

By default, a hexadecimal literal is a binary string, where each pair of
hexadecimal digits represents a character:

     mysql> SELECT X'4D7953514C', CHARSET(X'4D7953514C');
     +---------------+------------------------+
     | X'4D7953514C' | CHARSET(X'4D7953514C') |
     +---------------+------------------------+
     | MySQL         | binary                 |
     +---------------+------------------------+
     mysql> SELECT 0x5461626c65, CHARSET(0x5461626c65);
     +--------------+-----------------------+
     | 0x5461626c65 | CHARSET(0x5461626c65) |
     +--------------+-----------------------+
     | Table        | binary                |
     +--------------+-----------------------+

A hexadecimal literal may have an optional character set introducer and
'COLLATE' clause, to designate it as a string that uses a particular
character set and collation:

     [_CHARSET_NAME] X'VAL' [COLLATE COLLATION_NAME]

Examples:

     SELECT _latin1 X'4D7953514C';
     SELECT _utf8 0x4D7953514C COLLATE utf8_danish_ci;

The examples use 'X'VAL'' notation, but '0xVAL' notation permits
introducers as well.  For information about introducers, see *note
charset-introducer::.

In numeric contexts, MySQL treats a hexadecimal literal like a *note
'BIGINT': integer-types. (64-bit integer).  To ensure numeric treatment
of a hexadecimal literal, use it in numeric context.  Ways to do this
include adding 0 or using 'CAST(... AS UNSIGNED)'.  For example, a
hexadecimal literal assigned to a user-defined variable is a binary
string by default.  To assign the value as a number, use it in numeric
context:

     mysql> SET @v1 = X'41';
     mysql> SET @v2 = X'41'+0;
     mysql> SET @v3 = CAST(X'41' AS UNSIGNED);
     mysql> SELECT @v1, @v2, @v3;
     +------+------+------+
     | @v1  | @v2  | @v3  |
     +------+------+------+
     | A    |   65 |   65 |
     +------+------+------+

An empty hexadecimal value ('X''') evaluates to a zero-length binary
string.  Converted to a number, it produces 0:

     mysql> SELECT CHARSET(X''), LENGTH(X'');
     +--------------+-------------+
     | CHARSET(X'') | LENGTH(X'') |
     +--------------+-------------+
     | binary       |           0 |
     +--------------+-------------+
     mysql> SELECT X''+0;
     +-------+
     | X''+0 |
     +-------+
     |     0 |
     +-------+

The 'X'VAL'' notation is based on standard SQL. The '0x' notation is
based on ODBC, for which hexadecimal strings are often used to supply
values for *note 'BLOB': blob. columns.

To convert a string or a number to a string in hexadecimal format, use
the 'HEX()' function:

     mysql> SELECT HEX('cat');
     +------------+
     | HEX('cat') |
     +------------+
     | 636174     |
     +------------+
     mysql> SELECT X'636174';
     +-----------+
     | X'636174' |
     +-----------+
     | cat       |
     +-----------+


File: manual.info.tmp,  Node: bit-value-literals,  Next: boolean-literals,  Prev: hexadecimal-literals,  Up: literals

9.1.5 Bit-Value Literals
------------------------

Bit-value literals are written using 'b'VAL'' or '0bVAL' notation.  VAL
is a binary value written using zeros and ones.  Lettercase of any
leading 'b' does not matter.  A leading '0b' is case-sensitive and
cannot be written as '0B'.

Legal bit-value literals:

     b'01'
     B'01'
     0b01

Illegal bit-value literals:

     b'2'    (2 is not a binary digit)
     0B01    (0B must be written as 0b)

By default, a bit-value literal is a binary string:

     mysql> SELECT b'1000001', CHARSET(b'1000001');
     +------------+---------------------+
     | b'1000001' | CHARSET(b'1000001') |
     +------------+---------------------+
     | A          | binary              |
     +------------+---------------------+
     mysql> SELECT 0b1100001, CHARSET(0b1100001);
     +-----------+--------------------+
     | 0b1100001 | CHARSET(0b1100001) |
     +-----------+--------------------+
     | a         | binary             |
     +-----------+--------------------+

A bit-value literal may have an optional character set introducer and
'COLLATE' clause, to designate it as a string that uses a particular
character set and collation:

     [_CHARSET_NAME] b'VAL' [COLLATE COLLATION_NAME]

Examples:

     SELECT _latin1 b'1000001';
     SELECT _utf8 0b1000001 COLLATE utf8_danish_ci;

The examples use 'b'VAL'' notation, but '0bVAL' notation permits
introducers as well.  For information about introducers, see *note
charset-introducer::.

In numeric contexts, MySQL treats a bit literal like an integer.  To
ensure numeric treatment of a bit literal, use it in numeric context.
Ways to do this include adding 0 or using 'CAST(... AS UNSIGNED)'.  For
example, a bit literal assigned to a user-defined variable is a binary
string by default.  To assign the value as a number, use it in numeric
context:

     mysql> SET @v1 = b'1100001';
     mysql> SET @v2 = b'1100001'+0;
     mysql> SET @v3 = CAST(b'1100001' AS UNSIGNED);
     mysql> SELECT @v1, @v2, @v3;
     +------+------+------+
     | @v1  | @v2  | @v3  |
     +------+------+------+
     | a    |   97 |   97 |
     +------+------+------+

An empty bit value ('b''') evaluates to a zero-length binary string.
Converted to a number, it produces 0:

     mysql> SELECT CHARSET(b''), LENGTH(b'');
     +--------------+-------------+
     | CHARSET(b'') | LENGTH(b'') |
     +--------------+-------------+
     | binary       |           0 |
     +--------------+-------------+
     mysql> SELECT b''+0;
     +-------+
     | b''+0 |
     +-------+
     |     0 |
     +-------+

Bit-value notation is convenient for specifying values to be assigned to
*note 'BIT': bit-type. columns:


     mysql> CREATE TABLE t (b BIT(8));
     mysql> INSERT INTO t SET b = b'11111111';
     mysql> INSERT INTO t SET b = b'1010';
     mysql> INSERT INTO t SET b = b'0101';

Bit values in result sets are returned as binary values, which may not
display well.  To convert a bit value to printable form, use it in
numeric context or use a conversion function such as 'BIN()' or 'HEX()'.
High-order 0 digits are not displayed in the converted value.

     mysql> SELECT b+0, BIN(b), OCT(b), HEX(b) FROM t;
     +------+----------+--------+--------+
     | b+0  | BIN(b)   | OCT(b) | HEX(b) |
     +------+----------+--------+--------+
     |  255 | 11111111 | 377    | FF     |
     |   10 | 1010     | 12     | A      |
     |    5 | 101      | 5      | 5      |
     +------+----------+--------+--------+


File: manual.info.tmp,  Node: boolean-literals,  Next: null-values,  Prev: bit-value-literals,  Up: literals

9.1.6 Boolean Literals
----------------------

The constants 'TRUE' and 'FALSE' evaluate to '1' and '0', respectively.
The constant names can be written in any lettercase.

     mysql> SELECT TRUE, true, FALSE, false;
             -> 1, 1, 0, 0


File: manual.info.tmp,  Node: null-values,  Prev: boolean-literals,  Up: literals

9.1.7 NULL Values
-----------------

The 'NULL' value means 'no data.'  'NULL' can be written in any
lettercase.  A synonym is '\N' (case-sensitive).

Be aware that the 'NULL' value is different from values such as '0' for
numeric types or the empty string for string types.  For more
information, see *note problems-with-null::.

For text file import or export operations performed with *note 'LOAD
DATA': load-data. or *note 'SELECT ... INTO OUTFILE': select-into,
'NULL' is represented by the '\N' sequence.  See *note load-data::.

For sorting with 'ORDER BY', 'NULL' values sort before other values for
ascending sorts, after other values for descending sorts.


File: manual.info.tmp,  Node: identifiers,  Next: keywords,  Prev: literals,  Up: language-structure

9.2 Schema Object Names
=======================

* Menu:

* identifier-length::            Identifier Length Limits
* identifier-qualifiers::        Identifier Qualifiers
* identifier-case-sensitivity::  Identifier Case Sensitivity
* identifier-mapping::           Mapping of Identifiers to File Names
* function-resolution::          Function Name Parsing and Resolution

Certain objects within MySQL, including database, table, index, column,
alias, view, stored procedure, partition, tablespace, and other object
names are known as identifiers.  This section describes the permissible
syntax for identifiers in MySQL. *note identifier-length::, indicates
the maximum length of each type of identifier.  *note
identifier-case-sensitivity::, describes which types of identifiers are
case-sensitive and under what conditions.

An identifier may be quoted or unquoted.  If an identifier contains
special characters or is a reserved word, you _must_ quote it whenever
you refer to it.  (Exception: A reserved word that follows a period in a
qualified name must be an identifier, so it need not be quoted.)
Reserved words are listed at *note keywords::.

Internally, identifiers are converted to and are stored as Unicode
(UTF-8).  The permissible Unicode characters in identifiers are those in
the Basic Multilingual Plane (BMP). Supplementary characters are not
permitted.  Identifiers thus may contain these characters:

   * Permitted characters in unquoted identifiers:

        * ASCII: [0-9,a-z,A-Z$_] (basic Latin letters, digits 0-9,
          dollar, underscore)

        * Extended: U+0080 ..  U+FFFF

   * Permitted characters in quoted identifiers include the full Unicode
     Basic Multilingual Plane (BMP), except U+0000:

        * ASCII: U+0001 ..  U+007F

        * Extended: U+0080 ..  U+FFFF

   * ASCII NUL (U+0000) and supplementary characters (U+10000 and
     higher) are not permitted in quoted or unquoted identifiers.

   * Identifiers may begin with a digit but unless quoted may not
     consist solely of digits.

   * Database, table, and column names cannot end with space characters.

The identifier quote character is the backtick ('`'):

     mysql> SELECT * FROM `select` WHERE `select`.id > 100;

If the 'ANSI_QUOTES' SQL mode is enabled, it is also permissible to
quote identifiers within double quotation marks:

     mysql> CREATE TABLE "test" (col INT);
     ERROR 1064: You have an error in your SQL syntax...
     mysql> SET sql_mode='ANSI_QUOTES';
     mysql> CREATE TABLE "test" (col INT);
     Query OK, 0 rows affected (0.00 sec)

The 'ANSI_QUOTES' mode causes the server to interpret double-quoted
strings as identifiers.  Consequently, when this mode is enabled, string
literals must be enclosed within single quotation marks.  They cannot be
enclosed within double quotation marks.  The server SQL mode is
controlled as described in *note sql-mode::.

Identifier quote characters can be included within an identifier if you
quote the identifier.  If the character to be included within the
identifier is the same as that used to quote the identifier itself, then
you need to double the character.  The following statement creates a
table named 'a`b' that contains a column named 'c"d':

     mysql> CREATE TABLE `a``b` (`c"d` INT);

In the select list of a query, a quoted column alias can be specified
using identifier or string quoting characters:

     mysql> SELECT 1 AS `one`, 2 AS 'two';
     +-----+-----+
     | one | two |
     +-----+-----+
     |   1 |   2 |
     +-----+-----+

Elsewhere in the statement, quoted references to the alias must use
identifier quoting or the reference is treated as a string literal.

It is recommended that you do not use names that begin with 'Me' or
'MeN', where M and N are integers.  For example, avoid using '1e' as an
identifier, because an expression such as '1e+3' is ambiguous.
Depending on context, it might be interpreted as the expression '1e + 3'
or as the number '1e+3'.

Be careful when using 'MD5()' to produce table names because it can
produce names in illegal or ambiguous formats such as those just
described.

A user variable cannot be used directly in an SQL statement as an
identifier or as part of an identifier.  See *note user-variables::, for
more information and examples of workarounds.

Special characters in database and table names are encoded in the
corresponding file system names as described in *note
identifier-mapping::.  If you have databases or tables from an older
version of MySQL that contain special characters and for which the
underlying directory names or file names have not been updated to use
the new encoding, the server displays their names with a prefix of
'#mysql50#'.  For information about referring to such names or
converting them to the newer encoding, see that section.


File: manual.info.tmp,  Node: identifier-length,  Next: identifier-qualifiers,  Prev: identifiers,  Up: identifiers

9.2.1 Identifier Length Limits
------------------------------

The following table describes the maximum length for each type of
identifier.

Identifier  Maximum
Type        Length
            (characters)
            
Database

64
(*note 'NDB': mysql-cluster.
storage
engine:
63)

Table

64
(*note 'NDB': mysql-cluster.
storage
engine:
63)

Column

64

Index

64

Constraint

64

Stored
Program

64

View

64

Tablespace

64

Server

64

Log File
Group

64

Alias

256 (see
exception
following
table)

Compound
Statement
Label

16

Aliases for column names in *note 'CREATE VIEW': create-view. statements
are checked against the maximum column length of 64 characters (not the
maximum alias length of 256 characters).

For constraint definitions that include no constraint name, the server
internally generates a name derived from the associated table name.  For
example, internally generated foreign key constraint names consist of
the table name plus '_ibfk_' and a number.  If the table name is close
to the length limit for constraint names, the additional characters
required for the constraint name may cause that name to exceed the
limit, resulting in an error.

Identifiers are stored using Unicode (UTF-8).  This applies to
identifiers in table definitions that are stored in '.frm' files and to
identifiers stored in the grant tables in the 'mysql' database.  The
sizes of the identifier string columns in the grant tables are measured
in characters.  You can use multibyte characters without reducing the
number of characters permitted for values stored in these columns.

NDB Cluster imposes a maximum length of 63 characters for names of
databases and tables.  See *note
mysql-cluster-limitations-database-objects::.

Values such as user name and host names in MySQL account names are
strings rather than identifiers.  For information about the maximum
length of such values as stored in grant tables, see *note
grant-tables-scope-column-properties::.


File: manual.info.tmp,  Node: identifier-qualifiers,  Next: identifier-case-sensitivity,  Prev: identifier-length,  Up: identifiers

9.2.2 Identifier Qualifiers
---------------------------

Object names may be unqualified or qualified.  An unqualified name is
permitted in contexts where interpretation of the name is unambiguous.
A qualified name includes at least one qualifier to clarify the
interpretive context by overriding a default context or providing
missing context.

For example, this statement creates a table using the unqualified name
't1':

     CREATE TABLE t1 (i INT);

Because 't1' includes no qualifier to specify a database, the statement
creates the table in the default database.  If there is no default
database, an error occurs.

This statement creates a table using the qualified name 'db1.t1':

     CREATE TABLE db1.t1 (i INT);

Because 'db1.t1' includes a database qualifier 'db1', the statement
creates 't1' in the database named 'db1', regardless of the default
database.  The qualifier _must_ be specified if there is no default
database.  The qualifier _may_ be specified if there is a default
database, to specify a database different from the default, or to make
the database explicit if the default is the same as the one specified.

Qualifiers have these characteristics:

   * An unqualified name consists of a single identifier.  A qualified
     name consists of multiple identifiers.

   * The components of a multiple-part name must be separated by period
     ('.') characters.  The initial parts of a multiple-part name act as
     qualifiers that affect the context within which to interpret the
     final identifier.

   * The qualifier character is a separate token and need not be
     contiguous with the associated identifiers.  For example,
     TBL_NAME.COL_NAME and TBL_NAME . COL_NAME are equivalent.

   * If any components of a multiple-part name require quoting, quote
     them individually rather than quoting the name as a whole.  For
     example, write '`my-table`.`my-column`', not
     '`my-table.my-column`'.

   * A reserved word that follows a period in a qualified name must be
     an identifier, so in that context it need not be quoted.

   * The syntax '.TBL_NAME' means the table TBL_NAME in the default
     database.  This syntax is accepted for ODBC compatibility because
     some ODBC programs prefix table names with a '.' character.

The permitted qualifiers for object names depend on the object type:

   * A database name is fully qualified and takes no qualifier:

          CREATE DATABASE db1;

   * A table, view, or stored program name may be given a database-name
     qualifier.  Examples of unqualified and qualified names in 'CREATE'
     statements:

          CREATE TABLE mytable ...;
          CREATE VIEW myview ...;
          CREATE PROCEDURE myproc ...;
          CREATE FUNCTION myfunc ...;
          CREATE EVENT myevent ...;

          CREATE TABLE mydb.mytable ...;
          CREATE VIEW mydb.myview ...;
          CREATE PROCEDURE mydb.myproc ...;
          CREATE FUNCTION mydb.myfunc ...;
          CREATE EVENT mydb.myevent ...;

   * A trigger is associated with a table, so any qualifier applies to
     the table name:

          CREATE TRIGGER mytrigger ... ON mytable ...;

          CREATE TRIGGER mytrigger ... ON mydb.mytable ...;

   * A column name may be given multiple qualifiers to indicate context
     in statements that reference it, as shown in the following table.

     Column Reference          Meaning
                               
     COL_NAME                  Column COL_NAME from whichever table used in
                               the statement contains a column of that name
                               
     TBL_NAME.COL_NAME         Column COL_NAME from table TBL_NAME of the
                               default database
                               
     DB_NAME.TBL_NAME.COL_NAME Column COL_NAME from table TBL_NAME of the
                               database DB_NAME

     In other words, a column name may be given a table-name qualifier,
     which itself may be given a database-name qualifier.  Examples of
     unqualified and qualified column references in 'SELECT' statements:

          SELECT c1 FROM mytable
          WHERE c2 > 100;

          SELECT mytable.c1 FROM mytable
          WHERE mytable.c2 > 100;

          SELECT mydb.mytable.c1 FROM mydb.mytable
          WHERE mydb.mytable.c2 > 100;

You need not specify a qualifier for an object reference in a statement
unless the unqualified reference is ambiguous.  Suppose that column 'c1'
occurs only in table 't1', 'c2' only in 't2', and 'c' in both 't1' and
't2'.  Any unqualified reference to 'c' is ambiguous in a statement that
refers to both tables and must be qualified as 't1.c' or 't2.c' to
indicate which table you mean:

     SELECT c1, c2, t1.c FROM t1 INNER JOIN t2
     WHERE t2.c > 100;

Similarly, to retrieve from a table 't' in database 'db1' and from a
table 't' in database 'db2' in the same statement, you must qualify the
table references: For references to columns in those tables, qualifiers
are required only for column names that appear in both tables.  Suppose
that column 'c1' occurs only in table 'db1.t', 'c2' only in 'db2.t', and
'c' in both 'db1.t' and 'db2.t'.  In this case, 'c' is ambiguous and
must be qualified but 'c1' and 'c2' need not be:

     SELECT c1, c2, db1.t.c FROM db1.t INNER JOIN db2.t
     WHERE db2.t.c > 100;

Table aliases enable qualified column references to be written more
simply:

     SELECT c1, c2, t1.c FROM db1.t AS t1 INNER JOIN db2.t AS t2
     WHERE t2.c > 100;


File: manual.info.tmp,  Node: identifier-case-sensitivity,  Next: identifier-mapping,  Prev: identifier-qualifiers,  Up: identifiers

9.2.3 Identifier Case Sensitivity
---------------------------------

In MySQL, databases correspond to directories within the data directory.
Each table within a database corresponds to at least one file within the
database directory (and possibly more, depending on the storage engine).
Triggers also correspond to files.  Consequently, the case sensitivity
of the underlying operating system plays a part in the case sensitivity
of database, table, and trigger names.  This means such names are not
case-sensitive in Windows, but are case-sensitive in most varieties of
Unix.  One notable exception is macOS, which is Unix-based but uses a
default file system type (HFS+) that is not case-sensitive.  However,
macOS also supports UFS volumes, which are case-sensitive just as on any
Unix.  See *note extensions-to-ansi::.  The 'lower_case_table_names'
system variable also affects how the server handles identifier case
sensitivity, as described later in this section.

*Note*:

Although database, table, and trigger names are not case sensitive on
some platforms, you should not refer to one of these using different
cases within the same statement.  The following statement would not work
because it refers to a table both as 'my_table' and as 'MY_TABLE':

     mysql> SELECT * FROM my_table WHERE MY_TABLE.col=1;

Column, index, stored routine, and event names are not case sensitive on
any platform, nor are column aliases.

However, names of logfile groups are case-sensitive.  This differs from
standard SQL.

By default, table aliases are case-sensitive on Unix, but not so on
Windows or macOS. The following statement would not work on Unix,
because it refers to the alias both as 'a' and as 'A':

     mysql> SELECT COL_NAME FROM TBL_NAME AS a
            WHERE a.COL_NAME = 1 OR A.COL_NAME = 2;

However, this same statement is permitted on Windows.  To avoid problems
caused by such differences, it is best to adopt a consistent convention,
such as always creating and referring to databases and tables using
lowercase names.  This convention is recommended for maximum portability
and ease of use.

How table and database names are stored on disk and used in MySQL is
affected by the 'lower_case_table_names' system variable, which you can
set when starting *note 'mysqld': mysqld.  'lower_case_table_names' can
take the values shown in the following table.  This variable does _not_
affect case sensitivity of trigger identifiers.  On Unix, the default
value of 'lower_case_table_names' is 0.  On Windows, the default value
is 1.  On macOS, the default value is 2.

Value   Meaning
        
'0'     Table and database names are stored on disk using the
        lettercase specified in the *note 'CREATE TABLE': create-table.
        or *note 'CREATE DATABASE': create-database. statement.  Name
        comparisons are case sensitive.  You should _not_ set this
        variable to 0 if you are running MySQL on a system that has
        case-insensitive file names (such as Windows or macOS). If you
        force this variable to 0 with '--lower-case-table-names=0' on a
        case-insensitive file system and access 'MyISAM' tablenames
        using different lettercases, index corruption may result.
        
'1'     Table names are stored in lowercase on disk and name
        comparisons are not case-sensitive.  MySQL converts all table
        names to lowercase on storage and lookup.  This behavior also
        applies to database names and table aliases.
        
'2'     Table and database names are stored on disk using the
        lettercase specified in the *note 'CREATE TABLE': create-table.
        or *note 'CREATE DATABASE': create-database. statement, but
        MySQL converts them to lowercase on lookup.  Name comparisons
        are not case sensitive.  This works _only_ on file systems that
        are not case-sensitive!  'InnoDB' table names are stored in
        lowercase, as for 'lower_case_table_names=1'.

If you are using MySQL on only one platform, you do not normally have to
change the 'lower_case_table_names' variable from its default value.
However, you may encounter difficulties if you want to transfer tables
between platforms that differ in file system case sensitivity.  For
example, on Unix, you can have two different tables named 'my_table' and
'MY_TABLE', but on Windows these two names are considered identical.  To
avoid data transfer problems arising from lettercase of database or
table names, you have two options:

   * Use 'lower_case_table_names=1' on all systems.  The main
     disadvantage with this is that when you use *note 'SHOW TABLES':
     show-tables. or *note 'SHOW DATABASES': show-databases, you do not
     see the names in their original lettercase.

   * Use 'lower_case_table_names=0' on Unix and
     'lower_case_table_names=2' on Windows.  This preserves the
     lettercase of database and table names.  The disadvantage of this
     is that you must ensure that your statements always refer to your
     database and table names with the correct lettercase on Windows.
     If you transfer your statements to Unix, where lettercase is
     significant, they do not work if the lettercase is incorrect.

     *Exception*: If you are using 'InnoDB' tables and you are trying to
     avoid these data transfer problems, you should set
     'lower_case_table_names' to 1 on all platforms to force names to be
     converted to lowercase.

If you plan to set the 'lower_case_table_names' system variable to 1 on
Unix, you must first convert your old database and table names to
lowercase before stopping *note 'mysqld': mysqld. and restarting it with
the new variable setting.  To do this for an individual table, use *note
'RENAME TABLE': rename-table.:

     RENAME TABLE T1 TO t1;

To convert one or more entire databases, dump them before setting
'lower_case_table_names', then drop the databases, and reload them after
setting 'lower_case_table_names':

  1. Use *note 'mysqldump': mysqldump. to dump each database:

          mysqldump --databases db1 > db1.sql
          mysqldump --databases db2 > db2.sql
          ...

     Do this for each database that must be recreated.

  2. Use 'DROP DATABASE' to drop each database.

  3. Stop the server, set 'lower_case_table_names', and restart the
     server.

  4. Reload the dump file for each database.  Because
     'lower_case_table_names' is set, each database and table name will
     be converted to lowercase as it is recreated:

          mysql < db1.sql
          mysql < db2.sql
          ...

Object names may be considered duplicates if their uppercase forms are
equal according to a binary collation.  That is true for names of
cursors, conditions, procedures, functions, savepoints, stored routine
parameters, stored program local variables, and plugins.  It is not true
for names of columns, constraints, databases, partitions, statements
prepared with *note 'PREPARE': prepare, tables, triggers, users, and
user-defined variables.

File system case sensitivity can affect searches in string columns of
'INFORMATION_SCHEMA' tables.  For more information, see *note
charset-collation-information-schema::.


File: manual.info.tmp,  Node: identifier-mapping,  Next: function-resolution,  Prev: identifier-case-sensitivity,  Up: identifiers

9.2.4 Mapping of Identifiers to File Names
------------------------------------------

There is a correspondence between database and table identifiers and
names in the file system.  For the basic structure, MySQL represents
each database as a directory in the data directory, and each table by
one or more files in the appropriate database directory.  For the table
format files ('.FRM'), the data is always stored in this structure and
location.

For the data and index files, the exact representation on disk is
storage engine specific.  These files may be stored in the same location
as the 'FRM' files, or the information may be stored in a separate file.
'InnoDB' data is stored in the InnoDB data files.  If you are using
tablespaces with 'InnoDB', then the specific tablespace files you create
are used instead.

Any character is legal in database or table identifiers except ASCII NUL
('X'00'').  MySQL encodes any characters that are problematic in the
corresponding file system objects when it creates database directories
or table files:

   * Basic Latin letters ('a..zA..Z'), digits ('0..9') and underscore
     ('_') are encoded as is.  Consequently, their case sensitivity
     directly depends on file system features.

   * All other national letters from alphabets that have
     uppercase/lowercase mapping are encoded as shown in the following
     table.  Values in the Code Range column are UCS-2 values.

     Code        Pattern     Number      Used        Unused      Blocks
     Range                                                       

     00C0..017F  [@][0..4][g..z]5*20= 10097          3           Latin-1
                                                                 Supplement +
                                                                 Latin Extended-A
                                                                 
     0370..03FF  [@][5..9][g..z]5*20= 10088          12          Greek and Coptic
                                                                 
     0400..052F  [@][g..z][0..6]20*7= 140137         3           Cyrillic +
                                                                 Cyrillic
                                                                 Supplement
                                                                 
     0530..058F  [@][g..z][7..8]20*2= 40 38          2           Armenian
                                                                 
     2160..217F  [@][g..z][9]20*1= 20    16          4           Number Forms
                                                                 
     0180..02AF  [@][g..z][a..k]20*11=220203         17          Latin Extended-B
                                                                 + IPA Extensions
                                                                 
     1E00..1EFF  [@][g..z][l..r]20*7= 140136         4           Latin Extended
                                                                 Additional
                                                                 
     1F00..1FFF  [@][g..z][s..z]20*8= 160144         16          Greek Extended
                                                                 
     ....        [@][a..f][g..z]6*20= 1200           120         RESERVED
     ....                                                        

     24B6..24E9  [@][@][a..z]26          26          0           Enclosed
                                                                 Alphanumerics
                                                                 
     FF21..FF5A  [@][a..z][@]26          26          0           Halfwidth and
                                                                 Fullwidth forms

     One of the bytes in the sequence encodes lettercase.  For example:
     'LATIN CAPITAL LETTER A WITH GRAVE' is encoded as '@0G', whereas
     'LATIN SMALL LETTER A WITH GRAVE' is encoded as '@0g'.  Here the
     third byte ('G' or 'g') indicates lettercase.  (On a
     case-insensitive file system, both letters will be treated as the
     same.)

     For some blocks, such as Cyrillic, the second byte determines
     lettercase.  For other blocks, such as Latin1 Supplement, the third
     byte determines lettercase.  If two bytes in the sequence are
     letters (as in Greek Extended), the leftmost letter character
     stands for lettercase.  All other letter bytes must be in
     lowercase.

   * All nonletter characters except underscore ('_'), as well as
     letters from alphabets that do not have uppercase/lowercase mapping
     (such as Hebrew) are encoded using hexadecimal representation using
     lowercase letters for hexadecimal digits 'a..f':

          0x003F -> @003f
          0xFFFF -> @ffff

     The hexadecimal values correspond to character values in the 'ucs2'
     double-byte character set.

On Windows, some names such as 'nul', 'prn', and 'aux' are encoded by
appending '@@@' to the name when the server creates the corresponding
file or directory.  This occurs on all platforms for portability of the
corresponding database object between platforms.

If you have databases or tables from a version of MySQL older than 5.1.6
that contain special characters and for which the underlying directory
names or file names have not been updated to use the new encoding, the
server displays their names with a prefix of '#mysql50#' in the output
from 'INFORMATION_SCHEMA' tables or *note 'SHOW': show. statements.  For
example, if you have a table named 'a@b' and its name encoding has not
been updated, *note 'SHOW TABLES': show-tables. displays it like this:

     mysql> SHOW TABLES;
     +----------------+
     | Tables_in_test |
     +----------------+
     | #mysql50#a@b   |
     +----------------+

To refer to such a name for which the encoding has not been updated, you
must supply the '#mysql50#' prefix:

     mysql> SHOW COLUMNS FROM `a@b`;
     ERROR 1146 (42S02): Table 'test.a@b' doesn't exist

     mysql> SHOW COLUMNS FROM `#mysql50#a@b`;
     +-------+---------+------+-----+---------+-------+
     | Field | Type    | Null | Key | Default | Extra |
     +-------+---------+------+-----+---------+-------+
     | i     | int(11) | YES  |     | NULL    |       |
     +-------+---------+------+-----+---------+-------+

To update old names to eliminate the need to use the special prefix to
refer to them, re-encode them with *note 'mysqlcheck': mysqlcheck.  The
following commands update all names to the new encoding:

     mysqlcheck --check-upgrade --all-databases
     mysqlcheck --fix-db-names --fix-table-names --all-databases

To check only specific databases or tables, omit '--all-databases' and
provide the appropriate database or table arguments.  For information
about *note 'mysqlcheck': mysqlcheck. invocation syntax, see *note
mysqlcheck::.

*Note*:

The '#mysql50#' prefix is intended only to be used internally by the
server.  You should not create databases or tables with names that use
this prefix.

Also, *note 'mysqlcheck': mysqlcheck. cannot fix names that contain
literal instances of the '@' character that is used for encoding special
characters.  If you have databases or tables that contain this
character, use *note 'mysqldump': mysqldump. to dump them before
upgrading to MySQL 5.1.6 or later, and then reload the dump file after
upgrading.


File: manual.info.tmp,  Node: function-resolution,  Prev: identifier-mapping,  Up: identifiers

9.2.5 Function Name Parsing and Resolution
------------------------------------------

MySQL supports built-in (native) functions, user-defined functions
(UDFs), and stored functions.  This section describes how the server
recognizes whether the name of a built-in function is used as a function
call or as an identifier, and how the server determines which function
to use in cases when functions of different types exist with a given
name.

   * *note function-name-parsing::

   * *note function-name-resolution::

*Built-In Function Name Parsing*

The parser uses default rules for parsing names of built-in functions.
These rules can be changed by enabling the 'IGNORE_SPACE' SQL mode.

When the parser encounters a word that is the name of a built-in
function, it must determine whether the name signifies a function call
or is instead a nonexpression reference to an identifier such as a table
or column name.  For example, in the following statements, the first
reference to 'count' is a function call, whereas the second reference is
a table name:

     SELECT COUNT(*) FROM mytable;
     CREATE TABLE count (i INT);

The parser should recognize the name of a built-in function as
indicating a function call only when parsing what is expected to be an
expression.  That is, in nonexpression context, function names are
permitted as identifiers.

However, some built-in functions have special parsing or implementation
considerations, so the parser uses the following rules by default to
distinguish whether their names are being used as function calls or as
identifiers in nonexpression context:

   * To use the name as a function call in an expression, there must be
     no whitespace between the name and the following '(' parenthesis
     character.

   * Conversely, to use the function name as an identifier, it must not
     be followed immediately by a parenthesis.

The requirement that function calls be written with no whitespace
between the name and the parenthesis applies only to the built-in
functions that have special considerations.  'COUNT' is one such name.
The 'sql_functions[]' array in the 'sql/lex.h' source file lists the
names of these special functions for which following whitespace
determines their interpretation.

The following list names the functions in MySQL 5.5 that are affected by
the 'IGNORE_SPACE' setting and listed as special in the 'sql/lex.h'
source file.  You may find it easiest to treat the no-whitespace
requirement as applying to all function calls.

   * 'ADDDATE'

   * 'BIT_AND'

   * 'BIT_OR'

   * 'BIT_XOR'

   * 'CAST'

   * 'COUNT'

   * 'CURDATE'

   * 'CURTIME'

   * 'DATE_ADD'

   * 'DATE_SUB'

   * 'EXTRACT'

   * 'GROUP_CONCAT'

   * 'MAX'

   * 'MID'

   * 'MIN'

   * 'NOW'

   * 'POSITION'

   * 'SESSION_USER'

   * 'STD'

   * 'STDDEV'

   * 'STDDEV_POP'

   * 'STDDEV_SAMP'

   * 'SUBDATE'

   * 'SUBSTR'

   * 'SUBSTRING'

   * 'SUM'

   * 'SYSDATE'

   * 'SYSTEM_USER'

   * 'TRIM'

   * 'VARIANCE'

   * 'VAR_POP'

   * 'VAR_SAMP'

For functions not listed as special in 'sql/lex.h', whitespace does not
matter.  They are interpreted as function calls only when used in
expression context and may be used freely as identifiers otherwise.
'ASCII' is one such name.  However, for these nonaffected function
names, interpretation may vary in expression context: 'FUNC_NAME ()' is
interpreted as a built-in function if there is one with the given name;
if not, 'FUNC_NAME ()' is interpreted as a user-defined function or
stored function if one exists with that name.

The 'IGNORE_SPACE' SQL mode can be used to modify how the parser treats
function names that are whitespace-sensitive:

   * With 'IGNORE_SPACE' disabled, the parser interprets the name as a
     function call when there is no whitespace between the name and the
     following parenthesis.  This occurs even when the function name is
     used in nonexpression context:

          mysql> CREATE TABLE count(i INT);
          ERROR 1064 (42000): You have an error in your SQL syntax ...
          near 'count(i INT)'

     To eliminate the error and cause the name to be treated as an
     identifier, either use whitespace following the name or write it as
     a quoted identifier (or both):

          CREATE TABLE count (i INT);
          CREATE TABLE `count`(i INT);
          CREATE TABLE `count` (i INT);

   * With 'IGNORE_SPACE' enabled, the parser loosens the requirement
     that there be no whitespace between the function name and the
     following parenthesis.  This provides more flexibility in writing
     function calls.  For example, either of the following function
     calls are legal:

          SELECT COUNT(*) FROM mytable;
          SELECT COUNT (*) FROM mytable;

     However, enabling 'IGNORE_SPACE' also has the side effect that the
     parser treats the affected function names as reserved words (see
     *note keywords::).  This means that a space following the name no
     longer signifies its use as an identifier.  The name can be used in
     function calls with or without following whitespace, but causes a
     syntax error in nonexpression context unless it is quoted.  For
     example, with 'IGNORE_SPACE' enabled, both of the following
     statements fail with a syntax error because the parser interprets
     'count' as a reserved word:

          CREATE TABLE count(i INT);
          CREATE TABLE count (i INT);

     To use the function name in nonexpression context, write it as a
     quoted identifier:

          CREATE TABLE `count`(i INT);
          CREATE TABLE `count` (i INT);

To enable the 'IGNORE_SPACE' SQL mode, use this statement:

     SET sql_mode = 'IGNORE_SPACE';

'IGNORE_SPACE' is also enabled by certain other composite modes such as
'ANSI' that include it in their value:

     SET sql_mode = 'ANSI';

Check *note sql-mode::, to see which composite modes enable
'IGNORE_SPACE'.

To minimize the dependency of SQL code on the 'IGNORE_SPACE' setting,
use these guidelines:

   * Avoid creating UDFs or stored functions that have the same name as
     a built-in function.

   * Avoid using function names in nonexpression context.  For example,
     these statements use 'count' (one of the affected function names
     affected by 'IGNORE_SPACE'), so they fail with or without
     whitespace following the name if 'IGNORE_SPACE' is enabled:

          CREATE TABLE count(i INT);
          CREATE TABLE count (i INT);

     If you must use a function name in nonexpression context, write it
     as a quoted identifier:

          CREATE TABLE `count`(i INT);
          CREATE TABLE `count` (i INT);

*Function Name Resolution*

The following rules describe how the server resolves references to
function names for function creation and invocation:

   * Built-in functions and user-defined functions

     An error occurs if you try to create a UDF with the same name as a
     built-in function.

   * Built-in functions and stored functions

     It is possible to create a stored function with the same name as a
     built-in function, but to invoke the stored function it is
     necessary to qualify it with a schema name.  For example, if you
     create a stored function named 'PI' in the 'test' schema, invoke it
     as 'test.PI()' because the server resolves 'PI()' without a
     qualifier as a reference to the built-in function.  The server
     generates a warning if the stored function name collides with a
     built-in function name.  The warning can be displayed with *note
     'SHOW WARNINGS': show-warnings.

   * User-defined functions and stored functions

     User-defined functions and stored functions share the same
     namespace, so you cannot create a UDF and a stored function with
     the same name.

The preceding function name resolution rules have implications for
upgrading to versions of MySQL that implement new built-in functions:

   * If you have already created a user-defined function with a given
     name and upgrade MySQL to a version that implements a new built-in
     function with the same name, the UDF becomes inaccessible.  To
     correct this, use *note 'DROP FUNCTION': drop-function. to drop the
     UDF and *note 'CREATE FUNCTION': create-function. to re-create the
     UDF with a different nonconflicting name.  Then modify any affected
     code to use the new name.

   * If a new version of MySQL implements a built-in function with the
     same name as an existing stored function, you have two choices:
     Rename the stored function to use a nonconflicting name, or change
     calls to the function so that they use a schema qualifier (that is,
     use 'SCHEMA_NAME.FUNC_NAME()' syntax).  In either case, modify any
     affected code accordingly.


File: manual.info.tmp,  Node: keywords,  Next: user-variables,  Prev: identifiers,  Up: language-structure

9.3 Keywords and Reserved Words
===============================

Keywords are words that have significance in SQL. Certain keywords, such
as *note 'SELECT': select, *note 'DELETE': delete, or *note 'BIGINT':
integer-types, are reserved and require special treatment for use as
identifiers such as table and column names.  This may also be true for
the names of built-in functions.

Nonreserved keywords are permitted as identifiers without quoting.
Reserved words are permitted as identifiers if you quote them as
described in *note identifiers:::

     mysql> CREATE TABLE interval (begin INT, end INT);
     ERROR 1064 (42000): You have an error in your SQL syntax ...
     near 'interval (begin INT, end INT)'

'BEGIN' and 'END' are keywords but not reserved, so their use as
identifiers does not require quoting.  'INTERVAL' is a reserved keyword
and must be quoted to be used as an identifier:

     mysql> CREATE TABLE `interval` (begin INT, end INT);
     Query OK, 0 rows affected (0.01 sec)

Exception: A word that follows a period in a qualified name must be an
identifier, so it need not be quoted even if it is reserved:

     mysql> CREATE TABLE mydb.interval (begin INT, end INT);
     Query OK, 0 rows affected (0.01 sec)

Names of built-in functions are permitted as identifiers but may require
care to be used as such.  For example, 'COUNT' is acceptable as a column
name.  However, by default, no whitespace is permitted in function
invocations between the function name and the following '(' character.
This requirement enables the parser to distinguish whether the name is
used in a function call or in nonfunction context.  For further details
on recognition of function names, see *note function-resolution::.

   * *note keywords-in-current-series::

   * *note keywords-new-in-current-series::

   * *note keywords-removed-in-current-series::

*MySQL 5.5 Keywords and Reserved Words*

The following list shows the keywords and reserved words in MySQL 5.5,
along with changes to individual words from version to version.
Reserved keywords are marked with (R). In addition, '_FILENAME' is
reserved.

At some point, you might upgrade to a higher version, so it is a good
idea to have a look at future reserved words, too.  You can find these
in the manuals that cover higher versions of MySQL. Most of the reserved
words in the list are forbidden by standard SQL as column or table names
(for example, 'GROUP').  A few are reserved because MySQL needs them and
uses a 'yacc' parser.

A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R |
S | T | U | V | W | X | Y | Z

A

   * 'ACCESSIBLE' (R)

   * 'ACTION'

   * 'ADD' (R)

   * 'AFTER'

   * 'AGAINST'

   * 'AGGREGATE'

   * 'ALGORITHM'

   * 'ALL' (R)

   * 'ALTER' (R)

   * 'ANALYZE' (R)

   * 'AND' (R)

   * 'ANY'

   * 'AS' (R)

   * 'ASC' (R)

   * 'ASCII'

   * 'ASENSITIVE' (R)

   * 'AT'

   * 'AUTHORS'

   * 'AUTOEXTEND_SIZE'

   * 'AUTO_INCREMENT'

   * 'AVG'

   * 'AVG_ROW_LENGTH'

B

   * 'BACKUP'

   * 'BEFORE' (R)

   * 'BEGIN'

   * 'BETWEEN' (R)

   * 'BIGINT' (R)

   * 'BINARY' (R)

   * 'BINLOG'

   * 'BIT'

   * 'BLOB' (R)

   * 'BLOCK'

   * 'BOOL'

   * 'BOOLEAN'

   * 'BOTH' (R)

   * 'BTREE'

   * 'BY' (R)

   * 'BYTE'

C

   * 'CACHE'

   * 'CALL' (R)

   * 'CASCADE' (R)

   * 'CASCADED'

   * 'CASE' (R)

   * 'CATALOG_NAME'

   * 'CHAIN'

   * 'CHANGE' (R)

   * 'CHANGED'

   * 'CHAR' (R)

   * 'CHARACTER' (R)

   * 'CHARSET'

   * 'CHECK' (R)

   * 'CHECKSUM'

   * 'CIPHER'

   * 'CLASS_ORIGIN'

   * 'CLIENT'

   * 'CLOSE'

   * 'COALESCE'

   * 'CODE'

   * 'COLLATE' (R)

   * 'COLLATION'

   * 'COLUMN' (R)

   * 'COLUMNS'

   * 'COLUMN_NAME'

   * 'COMMENT'

   * 'COMMIT'

   * 'COMMITTED'

   * 'COMPACT'

   * 'COMPLETION'

   * 'COMPRESSED'

   * 'CONCURRENT'

   * 'CONDITION' (R)

   * 'CONNECTION'

   * 'CONSISTENT'

   * 'CONSTRAINT' (R)

   * 'CONSTRAINT_CATALOG'

   * 'CONSTRAINT_NAME'

   * 'CONSTRAINT_SCHEMA'

   * 'CONTAINS'

   * 'CONTEXT'

   * 'CONTINUE' (R)

   * 'CONTRIBUTORS'

   * 'CONVERT' (R)

   * 'CPU'

   * 'CREATE' (R)

   * 'CROSS' (R)

   * 'CUBE'

   * 'CURRENT_DATE' (R)

   * 'CURRENT_TIME' (R)

   * 'CURRENT_TIMESTAMP' (R)

   * 'CURRENT_USER' (R)

   * 'CURSOR' (R)

   * 'CURSOR_NAME'

D

   * 'DATA'

   * 'DATABASE' (R)

   * 'DATABASES' (R)

   * 'DATAFILE'

   * 'DATE'

   * 'DATETIME'

   * 'DAY'

   * 'DAY_HOUR' (R)

   * 'DAY_MICROSECOND' (R)

   * 'DAY_MINUTE' (R)

   * 'DAY_SECOND' (R)

   * 'DEALLOCATE'

   * 'DEC' (R)

   * 'DECIMAL' (R)

   * 'DECLARE' (R)

   * 'DEFAULT' (R)

   * 'DEFINER'

   * 'DELAYED' (R)

   * 'DELAY_KEY_WRITE'

   * 'DELETE' (R)

   * 'DESC' (R)

   * 'DESCRIBE' (R)

   * 'DES_KEY_FILE'

   * 'DETERMINISTIC' (R)

   * 'DIRECTORY'

   * 'DISABLE'

   * 'DISCARD'

   * 'DISK'

   * 'DISTINCT' (R)

   * 'DISTINCTROW' (R)

   * 'DIV' (R)

   * 'DO'

   * 'DOUBLE' (R)

   * 'DROP' (R)

   * 'DUAL' (R)

   * 'DUMPFILE'

   * 'DUPLICATE'

   * 'DYNAMIC'

E

   * 'EACH' (R)

   * 'ELSE' (R)

   * 'ELSEIF' (R)

   * 'ENABLE'

   * 'ENCLOSED' (R)

   * 'END'

   * 'ENDS'

   * 'ENGINE'

   * 'ENGINES'

   * 'ENUM'

   * 'ERROR'; added in 5.5.3 (nonreserved)

   * 'ERRORS'

   * 'ESCAPE'

   * 'ESCAPED' (R)

   * 'EVENT'

   * 'EVENTS'

   * 'EVERY'

   * 'EXECUTE'

   * 'EXISTS' (R)

   * 'EXIT' (R)

   * 'EXPANSION'

   * 'EXPLAIN' (R)

   * 'EXTENDED'

   * 'EXTENT_SIZE'

F

   * 'FALSE' (R)

   * 'FAST'

   * 'FAULTS'

   * 'FETCH' (R)

   * 'FIELDS'

   * 'FILE'

   * 'FIRST'

   * 'FIXED'

   * 'FLOAT' (R)

   * 'FLOAT4' (R)

   * 'FLOAT8' (R)

   * 'FLUSH'

   * 'FOR' (R)

   * 'FORCE' (R)

   * 'FOREIGN' (R)

   * 'FOUND'

   * 'FRAC_SECOND'; removed in 5.5.3

   * 'FROM' (R)

   * 'FULL'

   * 'FULLTEXT' (R)

   * 'FUNCTION'

G

   * 'GENERAL'; added in 5.5.3 (reserved); became nonreserved in 5.5.8

   * 'GEOMETRY'

   * 'GEOMETRYCOLLECTION'

   * 'GET_FORMAT'

   * 'GLOBAL'

   * 'GRANT' (R)

   * 'GRANTS'

   * 'GROUP' (R)

H

   * 'HANDLER'

   * 'HASH'

   * 'HAVING' (R)

   * 'HELP'

   * 'HIGH_PRIORITY' (R)

   * 'HOST'

   * 'HOSTS'

   * 'HOUR'

   * 'HOUR_MICROSECOND' (R)

   * 'HOUR_MINUTE' (R)

   * 'HOUR_SECOND' (R)

I

   * 'IDENTIFIED'

   * 'IF' (R)

   * 'IGNORE' (R)

   * 'IGNORE_SERVER_IDS'; became nonreserved in 5.5.8

   * 'IMPORT'

   * 'IN' (R)

   * 'INDEX' (R)

   * 'INDEXES'

   * 'INFILE' (R)

   * 'INITIAL_SIZE'

   * 'INNER' (R)

   * 'INNOBASE'; removed in 5.5.3

   * 'INNODB'; removed in 5.5.3

   * 'INOUT' (R)

   * 'INSENSITIVE' (R)

   * 'INSERT' (R)

   * 'INSERT_METHOD'

   * 'INSTALL'

   * 'INT' (R)

   * 'INT1' (R)

   * 'INT2' (R)

   * 'INT3' (R)

   * 'INT4' (R)

   * 'INT8' (R)

   * 'INTEGER' (R)

   * 'INTERVAL' (R)

   * 'INTO' (R)

   * 'INVOKER'

   * 'IO'

   * 'IO_THREAD'

   * 'IPC'

   * 'IS' (R)

   * 'ISOLATION'

   * 'ISSUER'

   * 'ITERATE' (R)

J

   * 'JOIN' (R)

K

   * 'KEY' (R)

   * 'KEYS' (R)

   * 'KEY_BLOCK_SIZE'

   * 'KILL' (R)

L

   * 'LANGUAGE'

   * 'LAST'

   * 'LEADING' (R)

   * 'LEAVE' (R)

   * 'LEAVES'

   * 'LEFT' (R)

   * 'LESS'

   * 'LEVEL'

   * 'LIKE' (R)

   * 'LIMIT' (R)

   * 'LINEAR' (R)

   * 'LINES' (R)

   * 'LINESTRING'

   * 'LIST'

   * 'LOAD' (R)

   * 'LOCAL'

   * 'LOCALTIME' (R)

   * 'LOCALTIMESTAMP' (R)

   * 'LOCK' (R)

   * 'LOCKS'

   * 'LOGFILE'

   * 'LOGS'

   * 'LONG' (R)

   * 'LONGBLOB' (R)

   * 'LONGTEXT' (R)

   * 'LOOP' (R)

   * 'LOW_PRIORITY' (R)

M

   * 'MASTER'

   * 'MASTER_CONNECT_RETRY'

   * 'MASTER_HEARTBEAT_PERIOD'; became nonreserved in 5.5.8

   * 'MASTER_HOST'

   * 'MASTER_LOG_FILE'

   * 'MASTER_LOG_POS'

   * 'MASTER_PASSWORD'

   * 'MASTER_PORT'

   * 'MASTER_SERVER_ID'

   * 'MASTER_SSL'

   * 'MASTER_SSL_CA'

   * 'MASTER_SSL_CAPATH'

   * 'MASTER_SSL_CERT'

   * 'MASTER_SSL_CIPHER'

   * 'MASTER_SSL_KEY'

   * 'MASTER_SSL_VERIFY_SERVER_CERT' (R)

   * 'MASTER_USER'

   * 'MATCH' (R)

   * 'MAXVALUE' (R)

   * 'MAX_CONNECTIONS_PER_HOUR'

   * 'MAX_QUERIES_PER_HOUR'

   * 'MAX_ROWS'

   * 'MAX_SIZE'

   * 'MAX_UPDATES_PER_HOUR'

   * 'MAX_USER_CONNECTIONS'

   * 'MEDIUM'

   * 'MEDIUMBLOB' (R)

   * 'MEDIUMINT' (R)

   * 'MEDIUMTEXT' (R)

   * 'MEMORY'

   * 'MERGE'

   * 'MESSAGE_TEXT'

   * 'MICROSECOND'

   * 'MIDDLEINT' (R)

   * 'MIGRATE'

   * 'MINUTE'

   * 'MINUTE_MICROSECOND' (R)

   * 'MINUTE_SECOND' (R)

   * 'MIN_ROWS'

   * 'MOD' (R)

   * 'MODE'

   * 'MODIFIES' (R)

   * 'MODIFY'

   * 'MONTH'

   * 'MULTILINESTRING'

   * 'MULTIPOINT'

   * 'MULTIPOLYGON'

   * 'MUTEX'

   * 'MYSQL_ERRNO'

N

   * 'NAME'

   * 'NAMES'

   * 'NATIONAL'

   * 'NATURAL' (R)

   * 'NCHAR'

   * 'NDB'

   * 'NDBCLUSTER'

   * 'NEW'

   * 'NEXT'

   * 'NO'

   * 'NODEGROUP'

   * 'NONE'

   * 'NOT' (R)

   * 'NO_WAIT'

   * 'NO_WRITE_TO_BINLOG' (R)

   * 'NULL' (R)

   * 'NUMERIC' (R)

   * 'NVARCHAR'

O

   * 'OFFSET'

   * 'OLD_PASSWORD'

   * 'ON' (R)

   * 'ONE'

   * 'ONE_SHOT'

   * 'OPEN'

   * 'OPTIMIZE' (R)

   * 'OPTION' (R)

   * 'OPTIONALLY' (R)

   * 'OPTIONS'

   * 'OR' (R)

   * 'ORDER' (R)

   * 'OUT' (R)

   * 'OUTER' (R)

   * 'OUTFILE' (R)

   * 'OWNER'

P

   * 'PACK_KEYS'

   * 'PAGE'

   * 'PARSER'

   * 'PARTIAL'

   * 'PARTITION'

   * 'PARTITIONING'

   * 'PARTITIONS'

   * 'PASSWORD'

   * 'PHASE'

   * 'PLUGIN'

   * 'PLUGINS'

   * 'POINT'

   * 'POLYGON'

   * 'PORT'

   * 'PRECISION' (R)

   * 'PREPARE'

   * 'PRESERVE'

   * 'PREV'

   * 'PRIMARY' (R)

   * 'PRIVILEGES'

   * 'PROCEDURE' (R)

   * 'PROCESSLIST'

   * 'PROFILE'

   * 'PROFILES'

   * 'PROXY'; added in 5.5.7 (nonreserved)

   * 'PURGE' (R)

Q

   * 'QUARTER'

   * 'QUERY'

   * 'QUICK'

R

   * 'RANGE' (R)

   * 'READ' (R)

   * 'READS' (R)

   * 'READ_ONLY'

   * 'READ_WRITE' (R)

   * 'REAL' (R)

   * 'REBUILD'

   * 'RECOVER'

   * 'REDOFILE'

   * 'REDO_BUFFER_SIZE'

   * 'REDUNDANT'

   * 'REFERENCES' (R)

   * 'REGEXP' (R)

   * 'RELAY'; added in 5.5.3 (nonreserved)

   * 'RELAYLOG'

   * 'RELAY_LOG_FILE'

   * 'RELAY_LOG_POS'

   * 'RELAY_THREAD'

   * 'RELEASE' (R)

   * 'RELOAD'

   * 'REMOVE'

   * 'RENAME' (R)

   * 'REORGANIZE'

   * 'REPAIR'

   * 'REPEAT' (R)

   * 'REPEATABLE'

   * 'REPLACE' (R)

   * 'REPLICATION'

   * 'REQUIRE' (R)

   * 'RESET'

   * 'RESIGNAL' (R)

   * 'RESTORE'

   * 'RESTRICT' (R)

   * 'RESUME'

   * 'RETURN' (R)

   * 'RETURNS'

   * 'REVOKE' (R)

   * 'RIGHT' (R)

   * 'RLIKE' (R)

   * 'ROLLBACK'

   * 'ROLLUP'

   * 'ROUTINE'

   * 'ROW'

   * 'ROWS'

   * 'ROW_FORMAT'

   * 'RTREE'

S

   * 'SAVEPOINT'

   * 'SCHEDULE'

   * 'SCHEMA' (R)

   * 'SCHEMAS' (R)

   * 'SCHEMA_NAME'

   * 'SECOND'

   * 'SECOND_MICROSECOND' (R)

   * 'SECURITY'

   * 'SELECT' (R)

   * 'SENSITIVE' (R)

   * 'SEPARATOR' (R)

   * 'SERIAL'

   * 'SERIALIZABLE'

   * 'SERVER'

   * 'SESSION'

   * 'SET' (R)

   * 'SHARE'

   * 'SHOW' (R)

   * 'SHUTDOWN'

   * 'SIGNAL' (R)

   * 'SIGNED'

   * 'SIMPLE'

   * 'SLAVE'

   * 'SLOW'; added in 5.5.3 (reserved); became nonreserved in 5.5.8

   * 'SMALLINT' (R)

   * 'SNAPSHOT'

   * 'SOCKET'

   * 'SOME'

   * 'SONAME'

   * 'SOUNDS'

   * 'SOURCE'

   * 'SPATIAL' (R)

   * 'SPECIFIC' (R)

   * 'SQL' (R)

   * 'SQLEXCEPTION' (R)

   * 'SQLSTATE' (R)

   * 'SQLWARNING' (R)

   * 'SQL_BIG_RESULT' (R)

   * 'SQL_BUFFER_RESULT'

   * 'SQL_CACHE'

   * 'SQL_CALC_FOUND_ROWS' (R)

   * 'SQL_NO_CACHE'

   * 'SQL_SMALL_RESULT' (R)

   * 'SQL_THREAD'

   * 'SQL_TSI_DAY'

   * 'SQL_TSI_FRAC_SECOND'; removed in 5.5.3

   * 'SQL_TSI_HOUR'

   * 'SQL_TSI_MINUTE'

   * 'SQL_TSI_MONTH'

   * 'SQL_TSI_QUARTER'

   * 'SQL_TSI_SECOND'

   * 'SQL_TSI_WEEK'

   * 'SQL_TSI_YEAR'

   * 'SSL' (R)

   * 'START'

   * 'STARTING' (R)

   * 'STARTS'

   * 'STATUS'

   * 'STOP'

   * 'STORAGE'

   * 'STRAIGHT_JOIN' (R)

   * 'STRING'

   * 'SUBCLASS_ORIGIN'

   * 'SUBJECT'

   * 'SUBPARTITION'

   * 'SUBPARTITIONS'

   * 'SUPER'

   * 'SUSPEND'

   * 'SWAPS'

   * 'SWITCHES'

T

   * 'TABLE' (R)

   * 'TABLES'

   * 'TABLESPACE'

   * 'TABLE_CHECKSUM'

   * 'TABLE_NAME'

   * 'TEMPORARY'

   * 'TEMPTABLE'

   * 'TERMINATED' (R)

   * 'TEXT'

   * 'THAN'

   * 'THEN' (R)

   * 'TIME'

   * 'TIMESTAMP'

   * 'TIMESTAMPADD'

   * 'TIMESTAMPDIFF'

   * 'TINYBLOB' (R)

   * 'TINYINT' (R)

   * 'TINYTEXT' (R)

   * 'TO' (R)

   * 'TRAILING' (R)

   * 'TRANSACTION'

   * 'TRIGGER' (R)

   * 'TRIGGERS'

   * 'TRUE' (R)

   * 'TRUNCATE'

   * 'TYPE'

   * 'TYPES'

U

   * 'UNCOMMITTED'

   * 'UNDEFINED'

   * 'UNDO' (R)

   * 'UNDOFILE'

   * 'UNDO_BUFFER_SIZE'

   * 'UNICODE'

   * 'UNINSTALL'

   * 'UNION' (R)

   * 'UNIQUE' (R)

   * 'UNKNOWN'

   * 'UNLOCK' (R)

   * 'UNSIGNED' (R)

   * 'UNTIL'

   * 'UPDATE' (R)

   * 'UPGRADE'

   * 'USAGE' (R)

   * 'USE' (R)

   * 'USER'

   * 'USER_RESOURCES'

   * 'USE_FRM'

   * 'USING' (R)

   * 'UTC_DATE' (R)

   * 'UTC_TIME' (R)

   * 'UTC_TIMESTAMP' (R)

V

   * 'VALUE'

   * 'VALUES' (R)

   * 'VARBINARY' (R)

   * 'VARCHAR' (R)

   * 'VARCHARACTER' (R)

   * 'VARIABLES'

   * 'VARYING' (R)

   * 'VIEW'

W

   * 'WAIT'

   * 'WARNINGS'

   * 'WEEK'

   * 'WHEN' (R)

   * 'WHERE' (R)

   * 'WHILE' (R)

   * 'WITH' (R)

   * 'WORK'

   * 'WRAPPER'

   * 'WRITE' (R)

X

   * 'X509'

   * 'XA'

   * 'XML'

   * 'XOR' (R)

Y

   * 'YEAR'

   * 'YEAR_MONTH' (R)

Z

   * 'ZEROFILL' (R)

*MySQL 5.5 New Keywords and Reserved Words*

The following list shows the keywords and reserved words that are added
in MySQL 5.5, compared to MySQL 5.1.  Reserved keywords are marked with
(R).

C | E | G | I | M | P | R | S | T | X

C

   * 'CATALOG_NAME'

   * 'CLASS_ORIGIN'

   * 'COLUMN_NAME'

   * 'CONSTRAINT_CATALOG'

   * 'CONSTRAINT_NAME'

   * 'CONSTRAINT_SCHEMA'

   * 'CURSOR_NAME'

E

   * 'ERROR'

G

   * 'GENERAL'

I

   * 'IGNORE_SERVER_IDS'

M

   * 'MASTER_HEARTBEAT_PERIOD'

   * 'MESSAGE_TEXT'

   * 'MYSQL_ERRNO'

P

   * 'PROXY'

R

   * 'RELAY'

   * 'RELAYLOG'

   * 'RESIGNAL' (R)

S

   * 'SCHEMA_NAME'

   * 'SIGNAL' (R)

   * 'SLOW'

   * 'SUBCLASS_ORIGIN'

T

   * 'TABLE_NAME'

X

   * 'XML'

*MySQL 5.5 Removed Keywords and Reserved Words*

The following list shows the keywords and reserved words that are
removed in MySQL 5.5, compared to MySQL 5.1.  Reserved keywords are
marked with (R).

   * 'FRAC_SECOND'

   * 'INNOBASE'

   * 'INNODB'

   * 'SQL_TSI_FRAC_SECOND'


File: manual.info.tmp,  Node: user-variables,  Next: expressions,  Prev: keywords,  Up: language-structure

9.4 User-Defined Variables
==========================

You can store a value in a user-defined variable in one statement and
refer to it later in another statement.  This enables you to pass values
from one statement to another.

User variables are written as '@VAR_NAME', where the variable name
VAR_NAME consists of alphanumeric characters, '.', '_', and '$'.  A user
variable name can contain other characters if you quote it as a string
or identifier (for example, '@'my-var'', '@"my-var"', or '@`my-var`').

User-defined variables are session specific.  A user variable defined by
one client cannot be seen or used by other clients.  All variables for a
given client session are automatically freed when that client exits.

User variable names are not case-sensitive.

One way to set a user-defined variable is by issuing a *note 'SET':
set-variable. statement:

     SET @VAR_NAME = EXPR [, @VAR_NAME = EXPR] ...

For *note 'SET': set-variable, either '=' or ':=' can be used as the
assignment operator.

User variables can be assigned a value from a limited set of data types:
integer, decimal, floating-point, binary or nonbinary string, or 'NULL'
value.  Assignment of decimal and real values does not preserve the
precision or scale of the value.  A value of a type other than one of
the permissible types is converted to a permissible type.  For example,
a value having a temporal or spatial data type is converted to a binary
string.

If a user variable is assigned a nonbinary (character) string value, it
has the same character set and collation as the string.  The
coercibility of user variables is implicit.  (This is the same
coercibility as for table column values.)

Hexadecimal or bit values assigned to user variables are treated as
binary strings.  To assign a hexadecimal or bit value as a number to a
user variable, use it in numeric context.  For example, add 0 or use
'CAST(... AS UNSIGNED)':

     mysql> SET @v1 = X'41';
     mysql> SET @v2 = X'41'+0;
     mysql> SET @v3 = CAST(X'41' AS UNSIGNED);
     mysql> SELECT @v1, @v2, @v3;
     +------+------+------+
     | @v1  | @v2  | @v3  |
     +------+------+------+
     | A    |   65 |   65 |
     +------+------+------+
     mysql> SET @v1 = b'1000001';
     mysql> SET @v2 = b'1000001'+0;
     mysql> SET @v3 = CAST(b'1000001' AS UNSIGNED);
     mysql> SELECT @v1, @v2, @v3;
     +------+------+------+
     | @v1  | @v2  | @v3  |
     +------+------+------+
     | A    |   65 |   65 |
     +------+------+------+

If the value of a user variable is selected in a result set, it is
returned to the client as a string.

If you refer to a variable that has not been initialized, it has a value
of 'NULL' and a type of string.

User variables may be used in most contexts where expressions are
permitted.  This does not currently include contexts that explicitly
require a literal value, such as in the 'LIMIT' clause of a *note
'SELECT': select. statement, or the 'IGNORE N LINES' clause of a *note
'LOAD DATA': load-data. statement.

You can also assign a value to a user variable in statements other than
*note 'SET': set-variable.  In this case, the assignment operator must
be ':=' and not '=' because the latter is treated as the comparison
operator '=' in statements other than *note 'SET': set-variable.:

     mysql> SET @t1=1, @t2=2, @t3:=4;
     mysql> SELECT @t1, @t2, @t3, @t4 := @t1+@t2+@t3;
     +------+------+------+--------------------+
     | @t1  | @t2  | @t3  | @t4 := @t1+@t2+@t3 |
     +------+------+------+--------------------+
     |    1 |    2 |    4 |                  7 |
     +------+------+------+--------------------+

As a general rule, other than in *note 'SET': set-variable. statements,
you should never assign a value to a user variable and read the value
within the same statement.  For example, to increment a variable, this
is okay:

     SET @a = @a + 1;

For other statements, such as *note 'SELECT': select, you might get the
results you expect, but this is not guaranteed.  In the following
statement, you might think that MySQL will evaluate '@a' first and then
do an assignment second:

     SELECT @a, @a:=@a+1, ...;

However, the order of evaluation for expressions involving user
variables is undefined.

Another issue with assigning a value to a variable and reading the value
within the same non-*note 'SET': set-variable. statement is that the
default result type of a variable is based on its type at the start of
the statement.  The following example illustrates this:

     mysql> SET @a='test';
     mysql> SELECT @a,(@a:=20) FROM TBL_NAME;

For this *note 'SELECT': select. statement, MySQL reports to the client
that column one is a string and converts all accesses of '@a' to
strings, even though @a is set to a number for the second row.  After
the *note 'SELECT': select. statement executes, '@a' is regarded as a
number for the next statement.

To avoid problems with this behavior, either do not assign a value to
and read the value of the same variable within a single statement, or
else set the variable to '0', '0.0', or '''' to define its type before
you use it.

In a *note 'SELECT': select. statement, each select expression is
evaluated only when sent to the client.  This means that in a 'HAVING',
'GROUP BY', or 'ORDER BY' clause, referring to a variable that is
assigned a value in the select expression list does _not_ work as
expected:

     mysql> SELECT (@aa:=id) AS a, (@aa+3) AS b FROM TBL_NAME HAVING b=5;

The reference to 'b' in the 'HAVING' clause refers to an alias for an
expression in the select list that uses '@aa'.  This does not work as
expected: '@aa' contains the value of 'id' from the previous selected
row, not from the current row.

User variables are intended to provide data values.  They cannot be used
directly in an SQL statement as an identifier or as part of an
identifier, such as in contexts where a table or database name is
expected, or as a reserved word such as *note 'SELECT': select.  This is
true even if the variable is quoted, as shown in the following example:

     mysql> SELECT c1 FROM t;
     +----+
     | c1 |
     +----+
     |  0 |
     +----+
     |  1 |
     +----+
     2 rows in set (0.00 sec)

     mysql> SET @col = "c1";
     Query OK, 0 rows affected (0.00 sec)

     mysql> SELECT @col FROM t;
     +------+
     | @col |
     +------+
     | c1   |
     +------+
     1 row in set (0.00 sec)

     mysql> SELECT `@col` FROM t;
     ERROR 1054 (42S22): Unknown column '@col' in 'field list'

     mysql> SET @col = "`c1`";
     Query OK, 0 rows affected (0.00 sec)

     mysql> SELECT @col FROM t;
     +------+
     | @col |
     +------+
     | `c1` |
     +------+
     1 row in set (0.00 sec)

An exception to this principle that user variables cannot be used to
provide identifiers, is when you are constructing a string for use as a
prepared statement to execute later.  In this case, user variables can
be used to provide any part of the statement.  The following example
illustrates how this can be done:

     mysql> SET @c = "c1";
     Query OK, 0 rows affected (0.00 sec)

     mysql> SET @s = CONCAT("SELECT ", @c, " FROM t");
     Query OK, 0 rows affected (0.00 sec)

     mysql> PREPARE stmt FROM @s;
     Query OK, 0 rows affected (0.04 sec)
     Statement prepared

     mysql> EXECUTE stmt;
     +----+
     | c1 |
     +----+
     |  0 |
     +----+
     |  1 |
     +----+
     2 rows in set (0.00 sec)

     mysql> DEALLOCATE PREPARE stmt;
     Query OK, 0 rows affected (0.00 sec)

See *note sql-prepared-statements::, for more information.

A similar technique can be used in application programs to construct SQL
statements using program variables, as shown here using PHP 5:

     <?php
       $mysqli = new mysqli("localhost", "user", "pass", "test");

       if( mysqli_connect_errno() )
         die("Connection failed: %s\n", mysqli_connect_error());

       $col = "c1";

       $query = "SELECT $col FROM t";

       $result = $mysqli->query($query);

       while($row = $result->fetch_assoc())
       {
         echo "<p>" . $row["$col"] . "</p>\n";
       }

       $result->close();

       $mysqli->close();
     ?>

Assembling an SQL statement in this fashion is sometimes known as
'Dynamic SQL'.


File: manual.info.tmp,  Node: expressions,  Next: comments,  Prev: user-variables,  Up: language-structure

9.5 Expressions
===============

This section lists the grammar rules that expressions must follow in
MySQL and provides additional information about the types of terms that
may appear in expressions.

   * *note expression-syntax::

   * *note expression-term-notes::

   * *note temporal-intervals::

*Expression Syntax*

The following grammar rules define expression syntax in MySQL. The
grammar shown here is based on that given in the 'sql/sql_yacc.yy' file
of MySQL source distributions.  For additional information about some of
the expression terms, see *note expression-term-notes::.

     EXPR:
         EXPR OR EXPR
       | EXPR || EXPR
       | EXPR XOR EXPR
       | EXPR AND EXPR
       | EXPR && EXPR
       | NOT EXPR
       | ! EXPR
       | BOOLEAN_PRIMARY IS [NOT] {TRUE | FALSE | UNKNOWN}
       | BOOLEAN_PRIMARY

     BOOLEAN_PRIMARY:
         BOOLEAN_PRIMARY IS [NOT] NULL
       | BOOLEAN_PRIMARY <=> PREDICATE
       | BOOLEAN_PRIMARY COMPARISON_OPERATOR PREDICATE
       | BOOLEAN_PRIMARY COMPARISON_OPERATOR {ALL | ANY} (SUBQUERY)
       | PREDICATE

     COMPARISON_OPERATOR: = | >= | > | <= | < | <> | !=

     PREDICATE:
         BIT_EXPR [NOT] IN (SUBQUERY)
       | BIT_EXPR [NOT] IN (EXPR [, EXPR] ...)
       | BIT_EXPR [NOT] BETWEEN BIT_EXPR AND PREDICATE
       | BIT_EXPR SOUNDS LIKE BIT_EXPR
       | BIT_EXPR [NOT] LIKE SIMPLE_EXPR [ESCAPE SIMPLE_EXPR]
       | BIT_EXPR [NOT] REGEXP BIT_EXPR
       | BIT_EXPR

     BIT_EXPR:
         BIT_EXPR | BIT_EXPR
       | BIT_EXPR & BIT_EXPR
       | BIT_EXPR << BIT_EXPR
       | BIT_EXPR >> BIT_EXPR
       | BIT_EXPR + BIT_EXPR
       | BIT_EXPR - BIT_EXPR
       | BIT_EXPR * BIT_EXPR
       | BIT_EXPR / BIT_EXPR
       | BIT_EXPR DIV BIT_EXPR
       | BIT_EXPR MOD BIT_EXPR
       | BIT_EXPR % BIT_EXPR
       | BIT_EXPR ^ BIT_EXPR
       | BIT_EXPR + INTERVAL_EXPR
       | BIT_EXPR - INTERVAL_EXPR
       | SIMPLE_EXPR

     SIMPLE_EXPR:
         LITERAL
       | IDENTIFIER
       | FUNCTION_CALL
       | SIMPLE_EXPR COLLATE COLLATION_NAME
       | PARAM_MARKER
       | VARIABLE
       | SIMPLE_EXPR || SIMPLE_EXPR
       | + SIMPLE_EXPR
       | - SIMPLE_EXPR
       | ~ SIMPLE_EXPR
       | ! SIMPLE_EXPR
       | BINARY SIMPLE_EXPR
       | (EXPR [, EXPR] ...)
       | ROW (EXPR, EXPR [, EXPR] ...)
       | (SUBQUERY)
       | EXISTS (SUBQUERY)
       | {IDENTIFIER EXPR}
       | MATCH_EXPR
       | CASE_EXPR
       | INTERVAL_EXPR

For operator precedence, see *note operator-precedence::.  The
precedence and meaning of some operators depends on the SQL mode:

   * By default, '||' is a logical 'OR' operator.  With
     'PIPES_AS_CONCAT' enabled, '||' is string concatenation, with a
     precedence between '^' and the unary operators.

   * By default, '!' has a higher precedence than 'NOT'.  With
     'HIGH_NOT_PRECEDENCE' enabled, '!' and 'NOT' have the same
     precedence.

See *note sql-mode::.

*Expression Term Notes*

For literal value syntax, see *note literals::.

For identifier syntax, see *note identifiers::.

Variables can be user variables, system variables, or stored program
local variables or parameters:

   * User variables: *note user-variables::

   * System variables: *note using-system-variables::

   * Stored program local variables: *note declare-local-variable::

   * Stored program parameters: *note create-procedure::

PARAM_MARKER is '?' as used in prepared statements for placeholders.
See *note prepare::.

'(SUBQUERY)' indicates a subquery that returns a single value; that is,
a scalar subquery.  See *note scalar-subqueries::.

'{IDENTIFIER EXPR}' is ODBC escape syntax and is accepted for ODBC
compatibility.  The value is EXPR.  The '{' and '}' curly braces in the
syntax should be written literally; they are not metasyntax as used
elsewhere in syntax descriptions.

MATCH_EXPR indicates a 'MATCH' expression.  See *note fulltext-search::.

CASE_EXPR indicates a 'CASE' expression.  See *note
control-flow-functions::.

INTERVAL_EXPR represents a temporal interval.  See *note
temporal-intervals::.

*Temporal Intervals*

INTERVAL_EXPR in expressions represents a temporal interval.  Intervals
have this syntax:

     INTERVAL EXPR UNIT

EXPR represents a quantity.  UNIT represents the unit for interpreting
the quantity; it is a specifier such as 'HOUR', 'DAY', or 'WEEK'.  The
'INTERVAL' keyword and the UNIT specifier are not case sensitive.

The following table shows the expected form of the EXPR argument for
each UNIT value.

*Temporal Interval Expression and Unit Arguments*

UNIT Value                           Expected EXPR Format
                                     
'MICROSECOND'                        'MICROSECONDS'
                                     
'SECOND'                             'SECONDS'
                                     
'MINUTE'                             'MINUTES'
                                     
'HOUR'                               'HOURS'
                                     
'DAY'                                'DAYS'
                                     
'WEEK'                               'WEEKS'
                                     
'MONTH'                              'MONTHS'
                                     
'QUARTER'                            'QUARTERS'
                                     
'YEAR'                               'YEARS'
                                     
'SECOND_MICROSECOND'                 ''SECONDS.MICROSECONDS''
                                     
'MINUTE_MICROSECOND'                 ''MINUTES:SECONDS.MICROSECONDS''
                                     
'MINUTE_SECOND'                      ''MINUTES:SECONDS''
                                     
'HOUR_MICROSECOND'                   ''HOURS:MINUTES:SECONDS.MICROSECONDS''
                                     
'HOUR_SECOND'                        ''HOURS:MINUTES:SECONDS''
                                     
'HOUR_MINUTE'                        ''HOURS:MINUTES''
                                     
'DAY_MICROSECOND'                    ''DAYS
                                     HOURS:MINUTES:SECONDS.MICROSECONDS''
                                     
'DAY_SECOND'                         ''DAYS HOURS:MINUTES:SECONDS''
                                     
'DAY_MINUTE'                         ''DAYS HOURS:MINUTES''
                                     
'DAY_HOUR'                           ''DAYS HOURS''
                                     
'YEAR_MONTH'                         ''YEARS-MONTHS''

MySQL permits any punctuation delimiter in the EXPR format.  Those shown
in the table are the suggested delimiters.

Temporal intervals are used for certain functions, such as 'DATE_ADD()'
and 'DATE_SUB()':

     mysql> SELECT DATE_ADD('2018-05-01',INTERVAL 1 DAY);
             -> '2018-05-02'
     mysql> SELECT DATE_SUB('2018-05-01',INTERVAL 1 YEAR);
             -> '2017-05-01'
     mysql> SELECT DATE_ADD('2020-12-31 23:59:59',
         ->                 INTERVAL 1 SECOND);
             -> '2021-01-01 00:00:00'
     mysql> SELECT DATE_ADD('2018-12-31 23:59:59',
         ->                 INTERVAL 1 DAY);
             -> '2019-01-01 23:59:59'
     mysql> SELECT DATE_ADD('2100-12-31 23:59:59',
         ->                 INTERVAL '1:1' MINUTE_SECOND);
             -> '2101-01-01 00:01:00'
     mysql> SELECT DATE_SUB('2025-01-01 00:00:00',
         ->                 INTERVAL '1 1:1:1' DAY_SECOND);
             -> '2024-12-30 22:58:59'
     mysql> SELECT DATE_ADD('1900-01-01 00:00:00',
         ->                 INTERVAL '-1 10' DAY_HOUR);
             -> '1899-12-30 14:00:00'
     mysql> SELECT DATE_SUB('1998-01-02', INTERVAL 31 DAY);
             -> '1997-12-02'
     mysql> SELECT DATE_ADD('1992-12-31 23:59:59.000002',
         ->            INTERVAL '1.999999' SECOND_MICROSECOND);
             -> '1993-01-01 00:00:01.000001'

Temporal arithmetic also can be performed in expressions using
'INTERVAL' together with the '+' or '-' operator:

     date + INTERVAL EXPR UNIT
     date - INTERVAL EXPR UNIT

'INTERVAL EXPR UNIT' is permitted on either side of the '+' operator if
the expression on the other side is a date or datetime value.  For the
'-' operator, 'INTERVAL EXPR UNIT' is permitted only on the right side,
because it makes no sense to subtract a date or datetime value from an
interval.

     mysql> SELECT '2018-12-31 23:59:59' + INTERVAL 1 SECOND;
             -> '2019-01-01 00:00:00'
     mysql> SELECT INTERVAL 1 DAY + '2018-12-31';
             -> '2019-01-01'
     mysql> SELECT '2025-01-01' - INTERVAL 1 SECOND;
             -> '2024-12-31 23:59:59'

The 'EXTRACT()' function uses the same kinds of UNIT specifiers as
'DATE_ADD()' or 'DATE_SUB()', but extracts parts from the date rather
than performing date arithmetic:

     mysql> SELECT EXTRACT(YEAR FROM '2019-07-02');
             -> 2019
     mysql> SELECT EXTRACT(YEAR_MONTH FROM '2019-07-02 01:02:03');
             -> 201907

Temporal intervals can be used in *note 'CREATE EVENT': create-event.
statements:

     CREATE EVENT myevent
         ON SCHEDULE AT CURRENT_TIMESTAMP + INTERVAL 1 HOUR
         DO
           UPDATE myschema.mytable SET mycol = mycol + 1;

If you specify an interval value that is too short (does not include all
the interval parts that would be expected from the UNIT keyword), MySQL
assumes that you have left out the leftmost parts of the interval value.
For example, if you specify a UNIT of 'DAY_SECOND', the value of EXPR is
expected to have days, hours, minutes, and seconds parts.  If you
specify a value like ''1:10'', MySQL assumes that the days and hours
parts are missing and the value represents minutes and seconds.  In
other words, ''1:10' DAY_SECOND' is interpreted in such a way that it is
equivalent to ''1:10' MINUTE_SECOND'.  This is analogous to the way that
MySQL interprets *note 'TIME': time. values as representing elapsed time
rather than as a time of day.

EXPR is treated as a string, so be careful if you specify a nonstring
value with 'INTERVAL'.  For example, with an interval specifier of
'HOUR_MINUTE', '6/4' is treated as 6 hours, four minutes, whereas '6/4'
evaluates to '1.5000' and is treated as 1 hour, 5000 minutes:

     mysql> SELECT '6/4', 6/4;
             -> 1.5000
     mysql> SELECT DATE_ADD('2019-01-01', INTERVAL '6/4' HOUR_MINUTE);
             -> '2019-01-01 06:04:00'
     mysql> SELECT DATE_ADD('2019-01-01', INTERVAL 6/4 HOUR_MINUTE);
             -> '2019-01-04 12:20:00'

To ensure interpretation of the interval value as you expect, a 'CAST()'
operation may be used.  To treat '6/4' as 1 hour, 5 minutes, cast it to
a *note 'DECIMAL': fixed-point-types. value with a single fractional
digit:

     mysql> SELECT CAST(6/4 AS DECIMAL(3,1));
             -> 1.5
     mysql> SELECT DATE_ADD('1970-01-01 12:00:00',
         ->                 INTERVAL CAST(6/4 AS DECIMAL(3,1)) HOUR_MINUTE);
             -> '1970-01-01 13:05:00'

If you add to or subtract from a date value something that contains a
time part, the result is automatically converted to a datetime value:

     mysql> SELECT DATE_ADD('2023-01-01', INTERVAL 1 DAY);
             -> '2023-01-02'
     mysql> SELECT DATE_ADD('2023-01-01', INTERVAL 1 HOUR);
             -> '2023-01-01 01:00:00'

If you add 'MONTH', 'YEAR_MONTH', or 'YEAR' and the resulting date has a
day that is larger than the maximum day for the new month, the day is
adjusted to the maximum days in the new month:

     mysql> SELECT DATE_ADD('2019-01-30', INTERVAL 1 MONTH);
             -> '2019-02-28'

Date arithmetic operations require complete dates and do not work with
incomplete dates such as ''2016-07-00'' or badly malformed dates:

     mysql> SELECT DATE_ADD('2016-07-00', INTERVAL 1 DAY);
             -> NULL
     mysql> SELECT '2005-03-32' + INTERVAL 1 MONTH;
             -> NULL


File: manual.info.tmp,  Node: comments,  Prev: expressions,  Up: language-structure

9.6 Comment Syntax
==================

MySQL Server supports three comment styles:

   * From a '#' character to the end of the line.

   * From a '-- ' sequence to the end of the line.  In MySQL, the '-- '
     (double-dash) comment style requires the second dash to be followed
     by at least one whitespace or control character (such as a space,
     tab, newline, and so on).  This syntax differs slightly from
     standard SQL comment syntax, as discussed in *note
     ansi-diff-comments::.

   * From a '/*' sequence to the following '*/' sequence, as in the C
     programming language.  This syntax enables a comment to extend over
     multiple lines because the beginning and closing sequences need not
     be on the same line.

The following example demonstrates all three comment styles:

     mysql> SELECT 1+1;     # This comment continues to the end of line
     mysql> SELECT 1+1;     -- This comment continues to the end of line
     mysql> SELECT 1 /* this is an in-line comment */ + 1;
     mysql> SELECT 1+
     /*
     this is a
     multiple-line comment
     */
     1;

Nested comments are not supported.  (Under some conditions, nested
comments might be permitted, but usually are not, and users should avoid
them.)

MySQL Server supports certain variants of C-style comments.  These
enable you to write code that includes MySQL extensions, but is still
portable, by using comments of the following form:

     /*! MYSQL-SPECIFIC CODE */

In this case, MySQL Server parses and executes the code within the
comment as it would any other SQL statement, but other SQL servers will
ignore the extensions.  For example, MySQL Server recognizes the
'STRAIGHT_JOIN' keyword in the following statement, but other servers
will not:

     SELECT /*! STRAIGHT_JOIN */ col1 FROM table1,table2 WHERE ...

If you add a version number after the '!' character, the syntax within
the comment is executed only if the MySQL version is greater than or
equal to the specified version number.  The 'KEY_BLOCK_SIZE' keyword in
the following comment is executed only by servers from MySQL 5.1.10 or
higher:

     CREATE TABLE t1(a INT, KEY (a)) /*!50110 KEY_BLOCK_SIZE=1024 */;

The comment syntax just described applies to how the *note 'mysqld':
mysqld. server parses SQL statements.  The *note 'mysql': mysql. client
program also performs some parsing of statements before sending them to
the server.  (It does this to determine statement boundaries within a
multiple-statement input line.)  For information about differences
between the server and *note 'mysql': mysql. client parsers, see *note
mysql-tips::.

Comments in '/*!12345 ... */' format are not stored on the server.  If
this format is used to comment stored programs, the comments are not
retained in the program body.

The use of short-form *note 'mysql': mysql. commands such as '\C' within
multiple-line '/* ... */' comments is not supported.


File: manual.info.tmp,  Node: charset,  Next: data-types,  Prev: language-structure,  Up: Top

10 Character Sets, Collations, Unicode
**************************************

* Menu:

* charset-general::              Character Sets and Collations in General
* charset-mysql::                Character Sets and Collations in MySQL
* charset-syntax::               Specifying Character Sets and Collations
* charset-connection::           Connection Character Sets and Collations
* charset-applications::         Configuring Application Character Set and Collation
* charset-errors::               Error Message Character Set
* charset-conversion::           Column Character Set Conversion
* charset-collations::           Collation Issues
* charset-unicode::              Unicode Support
* charset-charsets::             Supported Character Sets and Collations
* charset-restrictions::         Restrictions on Character Sets
* error-message-language::       Setting the Error Message Language
* adding-character-set::         Adding a Character Set
* adding-collation::             Adding a Collation to a Character Set
* charset-configuration::        Character Set Configuration
* locale-support::               MySQL Server Locale Support

MySQL includes character set support that enables you to store data
using a variety of character sets and perform comparisons according to a
variety of collations.  You can specify character sets at the server,
database, table, and column level.

This chapter discusses the following topics:

   * What are character sets and collations?

   * The multiple-level default system for character set assignment.

   * Syntax for specifying character sets and collations.

   * Affected functions and operations.

   * Unicode support.

   * The character sets and collations that are available, with notes.

   * Selecting the language for error messages.

   * Selecting the locale for day and month names.

Character set issues affect not only data storage, but also
communication between client programs and the MySQL server.  If you want
the client program to communicate with the server using a character set
different from the default, you'll need to indicate which one.  For
example, to use the 'utf8' Unicode character set, issue this statement
after connecting to the server:

     SET NAMES 'utf8';

For more information about configuring character sets for application
use and character set-related issues in client/server communication, see
*note charset-applications::, and *note charset-connection::.


File: manual.info.tmp,  Node: charset-general,  Next: charset-mysql,  Prev: charset,  Up: charset

10.1 Character Sets and Collations in General
=============================================

A _character set_ is a set of symbols and encodings.  A _collation_ is a
set of rules for comparing characters in a character set.  Let's make
the distinction clear with an example of an imaginary character set.

Suppose that we have an alphabet with four letters: 'A', 'B', 'a', 'b'.
We give each letter a number: 'A' = 0, 'B' = 1, 'a' = 2, 'b' = 3.  The
letter 'A' is a symbol, the number 0 is the _encoding_ for 'A', and the
combination of all four letters and their encodings is a _character
set_.

Suppose that we want to compare two string values, 'A' and 'B'.  The
simplest way to do this is to look at the encodings: 0 for 'A' and 1 for
'B'.  Because 0 is less than 1, we say 'A' is less than 'B'.  What we've
just done is apply a collation to our character set.  The collation is a
set of rules (only one rule in this case): 'compare the encodings.' We
call this simplest of all possible collations a _binary_ collation.

But what if we want to say that the lowercase and uppercase letters are
equivalent?  Then we would have at least two rules: (1) treat the
lowercase letters 'a' and 'b' as equivalent to 'A' and 'B'; (2) then
compare the encodings.  We call this a _case-insensitive_ collation.  It
is a little more complex than a binary collation.

In real life, most character sets have many characters: not just 'A' and
'B' but whole alphabets, sometimes multiple alphabets or eastern writing
systems with thousands of characters, along with many special symbols
and punctuation marks.  Also in real life, most collations have many
rules, not just for whether to distinguish lettercase, but also for
whether to distinguish accents (an 'accent' is a mark attached to a
character as in German 'O"'), and for multiple-character mappings (such
as the rule that 'O"' = 'OE' in one of the two German collations).

MySQL can do these things for you:

   * Store strings using a variety of character sets.

   * Compare strings using a variety of collations.

   * Mix strings with different character sets or collations in the same
     server, the same database, or even the same table.

   * Enable specification of character set and collation at any level.

To use these features effectively, you must know what character sets and
collations are available, how to change the defaults, and how they
affect the behavior of string operators and functions.


File: manual.info.tmp,  Node: charset-mysql,  Next: charset-syntax,  Prev: charset-general,  Up: charset

10.2 Character Sets and Collations in MySQL
===========================================

* Menu:

* charset-repertoire::           Character Set Repertoire
* charset-metadata::             UTF-8 for Metadata

MySQL Server supports multiple character sets.  To display the available
character sets, use the 'INFORMATION_SCHEMA' *note 'CHARACTER_SETS':
character-sets-table. table or the *note 'SHOW CHARACTER SET':
show-character-set. statement.  A partial listing follows.  For more
complete information, see *note charset-charsets::.

     mysql> SHOW CHARACTER SET;
     +----------+---------------------------------+---------------------+--------+
     | Charset  | Description                     | Default collation   | Maxlen |
     +----------+---------------------------------+---------------------+--------+
     | big5     | Big5 Traditional Chinese        | big5_chinese_ci     |      2 |
     ...
     | latin1   | cp1252 West European            | latin1_swedish_ci   |      1 |
     | latin2   | ISO 8859-2 Central European     | latin2_general_ci   |      1 |
     ...
     | utf8     | UTF-8 Unicode                   | utf8_general_ci     |      3 |
     | ucs2     | UCS-2 Unicode                   | ucs2_general_ci     |      2 |
     ...
     | utf8mb4  | UTF-8 Unicode                   | utf8mb4_general_ci  |      4 |
     ...
     | binary   | Binary pseudo charset           | binary              |      1 |
     ...

By default, the *note 'SHOW CHARACTER SET': show-character-set.
statement displays all available character sets.  It takes an optional
'LIKE' or 'WHERE' clause that indicates which character set names to
match.  For example:

     mysql> SHOW CHARACTER SET LIKE 'latin%';
     +---------+-----------------------------+-------------------+--------+
     | Charset | Description                 | Default collation | Maxlen |
     +---------+-----------------------------+-------------------+--------+
     | latin1  | cp1252 West European        | latin1_swedish_ci |      1 |
     | latin2  | ISO 8859-2 Central European | latin2_general_ci |      1 |
     | latin5  | ISO 8859-9 Turkish          | latin5_turkish_ci |      1 |
     | latin7  | ISO 8859-13 Baltic          | latin7_general_ci |      1 |
     +---------+-----------------------------+-------------------+--------+

A given character set always has at least one collation, and most
character sets have several.  To list the display collations for a
character set, use the 'INFORMATION_SCHEMA' *note 'COLLATIONS':
collations-table. table or the *note 'SHOW COLLATION': show-collation.
statement.

By default, the *note 'SHOW COLLATION': show-collation. statement
displays all available collations.  It takes an optional 'LIKE' or
'WHERE' clause that indicates which collation names to display.  For
example, to see the collations for the default character set, 'latin1'
(cp1252 West European), use this statement:

     mysql> SHOW COLLATION WHERE Charset = 'latin1';
     +-------------------+---------+----+---------+----------+---------+
     | Collation         | Charset | Id | Default | Compiled | Sortlen |
     +-------------------+---------+----+---------+----------+---------+
     | latin1_german1_ci | latin1  |  5 |         | Yes      |       1 |
     | latin1_swedish_ci | latin1  |  8 | Yes     | Yes      |       1 |
     | latin1_danish_ci  | latin1  | 15 |         | Yes      |       1 |
     | latin1_german2_ci | latin1  | 31 |         | Yes      |       2 |
     | latin1_bin        | latin1  | 47 |         | Yes      |       1 |
     | latin1_general_ci | latin1  | 48 |         | Yes      |       1 |
     | latin1_general_cs | latin1  | 49 |         | Yes      |       1 |
     | latin1_spanish_ci | latin1  | 94 |         | Yes      |       1 |
     +-------------------+---------+----+---------+----------+---------+

The 'latin1' collations have the following meanings.

Collation                     Meaning
                              
'latin1_bin'                  Binary according to 'latin1' encoding
                              
'latin1_danish_ci'            Danish/Norwegian
                              
'latin1_general_ci'           Multilingual (Western European)
                              
'latin1_general_cs'           Multilingual (ISO Western European),
                              case-sensitive
                              
'latin1_german1_ci'           German DIN-1 (dictionary order)
                              
'latin1_german2_ci'           German DIN-2 (phone book order)
                              
'latin1_spanish_ci'           Modern Spanish
                              
'latin1_swedish_ci'           Swedish/Finnish

Collations have these general characteristics:

   * Two different character sets cannot have the same collation.

   * Each character set has a _default collation_.  For example, the
     default collations for 'latin1' and 'utf8' are 'latin1_swedish_ci'
     and 'utf8_general_ci', respectively.  The 'INFORMATION_SCHEMA'
     *note 'CHARACTER_SETS': character-sets-table. table and the *note
     'SHOW CHARACTER SET': show-character-set. statement indicate the
     default collation for each character set.  The 'INFORMATION_SCHEMA'
     *note 'COLLATIONS': collations-table. table and the *note 'SHOW
     COLLATION': show-collation. statement have a column that indicates
     for each collation whether it is the default for its character set
     ('Yes' if so, empty if not).

   * Collation names start with the name of the character set with which
     they are associated, generally followed by one or more suffixes
     indicating other collation characteristics.  For additional
     information about naming conventions, see *note
     charset-collation-names::.

When a character set has multiple collations, it might not be clear
which collation is most suitable for a given application.  To avoid
choosing an inappropriate collation, perform some comparisons with
representative data values to make sure that a given collation sorts
values the way you expect.


File: manual.info.tmp,  Node: charset-repertoire,  Next: charset-metadata,  Prev: charset-mysql,  Up: charset-mysql

10.2.1 Character Set Repertoire
-------------------------------

The _repertoire_ of a character set is the collection of characters in
the set.

String expressions have a repertoire attribute, which can have two
values:

   * 'ASCII': The expression can contain only characters in the Unicode
     range 'U+0000' to 'U+007F'.

   * 'UNICODE': The expression can contain characters in the Unicode
     range 'U+0000' to 'U+10FFFF'.  This includes characters in the
     Basic Multilingual Plane (BMP) range ('U+0000' to 'U+FFFF') and
     supplementary characters outside the BMP range ('U+10000' to
     'U+10FFFF').

The 'ASCII' range is a subset of 'UNICODE' range, so a string with
'ASCII' repertoire can be converted safely without loss of information
to the character set of any string with 'UNICODE' repertoire or to a
character set that is a superset of 'ASCII'.  (All MySQL character sets
are supersets of 'ASCII' with the exception of 'swe7', which reuses some
punctuation characters for Swedish accented characters.)  The use of
repertoire enables character set conversion in expressions for many
cases where MySQL would otherwise return an 'illegal mix of collations'
error.

The following discussion provides examples of expressions and their
repertoires, and describes how the use of repertoire changes string
expression evaluation:

   * The repertoire for a string constant depends on string content and
     may differ from the repertoire of the string character set.
     Consider these statements:

          SET NAMES utf8; SELECT 'abc';
          SELECT _utf8'def';
          SELECT N'MySQL';

     Although the character set is 'utf8' in each of the preceding
     cases, the strings do not actually contain any characters outside
     the ASCII range, so their repertoire is 'ASCII' rather than
     'UNICODE'.

   * A column having the 'ascii' character set has 'ASCII' repertoire
     because of its character set.  In the following table, 'c1' has
     'ASCII' repertoire:

          CREATE TABLE t1 (c1 CHAR(1) CHARACTER SET ascii);

     The following example illustrates how repertoire enables a result
     to be determined in a case where an error occurs without
     repertoire:

          CREATE TABLE t1 (
            c1 CHAR(1) CHARACTER SET latin1,
            c2 CHAR(1) CHARACTER SET ascii
          );
          INSERT INTO t1 VALUES ('a','b');
          SELECT CONCAT(c1,c2) FROM t1;

     Without repertoire, this error occurs:

          ERROR 1267 (HY000): Illegal mix of collations (latin1_swedish_ci,IMPLICIT)
          and (ascii_general_ci,IMPLICIT) for operation 'concat'

     Using repertoire, subset to superset ('ascii' to 'latin1')
     conversion can occur and a result is returned:

          +---------------+
          | CONCAT(c1,c2) |
          +---------------+
          | ab            |
          +---------------+

   * Functions with one string argument inherit the repertoire of their
     argument.  The result of 'UPPER(_utf8'ABC')' has 'ASCII' repertoire
     because its argument has 'ASCII' repertoire.

   * For functions that return a string but do not have string arguments
     and use 'character_set_connection' as the result character set, the
     result repertoire is 'ASCII' if 'character_set_connection' is
     'ascii', and 'UNICODE' otherwise:

          FORMAT(NUMERIC_COLUMN, 4);

     Use of repertoire changes how MySQL evaluates the following
     example:

          SET NAMES ascii;
          CREATE TABLE t1 (a INT, b VARCHAR(10) CHARACTER SET latin1);
          INSERT INTO t1 VALUES (1,'b');
          SELECT CONCAT(FORMAT(a, 4), b) FROM t1;

     Without repertoire, this error occurs:

          ERROR 1267 (HY000): Illegal mix of collations (ascii_general_ci,COERCIBLE)
          and (latin1_swedish_ci,IMPLICIT) for operation 'concat'

     With repertoire, a result is returned:

          +-------------------------+
          | CONCAT(FORMAT(a, 4), b) |
          +-------------------------+
          | 1.0000b                 |
          +-------------------------+

   * Functions with two or more string arguments use the 'widest'
     argument repertoire for the result repertoire ('UNICODE' is wider
     than 'ASCII').  Consider the following 'CONCAT()' calls:

          CONCAT(_ucs2 X'0041', _ucs2 X'0042')
          CONCAT(_ucs2 X'0041', _ucs2 X'00C2')

     For the first call, the repertoire is 'ASCII' because both
     arguments are within the range of the 'ascii' character set.  For
     the second call, the repertoire is 'UNICODE' because the second
     argument is outside the 'ascii' character set range.

   * The repertoire for function return values is determined based only
     on the repertoire of the arguments that affect the result's
     character set and collation.

          IF(column1 < column2, 'smaller', 'greater')

     The result repertoire is 'ASCII' because the two string arguments
     (the second argument and the third argument) both have 'ASCII'
     repertoire.  The first argument does not matter for the result
     repertoire, even if the expression uses string values.


File: manual.info.tmp,  Node: charset-metadata,  Prev: charset-repertoire,  Up: charset-mysql

10.2.2 UTF-8 for Metadata
-------------------------

_Metadata_ is 'the data about the data.' Anything that _describes_ the
database--as opposed to being the _contents_ of the database--is
metadata.  Thus column names, database names, user names, version names,
and most of the string results from *note 'SHOW': show. are metadata.
This is also true of the contents of tables in 'INFORMATION_SCHEMA'
because those tables by definition contain information about database
objects.

Representation of metadata must satisfy these requirements:

   * All metadata must be in the same character set.  Otherwise, neither
     the *note 'SHOW': show. statements nor *note 'SELECT': select.
     statements for tables in 'INFORMATION_SCHEMA' would work properly
     because different rows in the same column of the results of these
     operations would be in different character sets.

   * Metadata must include all characters in all languages.  Otherwise,
     users would not be able to name columns and tables using their own
     languages.

To satisfy both requirements, MySQL stores metadata in a Unicode
character set, namely UTF-8.  This does not cause any disruption if you
never use accented or non-Latin characters.  But if you do, you should
be aware that metadata is in UTF-8.

The metadata requirements mean that the return values of the 'USER()',
'CURRENT_USER()', 'SESSION_USER()', 'SYSTEM_USER()', 'DATABASE()', and
'VERSION()' functions have the UTF-8 character set by default.

The server sets the 'character_set_system' system variable to the name
of the metadata character set:

     mysql> SHOW VARIABLES LIKE 'character_set_system';
     +----------------------+-------+
     | Variable_name        | Value |
     +----------------------+-------+
     | character_set_system | utf8  |
     +----------------------+-------+

Storage of metadata using Unicode does _not_ mean that the server
returns headers of columns and the results of *note 'DESCRIBE':
describe. functions in the 'character_set_system' character set by
default.  When you use 'SELECT column1 FROM t', the name 'column1'
itself is returned from the server to the client in the character set
determined by the value of the 'character_set_results' system variable,
which has a default value of 'utf8'.  If you want the server to pass
metadata results back in a different character set, use the *note 'SET
NAMES': set-names. statement to force the server to perform character
set conversion.  *note 'SET NAMES': set-names. sets the
'character_set_results' and other related system variables.  (See *note
charset-connection::.)  Alternatively, a client program can perform the
conversion after receiving the result from the server.  It is more
efficient for the client to perform the conversion, but this option is
not always available for all clients.

If 'character_set_results' is set to 'NULL', no conversion is performed
and the server returns metadata using its original character set (the
set indicated by 'character_set_system').

Error messages returned from the server to the client are converted to
the client character set automatically, as with metadata.

If you are using (for example) the 'USER()' function for comparison or
assignment within a single statement, don't worry.  MySQL performs some
automatic conversion for you.

     SELECT * FROM t1 WHERE USER() = latin1_column;

This works because the contents of 'latin1_column' are automatically
converted to UTF-8 before the comparison.

     INSERT INTO t1 (latin1_column) SELECT USER();

This works because the contents of 'USER()' are automatically converted
to 'latin1' before the assignment.

Although automatic conversion is not in the SQL standard, the standard
does say that every character set is (in terms of supported characters)
a 'subset' of Unicode.  Because it is a well-known principle that 'what
applies to a superset can apply to a subset,' we believe that a
collation for Unicode can apply for comparisons with non-Unicode
strings.  For more information about coercion of strings, see *note
charset-collation-coercibility::.


File: manual.info.tmp,  Node: charset-syntax,  Next: charset-connection,  Prev: charset-mysql,  Up: charset

10.3 Specifying Character Sets and Collations
=============================================

* Menu:

* charset-collation-names::      Collation Naming Conventions
* charset-server::               Server Character Set and Collation
* charset-database::             Database Character Set and Collation
* charset-table::                Table Character Set and Collation
* charset-column::               Column Character Set and Collation
* charset-literal::              Character String Literal Character Set and Collation
* charset-national::             The National Character Set
* charset-introducer::           Character Set Introducers
* charset-examples::             Examples of Character Set and Collation Assignment
* charset-compatibility::        Compatibility with Other DBMSs

There are default settings for character sets and collations at four
levels: server, database, table, and column.  The description in the
following sections may appear complex, but it has been found in practice
that multiple-level defaulting leads to natural and obvious results.

'CHARACTER SET' is used in clauses that specify a character set.
'CHARSET' can be used as a synonym for 'CHARACTER SET'.

Character set issues affect not only data storage, but also
communication between client programs and the MySQL server.  If you want
the client program to communicate with the server using a character set
different from the default, you'll need to indicate which one.  For
example, to use the 'utf8' Unicode character set, issue this statement
after connecting to the server:

     SET NAMES 'utf8';

For more information about character set-related issues in client/server
communication, see *note charset-connection::.


File: manual.info.tmp,  Node: charset-collation-names,  Next: charset-server,  Prev: charset-syntax,  Up: charset-syntax

10.3.1 Collation Naming Conventions
-----------------------------------

MySQL collation names follow these conventions:

   * A collation name starts with the name of the character set with
     which it is associated, generally followed by one or more suffixes
     indicating other collation characteristics.  For example,
     'utf8_general_ci' and 'latin1_swedish_ci' are collations for the
     'utf8' and 'latin1' character sets, respectively.  The 'binary'
     character set has a single collation, also named 'binary', with no
     suffixes.

   * A language-specific collation includes a language name.  For
     example, 'utf8_turkish_ci' and 'utf8_hungarian_ci' sort characters
     for the 'utf8' character set using the rules of Turkish and
     Hungarian, respectively.

   * Collation suffixes indicate whether a collation is case-sensitive,
     accent-sensitive, or kana-sensitive (or some combination thereof),
     or binary.  The following table shows the suffixes used to indicate
     these characteristics.

     *Collation Suffix Meanings*

     Suffix         Meaning
                    
     '_ai'          Accent-insensitive
                    
     '_as'          Accent-sensitive
                    
     '_ci'          Case-insensitive
                    
     '_cs'          Case-sensitive
                    
     '_bin'         Binary

     For nonbinary collation names that do not specify accent
     sensitivity, it is determined by case sensitivity.  If a collation
     name does not contain '_ai' or '_as', '_ci' in the name implies
     '_ai' and '_cs' in the name implies '_as'.  For example,
     'latin1_general_ci' is explicitly case-insensitive and implicitly
     accent-insensitive, and 'latin1_general_cs' is explicitly
     case-sensitive and implicitly accent-sensitive.

     For the 'binary' collation of the 'binary' character set,
     comparisons are based on numeric byte values.  For the '_bin'
     collation of a nonbinary character set, comparisons are based on
     numeric character code values, which differ from byte values for
     multibyte characters.  For information about the differences
     between the 'binary' collation of the 'binary' character set and
     the '_bin' collations of nonbinary character sets, see *note
     charset-binary-collations::.

   * For Unicode character sets, the 'XXX_general_mysql500_ci'
     collations preserve the pre-5.1.24 ordering of the original
     'XXX_general_ci' collations and permit upgrades for tables created
     before MySQL 5.1.24 (Bug #27877).


File: manual.info.tmp,  Node: charset-server,  Next: charset-database,  Prev: charset-collation-names,  Up: charset-syntax

10.3.2 Server Character Set and Collation
-----------------------------------------

MySQL Server has a server character set and a server collation.  These
can be set at server startup on the command line or in an option file
and changed at runtime.

Initially, the server character set and collation depend on the options
that you use when you start *note 'mysqld': mysqld.  You can use
'--character-set-server' for the character set.  Along with it, you can
add '--collation-server' for the collation.  If you don't specify a
character set, that is the same as saying
'--character-set-server=latin1'.  If you specify only a character set
(for example, 'latin1') but not a collation, that is the same as saying
'--character-set-server=latin1' '--collation-server=latin1_swedish_ci'
because 'latin1_swedish_ci' is the default collation for 'latin1'.
Therefore, the following three commands all have the same effect:

     mysqld
     mysqld --character-set-server=latin1
     mysqld --character-set-server=latin1 \
       --collation-server=latin1_swedish_ci

One way to change the settings is by recompiling.  To change the default
server character set and collation when building from sources, use the
'DEFAULT_CHARSET' and 'DEFAULT_COLLATION' options for 'CMake'.  For
example:

     cmake . -DDEFAULT_CHARSET=latin1

Or:

     cmake . -DDEFAULT_CHARSET=latin1 \
       -DDEFAULT_COLLATION=latin1_german1_ci

Both *note 'mysqld': mysqld. and 'CMake' verify that the character
set/collation combination is valid.  If not, each program displays an
error message and terminates.

The server character set and collation are used as default values if the
database character set and collation are not specified in *note 'CREATE
DATABASE': create-database. statements.  They have no other purpose.

The current server character set and collation can be determined from
the values of the 'character_set_server' and 'collation_server' system
variables.  These variables can be changed at runtime.


File: manual.info.tmp,  Node: charset-database,  Next: charset-table,  Prev: charset-server,  Up: charset-syntax

10.3.3 Database Character Set and Collation
-------------------------------------------

Every database has a database character set and a database collation.
The *note 'CREATE DATABASE': create-database. and *note 'ALTER
DATABASE': alter-database. statements have optional clauses for
specifying the database character set and collation:

     CREATE DATABASE DB_NAME
         [[DEFAULT] CHARACTER SET CHARSET_NAME]
         [[DEFAULT] COLLATE COLLATION_NAME]

     ALTER DATABASE DB_NAME
         [[DEFAULT] CHARACTER SET CHARSET_NAME]
         [[DEFAULT] COLLATE COLLATION_NAME]

The keyword 'SCHEMA' can be used instead of 'DATABASE'.

All database options are stored in a text file named 'db.opt' that can
be found in the database directory.

The 'CHARACTER SET' and 'COLLATE' clauses make it possible to create
databases with different character sets and collations on the same MySQL
server.

Example:

     CREATE DATABASE DB_NAME CHARACTER SET latin1 COLLATE latin1_swedish_ci;

MySQL chooses the database character set and database collation in the
following manner:

   * If both 'CHARACTER SET CHARSET_NAME' and 'COLLATE COLLATION_NAME'
     are specified, character set CHARSET_NAME and collation
     COLLATION_NAME are used.

   * If 'CHARACTER SET CHARSET_NAME' is specified without 'COLLATE',
     character set CHARSET_NAME and its default collation are used.  To
     see the default collation for each character set, use the *note
     'SHOW CHARACTER SET': show-character-set. statement or query the
     'INFORMATION_SCHEMA' *note 'CHARACTER_SETS': character-sets-table.
     table.

   * If 'COLLATE COLLATION_NAME' is specified without 'CHARACTER SET',
     the character set associated with COLLATION_NAME and collation
     COLLATION_NAME are used.

   * Otherwise (neither 'CHARACTER SET' nor 'COLLATE' is specified), the
     server character set and server collation are used.

The character set and collation for the default database can be
determined from the values of the 'character_set_database' and
'collation_database' system variables.  The server sets these variables
whenever the default database changes.  If there is no default database,
the variables have the same value as the corresponding server-level
system variables, 'character_set_server' and 'collation_server'.

To see the default character set and collation for a given database, use
these statements:

     USE DB_NAME;
     SELECT @@character_set_database, @@collation_database;

Alternatively, to display the values without changing the default
database:

     SELECT DEFAULT_CHARACTER_SET_NAME, DEFAULT_COLLATION_NAME
     FROM INFORMATION_SCHEMA.SCHEMATA WHERE SCHEMA_NAME = 'DB_NAME';

The database character set and collation affect these aspects of server
operation:

   * For *note 'CREATE TABLE': create-table. statements, the database
     character set and collation are used as default values for table
     definitions if the table character set and collation are not
     specified.  To override this, provide explicit 'CHARACTER SET' and
     'COLLATE' table options.

   * For *note 'LOAD DATA': load-data. statements that include no
     'CHARACTER SET' clause, the server uses the character set indicated
     by the 'character_set_database' system variable to interpret the
     information in the file.  To override this, provide an explicit
     'CHARACTER SET' clause.

   * For stored routines (procedures and functions), the database
     character set and collation in effect at routine creation time are
     used as the character set and collation of character data
     parameters for which the declaration includes no 'CHARACTER SET' or
     a 'COLLATE' attribute.  To override this, provide 'CHARACTER SET'
     and 'COLLATE' explicitly.


File: manual.info.tmp,  Node: charset-table,  Next: charset-column,  Prev: charset-database,  Up: charset-syntax

10.3.4 Table Character Set and Collation
----------------------------------------

Every table has a table character set and a table collation.  The *note
'CREATE TABLE': create-table. and *note 'ALTER TABLE': alter-table.
statements have optional clauses for specifying the table character set
and collation:

     CREATE TABLE TBL_NAME (COLUMN_LIST)
         [[DEFAULT] CHARACTER SET CHARSET_NAME]
         [COLLATE COLLATION_NAME]]

     ALTER TABLE TBL_NAME
         [[DEFAULT] CHARACTER SET CHARSET_NAME]
         [COLLATE COLLATION_NAME]

Example:

     CREATE TABLE t1 ( ... )
     CHARACTER SET latin1 COLLATE latin1_danish_ci;

MySQL chooses the table character set and collation in the following
manner:

   * If both 'CHARACTER SET CHARSET_NAME' and 'COLLATE COLLATION_NAME'
     are specified, character set CHARSET_NAME and collation
     COLLATION_NAME are used.

   * If 'CHARACTER SET CHARSET_NAME' is specified without 'COLLATE',
     character set CHARSET_NAME and its default collation are used.  To
     see the default collation for each character set, use the *note
     'SHOW CHARACTER SET': show-character-set. statement or query the
     'INFORMATION_SCHEMA' *note 'CHARACTER_SETS': character-sets-table.
     table.

   * If 'COLLATE COLLATION_NAME' is specified without 'CHARACTER SET',
     the character set associated with COLLATION_NAME and collation
     COLLATION_NAME are used.

   * Otherwise (neither 'CHARACTER SET' nor 'COLLATE' is specified), the
     database character set and collation are used.

The table character set and collation are used as default values for
column definitions if the column character set and collation are not
specified in individual column definitions.  The table character set and
collation are MySQL extensions; there are no such things in standard
SQL.


File: manual.info.tmp,  Node: charset-column,  Next: charset-literal,  Prev: charset-table,  Up: charset-syntax

10.3.5 Column Character Set and Collation
-----------------------------------------

Every 'character' column (that is, a column of type *note 'CHAR': char,
*note 'VARCHAR': char, a *note 'TEXT': blob. type, or any synonym) has a
column character set and a column collation.  Column definition syntax
for *note 'CREATE TABLE': create-table. and *note 'ALTER TABLE':
alter-table. has optional clauses for specifying the column character
set and collation:

     COL_NAME {CHAR | VARCHAR | TEXT} (COL_LENGTH)
         [CHARACTER SET CHARSET_NAME]
         [COLLATE COLLATION_NAME]

These clauses can also be used for *note 'ENUM': enum. and *note 'SET':
set. columns:

     COL_NAME {ENUM | SET} (VAL_LIST)
         [CHARACTER SET CHARSET_NAME]
         [COLLATE COLLATION_NAME]

Examples:

     CREATE TABLE t1
     (
         col1 VARCHAR(5)
           CHARACTER SET latin1
           COLLATE latin1_german1_ci
     );

     ALTER TABLE t1 MODIFY
         col1 VARCHAR(5)
           CHARACTER SET latin1
           COLLATE latin1_swedish_ci;

MySQL chooses the column character set and collation in the following
manner:

   * If both 'CHARACTER SET CHARSET_NAME' and 'COLLATE COLLATION_NAME'
     are specified, character set CHARSET_NAME and collation
     COLLATION_NAME are used.

          CREATE TABLE t1
          (
              col1 CHAR(10) CHARACTER SET utf8 COLLATE utf8_unicode_ci
          ) CHARACTER SET latin1 COLLATE latin1_bin;

     The character set and collation are specified for the column, so
     they are used.  The column has character set 'utf8' and collation
     'utf8_unicode_ci'.

   * If 'CHARACTER SET CHARSET_NAME' is specified without 'COLLATE',
     character set CHARSET_NAME and its default collation are used.

          CREATE TABLE t1
          (
              col1 CHAR(10) CHARACTER SET utf8
          ) CHARACTER SET latin1 COLLATE latin1_bin;

     The character set is specified for the column, but the collation is
     not.  The column has character set 'utf8' and the default collation
     for 'utf8', which is 'utf8_general_ci'.  To see the default
     collation for each character set, use the *note 'SHOW CHARACTER
     SET': show-character-set. statement or query the
     'INFORMATION_SCHEMA' *note 'CHARACTER_SETS': character-sets-table.
     table.

   * If 'COLLATE COLLATION_NAME' is specified without 'CHARACTER SET',
     the character set associated with COLLATION_NAME and collation
     COLLATION_NAME are used.

          CREATE TABLE t1
          (
              col1 CHAR(10) COLLATE utf8_polish_ci
          ) CHARACTER SET latin1 COLLATE latin1_bin;

     The collation is specified for the column, but the character set is
     not.  The column has collation 'utf8_polish_ci' and the character
     set is the one associated with the collation, which is 'utf8'.

   * Otherwise (neither 'CHARACTER SET' nor 'COLLATE' is specified), the
     table character set and collation are used.

          CREATE TABLE t1
          (
              col1 CHAR(10)
          ) CHARACTER SET latin1 COLLATE latin1_bin;

     Neither the character set nor collation is specified for the
     column, so the table defaults are used.  The column has character
     set 'latin1' and collation 'latin1_bin'.

The 'CHARACTER SET' and 'COLLATE' clauses are standard SQL.

If you use *note 'ALTER TABLE': alter-table. to convert a column from
one character set to another, MySQL attempts to map the data values, but
if the character sets are incompatible, there may be data loss.


File: manual.info.tmp,  Node: charset-literal,  Next: charset-national,  Prev: charset-column,  Up: charset-syntax

10.3.6 Character String Literal Character Set and Collation
-----------------------------------------------------------

Every character string literal has a character set and a collation.

For the simple statement 'SELECT 'STRING'', the string has the
connection default character set and collation defined by the
'character_set_connection' and 'collation_connection' system variables.

A character string literal may have an optional character set introducer
and 'COLLATE' clause, to designate it as a string that uses a particular
character set and collation:

     [_CHARSET_NAME]'STRING' [COLLATE COLLATION_NAME]

The '_CHARSET_NAME' expression is formally called an _introducer_.  It
tells the parser, 'the string that follows uses character set
CHARSET_NAME.' An introducer does not change the string to the
introducer character set like 'CONVERT()' would do.  It does not change
the string value, although padding may occur.  The introducer is just a
signal.  See *note charset-introducer::.

Examples:

     SELECT 'abc';
     SELECT _latin1'abc';
     SELECT _binary'abc';
     SELECT _utf8'abc' COLLATE utf8_danish_ci;

Character set introducers and the 'COLLATE' clause are implemented
according to standard SQL specifications.

MySQL determines the character set and collation of a character string
literal in the following manner:

   * If both _CHARSET_NAME and 'COLLATE COLLATION_NAME' are specified,
     character set CHARSET_NAME and collation COLLATION_NAME are used.
     COLLATION_NAME must be a permitted collation for CHARSET_NAME.

   * If _CHARSET_NAME is specified but 'COLLATE' is not specified,
     character set CHARSET_NAME and its default collation are used.  To
     see the default collation for each character set, use the *note
     'SHOW CHARACTER SET': show-character-set. statement or query the
     'INFORMATION_SCHEMA' *note 'CHARACTER_SETS': character-sets-table.
     table.

   * If _CHARSET_NAME is not specified but 'COLLATE COLLATION_NAME' is
     specified, the connection default character set given by the
     'character_set_connection' system variable and collation
     COLLATION_NAME are used.  COLLATION_NAME must be a permitted
     collation for the connection default character set.

   * Otherwise (neither _CHARSET_NAME nor 'COLLATE COLLATION_NAME' is
     specified), the connection default character set and collation
     given by the 'character_set_connection' and 'collation_connection'
     system variables are used.

Examples:

   * A nonbinary string with 'latin1' character set and
     'latin1_german1_ci' collation:

          SELECT _latin1'Mu"ller' COLLATE latin1_german1_ci;

   * A nonbinary string with 'utf8' character set and its default
     collation (that is, 'utf8_general_ci'):

          SELECT _utf8'Mu"ller';

   * A binary string with 'binary' character set and its default
     collation (that is, 'binary'):

          SELECT _binary'Mu"ller';

   * A nonbinary string with the connection default character set and
     'utf8_general_ci' collation (fails if the connection character set
     is not 'utf8'):

          SELECT 'Mu"ller' COLLATE utf8_general_ci;

   * A string with the connection default character set and collation:

          SELECT 'Mu"ller';

An introducer indicates the character set for the following string, but
does not change how the parser performs escape processing within the
string.  Escapes are always interpreted by the parser according to the
character set given by 'character_set_connection'.

The following examples show that escape processing occurs using
'character_set_connection' even in the presence of an introducer.  The
examples use *note 'SET NAMES': set-names. (which changes
'character_set_connection', as discussed in *note charset-connection::),
and display the resulting strings using the 'HEX()' function so that the
exact string contents can be seen.

Example 1:

     mysql> SET NAMES latin1;
     mysql> SELECT HEX('a`\n'), HEX(_sjis'a`\n');
     +------------+-----------------+
     | HEX('a`\n')  | HEX(_sjis'a`\n')  |
     +------------+-----------------+
     | E00A       | E00A            |
     +------------+-----------------+

Here, 'a`' (hexadecimal value 'E0') is followed by '\n', the escape
sequence for newline.  The escape sequence is interpreted using the
'character_set_connection' value of 'latin1' to produce a literal
newline (hexadecimal value '0A').  This happens even for the second
string.  That is, the '_sjis' introducer does not affect the parser's
escape processing.

Example 2:

     mysql> SET NAMES sjis;
     mysql> SELECT HEX('a`\n'), HEX(_latin1'a`\n');
     +------------+-------------------+
     | HEX('a`\n')  | HEX(_latin1'a`\n')  |
     +------------+-------------------+
     | E05C6E     | E05C6E            |
     +------------+-------------------+

Here, 'character_set_connection' is 'sjis', a character set in which the
sequence of 'a`' followed by '\' (hexadecimal values '05' and '5C') is a
valid multibyte character.  Hence, the first two bytes of the string are
interpreted as a single 'sjis' character, and the '\' is not interpreted
as an escape character.  The following 'n' (hexadecimal value '6E') is
not interpreted as part of an escape sequence.  This is true even for
the second string; the '_latin1' introducer does not affect escape
processing.


File: manual.info.tmp,  Node: charset-national,  Next: charset-introducer,  Prev: charset-literal,  Up: charset-syntax

10.3.7 The National Character Set
---------------------------------

Standard SQL defines *note 'NCHAR': char. or *note 'NATIONAL CHAR':
char. as a way to indicate that a *note 'CHAR': char. column should use
some predefined character set.  MySQL uses 'utf8' as this predefined
character set.  For example, these data type declarations are
equivalent:

     CHAR(10) CHARACTER SET utf8
     NATIONAL CHARACTER(10)
     NCHAR(10)

As are these:

     VARCHAR(10) CHARACTER SET utf8
     NATIONAL VARCHAR(10)
     NVARCHAR(10)
     NCHAR VARCHAR(10)
     NATIONAL CHARACTER VARYING(10)
     NATIONAL CHAR VARYING(10)

You can use 'N'LITERAL'' (or 'n'LITERAL'') to create a string in the
national character set.  These statements are equivalent:

     SELECT N'some text';
     SELECT n'some text';
     SELECT _utf8'some text';


File: manual.info.tmp,  Node: charset-introducer,  Next: charset-examples,  Prev: charset-national,  Up: charset-syntax

10.3.8 Character Set Introducers
--------------------------------

A character string literal, hexadecimal literal, or bit-value literal
may have an optional character set introducer and 'COLLATE' clause, to
designate it as a string that uses a particular character set and
collation:

     [_CHARSET_NAME] LITERAL [COLLATE COLLATION_NAME]

The '_CHARSET_NAME' expression is formally called an _introducer_.  It
tells the parser, 'the string that follows uses character set
CHARSET_NAME.' An introducer does not change the string to the
introducer character set like 'CONVERT()' would do.  It does not change
the string value, although padding may occur.  The introducer is just a
signal.

For character string literals, space between the introducer and the
string is permitted but optional.

For character set literals, an introducer indicates the character set
for the following string, but does not change how the parser performs
escape processing within the string.  Escapes are always interpreted by
the parser according to the character set given by
'character_set_connection'.  For additional discussion and examples, see
*note charset-literal::.

Examples:

     SELECT 'abc';
     SELECT _latin1'abc';
     SELECT _binary'abc';
     SELECT _utf8'abc' COLLATE utf8_danish_ci;

     SELECT _latin1 X'4D7953514C';
     SELECT _utf8 0x4D7953514C COLLATE utf8_danish_ci;

     SELECT _latin1 b'1000001';
     SELECT _utf8 0b1000001 COLLATE utf8_danish_ci;

Character set introducers and the 'COLLATE' clause are implemented
according to standard SQL specifications.

Character string literals can be designated as binary strings by using
the '_binary' introducer.  Hexadecimal literals and bit-value literals
are binary strings by default, so '_binary' is permitted, but
unnecessary.

MySQL determines the character set and collation of a character string
literal, hexadecimal literal, or bit-value literal in the following
manner:

   * If both _CHARSET_NAME and 'COLLATE COLLATION_NAME' are specified,
     character set CHARSET_NAME and collation COLLATION_NAME are used.
     COLLATION_NAME must be a permitted collation for CHARSET_NAME.

   * If _CHARSET_NAME is specified but 'COLLATE' is not specified,
     character set CHARSET_NAME and its default collation are used.  To
     see the default collation for each character set, use the *note
     'SHOW CHARACTER SET': show-character-set. statement or query the
     'INFORMATION_SCHEMA' *note 'CHARACTER_SETS': character-sets-table.
     table.

   * If _CHARSET_NAME is not specified but 'COLLATE COLLATION_NAME' is
     specified:

        * For a character string literal, the connection default
          character set given by the 'character_set_connection' system
          variable and collation COLLATION_NAME are used.
          COLLATION_NAME must be a permitted collation for the
          connection default character set.

        * For a hexadecimal literal or bit-value literal, the only
          permitted collation is 'binary' because these types of
          literals are binary strings by default.

   * Otherwise (neither _CHARSET_NAME nor 'COLLATE COLLATION_NAME' is
     specified):

        * For a character string literal, the connection default
          character set and collation given by the
          'character_set_connection' and 'collation_connection' system
          variables are used.

        * For a hexadecimal literal or bit-value literal, the character
          set and collation are 'binary'.

Examples:

   * Nonbinary strings with 'latin1' character set and
     'latin1_german1_ci' collation:

          SELECT _latin1'Mu"ller' COLLATE latin1_german1_ci;
          SELECT _latin1 X'0A0D' COLLATE latin1_german1_ci;
          SELECT _latin1 b'0110' COLLATE latin1_german1_ci;

   * Nonbinary strings with 'utf8' character set and its default
     collation (that is, 'utf8_general_ci'):

          SELECT _utf8'Mu"ller';
          SELECT _utf8 X'0A0D';
          SELECT _utf8 b'0110';

   * Binary strings with 'binary' character set and its default
     collation (that is, 'binary'):

          SELECT _binary'Mu"ller';
          SELECT X'0A0D';
          SELECT b'0110';

     The hexadecimal literal and bit-value literal need no introducer
     because they are binary strings by default.

   * A nonbinary string with the connection default character set and
     'utf8_general_ci' collation (fails if the connection character set
     is not 'utf8'):

          SELECT 'Mu"ller' COLLATE utf8_general_ci;

     This construction ('COLLATE' only) does not work for hexadecimal
     literals or bit literals because their character set is 'binary' no
     matter the connection character set, and 'binary' is not compatible
     with the 'utf8_general_ci' collation.  The only permitted 'COLLATE'
     clause in the absence of an introducer is 'COLLATE binary'.

   * A string with the connection default character set and collation:

          SELECT 'Mu"ller';


File: manual.info.tmp,  Node: charset-examples,  Next: charset-compatibility,  Prev: charset-introducer,  Up: charset-syntax

10.3.9 Examples of Character Set and Collation Assignment
---------------------------------------------------------

The following examples show how MySQL determines default character set
and collation values.

*Example 1: Table and Column Definition*

     CREATE TABLE t1
     (
         c1 CHAR(10) CHARACTER SET latin1 COLLATE latin1_german1_ci
     ) DEFAULT CHARACTER SET latin2 COLLATE latin2_bin;

Here we have a column with a 'latin1' character set and a
'latin1_german1_ci' collation.  The definition is explicit, so that is
straightforward.  Notice that there is no problem with storing a
'latin1' column in a 'latin2' table.

*Example 2: Table and Column Definition*

     CREATE TABLE t1
     (
         c1 CHAR(10) CHARACTER SET latin1
     ) DEFAULT CHARACTER SET latin1 COLLATE latin1_danish_ci;

This time we have a column with a 'latin1' character set and a default
collation.  Although it might seem natural, the default collation is not
taken from the table level.  Instead, because the default collation for
'latin1' is always 'latin1_swedish_ci', column 'c1' has a collation of
'latin1_swedish_ci' (not 'latin1_danish_ci').

*Example 3: Table and Column Definition*

     CREATE TABLE t1
     (
         c1 CHAR(10)
     ) DEFAULT CHARACTER SET latin1 COLLATE latin1_danish_ci;

We have a column with a default character set and a default collation.
In this circumstance, MySQL checks the table level to determine the
column character set and collation.  Consequently, the character set for
column 'c1' is 'latin1' and its collation is 'latin1_danish_ci'.

*Example 4: Database, Table, and Column Definition*

     CREATE DATABASE d1
         DEFAULT CHARACTER SET latin2 COLLATE latin2_czech_ci;
     USE d1;
     CREATE TABLE t1
     (
         c1 CHAR(10)
     );

We create a column without specifying its character set and collation.
We're also not specifying a character set and a collation at the table
level.  In this circumstance, MySQL checks the database level to
determine the table settings, which thereafter become the column
settings.)  Consequently, the character set for column 'c1' is 'latin2'
and its collation is 'latin2_czech_ci'.


File: manual.info.tmp,  Node: charset-compatibility,  Prev: charset-examples,  Up: charset-syntax

10.3.10 Compatibility with Other DBMSs
--------------------------------------

For MaxDB compatibility these two statements are the same:

     CREATE TABLE t1 (f1 CHAR(N) UNICODE);
     CREATE TABLE t1 (f1 CHAR(N) CHARACTER SET ucs2);


File: manual.info.tmp,  Node: charset-connection,  Next: charset-applications,  Prev: charset-syntax,  Up: charset

10.4 Connection Character Sets and Collations
=============================================

A 'connection' is what a client program makes when it connects to the
server, to begin a session within which it interacts with the server.
The client sends SQL statements, such as queries, over the session
connection.  The server sends responses, such as result sets or error
messages, over the connection back to the client.

   * *note charset-connection-system-variables::

   * *note charset-connection-impermissible-client-charset::

   * *note charset-connection-client-configuration::

   * *note charset-connection-sql-statements::

   * *note charset-connection-error-handling::

*Connection Character Set and Collation System Variables*

Several character set and collation system variables relate to a
client's interaction with the server.  Some of these have been mentioned
in earlier sections:

   * The 'character_set_server' and 'collation_server' system variables
     indicate the server character set and collation.  See *note
     charset-server::.

   * The 'character_set_database' and 'collation_database' system
     variables indicate the character set and collation of the default
     database.  See *note charset-database::.

Additional character set and collation system variables are involved in
handling traffic for the connection between a client and the server.
Every client has session-specific connection-related character set and
collation system variables.  These session system variable values are
initialized at connect time, but can be changed within the session.

Several questions about character set and collation handling for client
connections can be answered in terms of system variables:

   * What character set are statements in when they leave the client?

     The server takes the 'character_set_client' system variable to be
     the character set in which statements are sent by the client.

     *Note*:

     Some character sets cannot be used as the client character set.
     See *note charset-connection-impermissible-client-charset::.

   * What character set should the server translate statements to after
     receiving them?

     To determine this, the server uses the 'character_set_connection'
     and 'collation_connection' system variables:

        * The server converts statements sent by the client from
          'character_set_client' to 'character_set_connection'.
          Exception: For string literals that have an introducer such as
          '_utf8mb4' or '_latin2', the introducer determines the
          character set.  See *note charset-introducer::.

        * 'collation_connection' is important for comparisons of literal
          strings.  For comparisons of strings with column values,
          'collation_connection' does not matter because columns have
          their own collation, which has a higher collation precedence
          (see *note charset-collation-coercibility::).

   * What character set should the server translate query results to
     before shipping them back to the client?

     The 'character_set_results' system variable indicates the character
     set in which the server returns query results to the client.  This
     includes result data such as column values, result metadata such as
     column names, and error messages.

     To tell the server to perform no conversion of result sets or error
     messages, set 'character_set_results' to 'NULL' or 'binary':

          SET character_set_results = NULL;
          SET character_set_results = binary;

     For more information about character sets and error messages, see
     *note charset-errors::.

To see the values of the character set and collation system variables
that apply to the current session, use this statement:

     SELECT * FROM INFORMATION_SCHEMA.SESSION_VARIABLES
     WHERE VARIABLE_NAME IN (
     'character_set_client', 'character_set_connection',
     'character_set_results', 'collation_connection'
     ) ORDER BY VARIABLE_NAME;

The following simpler statements also display the connection variables,
but include other related variables as well.  They can be useful to see
_all_ character set and collation system variables:

     SHOW SESSION VARIABLES LIKE 'character\_set\_%';
     SHOW SESSION VARIABLES LIKE 'collation\_%';

Clients can fine-tune the settings for these variables, or depend on the
defaults (in which case, you can skip the rest of this section).  If you
do not use the defaults, you must change the character settings _for
each connection to the server._

*Impermissible Client Character Sets*

The 'character_set_client' system variable cannot be set to certain
character sets:

     ucs2
     utf16
     utf32

Attempting to use any of those character sets as the client character
set produces an error:

     mysql> SET character_set_client = 'ucs2';
     ERROR 1231 (42000): Variable 'character_set_client'
     can't be set to the value of 'ucs2'

The same error occurs if any of those character sets are used in the
following contexts, all of which result in an attempt to set
'character_set_client' to the named character set:

   * The '--default-character-set=CHARSET_NAME' command option used by
     MySQL client programs such as *note 'mysql': mysql. and *note
     'mysqladmin': mysqladmin.

   * The *note 'SET NAMES 'CHARSET_NAME'': set-names. statement.

   * The *note 'SET CHARACTER SET 'CHARSET_NAME'': set-character-set.
     statement.

*Client Program Connection Character Set Configuration*

When a client connects to the server, it indicates which character set
it wants to use for communication with the server.  (Actually, the
client indicates the default collation for that character set, from
which the server can determine the character set.)  The server uses this
information to set the 'character_set_client', 'character_set_results',
'character_set_connection' system variables to the character set, and
'collation_connection' to the character set default collation.  In
effect, the server performs the equivalent of a *note 'SET NAMES':
set-names. operation.

If the server does not support the requested character set or collation,
it falls back to using the server character set and collation to
configure the connection.  For additional detail about this fallback
behavior, see *note charset-connection-error-handling::.

The *note 'mysql': mysql, *note 'mysqladmin': mysqladmin, *note
'mysqlcheck': mysqlcheck, *note 'mysqlimport': mysqlimport, and *note
'mysqlshow': mysqlshow. client programs determine the default character
set to use as follows:

   * In the absence of other information, each client uses the
     compiled-in default character set, usually 'latin1'.

   * Each client can autodetect which character set to use based on the
     operating system setting, such as the value of the 'LANG' or
     'LC_ALL' locale environment variable on Unix systems or the code
     page setting on Windows systems.  For systems on which the locale
     is available from the OS, the client uses it to set the default
     character set rather than using the compiled-in default.  For
     example, setting 'LANG' to 'ru_RU.KOI8-R' causes the 'koi8r'
     character set to be used.  Thus, users can configure the locale in
     their environment for use by MySQL clients.

     The OS character set is mapped to the closest MySQL character set
     if there is no exact match.  If the client does not support the
     matching character set, it uses the compiled-in default.  For
     example, 'ucs2' is not supported as a connection character set, so
     it maps to the compiled-in default.

     C applications can use character set autodetection based on the OS
     setting by invoking *note 'mysql_options()': mysql-options. as
     follows before connecting to the server:

          mysql_options(mysql,
                        MYSQL_SET_CHARSET_NAME,
                        MYSQL_AUTODETECT_CHARSET_NAME);

   * Each client supports a '--default-character-set' option, which
     enables users to specify the character set explicitly to override
     whatever default the client otherwise determines.

     *Note*:

     Some character sets cannot be used as the client character set.
     Attempting to use them with '--default-character-set' produces an
     error.  See *note
     charset-connection-impermissible-client-charset::.

*Note*:

Before MySQL 5.5, in the absence of other information, the MySQL client
programs used the compiled-in default character set, usually 'latin1'.
An implication of this difference is that if your environment is
configured to use a non-'latin1' locale, MySQL client programs will use
a different connection character set than previously, as though you had
issued an implicit *note 'SET NAMES': set-names. statement.  If the
previous behavior is required, start the client with the
'--default-character-set=latin1' option.

With the *note 'mysql': mysql. client, to use a character set different
from the default, you could explicitly execute a *note 'SET NAMES':
set-names. statement every time you connect to the server (see *note
charset-connection-client-configuration::).  To accomplish the same
result more easily, specify the character set in your option file.  For
example, the following option file setting changes the three
connection-related character set system variables set to 'koi8r' each
time you invoke *note 'mysql': mysql.:

     [mysql]
     default-character-set=koi8r

If you are using the *note 'mysql': mysql. client with auto-reconnect
enabled (which is not recommended), it is preferable to use the
'charset' command rather than *note 'SET NAMES': set-names.  For
example:

     mysql> charset koi8r
     Charset changed

The 'charset' command issues a *note 'SET NAMES': set-names. statement,
and also changes the default character set that *note 'mysql': mysql.
uses when it reconnects after the connection has dropped.

When configuration client programs, you must also consider the
environment within which they execute.  See *note
charset-applications::.

*SQL Statements for Connection Character Set Configuration*

After a connection has been established, clients can change the
character set and collation system variables for the current session.
These variables can be changed individually using *note 'SET':
set-statement. statements, but two more convenient statements affect the
connection-related character set sytem variables as a group:

   * 'SET NAMES 'CHARSET_NAME' [COLLATE 'COLLATION_NAME']'

     *note 'SET NAMES': set-names. indicates what character set the
     client will use to send SQL statements to the server.  Thus, *note
     'SET NAMES 'cp1251'': set-names. tells the server, 'future incoming
     messages from this client are in character set 'cp1251'.' It also
     specifies the character set that the server should use for sending
     results back to the client.  (For example, it indicates what
     character set to use for column values if you use a *note 'SELECT':
     select. statement that produces a result set.)

     A *note 'SET NAMES 'CHARSET_NAME'': set-names. statement is
     equivalent to these three statements:

          SET character_set_client = CHARSET_NAME;
          SET character_set_results = CHARSET_NAME;
          SET character_set_connection = CHARSET_NAME;

     Setting 'character_set_connection' to CHARSET_NAME also implicitly
     sets 'collation_connection' to the default collation for
     CHARSET_NAME.  It is unnecessary to set that collation explicitly.
     To specify a particular collation to use for
     'collation_connection', add a 'COLLATE' clause:

          SET NAMES 'CHARSET_NAME' COLLATE 'COLLATION_NAME'

   * 'SET CHARACTER SET 'CHARSET_NAME''

     *note 'SET CHARACTER SET': set-character-set. is similar to *note
     'SET NAMES': set-names. but sets 'character_set_connection' and
     'collation_connection' to 'character_set_database' and
     'collation_database' (which, as mentioned previously, indicate the
     character set and collation of the default database).

     A *note 'SET CHARACTER SET CHARSET_NAME': set-character-set.
     statement is equivalent to these three statements:

          SET character_set_client = CHARSET_NAME;
          SET character_set_results = CHARSET_NAME;
          SET collation_connection = @@collation_database;

     Setting 'collation_connection' also implicitly sets
     'character_set_connection' to the character set associated with the
     collation (equivalent to executing 'SET character_set_connection =
     @@character_set_database').  It is unnecessary to set
     'character_set_connection' explicitly.

*Note*:

Some character sets cannot be used as the client character set.
Attempting to use them with *note 'SET NAMES': set-names. or *note 'SET
CHARACTER SET': set-character-set. produces an error.  See *note
charset-connection-impermissible-client-charset::.

Example: Suppose that 'column1' is defined as 'CHAR(5) CHARACTER SET
latin2'.  If you do not say *note 'SET NAMES': set-names. or *note 'SET
CHARACTER SET': set-character-set, then for 'SELECT column1 FROM t', the
server sends back all the values for 'column1' using the character set
that the client specified when it connected.  On the other hand, if you
say 'SET NAMES 'latin1'' or 'SET CHARACTER SET 'latin1'' before issuing
the *note 'SELECT': select. statement, the server converts the 'latin2'
values to 'latin1' just before sending results back.  Conversion may be
lossy for characters that are not in both character sets.

*Connection Character Set Error Handling*

Attempts to use an inappropriate connection character set or collation
can produce an error, or cause the server to fall back to its default
character set and collation for a given connection.  This section
describes problems that can occur when configuring the connection
character set.  These problems can occur when establishing a connection
or when changing the character set within an established connection.

   * *note charset-connection-error-handling-connect-time::

   * *note charset-connection-error-handling-runtime::

*Connect-Time Error Handling*

Some character sets cannot be used as the client character set; see
*note charset-connection-impermissible-client-charset::.  If you specify
a character set that is valid but not permitted as a client character
set, the server returns an error:

     shell> mysql --default-character-set=ucs2
     ERROR 1231 (42000): Variable 'character_set_client' can't be set to
     the value of 'ucs2'

If you specify a character set that the client does not recognize, it
produces an error:

     shell> mysql --default-character-set=bogus
     mysql: Character set 'bogus' is not a compiled character set and is
     not specified in the '/usr/local/mysql/share/charsets/Index.xml' file
     ERROR 2019 (HY000): Can't initialize character set bogus
     (path: /usr/local/mysql/share/charsets/)

If you specify a character set that the client recognizes but the server
does not, the server falls back to its default character set and
collation.  Suppose that the server is configured to use 'latin1' and
'latin1_swedish_ci' as its defaults, and that it does not recognize
'gb18030' as a valid character set.  A client that specifies
'--default-character-set=gb18030' is able to connect to the server, but
the resulting character set is not what the client wants:

     mysql> SHOW SESSION VARIABLES LIKE 'character\_set\_%';
     +--------------------------+--------+
     | Variable_name            | Value  |
     +--------------------------+--------+
     | character_set_client     | latin1 |
     | character_set_connection | latin1 |
     ...
     | character_set_results    | latin1 |
     ...
     +--------------------------+--------+
     mysql> SHOW SESSION VARIABLES LIKE 'collation_connection';
     +----------------------+-------------------+
     | Variable_name        | Value             |
     +----------------------+-------------------+
     | collation_connection | latin1_swedish_ci |
     +----------------------+-------------------+

You can see that the connection system variables have been set to
reflect a character set and collation of 'latin1' and
'latin1_swedish_ci'.  This occurs because the server cannot satisfy the
client character set request and falls back to its defaults.

In this case, the client cannot use the character set that it wants
because the server does not support it.  The client must either be
willing to use a different character set, or connect to a different
server that supports the desired character set.

The same problem occurs in a more subtle context: When the client tells
the server to use a character set that the server recognizes, but the
default collation for that character set on the client side is not known
on the server side.  This occurs, for example, when a MySQL 8.0 client
wants to connect to a MySQL 5.7 server using 'utf8mb4' as the client
character set.  A client that specifies
'--default-character-set=utf8mb4' is able to connect to the server.
However, as in the previous example, the server falls back to its
default character set and collation, not what the client requested:

     mysql> SHOW SESSION VARIABLES LIKE 'character\_set\_%';
     +--------------------------+--------+
     | Variable_name            | Value  |
     +--------------------------+--------+
     | character_set_client     | latin1 |
     | character_set_connection | latin1 |
     ...
     | character_set_results    | latin1 |
     ...
     +--------------------------+--------+
     mysql> SHOW SESSION VARIABLES LIKE 'collation_connection';
     +----------------------+-------------------+
     | Variable_name        | Value             |
     +----------------------+-------------------+
     | collation_connection | latin1_swedish_ci |
     +----------------------+-------------------+

Why does this occur?  After all, 'utf8mb4' is known to the 8.0 client
and the 5.7 server, so both of them recognize it.  To understand this
behavior, it is necessary to understand that when the client tells the
server which character set it wants to use, it really tells the server
the default collation for that character set.  Therefore, the
aforementioned behavior occurs due to a combination of factors:

   * The default collation for 'utf8mb4' differs between MySQL 5.7 and
     8.0 ('utf8mb4_general_ci' for 5.7, 'utf8mb4_0900_ai_ci' for 8.0).

   * When the 8.0 client requests a character set of 'utf8mb4', what it
     sends to the server is the default 8.0 'utf8mb4' collation; that
     is, the 'utf8mb4_0900_ai_ci'.

   * 'utf8mb4_0900_ai_ci' is implemented only as of MySQL 8.0, so the
     5.7 server does not recognize it.

   * Because the 5.7 server does not recognize 'utf8mb4_0900_ai_ci', it
     cannot satisfy the client character set request, and falls back to
     its default character set and collation ('latin1' and
     'latin1_swedish_ci').

In this case, the client can still use 'utf8mb4' by issuing a 'SET NAMES
'utf8mb4'' statement after connecting.  The resulting collation is the
5.7 default 'utf8mb4' collation; that is, 'utf8mb4_general_ci'.  If the
client additionally wants a collation of 'utf8mb4_0900_ai_ci', it cannot
achieve that because the server does not recognize that collation.  The
client must either be willing to use a different 'utf8mb4' collation, or
connect to a server from MySQL 8.0 or higher.

*Runtime Error Handling*

Within an established connection, the client can request a change of
connection character set and collation with *note 'SET NAMES':
set-names. or *note 'SET CHARACTER SET': set-character-set.

Some character sets cannot be used as the client character set; see
*note charset-connection-impermissible-client-charset::.  If you specify
a character set that is valid but not permitted as a client character
set, the server returns an error:

     mysql> SET NAMES 'ucs2';
     ERROR 1231 (42000): Variable 'character_set_client' can't be set to
     the value of 'ucs2'

If the server does not recognize the character set (or the collation),
it produces an error:

     mysql> SET NAMES 'bogus';
     ERROR 1115 (42000): Unknown character set: 'bogus'

     mysql> SET NAMES 'utf8mb4' COLLATE 'bogus';
     ERROR 1273 (HY000): Unknown collation: 'bogus'

*Tip*:

A client that wants to verify whether its requested character set was
honored by the server can execute the following statement after
connecting and checking that the result is the expected character set:

     SELECT @@character_set_client;


File: manual.info.tmp,  Node: charset-applications,  Next: charset-errors,  Prev: charset-connection,  Up: charset

10.5 Configuring Application Character Set and Collation
========================================================

For applications that store data using the default MySQL character set
and collation ('latin1', 'latin1_swedish_ci'), no special configuration
should be needed.  If applications require data storage using a
different character set or collation, you can configure character set
information several ways:

   * Specify character settings per database.  For example, applications
     that use one database might use the default of 'latin1', whereas
     applications that use another database might use 'sjis'.

   * Specify character settings at server startup.  This causes the
     server to use the given settings for all applications that do not
     make other arrangements.

   * Specify character settings at configuration time, if you build
     MySQL from source.  This causes the server to use the given
     settings as the defaults for all applications, without having to
     specify them at server startup.

When different applications require different character settings, the
per-database technique provides a good deal of flexibility.  If most or
all applications use the same character set, specifying character
settings at server startup or configuration time may be most convenient.

For the per-database or server-startup techniques, the settings control
the character set for data storage.  Applications must also tell the
server which character set to use for client/server communications, as
described in the following instructions.

The examples shown here assume use of the 'utf8' character set and
'utf8_general_ci' collation in particular contexts as an alternative to
the defaults of 'latin1' and 'latin1_swedish_ci'.

   * Specify character settings per database

     To create a database such that its tables will use a given default
     character set and collation for data storage, use a *note 'CREATE
     DATABASE': create-database. statement like this:

          CREATE DATABASE mydb
            CHARACTER SET utf8
            COLLATE utf8_general_ci;

     Tables created in the database will use 'utf8' and
     'utf8_general_ci' by default for any character columns.

     Applications that use the database should also configure their
     connection to the server each time they connect.  This can be done
     by executing a 'SET NAMES 'utf8'' statement after connecting.  The
     statement can be used regardless of connection method (the *note
     'mysql': mysql. client, PHP scripts, and so forth).

     In some cases, it may be possible to configure the connection to
     use the desired character set some other way.  For example, to
     connect using *note 'mysql': mysql, you can specify the
     '--default-character-set=utf8' command-line option to achieve the
     same effect as 'SET NAMES 'utf8''.

     For more information about configuring client connections, see
     *note charset-connection::.

     *Note*:

     If you use *note 'ALTER DATABASE': alter-database. to change the
     database default character set or collation, existing stored
     routines in the database that use those defaults must be dropped
     and recreated so that they use the new defaults.  (In a stored
     routine, variables with character data types use the database
     defaults if the character set or collation are not specified
     explicitly.  See *note create-procedure::.)

   * Specify character settings at server startup

     To select a character set and collation at server startup, use the
     '--character-set-server' and '--collation-server' options.  For
     example, to specify the options in an option file, include these
     lines:

          [mysqld]
          character-set-server=utf8
          collation-server=utf8_general_ci

     These settings apply server-wide and apply as the defaults for
     databases created by any application, and for tables created in
     those databases.

     It is still necessary for applications to configure their
     connection using *note 'SET NAMES': set-names. or equivalent after
     they connect, as described previously.  You might be tempted to
     start the server with the '--init_connect="SET NAMES 'utf8'"'
     option to cause *note 'SET NAMES': set-names. to be executed
     automatically for each client that connects.  However, this may
     yield inconsistent results because the 'init_connect' value is not
     executed for users who have the 'SUPER' privilege.

   * Specify character settings at MySQL configuration time

     To select a character set and collation if you configure and build
     MySQL from source, use the 'DEFAULT_CHARSET' and
     'DEFAULT_COLLATION' 'CMake' options:

          cmake . -DDEFAULT_CHARSET=utf8 \
            -DDEFAULT_COLLATION=utf8_general_ci

     The resulting server uses 'utf8' and 'utf8_general_ci' as the
     default for databases and tables and for client connections.  It is
     unnecessary to use '--character-set-server' and
     '--collation-server' to specify those defaults at server startup.
     It is also unnecessary for applications to configure their
     connection using *note 'SET NAMES': set-names. or equivalent after
     they connect to the server.

Regardless of how you configure the MySQL character set for application
use, you must also consider the environment within which those
applications execute.  For example, if you will send statements using
UTF-8 text taken from a file that you create in an editor, you should
edit the file with the locale of your environment set to UTF-8 so that
the file encoding is correct and so that the operating system handles it
correctly.  If you use the *note 'mysql': mysql. client from within a
terminal window, the window must be configured to use UTF-8 or
characters may not display properly.  For a script that executes in a
Web environment, the script must handle character encoding properly for
its interaction with the MySQL server, and it must generate pages that
correctly indicate the encoding so that browsers know how to display the
content of the pages.  For example, you can include this '<meta>' tag
within your '<head>' element:

     <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />


File: manual.info.tmp,  Node: charset-errors,  Next: charset-conversion,  Prev: charset-applications,  Up: charset

10.6 Error Message Character Set
================================

This section describes how the MySQL server uses character sets for
constructing error messages.  For information about the language of
error messages (rather than the character set), see *note
error-message-language::.  For general information about configuring
error logging, see *note error-log::.

   * *note charset-errors-construction::

   * *note charset-errors-disposition::

*Character Set for Error Message Construction*

The server constructs error messages as follows:

   * The message template uses UTF-8 ('utf8mb3').

   * Parameters in the message template are replaced with values that
     apply to a specific error occurrence:

        * Identifiers such as table or column names use UTF-8 internally
          so they are copied as is.

        * Character (nonbinary) string values are converted from their
          character set to UTF-8.

        * Binary string values are copied as is for bytes in the range
          '0x20' to '0x7E', and using '\x' hexadecimal encoding for
          bytes outside that range.  For example, if a duplicate-key
          error occurs for an attempt to insert '0x41CF9F' into a *note
          'VARBINARY': binary-varbinary. unique column, the resulting
          error message uses UTF-8 with some bytes hexadecimal encoded:

               Duplicate entry 'A\xC3\x9F' for key 1

*Character Set for Error Message Disposition*

An error message, once constructed, can be written by the server to the
error log or sent to clients:

   * If the server writes the error message to the error log, it writes
     it in UTF-8, as constructed, without conversion to another
     character set.

   * If the server sends the error message to a client program, the
     server converts it from UTF-8 to the character set specified by the
     'character_set_results' system variable.  If
     'character_set_results' has a value of 'NULL' or 'binary', no
     conversion occurs.  No conversion occurs if the variable value is
     'utf8mb3' or 'utf8mb4', either, because those character sets have a
     repertoire that includes all UTF-8 characters used in message
     construction.

     If characters cannot be represented in 'character_set_results',
     some encoding may occur during the conversion.  The encoding uses
     Unicode code point values:

        * Characters in the Basic Multilingual Plane (BMP) range
          ('0x0000' to '0xFFFF') are written using '\NNNN' notation.

        * Characters outside the BMP range ('0x10000' to '0x10FFFF') are
          written using '\+NNNNNN' notation.

     Clients can set 'character_set_results' to control the character
     set in which they receive error messages.  The variable can be set
     directly, or indirectly by means such as *note 'SET NAMES':
     set-names.  For more information about 'character_set_results', see
     *note charset-connection::.


File: manual.info.tmp,  Node: charset-conversion,  Next: charset-collations,  Prev: charset-errors,  Up: charset

10.7 Column Character Set Conversion
====================================

To convert a binary or nonbinary string column to use a particular
character set, use *note 'ALTER TABLE': alter-table.  For successful
conversion to occur, one of the following conditions must apply:

   * If the column has a binary data type (*note 'BINARY':
     binary-varbinary, *note 'VARBINARY': binary-varbinary, *note
     'BLOB': blob.), all the values that it contains must be encoded
     using a single character set (the character set you're converting
     the column to).  If you use a binary column to store information in
     multiple character sets, MySQL has no way to know which values use
     which character set and cannot convert the data properly.

   * If the column has a nonbinary data type (*note 'CHAR': char, *note
     'VARCHAR': char, *note 'TEXT': blob.), its contents should be
     encoded in the column character set, not some other character set.
     If the contents are encoded in a different character set, you can
     convert the column to use a binary data type first, and then to a
     nonbinary column with the desired character set.

Suppose that a table 't' has a binary column named 'col1' defined as
'VARBINARY(50)'.  Assuming that the information in the column is encoded
using a single character set, you can convert it to a nonbinary column
that has that character set.  For example, if 'col1' contains binary
data representing characters in the 'greek' character set, you can
convert it as follows:

     ALTER TABLE t MODIFY col1 VARCHAR(50) CHARACTER SET greek;

If your original column has a type of 'BINARY(50)', you could convert it
to 'CHAR(50)', but the resulting values will be padded with '0x00' bytes
at the end, which may be undesirable.  To remove these bytes, use the
'TRIM()' function:

     UPDATE t SET col1 = TRIM(TRAILING 0x00 FROM col1);

Suppose that table 't' has a nonbinary column named 'col1' defined as
'CHAR(50) CHARACTER SET latin1' but you want to convert it to use 'utf8'
so that you can store values from many languages.  The following
statement accomplishes this:

     ALTER TABLE t MODIFY col1 CHAR(50) CHARACTER SET utf8;

Conversion may be lossy if the column contains characters that are not
in both character sets.

A special case occurs if you have old tables from before MySQL 4.1 where
a nonbinary column contains values that actually are encoded in a
character set different from the server's default character set.  For
example, an application might have stored 'sjis' values in a column,
even though MySQL's default character set was different.  It is possible
to convert the column to use the proper character set but an additional
step is required.  Suppose that the server's default character set was
'latin1' and 'col1' is defined as 'CHAR(50)' but its contents are 'sjis'
values.  The first step is to convert the column to a binary data type,
which removes the existing character set information without performing
any character conversion:

     ALTER TABLE t MODIFY col1 BLOB;

The next step is to convert the column to a nonbinary data type with the
proper character set:

     ALTER TABLE t MODIFY col1 CHAR(50) CHARACTER SET sjis;

This procedure requires that the table not have been modified already
with statements such as *note 'INSERT': insert. or *note 'UPDATE':
update. after an upgrade to MySQL 4.1 or higher.  In that case, MySQL
would store new values in the column using 'latin1', and the column will
contain a mix of 'sjis' and 'latin1' values and cannot be converted
properly.

If you specified attributes when creating a column initially, you should
also specify them when altering the table with *note 'ALTER TABLE':
alter-table.  For example, if you specified 'NOT NULL' and an explicit
'DEFAULT' value, you should also provide them in the *note 'ALTER
TABLE': alter-table. statement.  Otherwise, the resulting column
definition will not include those attributes.

To convert all character columns in a table, the 'ALTER TABLE ...
CONVERT TO CHARACTER SET CHARSET' statement may be useful.  See *note
alter-table::.


File: manual.info.tmp,  Node: charset-collations,  Next: charset-unicode,  Prev: charset-conversion,  Up: charset

10.8 Collation Issues
=====================

* Menu:

* charset-collate::              Using COLLATE in SQL Statements
* charset-collate-precedence::   COLLATE Clause Precedence
* charset-collation-compatibility::  Character Set and Collation Compatibility
* charset-collation-coercibility::  Collation Coercibility in Expressions
* charset-binary-collations::    The binary Collation Compared to _bin Collations
* charset-collation-effect::     Examples of the Effect of Collation
* charset-collation-information-schema::  Using Collation in INFORMATION_SCHEMA Searches

The following sections discuss various aspects of character set
collations.


File: manual.info.tmp,  Node: charset-collate,  Next: charset-collate-precedence,  Prev: charset-collations,  Up: charset-collations

10.8.1 Using COLLATE in SQL Statements
--------------------------------------

With the 'COLLATE' clause, you can override whatever the default
collation is for a comparison.  'COLLATE' may be used in various parts
of SQL statements.  Here are some examples:

   * With 'ORDER BY':

          SELECT k
          FROM t1
          ORDER BY k COLLATE latin1_german2_ci;

   * With 'AS':

          SELECT k COLLATE latin1_german2_ci AS k1
          FROM t1
          ORDER BY k1;

   * With 'GROUP BY':

          SELECT k
          FROM t1
          GROUP BY k COLLATE latin1_german2_ci;

   * With aggregate functions:

          SELECT MAX(k COLLATE latin1_german2_ci)
          FROM t1;

   * With 'DISTINCT':

          SELECT DISTINCT k COLLATE latin1_german2_ci
          FROM t1;

   * With 'WHERE':

               SELECT *
               FROM t1
               WHERE _latin1 'Mu"ller' COLLATE latin1_german2_ci = k;

               SELECT *
               FROM t1
               WHERE k LIKE _latin1 'Mu"ller' COLLATE latin1_german2_ci;

   * With 'HAVING':

          SELECT k
          FROM t1
          GROUP BY k
          HAVING k = _latin1 'Mu"ller' COLLATE latin1_german2_ci;


File: manual.info.tmp,  Node: charset-collate-precedence,  Next: charset-collation-compatibility,  Prev: charset-collate,  Up: charset-collations

10.8.2 COLLATE Clause Precedence
--------------------------------

The 'COLLATE' clause has high precedence (higher than '||'), so the
following two expressions are equivalent:

     x || y COLLATE z
     x || (y COLLATE z)


File: manual.info.tmp,  Node: charset-collation-compatibility,  Next: charset-collation-coercibility,  Prev: charset-collate-precedence,  Up: charset-collations

10.8.3 Character Set and Collation Compatibility
------------------------------------------------

Each character set has one or more collations, but each collation is
associated with one and only one character set.  Therefore, the
following statement causes an error message because the 'latin2_bin'
collation is not legal with the 'latin1' character set:

     mysql> SELECT _latin1 'x' COLLATE latin2_bin;
     ERROR 1253 (42000): COLLATION 'latin2_bin' is not valid
     for CHARACTER SET 'latin1'


File: manual.info.tmp,  Node: charset-collation-coercibility,  Next: charset-binary-collations,  Prev: charset-collation-compatibility,  Up: charset-collations

10.8.4 Collation Coercibility in Expressions
--------------------------------------------

In the great majority of statements, it is obvious what collation MySQL
uses to resolve a comparison operation.  For example, in the following
cases, it should be clear that the collation is the collation of column
'x':

     SELECT x FROM T ORDER BY x;
     SELECT x FROM T WHERE x = x;
     SELECT DISTINCT x FROM T;

However, with multiple operands, there can be ambiguity.  For example:

     SELECT x FROM T WHERE x = 'Y';

Should the comparison use the collation of the column 'x', or of the
string literal ''Y''?  Both 'x' and ''Y'' have collations, so which
collation takes precedence?

A mix of collations may also occur in contexts other than comparison.
For example, a multiple-argument concatenation operation such as
'CONCAT(x,'Y')' combines its arguments to produce a single string.  What
collation should the result have?

To resolve questions like these, MySQL checks whether the collation of
one item can be coerced to the collation of the other.  MySQL assigns
coercibility values as follows:

   * An explicit 'COLLATE' clause has a coercibility of 0 (not coercible
     at all).

   * The concatenation of two strings with different collations has a
     coercibility of 1.

   * The collation of a column or a stored routine parameter or local
     variable has a coercibility of 2.

   * A 'system constant' (the string returned by functions such as
     'USER()' or 'VERSION()') has a coercibility of 3.

   * The collation of a literal has a coercibility of 4.

   * The collation of a numeric or temporal value has a coercibility of
     5.

   * 'NULL' or an expression that is derived from 'NULL' has a
     coercibility of 6.

MySQL uses coercibility values with the following rules to resolve
ambiguities:

   * Use the collation with the lowest coercibility value.

   * If both sides have the same coercibility, then:

        * If both sides are Unicode, or both sides are not Unicode, it
          is an error.

        * If one of the sides has a Unicode character set, and another
          side has a non-Unicode character set, the side with Unicode
          character set wins, and automatic character set conversion is
          applied to the non-Unicode side.  For example, the following
          statement does not return an error:

               SELECT CONCAT(utf8_column, latin1_column) FROM t1;

          It returns a result that has a character set of 'utf8' and the
          same collation as 'utf8_column'.  Values of 'latin1_column'
          are automatically converted to 'utf8' before concatenating.

        * For an operation with operands from the same character set but
          that mix a '_bin' collation and a '_ci' or '_cs' collation,
          the '_bin' collation is used.  This is similar to how
          operations that mix nonbinary and binary strings evaluate the
          operands as binary strings, except that it is for collations
          rather than data types.

Although automatic conversion is not in the SQL standard, the standard
does say that every character set is (in terms of supported characters)
a 'subset' of Unicode.  Because it is a well-known principle that 'what
applies to a superset can apply to a subset,' we believe that a
collation for Unicode can apply for comparisons with non-Unicode
strings.

The following table illustrates some applications of the preceding
rules.

Comparison                           Collation Used
                                     
'column1 = 'A''                      Use collation of 'column1'
                                     
'column1 = 'A' COLLATE x'            Use collation of ''A' COLLATE x'
                                     
'column1 COLLATE x = 'A' COLLATE     Error
y'

To determine the coercibility of a string expression, use the
'COERCIBILITY()' function (see *note information-functions::):

     mysql> SELECT COERCIBILITY('A' COLLATE latin1_swedish_ci);
             -> 0
     mysql> SELECT COERCIBILITY(VERSION());
             -> 3
     mysql> SELECT COERCIBILITY('A');
             -> 4
     mysql> SELECT COERCIBILITY(1000);
             -> 5

For implicit conversion of a numeric or temporal value to a string, such
as occurs for the argument '1' in the expression 'CONCAT(1, 'abc')', the
result is a character (nonbinary) string that has a character set and
collation determined by the 'character_set_connection' and
'collation_connection' system variables.  See *note type-conversion::.


File: manual.info.tmp,  Node: charset-binary-collations,  Next: charset-collation-effect,  Prev: charset-collation-coercibility,  Up: charset-collations

10.8.5 The binary Collation Compared to _bin Collations
-------------------------------------------------------

This section describes how the 'binary' collation for binary strings
compares to '_bin' collations for nonbinary strings.

Binary strings (as stored using the *note 'BINARY': binary-varbinary,
*note 'VARBINARY': binary-varbinary, and *note 'BLOB': blob. data types)
have a character set and collation named 'binary'.  Binary strings are
sequences of bytes and the numeric values of those bytes determine
comparison and sort order.  See *note charset-binary-set::.

Nonbinary strings (as stored using the *note 'CHAR': char, *note
'VARCHAR': char, and *note 'TEXT': blob. data types) have a character
set and collation other than 'binary'.  A given nonbinary character set
can have several collations, each of which defines a particular
comparison and sort order for the characters in the set.  One of these
is the binary collation, indicated by a '_bin' suffix in the collation
name.  For example, the binary collation for 'utf8' and 'latin1' is
named 'utf8_bin' and 'latin1_bin', respectively.

The 'binary' collation differs from '_bin' collations in several
respects, discussed in the following sections:

   * *note charset-binary-collations-comparison-units::

   * *note charset-binary-collations-charset-conversion::

   * *note charset-binary-collations-lettercase-conversion::

   * *note charset-binary-collations-trailing-space-comparisons::

   * *note charset-binary-collations-trailing-space-inserts-retrievals::

*The Unit for Comparison and Sorting*

Binary strings are sequences of bytes.  For the 'binary' collation,
comparison and sorting are based on numeric byte values.  Nonbinary
strings are sequences of characters, which might be multibyte.
Collations for nonbinary strings define an ordering of the character
values for comparison and sorting.  For '_bin' collations, this ordering
is based on numeric character code values, which is similar to ordering
for binary strings except that character code values might be multibyte.

*Character Set Conversion*

A nonbinary string has a character set and is automatically converted to
another character set in many cases, even when the string has a '_bin'
collation:

   * When assigning column values to another column that has a different
     character set:

          UPDATE t1 SET utf8_bin_column=latin1_column;
          INSERT INTO t1 (latin1_column) SELECT utf8_bin_column FROM t2;

   * When assigning column values for *note 'INSERT': insert. or *note
     'UPDATE': update. using a string literal:

          SET NAMES latin1;
          INSERT INTO t1 (utf8_bin_column) VALUES ('string-in-latin1');

   * When sending results from the server to a client:

          SET NAMES latin1;
          SELECT utf8_bin_column FROM t2;

For binary string columns, no conversion occurs.  For cases similar to
those preceding, the string value is copied byte-wise.

*Lettercase Conversion*

Collations for nonbinary character sets provide information about
lettercase of characters, so characters in a nonbinary string can be
converted from one lettercase to another, even for '_bin' collations
that ignore lettercase for ordering:

     mysql> SET NAMES utf8mb4 COLLATE utf8mb4_bin;
     mysql> SELECT LOWER('aA'), UPPER('zZ');
     +-------------+-------------+
     | LOWER('aA') | UPPER('zZ') |
     +-------------+-------------+
     | aa          | ZZ          |
     +-------------+-------------+

The concept of lettercase does not apply to bytes in a binary string.
To perform lettercase conversion, the string must first be converted to
a nonbinary string using a character set appropriate for the data stored
in the string:

     mysql> SET NAMES binary;
     mysql> SELECT LOWER('aA'), LOWER(CONVERT('aA' USING utf8mb4));
     +-------------+------------------------------------+
     | LOWER('aA') | LOWER(CONVERT('aA' USING utf8mb4)) |
     +-------------+------------------------------------+
     | aA          | aa                                 |
     +-------------+------------------------------------+

*Trailing Space Handling in Comparisons*

Nonbinary strings have 'PAD SPACE' behavior for all collations,
including '_bin' collations.  Trailing spaces are insignificant in
comparisons:

     mysql> SET NAMES utf8 COLLATE utf8_bin;
     mysql> SELECT 'a ' = 'a';
     +------------+
     | 'a ' = 'a' |
     +------------+
     |          1 |
     +------------+

For binary strings, all characters are significant in comparisons,
including trailing spaces:

     mysql> SET NAMES binary;
     mysql> SELECT 'a ' = 'a';
     +------------+
     | 'a ' = 'a' |
     +------------+
     |          0 |
     +------------+

*Trailing Space Handling for Inserts and Retrievals*

'CHAR(N)' columns store nonbinary strings N characters long.  For
inserts, values shorter than N characters are extended with spaces.  For
retrievals, trailing spaces are removed.

'BINARY(N)' columns store binary strings N bytes long.  For inserts,
values shorter than N bytes are extended with '0x00' bytes.  For
retrievals, nothing is removed; a value of the declared length is always
returned.

     mysql> CREATE TABLE t1 (
              a CHAR(10) CHARACTER SET utf8 COLLATE utf8_bin,
              b BINARY(10)
            );
     mysql> INSERT INTO t1 VALUES ('x','x');
     mysql> INSERT INTO t1 VALUES ('x ','x ');
     mysql> SELECT a, b, HEX(a), HEX(b) FROM t1;
     +------+------------+--------+----------------------+
     | a    | b          | HEX(a) | HEX(b)               |
     +------+------------+--------+----------------------+
     | x    | x          | 78     | 78000000000000000000 |
     | x    | x          | 78     | 78200000000000000000 |
     +------+------------+--------+----------------------+


File: manual.info.tmp,  Node: charset-collation-effect,  Next: charset-collation-information-schema,  Prev: charset-binary-collations,  Up: charset-collations

10.8.6 Examples of the Effect of Collation
------------------------------------------

*Example 1: Sorting German Umlauts*

Suppose that column 'X' in table 'T' has these 'latin1' column values:

     Muffler
     Mu"ller
     MX Systems
     MySQL

Suppose also that the column values are retrieved using the following
statement:

     SELECT X FROM T ORDER BY X COLLATE COLLATION_NAME;

The following table shows the resulting order of the values if we use
'ORDER BY' with different collations.

'latin1_swedish_ci'    'latin1_german1_ci'    'latin1_german2_ci'
                                              
Muffler                Muffler                Mu"ller
                                              
MX Systems             Mu"ller                Muffler
                                              
Mu"ller                MX Systems             MX Systems
                                              
MySQL                  MySQL                  MySQL
                       

The character that causes the different sort orders in this example is
the U with two dots over it ('u"'), which the Germans call 'U-umlaut.'

   * The first column shows the result of the *note 'SELECT': select.
     using the Swedish/Finnish collating rule, which says that U-umlaut
     sorts with Y.

   * The second column shows the result of the *note 'SELECT': select.
     using the German DIN-1 rule, which says that U-umlaut sorts with U.

   * The third column shows the result of the *note 'SELECT': select.
     using the German DIN-2 rule, which says that U-umlaut sorts with
     UE.

*Example 2: Searching for German Umlauts*

Suppose that you have three tables that differ only by the character set
and collation used:

     mysql> SET NAMES utf8;
     mysql> CREATE TABLE german1 (
              c CHAR(10)
            ) CHARACTER SET latin1 COLLATE latin1_german1_ci;
     mysql> CREATE TABLE german2 (
              c CHAR(10)
            ) CHARACTER SET latin1 COLLATE latin1_german2_ci;
     mysql> CREATE TABLE germanutf8 (
              c CHAR(10)
            ) CHARACTER SET utf8 COLLATE utf8_unicode_ci;

Each table contains two records:

     mysql> INSERT INTO german1 VALUES ('Bar'), ('Ba"r');
     mysql> INSERT INTO german2 VALUES ('Bar'), ('Ba"r');
     mysql> INSERT INTO germanutf8 VALUES ('Bar'), ('Ba"r');

Two of the above collations have an 'A = A"' equality, and one has no
such equality ('latin1_german2_ci').  For that reason, you'll get these
results in comparisons:

     mysql> SELECT * FROM german1 WHERE c = 'Ba"r';
     +------+
     | c    |
     +------+
     | Bar  |
     | Ba"r  |
     +------+
     mysql> SELECT * FROM german2 WHERE c = 'Ba"r';
     +------+
     | c    |
     +------+
     | Ba"r  |
     +------+
     mysql> SELECT * FROM germanutf8 WHERE c = 'Ba"r';
     +------+
     | c    |
     +------+
     | Bar  |
     | Ba"r  |
     +------+

This is not a bug but rather a consequence of the sorting properties of
'latin1_german1_ci' and 'utf8_unicode_ci' (the sorting shown is done
according to the German DIN 5007 standard).


File: manual.info.tmp,  Node: charset-collation-information-schema,  Prev: charset-collation-effect,  Up: charset-collations

10.8.7 Using Collation in INFORMATION_SCHEMA Searches
-----------------------------------------------------

String columns in 'INFORMATION_SCHEMA' tables have a collation of
'utf8_general_ci', which is case-insensitive.  However, for values that
correspond to objects that are represented in the file system, such as
databases and tables, searches in 'INFORMATION_SCHEMA' string columns
can be case-sensitive or case-insensitive, depending on the
characteristics of the underlying file system and the value of the
'lower_case_table_names' system variable.  For example, searches may be
case-sensitive if the file system is case-sensitive.  This section
describes this behavior and how to modify it if necessary; see also Bug
#34921.

Suppose that a query searches the 'SCHEMATA.SCHEMA_NAME' column for the
'test' database.  On Linux, file systems are case-sensitive, so
comparisons of 'SCHEMATA.SCHEMA_NAME' with ''test'' match, but
comparisons with ''TEST'' do not:

     mysql> SELECT SCHEMA_NAME FROM INFORMATION_SCHEMA.SCHEMATA
            WHERE SCHEMA_NAME = 'test';
     +-------------+
     | SCHEMA_NAME |
     +-------------+
     | test        |
     +-------------+

     mysql> SELECT SCHEMA_NAME FROM INFORMATION_SCHEMA.SCHEMATA
            WHERE SCHEMA_NAME = 'TEST';
     Empty set (0.00 sec)

These results occur with the 'lower_case_table_names' system variable
set to 0.  Changing the value of 'lower_case_table_names' to 1 or 2
causes the second query to return the same (nonempty) result as the
first query.

On Windows or macOS, file systems are not case-sensitive, so comparisons
match both ''test'' and ''TEST'':

     mysql> SELECT SCHEMA_NAME FROM INFORMATION_SCHEMA.SCHEMATA
            WHERE SCHEMA_NAME = 'test';
     +-------------+
     | SCHEMA_NAME |
     +-------------+
     | test        |
     +-------------+

     mysql> SELECT SCHEMA_NAME FROM INFORMATION_SCHEMA.SCHEMATA
            WHERE SCHEMA_NAME = 'TEST';
     +-------------+
     | SCHEMA_NAME |
     +-------------+
     | TEST        |
     +-------------+

The value of 'lower_case_table_names' makes no difference in this
context.

The preceding behavior occurs because the 'utf8_general_ci' collation is
not used for 'INFORMATION_SCHEMA' queries when searching for values that
correspond to objects represented in the file system.  It is a result of
file system-scanning optimizations implemented for 'INFORMATION_SCHEMA'
searches.  For information about these optimizations, see *note
information-schema-optimization::.

If the result of a string operation on an 'INFORMATION_SCHEMA' column
differs from expectations, a workaround is to use an explicit 'COLLATE'
clause to force a suitable collation (see *note charset-collate::).  For
example, to perform a case-insensitive search, use 'COLLATE' with the
'INFORMATION_SCHEMA' column name:

     mysql> SELECT SCHEMA_NAME FROM INFORMATION_SCHEMA.SCHEMATA
            WHERE SCHEMA_NAME COLLATE utf8_general_ci = 'test';
     +-------------+
     | SCHEMA_NAME |
     +-------------+
     | test        |
     +-------------+

     mysql> SELECT SCHEMA_NAME FROM INFORMATION_SCHEMA.SCHEMATA
            WHERE SCHEMA_NAME COLLATE utf8_general_ci = 'TEST';
     +-------------+
     | SCHEMA_NAME |
     +-------------+
     | test        |
     +-------------+

In the preceding queries, it is important to apply the 'COLLATE' clause
to the 'INFORMATION_SCHEMA' column name.  Applying 'COLLATE' to the
comparison value has no effect.

You can also use the 'UPPER()' or 'LOWER()' function:

     WHERE UPPER(SCHEMA_NAME) = 'TEST'
     WHERE LOWER(SCHEMA_NAME) = 'test'

Although a case-insensitive comparison can be performed even on
platforms with case-sensitive file systems, as just shown, it is not
necessarily always the right thing to do.  On such platforms, it is
possible to have multiple objects with names that differ only in
lettercase.  For example, tables named 'city', 'CITY', and 'City' can
all exist simultaneously.  Consider whether a search should match all
such names or just one and write queries accordingly.  The first of the
following comparisons (with 'utf8_bin') is case-sensitive; the others
are not:

     WHERE TABLE_NAME COLLATE utf8_bin = 'City'
     WHERE TABLE_NAME COLLATE utf8_general_ci = 'city'
     WHERE UPPER(TABLE_NAME) = 'CITY'
     WHERE LOWER(TABLE_NAME) = 'city'

Searches in 'INFORMATION_SCHEMA' string columns for values that refer to
'INFORMATION_SCHEMA' itself do use the 'utf8_general_ci' collation
because 'INFORMATION_SCHEMA' is a 'virtual' database not represented in
the file system.  For example, comparisons with 'SCHEMATA.SCHEMA_NAME'
match ''information_schema'' or ''INFORMATION_SCHEMA'' regardless of
platform:

     mysql> SELECT SCHEMA_NAME FROM INFORMATION_SCHEMA.SCHEMATA
            WHERE SCHEMA_NAME = 'information_schema';
     +--------------------+
     | SCHEMA_NAME        |
     +--------------------+
     | information_schema |
     +--------------------+

     mysql> SELECT SCHEMA_NAME FROM INFORMATION_SCHEMA.SCHEMATA
            WHERE SCHEMA_NAME = 'INFORMATION_SCHEMA';
     +--------------------+
     | SCHEMA_NAME        |
     +--------------------+
     | information_schema |
     +--------------------+


File: manual.info.tmp,  Node: charset-unicode,  Next: charset-charsets,  Prev: charset-collations,  Up: charset

10.9 Unicode Support
====================

* Menu:

* charset-unicode-utf8mb4::      The utf8mb4 Character Set (4-Byte UTF-8 Unicode Encoding)
* charset-unicode-utf8mb3::      The utf8mb3 Character Set (3-Byte UTF-8 Unicode Encoding)
* charset-unicode-utf8::         The utf8 Character Set (Alias for utf8mb3)
* charset-unicode-ucs2::         The ucs2 Character Set (UCS-2 Unicode Encoding)
* charset-unicode-utf16::        The utf16 Character Set (UTF-16 Unicode Encoding)
* charset-unicode-utf32::        The utf32 Character Set (UTF-32 Unicode Encoding)
* charset-unicode-conversion::   Converting Between 3-Byte and 4-Byte Unicode Character Sets

The Unicode Standard includes characters from the Basic Multilingual
Plane (BMP) and supplementary characters that lie outside the BMP. This
section describes support for Unicode in MySQL. For information about
the Unicode Standard itself, visit the Unicode Consortium website
(http://www.unicode.org/).

BMP characters have these characteristics:

   * Their code point values are between 0 and 65535 (or 'U+0000' and
     'U+FFFF').

   * They can be encoded in a variable-length encoding using 8, 16, or
     24 bits (1 to 3 bytes).

   * They can be encoded in a fixed-length encoding using 16 bits (2
     bytes).

   * They are sufficient for almost all characters in major languages.

Supplementary characters lie outside the BMP:

   * Their code point values are between 'U+10000' and 'U+10FFFF').

   * Unicode support for supplementary characters requires character
     sets that have a range outside BMP characters and therefore take
     more space than BMP characters (up to 4 bytes per character).

The UTF-8 (Unicode Transformation Format with 8-bit units) method for
encoding Unicode data is implemented according to RFC 3629, which
describes encoding sequences that take from one to four bytes.  The idea
of UTF-8 is that various Unicode characters are encoded using byte
sequences of different lengths:

   * Basic Latin letters, digits, and punctuation signs use one byte.

   * Most European and Middle East script letters fit into a 2-byte
     sequence: extended Latin letters (with tilde, macron, acute, grave
     and other accents), Cyrillic, Greek, Armenian, Hebrew, Arabic,
     Syriac, and others.

   * Korean, Chinese, and Japanese ideographs use 3-byte or 4-byte
     sequences.

MySQL supports these Unicode character sets:

   * 'utf8mb4': A UTF-8 encoding of the Unicode character set using one
     to four bytes per character.

   * 'utf8mb3': A UTF-8 encoding of the Unicode character set using one
     to three bytes per character.

   * 'utf8': An alias for 'utf8mb3'.

   * 'ucs2': The UCS-2 encoding of the Unicode character set using two
     bytes per character.

   * 'utf16': The UTF-16 encoding for the Unicode character set using
     two or four bytes per character.  Like 'ucs2' but with an extension
     for supplementary characters.

   * 'utf32': The UTF-32 encoding for the Unicode character set using
     four bytes per character.

*note charset-unicode-charset-characteristics::, summarizes the general
characteristics of Unicode character sets supported by MySQL.

*Unicode Character Set General Characteristics*

Character      Supported Characters          Required Storage Per
Set                                          Character
                                             
'utf8mb3',     BMP only                      1, 2, or 3 bytes
'utf8'                                       

'ucs2'         BMP only                      2 bytes
                                             
'utf8mb4'      BMP and supplementary         1, 2, 3, or 4 bytes
                                             
'utf16'        BMP and supplementary         2 or 4 bytes
                                             
'utf32'        BMP and supplementary         4 bytes
               

Characters outside the BMP compare as REPLACEMENT CHARACTER and convert
to ''?'' when converted to a Unicode character set that supports only
BMP characters ('utf8mb3' or 'ucs2').

If you use character sets that support supplementary characters and thus
are 'wider' than the BMP-only 'utf8mb3' and 'ucs2' character sets, there
are potential incompatibility issues for your applications; see *note
charset-unicode-conversion::.  That section also describes how to
convert tables from the (3-byte) 'utf8mb3' to the (4-byte) 'utf8mb4',
and what constraints may apply in doing so.

A similar set of collations is available for each Unicode character set.
For example, each has a Danish collation, the names of which are
'utf8mb4_danish_ci', 'utf8mb3_danish_ci', 'utf8_danish_ci',
'ucs2_danish_ci', 'utf16_danish_ci', and 'utf32_danish_ci'.  For
information about Unicode collations and their differentiating
properties, including collation properties for supplementary characters,
see *note charset-unicode-sets::.

Although many of the supplementary characters come from East Asian
languages, what MySQL 5.5 adds is support for more Japanese and Chinese
characters in Unicode character sets, not support for new Japanese and
Chinese character sets.

The MySQL implementation of UCS-2, UTF-16, and UTF-32 stores characters
in big-endian byte order and does not use a byte order mark (BOM) at the
beginning of values.  Other database systems might use little-endian
byte order or a BOM. In such cases, conversion of values will need to be
performed when transferring data between those systems and MySQL.

MySQL uses no BOM for UTF-8 values.

Client applications that communicate with the server using Unicode
should set the client character set accordingly (for example, by issuing
a 'SET NAMES 'utf8mb4'' statement).  Some character sets cannot be used
as the client character set.  Attempting to use them with *note 'SET
NAMES': set-names. or *note 'SET CHARACTER SET': set-character-set.
produces an error.  See *note
charset-connection-impermissible-client-charset::.

The following sections provide additional detail on the Unicode
character sets in MySQL.


File: manual.info.tmp,  Node: charset-unicode-utf8mb4,  Next: charset-unicode-utf8mb3,  Prev: charset-unicode,  Up: charset-unicode

10.9.1 The utf8mb4 Character Set (4-Byte UTF-8 Unicode Encoding)
----------------------------------------------------------------

The 'utfmb4' character set has these characteristics:

   * Supports BMP and supplementary characters.

   * Requires a maximum of four bytes per multibyte character.

'utf8mb4' contrasts with the 'utf8mb3' character set, which supports
only BMP characters and uses a maximum of three bytes per character:

   * For a BMP character, 'utf8mb4' and 'utf8mb3' have identical storage
     characteristics: same code values, same encoding, same length.

   * For a supplementary character, 'utf8mb4' requires four bytes to
     store it, whereas 'utf8mb3' cannot store the character at all.
     When converting 'utf8mb3' columns to 'utf8mb4', you need not worry
     about converting supplementary characters because there will be
     none.

'utf8mb4' is a superset of 'utf8mb3', so for an operation such as the
following concatenation, the result has character set 'utf8mb4' and the
collation of 'utf8mb4_col':

     SELECT CONCAT(utf8mb3_col, utf8mb4_col);

Similarly, the following comparison in the 'WHERE' clause works
according to the collation of 'utf8mb4_col':

     SELECT * FROM utf8mb3_tbl, utf8mb4_tbl
     WHERE utf8mb3_tbl.utf8mb3_col = utf8mb4_tbl.utf8mb4_col;

For information about data type storage as it relates to multibyte
character sets, see *note data-types-storage-reqs-strings::.


File: manual.info.tmp,  Node: charset-unicode-utf8mb3,  Next: charset-unicode-utf8,  Prev: charset-unicode-utf8mb4,  Up: charset-unicode

10.9.2 The utf8mb3 Character Set (3-Byte UTF-8 Unicode Encoding)
----------------------------------------------------------------

The 'utf8mb3' character set has these characteristics:

   * Supports BMP characters only (no support for supplementary
     characters)

   * Requires a maximum of three bytes per multibyte character.

Applications that use UTF-8 data but require supplementary character
support should use 'utf8mb4' rather than 'utf8mb3' (see *note
charset-unicode-utf8mb4::).

Exactly the same set of characters is available in 'utf8mb3' and 'ucs2'.
That is, they have the same repertoire.

'utf8' is an alias for 'utf8mb3'; the character limit is implicit,
rather than explicit in the name.

'utf8mb3' can be used in 'CHARACTER SET' clauses, and
'utf8mb3_COLLATION_SUBSTRING' in 'COLLATE' clauses, where
COLLATION_SUBSTRING is 'bin', 'czech_ci', 'danish_ci', 'esperanto_ci',
'estonian_ci', and so forth.  For example:

     CREATE TABLE t (s1 CHAR(1) CHARACTER SET utf8mb3;
     SELECT * FROM t WHERE s1 COLLATE utf8mb3_general_ci = 'x';
     DECLARE x VARCHAR(5) CHARACTER SET utf8mb3 COLLATE utf8mb3_danish_ci;
     SELECT CAST('a' AS CHAR CHARACTER SET utf8) COLLATE utf8_czech_ci;

MySQL immediately converts instances of 'utf8mb3' in statements to
'utf8', so in statements such as 'SHOW CREATE TABLE' or 'SELECT
CHARACTER_SET_NAME FROM INFORMATION_SCHEMA.COLUMNS' or 'SELECT
COLLATION_NAME FROM INFORMATION_SCHEMA.COLUMNS', users will see the name
'utf8' or 'utf8_COLLATION_SUBSTRING'.

'utf8mb3' is also valid in contexts other than 'CHARACTER SET' clauses.
For example:

     mysqld --character-set-server=utf8mb3

     SET NAMES 'utf8mb3'; /* and other SET statements that have similar effect */
     SELECT _utf8mb3 'a';

There is no 'utf8mb3' collation corresponding to the 'utf8' collation
for collation names that include a version number to indicate the
Unicode Collation Algorithm version on which the collation is based (for
example, 'utf8_unicode_520_ci').

For information about data type storage as it relates to multibyte
character sets, see *note data-types-storage-reqs-strings::.


File: manual.info.tmp,  Node: charset-unicode-utf8,  Next: charset-unicode-ucs2,  Prev: charset-unicode-utf8mb3,  Up: charset-unicode

10.9.3 The utf8 Character Set (Alias for utf8mb3)
-------------------------------------------------

'utf8' is an alias for the 'utf8mb3' character set.  For more
information, see *note charset-unicode-utf8mb3::.


File: manual.info.tmp,  Node: charset-unicode-ucs2,  Next: charset-unicode-utf16,  Prev: charset-unicode-utf8,  Up: charset-unicode

10.9.4 The ucs2 Character Set (UCS-2 Unicode Encoding)
------------------------------------------------------

In UCS-2, every character is represented by a 2-byte Unicode code with
the most significant byte first.  For example: 'LATIN CAPITAL LETTER A'
has the code '0x0041' and it is stored as a 2-byte sequence: '0x00
0x41'.  'CYRILLIC SMALL LETTER YERU' (Unicode '0x044B') is stored as a
2-byte sequence: '0x04 0x4B'.  For Unicode characters and their codes,
please refer to the Unicode Consortium website
(http://www.unicode.org/).

The 'ucs2' character set has these characteristics:

   * Supports BMP characters only (no support for supplementary
     characters)

   * Uses a fixed-length 16-bit encoding and requires two bytes per
     character.


File: manual.info.tmp,  Node: charset-unicode-utf16,  Next: charset-unicode-utf32,  Prev: charset-unicode-ucs2,  Up: charset-unicode

10.9.5 The utf16 Character Set (UTF-16 Unicode Encoding)
--------------------------------------------------------

The 'utf16' character set is the 'ucs2' character set with an extension
that enables encoding of supplementary characters:

   * For a BMP character, 'utf16' and 'ucs2' have identical storage
     characteristics: same code values, same encoding, same length.

   * For a supplementary character, 'utf16' has a special sequence for
     representing the character using 32 bits.  This is called the
     'surrogate' mechanism: For a number greater than '0xffff', take 10
     bits and add them to '0xd800' and put them in the first 16-bit
     word, take 10 more bits and add them to '0xdc00' and put them in
     the next 16-bit word.  Consequently, all supplementary characters
     require 32 bits, where the first 16 bits are a number between
     '0xd800' and '0xdbff', and the last 16 bits are a number between
     '0xdc00' and '0xdfff'.  Examples are in Section 15.5 Surrogates
     Area (http://www.unicode.org/versions/Unicode4.0.0/ch15.pdf) of the
     Unicode 4.0 document.

Because 'utf16' supports surrogates and 'ucs2' does not, there is a
validity check that applies only in 'utf16': You cannot insert a top
surrogate without a bottom surrogate, or vice versa.  For example:

     INSERT INTO t (ucs2_column) VALUES (0xd800); /* legal */
     INSERT INTO t (utf16_column)VALUES (0xd800); /* illegal */

There is no validity check for characters that are technically valid but
are not true Unicode (that is, characters that Unicode considers to be
'unassigned code points' or 'private use' characters or even 'illegals'
like '0xffff').  For example, since 'U+F8FF' is the Apple Logo, this is
legal:

     INSERT INTO t (utf16_column)VALUES (0xf8ff); /* legal */

Such characters cannot be expected to mean the same thing to everyone.

Because MySQL must allow for the worst case (that one character requires
four bytes) the maximum length of a 'utf16' column or index is only half
of the maximum length for a 'ucs2' column or index.  For example, the
maximum length of a 'MEMORY' table index key is 3072 bytes, so these
statements create tables with the longest permitted indexes for 'ucs2'
and 'utf16' columns:

     CREATE TABLE tf (s1 VARCHAR(1536) CHARACTER SET ucs2) ENGINE=MEMORY;
     CREATE INDEX i ON tf (s1);
     CREATE TABLE tg (s1 VARCHAR(768) CHARACTER SET utf16) ENGINE=MEMORY;
     CREATE INDEX i ON tg (s1);


File: manual.info.tmp,  Node: charset-unicode-utf32,  Next: charset-unicode-conversion,  Prev: charset-unicode-utf16,  Up: charset-unicode

10.9.6 The utf32 Character Set (UTF-32 Unicode Encoding)
--------------------------------------------------------

The 'utf32' character set is fixed length (like 'ucs2' and unlike
'utf16').  'utf32' uses 32 bits for every character, unlike 'ucs2'
(which uses 16 bits for every character), and unlike 'utf16' (which uses
16 bits for some characters and 32 bits for others).

'utf32' takes twice as much space as 'ucs2' and more space than 'utf16',
but 'utf32' has the same advantage as 'ucs2' that it is predictable for
storage: The required number of bytes for 'utf32' equals the number of
characters times 4.  Also, unlike 'utf16', there are no tricks for
encoding in 'utf32', so the stored value equals the code value.

To demonstrate how the latter advantage is useful, here is an example
that shows how to determine a 'utf8mb4' value given the 'utf32' code
value:

     /* Assume code value = 100cc LINEAR B WHEELED CHARIOT */
     CREATE TABLE tmp (utf32_col CHAR(1) CHARACTER SET utf32,
                       utf8mb4_col CHAR(1) CHARACTER SET utf8mb4);
     INSERT INTO tmp VALUES (0x000100cc,NULL);
     UPDATE tmp SET utf8mb4_col = utf32_col;
     SELECT HEX(utf32_col),HEX(utf8mb4_col) FROM tmp;

MySQL is very forgiving about additions of unassigned Unicode characters
or private-use-area characters.  There is in fact only one validity
check for 'utf32': No code value may be greater than '0x10ffff'.  For
example, this is illegal:

     INSERT INTO t (utf32_column) VALUES (0x110000); /* illegal */


File: manual.info.tmp,  Node: charset-unicode-conversion,  Prev: charset-unicode-utf32,  Up: charset-unicode

10.9.7 Converting Between 3-Byte and 4-Byte Unicode Character Sets
------------------------------------------------------------------

This section describes issues that you may face when converting
character data between the 'utf8mb3' and 'utf8mb4' character sets.

*Note*:

This discussion focuses primarily on converting between 'utf8mb3' and
'utf8mb4', but similar principles apply to converting between the 'ucs2'
character set and character sets such as 'utf16' or 'utf32'.

The 'utf8mb3' and 'utf8mb4' character sets differ as follows:

   * 'utf8mb3' supports only characters in the Basic Multilingual Plane
     (BMP). 'utf8mb4' additionally supports supplementary characters
     that lie outside the BMP.

   * 'utf8mb3' uses a maximum of three bytes per character.  'utf8mb4'
     uses a maximum of four bytes per character.

*Note*:

This discussion refers to the 'utf8mb3' and 'utf8mb4' character set
names to be explicit about referring to 3-byte and 4-byte UTF-8
character set data.  The exception is that in table definitions, 'utf8'
is used because MySQL converts instances of 'utf8mb3' specified in such
definitions to 'utf8', which is an alias for 'utf8mb3'.

One advantage of converting from 'utf8mb3' to 'utf8mb4' is that this
enables applications to use supplementary characters.  One tradeoff is
that this may increase data storage space requirements.

In terms of table content, conversion from 'utf8mb3' to 'utf8mb4'
presents no problems:

   * For a BMP character, 'utf8mb4' and 'utf8mb3' have identical storage
     characteristics: same code values, same encoding, same length.

   * For a supplementary character, 'utf8mb4' requires four bytes to
     store it, whereas 'utf8mb3' cannot store the character at all.
     When converting 'utf8mb3' columns to 'utf8mb4', you need not worry
     about converting supplementary characters because there will be
     none.

In terms of table structure, these are the primary potential
incompatibilities:

   * For the variable-length character data types (*note 'VARCHAR':
     char. and the *note 'TEXT': blob. types), the maximum permitted
     length in characters is less for 'utf8mb4' columns than for
     'utf8mb3' columns.

   * For all character data types (*note 'CHAR': char, *note 'VARCHAR':
     char, and the *note 'TEXT': blob. types), the maximum number of
     characters that can be indexed is less for 'utf8mb4' columns than
     for 'utf8mb3' columns.

Consequently, to convert tables from 'utf8mb3' to 'utf8mb4', it may be
necessary to change some column or index definitions.

Tables can be converted from 'utf8mb3' to 'utf8mb4' by using *note
'ALTER TABLE': alter-table.  Suppose that a table has this definition:

     CREATE TABLE t1 (
       col1 CHAR(10) CHARACTER SET utf8 COLLATE utf8_unicode_ci NOT NULL,
       col2 CHAR(10) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL
     ) CHARACTER SET utf8;

The following statement converts 't1' to use 'utf8mb4':

     ALTER TABLE t1
       DEFAULT CHARACTER SET utf8mb4,
       MODIFY col1 CHAR(10)
         CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci NOT NULL,
       MODIFY col2 CHAR(10)
         CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL;

The catch when converting from 'utf8mb3' to 'utf8mb4' is that the
maximum length of a column or index key is unchanged in terms of
_bytes_.  Therefore, it is smaller in terms of _characters_ because the
maximum length of a character is four bytes instead of three.  For the
*note 'CHAR': char, *note 'VARCHAR': char, and *note 'TEXT': blob. data
types, watch for these issues when converting your MySQL tables:

   * Check all definitions of 'utf8mb3' columns and make sure they will
     not exceed the maximum length for the storage engine.

   * Check all indexes on 'utf8mb3' columns and make sure they will not
     exceed the maximum length for the storage engine.  Sometimes the
     maximum can change due to storage engine enhancements.

If the preceding conditions apply, you must either reduce the defined
length of columns or indexes, or continue to use 'utf8mb3' rather than
'utf8mb4'.

Here are some examples where structural changes may be needed:

   * A *note 'TINYTEXT': blob. column can hold up to 255 bytes, so it
     can hold up to 85 3-byte or 63 4-byte characters.  Suppose that you
     have a *note 'TINYTEXT': blob. column that uses 'utf8mb3' but must
     be able to contain more than 63 characters.  You cannot convert it
     to 'utf8mb4' unless you also change the data type to a longer type
     such as *note 'TEXT': blob.

     Similarly, a very long *note 'VARCHAR': char. column may need to be
     changed to one of the longer *note 'TEXT': blob. types if you want
     to convert it from 'utf8mb3' to 'utf8mb4'.

   * 'InnoDB' has a maximum index length of 767 bytes, so for 'utf8mb3'
     or 'utf8mb4' columns, you can index a maximum of 255 or 191
     characters, respectively.  If you currently have 'utf8mb3' columns
     with indexes longer than 191 characters, you will need to index a
     smaller number of characters.  In an 'InnoDB' table, these column
     and index definitions are legal:

          col1 VARCHAR(500) CHARACTER SET utf8, INDEX (col1(255))

     To use 'utf8mb4' instead, the index must be smaller:

          col1 VARCHAR(500) CHARACTER SET utf8mb4, INDEX (col1(191))

The preceding types of changes are most likely to be required only if
you have very long columns or indexes.  Otherwise, you should be able to
convert your tables from 'utf8mb3' to 'utf8mb4' without problems, using
*note 'ALTER TABLE': alter-table. as described previously.

The following items summarize other potential incompatibilities:

   * 'SET NAMES 'utf8mb4'' causes use of the 4-byte character set for
     connection character sets.  As long as no 4-byte characters are
     sent from the server, there should be no problems.  Otherwise,
     applications that expect to receive a maximum of three bytes per
     character may have problems.  Conversely, applications that expect
     to send 4-byte characters must ensure that the server understands
     them.

   * For replication, if character sets that support supplementary
     characters are to be used on the master, all slaves must understand
     them as well.

     Also, keep in mind the general principle that if a table has
     different definitions on the master and slave, this can lead to
     unexpected results.  For example, the differences in maximum index
     key length make it risky to use 'utf8mb3' on the master and
     'utf8mb4' on the slave.

If you have converted to 'utf8mb4', 'utf16', or 'utf32', and then decide
to convert back to 'utf8mb3' or 'ucs2' (for example, to downgrade to an
older version of MySQL), these considerations apply:

   * 'utf8mb3' and 'ucs2' data should present no problems.

   * The server must be recent enough to recognize definitions referring
     to the character set from which you are converting.

   * For object definitions that refer to the 'utf8mb4' character set,
     you can dump them with *note 'mysqldump': mysqldump. prior to
     downgrading, edit the dump file to change instances of 'utf8mb4' to
     'utf8', and reload the file in the older server, as long as there
     are no 4-byte characters in the data.  The older server will see
     'utf8' in the dump file object definitions and create new objects
     that use the (3-byte) 'utf8' character set.


File: manual.info.tmp,  Node: charset-charsets,  Next: charset-restrictions,  Prev: charset-unicode,  Up: charset

10.10 Supported Character Sets and Collations
=============================================

* Menu:

* charset-unicode-sets::         Unicode Character Sets
* charset-we-sets::              West European Character Sets
* charset-ce-sets::              Central European Character Sets
* charset-se-me-sets::           South European and Middle East Character Sets
* charset-baltic-sets::          Baltic Character Sets
* charset-cyrillic-sets::        Cyrillic Character Sets
* charset-asian-sets::           Asian Character Sets
* charset-binary-set::           The Binary Character Set

This section indicates which character sets MySQL supports.  There is
one subsection for each group of related character sets.  For each
character set, the permissible collations are listed.

To list the available character sets and their default collations, use
the *note 'SHOW CHARACTER SET': show-character-set. statement or query
the 'INFORMATION_SCHEMA' *note 'CHARACTER_SETS': character-sets-table.
table.  For example:

     mysql> SHOW CHARACTER SET;
     +----------+-----------------------------+---------------------+--------+
     | Charset  | Description                 | Default collation   | Maxlen |
     +----------+-----------------------------+---------------------+--------+
     | big5     | Big5 Traditional Chinese    | big5_chinese_ci     |      2 |
     | dec8     | DEC West European           | dec8_swedish_ci     |      1 |
     | cp850    | DOS West European           | cp850_general_ci    |      1 |
     | hp8      | HP West European            | hp8_english_ci      |      1 |
     | koi8r    | KOI8-R Relcom Russian       | koi8r_general_ci    |      1 |
     | latin1   | cp1252 West European        | latin1_swedish_ci   |      1 |
     | latin2   | ISO 8859-2 Central European | latin2_general_ci   |      1 |
     | swe7     | 7bit Swedish                | swe7_swedish_ci     |      1 |
     | ascii    | US ASCII                    | ascii_general_ci    |      1 |
     | ujis     | EUC-JP Japanese             | ujis_japanese_ci    |      3 |
     | sjis     | Shift-JIS Japanese          | sjis_japanese_ci    |      2 |
     | hebrew   | ISO 8859-8 Hebrew           | hebrew_general_ci   |      1 |
     | tis620   | TIS620 Thai                 | tis620_thai_ci      |      1 |
     | euckr    | EUC-KR Korean               | euckr_korean_ci     |      2 |
     | koi8u    | KOI8-U Ukrainian            | koi8u_general_ci    |      1 |
     | gb2312   | GB2312 Simplified Chinese   | gb2312_chinese_ci   |      2 |
     | greek    | ISO 8859-7 Greek            | greek_general_ci    |      1 |
     | cp1250   | Windows Central European    | cp1250_general_ci   |      1 |
     | gbk      | GBK Simplified Chinese      | gbk_chinese_ci      |      2 |
     | latin5   | ISO 8859-9 Turkish          | latin5_turkish_ci   |      1 |
     | armscii8 | ARMSCII-8 Armenian          | armscii8_general_ci |      1 |
     | utf8     | UTF-8 Unicode               | utf8_general_ci     |      3 |
     | ucs2     | UCS-2 Unicode               | ucs2_general_ci     |      2 |
     | cp866    | DOS Russian                 | cp866_general_ci    |      1 |
     | keybcs2  | DOS Kamenicky Czech-Slovak  | keybcs2_general_ci  |      1 |
     | macce    | Mac Central European        | macce_general_ci    |      1 |
     | macroman | Mac West European           | macroman_general_ci |      1 |
     | cp852    | DOS Central European        | cp852_general_ci    |      1 |
     | latin7   | ISO 8859-13 Baltic          | latin7_general_ci   |      1 |
     | utf8mb4  | UTF-8 Unicode               | utf8mb4_general_ci  |      4 |
     | cp1251   | Windows Cyrillic            | cp1251_general_ci   |      1 |
     | utf16    | UTF-16 Unicode              | utf16_general_ci    |      4 |
     | cp1256   | Windows Arabic              | cp1256_general_ci   |      1 |
     | cp1257   | Windows Baltic              | cp1257_general_ci   |      1 |
     | utf32    | UTF-32 Unicode              | utf32_general_ci    |      4 |
     | binary   | Binary pseudo charset       | binary              |      1 |
     | geostd8  | GEOSTD8 Georgian            | geostd8_general_ci  |      1 |
     | cp932    | SJIS for Windows Japanese   | cp932_japanese_ci   |      2 |
     | eucjpms  | UJIS for Windows Japanese   | eucjpms_japanese_ci |      3 |
     +----------+-----------------------------+---------------------+--------+

In cases where a character set has multiple collations, it might not be
clear which collation is most suitable for a given application.  To
avoid choosing the wrong collation, it can be helpful to perform some
comparisons with representative data values to make sure that a given
collation sorts values the way you expect.


File: manual.info.tmp,  Node: charset-unicode-sets,  Next: charset-we-sets,  Prev: charset-charsets,  Up: charset-charsets

10.10.1 Unicode Character Sets
------------------------------

This section describes the collations available for Unicode character
sets and their differentiating properties.  For general information
about Unicode, see *note charset-unicode::.

MySQL supports multiple Unicode character sets:

   * 'utf8mb4': A UTF-8 encoding of the Unicode character set using one
     to four bytes per character.

   * 'utf8mb3': A UTF-8 encoding of the Unicode character set using one
     to three bytes per character.

   * 'utf8': An alias for 'utf8mb3'.

   * 'ucs2': The UCS-2 encoding of the Unicode character set using two
     bytes per character.

   * 'utf16': The UTF-16 encoding for the Unicode character set using
     two or four bytes per character.  Like 'ucs2' but with an extension
     for supplementary characters.

   * 'utf32': The UTF-32 encoding for the Unicode character set using
     four bytes per character.

'utf8mb4', 'utf16', and 'utf32' support Basic Multilingual Plane (BMP)
characters and supplementary characters that lie outside the BMP. 'utf8'
and 'ucs2' support only BMP characters.

Most Unicode character sets have a general collation (indicated by
'_general' in the name or by the absence of a language specifier), a
binary collation (indicated by '_bin' in the name), and several
language-specific collations (indicated by language specifiers).  For
example, for 'utf8mb4', 'utf8mb4_general_ci' and 'utf8mb4_bin' are its
general and binary collations, and 'utf8mb4_danish_ci' is one of its
language-specific collations.

   * *note charset-unicode-sets-uca::

   * *note charset-unicode-sets-language-specific-collations::

   * *note charset-unicode-sets-general-versus-unicode::

   * *note charset-unicode-sets-collating-weights::

   * *note charset-unicode-sets-miscellaneous::

*Unicode Collation Algorithm (UCA) Versions*

MySQL implements the 'XXX_unicode_ci' collations according to the
Unicode Collation Algorithm (UCA) described at
<http://www.unicode.org/reports/tr10/>.  The collation uses the
version-4.0.0 UCA weight keys:
<http://www.unicode.org/Public/UCA/4.0.0/allkeys-4.0.0.txt>.  The
'XXX_unicode_ci' collations have only partial support for the Unicode
Collation Algorithm.  Some characters are not supported, and combining
marks are not fully supported.  This affects primarily Vietnamese,
Yoruba, and some smaller languages such as Navajo.  A combined character
is considered different from the same character written with a single
unicode character in string comparisons, and the two characters are
considered to have a different length (for example, as returned by the
'CHAR_LENGTH()' function or in result set metadata).

*Language-Specific Collations*

MySQL implements language-specific Unicode collations if the ordering
based only on the Unicode Collation Algorithm (UCA) does not work well
for a language.  Language-specific collations are UCA-based, with
additional language tailoring rules.  Examples of such rules appear
later in this section.  For questions about particular language
orderings, <unicode.org> provides Common Locale Data Repository (CLDR)
collation charts at
<http://www.unicode.org/cldr/charts/30/collation/index.html>.

A language name shown in the following table indicates a
language-specific collation.  Unicode character sets may include
collations for one or more of these languages.

*Unicode Collation Language Specifiers*

Language                      Language Specifier
                              
Classical Latin               'roman'
                              
Czech                         'czech'
                              
Danish                        'danish'
                              
Esperanto                     'esperanto'
                              
Estonian                      'estonian'
                              
Hungarian                     'hungarian'
                              
Icelandic                     'icelandic'
                              
Latvian                       'latvian'
                              
Lithuanian                    'lithuanian'
                              
Persian                       'persian'
                              
Polish                        'polish'
                              
Romanian                      'romanian'
                              
Sinhala                       'sinhala'
                              
Slovak                        'slovak'
                              
Slovenian                     'slovenian'
                              
Modern Spanish                'spanish'
                              
Traditional Spanish           'spanish2'
                              
Swedish                       'swedish'
                              
Turkish                       'turkish'

Danish collations may also be used for Norwegian.

For Classical Latin collations, 'I' and 'J' compare as equal, and 'U'
and 'V' compare as equal.

Spanish collations are available for modern and traditional Spanish.
For both, 'ñ' (n-tilde) is a separate letter between 'n' and 'o'.  In
addition, for traditional Spanish, 'ch' is a separate letter between 'c'
and 'd', and 'll' is a separate letter between 'l' and 'm'.

Traditional Spanish collations may also be used for Asturian and
Galician.

Swedish collations include Swedish rules.  For example, in Swedish, the
following relationship holds, which is not something expected by a
German or French speaker:

     U" = Y < O"

*_general_ci Versus _unicode_ci Collations*

For any Unicode character set, operations performed using the
'XXX_general_ci' collation are faster than those for the
'XXX_unicode_ci' collation.  For example, comparisons for the
'utf8_general_ci' collation are faster, but slightly less correct, than
comparisons for 'utf8_unicode_ci'.  The reason is that 'utf8_unicode_ci'
supports mappings such as expansions; that is, when one character
compares as equal to combinations of other characters.  For example,
'ss' is equal to 'ss' in German and some other languages.
'utf8_unicode_ci' also supports contractions and ignorable characters.
'utf8_general_ci' is a legacy collation that does not support
expansions, contractions, or ignorable characters.  It can make only
one-to-one comparisons between characters.

To further illustrate, the following equalities hold in both
'utf8_general_ci' and 'utf8_unicode_ci' (for the effect of this in
comparisons or searches, see *note charset-collation-effect::):

     A" = A
     O" = O
     U" = U

A difference between the collations is that this is true for
'utf8_general_ci':

     ss = s

Whereas this is true for 'utf8_unicode_ci', which supports the German
DIN-1 ordering (also known as dictionary order):

     ss = ss

MySQL implements 'utf8' language-specific collations if the ordering
with 'utf8_unicode_ci' does not work well for a language.  For example,
'utf8_unicode_ci' works fine for German dictionary order and French, so
there is no need to create special 'utf8' collations.

'utf8_general_ci' also is satisfactory for both German and French,
except that 'ss' is equal to 's', and not to 'ss'.  If this is
acceptable for your application, you should use 'utf8_general_ci'
because it is faster.  Otherwise, use 'utf8_unicode_ci' because it is
more accurate.

*Character Collating Weights*

For all Unicode collations except the '_bin' (binary) collations, MySQL
performs a table lookup to find a character's collating weight.  If a
character is not in the table (for example, because it is a 'new'
character), collating weight determination becomes more complex:

   * For BMP characters in general collations ('XXX_general_ci'), the
     weight is the code point.

   * For BMP characters in UCA collations (for example, 'XXX_unicode_ci'
     and language-specific collations), the following algorithm applies:

          if (code >= 0x3400 && code <= 0x4DB5)
            base= 0xFB80; /* CJK Ideograph Extension */
          else if (code >= 0x4E00 && code <= 0x9FA5)
            base= 0xFB40; /* CJK Ideograph */
          else
            base= 0xFBC0; /* All other characters */
          aaaa= base +  (code >> 15);
          bbbb= (code & 0x7FFF) | 0x8000;

     The result is a sequence of two collating elements, 'aaaa' followed
     by 'bbbb'.

     Thus, 'U+04cf CYRILLIC SMALL LETTER PALOCHKA' currently is, with
     all UCA collations, greater than 'U+04c0 CYRILLIC LETTER PALOCHKA'.
     Eventually, after further collation tuning, all palochkas will sort
     together.

   * For supplementary characters in general collations, the weight is
     the weight for '0xfffd REPLACEMENT CHARACTER'.  For supplementary
     characters in UCA 4.0.0 collations, their collating weight is
     '0xfffd'.  That is, to MySQL, all supplementary characters are
     equal to each other, and greater than almost all BMP characters.

     An example with Deseret characters and 'COUNT(DISTINCT)':

          CREATE TABLE t (s1 VARCHAR(5) CHARACTER SET utf32 COLLATE utf32_unicode_ci);
          INSERT INTO t VALUES (0xfffd);   /* REPLACEMENT CHARACTER */
          INSERT INTO t VALUES (0x010412); /* DESERET CAPITAL LETTER BEE */
          INSERT INTO t VALUES (0x010413); /* DESERET CAPITAL LETTER TEE */
          SELECT COUNT(DISTINCT s1) FROM t;

     The result is 2 because in the MySQL 'XXX_unicode_ci' collations,
     the replacement character has a weight of '0x0dc6', whereas Deseret
     Bee and Deseret Tee both have a weight of '0xfffd'.  (Were the
     'utf32_general_ci' collation used instead, the result is 1 because
     all three characters have a weight of '0xfffd' in that collation.)

The rule that all supplementary characters are equal to each other is
nonoptimal but is not expected to cause trouble.  These characters are
very rare, so it is very rare that a multi-character string consists
entirely of supplementary characters.  In Japan, since the supplementary
characters are obscure Kanji ideographs, the typical user does not care
what order they are in, anyway.  If you really want rows sorted by the
MySQL rule and secondarily by code point value, it is easy:

     ORDER BY s1 COLLATE utf32_unicode_ci, s1 COLLATE utf32_bin

There is a difference between 'ordering by the character's code value'
and 'ordering by the character's binary representation,' a difference
that appears only with 'utf16_bin', because of surrogates.

Suppose that 'utf16_bin' (the binary collation for 'utf16') was a binary
comparison 'byte by byte' rather than 'character by character.' If that
were so, the order of characters in 'utf16_bin' would differ from the
order in 'utf8_bin'.  For example, the following chart shows two rare
characters.  The first character is in the range 'E000'-'FFFF', so it is
greater than a surrogate but less than a supplementary.  The second
character is a supplementary.

     Code point  Character                    utf8         utf16
     ----------  ---------                    ----         -----
     0FF9D       HALFWIDTH KATAKANA LETTER N  EF BE 9D     FF 9D
     10384       UGARITIC LETTER DELTA        F0 90 8E 84  D8 00 DF 84

The two characters in the chart are in order by code point value because
'0xff9d' < '0x10384'.  And they are in order by 'utf8' value because
'0xef' < '0xf0'.  But they are not in order by 'utf16' value, if we use
byte-by-byte comparison, because '0xff' > '0xd8'.

So MySQL's 'utf16_bin' collation is not 'byte by byte.' It is 'by code
point.' When MySQL sees a supplementary-character encoding in 'utf16',
it converts to the character's code-point value, and then compares.
Therefore, 'utf8_bin' and 'utf16_bin' are the same ordering.  This is
consistent with the SQL:2008 standard requirement for a UCS_BASIC
collation: 'UCS_BASIC is a collation in which the ordering is determined
entirely by the Unicode scalar values of the characters in the strings
being sorted. It is applicable to the UCS character repertoire. Since
every character repertoire is a subset of the UCS repertoire, the
UCS_BASIC collation is potentially applicable to every character set.
NOTE 11: The Unicode scalar value of a character is its code point
treated as an unsigned integer.'

If the character set is 'ucs2', comparison is byte-by-byte, but 'ucs2'
strings should not contain surrogates, anyway.

*Miscellaneous Information*

The 'XXX_general_mysql500_ci' collations were added in MySQL 5.5.21.
They preserve the pre-5.1.24 ordering of the original 'XXX_general_ci'
collations and permit upgrades for tables created before MySQL 5.1.24
(Bug #27877).


File: manual.info.tmp,  Node: charset-we-sets,  Next: charset-ce-sets,  Prev: charset-unicode-sets,  Up: charset-charsets

10.10.2 West European Character Sets
------------------------------------

Western European character sets cover most West European languages, such
as French, Spanish, Catalan, Basque, Portuguese, Italian, Albanian,
Dutch, German, Danish, Swedish, Norwegian, Finnish, Faroese, Icelandic,
Irish, Scottish, and English.

   * 'ascii' (US ASCII) collations:

        * 'ascii_bin'

        * 'ascii_general_ci' (default)

   * 'cp850' (DOS West European) collations:

        * 'cp850_bin'

        * 'cp850_general_ci' (default)

   * 'dec8' (DEC Western European) collations:

        * 'dec8_bin'

        * 'dec8_swedish_ci' (default)

   * 'hp8' (HP Western European) collations:

        * 'hp8_bin'

        * 'hp8_english_ci' (default)

   * 'latin1' (cp1252 West European) collations:

        * 'latin1_bin'

        * 'latin1_danish_ci'

        * 'latin1_general_ci'

        * 'latin1_general_cs'

        * 'latin1_german1_ci'

        * 'latin1_german2_ci'

        * 'latin1_spanish_ci'

        * 'latin1_swedish_ci' (default)

     'latin1' is the default character set.  MySQL's 'latin1' is the
     same as the Windows 'cp1252' character set.  This means it is the
     same as the official 'ISO 8859-1' or IANA (Internet Assigned
     Numbers Authority) 'latin1', except that IANA 'latin1' treats the
     code points between '0x80' and '0x9f' as 'undefined,' whereas
     'cp1252', and therefore MySQL's 'latin1', assign characters for
     those positions.  For example, '0x80' is the Euro sign.  For the
     'undefined' entries in 'cp1252', MySQL translates '0x81' to Unicode
     '0x0081', '0x8d' to '0x008d', '0x8f' to '0x008f', '0x90' to
     '0x0090', and '0x9d' to '0x009d'.

     The 'latin1_swedish_ci' collation is the default that probably is
     used by the majority of MySQL customers.  Although it is frequently
     said that it is based on the Swedish/Finnish collation rules, there
     are Swedes and Finns who disagree with this statement.

     The 'latin1_german1_ci' and 'latin1_german2_ci' collations are
     based on the DIN-1 and DIN-2 standards, where DIN stands for
     _Deutsches Institut fu"r Normung_ (the German equivalent of ANSI).
     DIN-1 is called the 'dictionary collation' and DIN-2 is called the
     'phone book collation.' For an example of the effect this has in
     comparisons or when doing searches, see *note
     charset-collation-effect::.

        * 'latin1_german1_ci' (dictionary) rules:

               A" = A
               O" = O
               U" = U
               ss = s

        * 'latin1_german2_ci' (phone-book) rules:

               A" = AE
               O" = OE
               U" = UE
               ss = ss

     In the 'latin1_spanish_ci' collation, 'ñ' (n-tilde) is a separate
     letter between 'n' and 'o'.

   * 'macroman' (Mac West European) collations:

        * 'macroman_bin'

        * 'macroman_general_ci' (default)

   * 'swe7' (7bit Swedish) collations:

        * 'swe7_bin'

        * 'swe7_swedish_ci' (default)


File: manual.info.tmp,  Node: charset-ce-sets,  Next: charset-se-me-sets,  Prev: charset-we-sets,  Up: charset-charsets

10.10.3 Central European Character Sets
---------------------------------------

MySQL provides some support for character sets used in the Czech
Republic, Slovakia, Hungary, Romania, Slovenia, Croatia, Poland, and
Serbia (Latin).

   * 'cp1250' (Windows Central European) collations:

        * 'cp1250_bin'

        * 'cp1250_croatian_ci'

        * 'cp1250_czech_cs'

        * 'cp1250_general_ci' (default)

        * 'cp1250_polish_ci'

   * 'cp852' (DOS Central European) collations:

        * 'cp852_bin'

        * 'cp852_general_ci' (default)

   * 'keybcs2' (DOS Kamenicky Czech-Slovak) collations:

        * 'keybcs2_bin'

        * 'keybcs2_general_ci' (default)

   * 'latin2' (ISO 8859-2 Central European) collations:

        * 'latin2_bin'

        * 'latin2_croatian_ci'

        * 'latin2_czech_cs'

        * 'latin2_general_ci' (default)

        * 'latin2_hungarian_ci'

   * 'macce' (Mac Central European) collations:

        * 'macce_bin'

        * 'macce_general_ci' (default)


File: manual.info.tmp,  Node: charset-se-me-sets,  Next: charset-baltic-sets,  Prev: charset-ce-sets,  Up: charset-charsets

10.10.4 South European and Middle East Character Sets
-----------------------------------------------------

South European and Middle Eastern character sets supported by MySQL
include Armenian, Arabic, Georgian, Greek, Hebrew, and Turkish.

   * 'armscii8' (ARMSCII-8 Armenian) collations:

        * 'armscii8_bin'

        * 'armscii8_general_ci' (default)

   * 'cp1256' (Windows Arabic) collations:

        * 'cp1256_bin'

        * 'cp1256_general_ci' (default)

   * 'geostd8' (GEOSTD8 Georgian) collations:

        * 'geostd8_bin'

        * 'geostd8_general_ci' (default)

   * 'greek' (ISO 8859-7 Greek) collations:

        * 'greek_bin'

        * 'greek_general_ci' (default)

   * 'hebrew' (ISO 8859-8 Hebrew) collations:

        * 'hebrew_bin'

        * 'hebrew_general_ci' (default)

   * 'latin5' (ISO 8859-9 Turkish) collations:

        * 'latin5_bin'

        * 'latin5_turkish_ci' (default)


File: manual.info.tmp,  Node: charset-baltic-sets,  Next: charset-cyrillic-sets,  Prev: charset-se-me-sets,  Up: charset-charsets

10.10.5 Baltic Character Sets
-----------------------------

The Baltic character sets cover Estonian, Latvian, and Lithuanian
languages.

   * 'cp1257' (Windows Baltic) collations:

        * 'cp1257_bin'

        * 'cp1257_general_ci' (default)

        * 'cp1257_lithuanian_ci'

   * 'latin7' (ISO 8859-13 Baltic) collations:

        * 'latin7_bin'

        * 'latin7_estonian_cs'

        * 'latin7_general_ci' (default)

        * 'latin7_general_cs'


File: manual.info.tmp,  Node: charset-cyrillic-sets,  Next: charset-asian-sets,  Prev: charset-baltic-sets,  Up: charset-charsets

10.10.6 Cyrillic Character Sets
-------------------------------

The Cyrillic character sets and collations are for use with Belarusian,
Bulgarian, Russian, Ukrainian, and Serbian (Cyrillic) languages.

   * 'cp1251' (Windows Cyrillic) collations:

        * 'cp1251_bin'

        * 'cp1251_bulgarian_ci'

        * 'cp1251_general_ci' (default)

        * 'cp1251_general_cs'

        * 'cp1251_ukrainian_ci'

   * 'cp866' (DOS Russian) collations:

        * 'cp866_bin'

        * 'cp866_general_ci' (default)

   * 'koi8r' (KOI8-R Relcom Russian) collations:

        * 'koi8r_bin'

        * 'koi8r_general_ci' (default)

   * 'koi8u' (KOI8-U Ukrainian) collations:

        * 'koi8u_bin'

        * 'koi8u_general_ci' (default)


File: manual.info.tmp,  Node: charset-asian-sets,  Next: charset-binary-set,  Prev: charset-cyrillic-sets,  Up: charset-charsets

10.10.7 Asian Character Sets
----------------------------

* Menu:

* charset-cp932::                The cp932 Character Set

The Asian character sets that we support include Chinese, Japanese,
Korean, and Thai.  These can be complicated.  For example, the Chinese
sets must allow for thousands of different characters.  See *note
charset-cp932::, for additional information about the 'cp932' and 'sjis'
character sets.

For answers to some common questions and problems relating support for
Asian character sets in MySQL, see *note faqs-cjk::.

   * 'big5' (Big5 Traditional Chinese) collations:

        * 'big5_bin'

        * 'big5_chinese_ci' (default)

   * 'cp932' (SJIS for Windows Japanese) collations:

        * 'cp932_bin'

        * 'cp932_japanese_ci' (default)

   * 'eucjpms' (UJIS for Windows Japanese) collations:

        * 'eucjpms_bin'

        * 'eucjpms_japanese_ci' (default)

   * 'euckr' (EUC-KR Korean) collations:

        * 'euckr_bin'

        * 'euckr_korean_ci' (default)

   * 'gb2312' (GB2312 Simplified Chinese) collations:

        * 'gb2312_bin'

        * 'gb2312_chinese_ci' (default)

   * 'gbk' (GBK Simplified Chinese) collations:

        * 'gbk_bin'

        * 'gbk_chinese_ci' (default)

   * 'sjis' (Shift-JIS Japanese) collations:

        * 'sjis_bin'

        * 'sjis_japanese_ci' (default)

   * 'tis620' (TIS620 Thai) collations:

        * 'tis620_bin'

        * 'tis620_thai_ci' (default)

   * 'ujis' (EUC-JP Japanese) collations:

        * 'ujis_bin'

        * 'ujis_japanese_ci' (default)

The 'big5_chinese_ci' collation sorts on number of strokes.


File: manual.info.tmp,  Node: charset-cp932,  Prev: charset-asian-sets,  Up: charset-asian-sets

10.10.7.1 The cp932 Character Set
.................................

*Why is 'cp932' needed?*

In MySQL, the 'sjis' character set corresponds to the 'Shift_JIS'
character set defined by IANA, which supports JIS X0201 and JIS X0208
characters.  (See <http://www.iana.org/assignments/character-sets>.)

However, the meaning of 'SHIFT JIS' as a descriptive term has become
very vague and it often includes the extensions to 'Shift_JIS' that are
defined by various vendors.

For example, 'SHIFT JIS' used in Japanese Windows environments is a
Microsoft extension of 'Shift_JIS' and its exact name is 'Microsoft
Windows Codepage : 932' or 'cp932'.  In addition to the characters
supported by 'Shift_JIS', 'cp932' supports extension characters such as
NEC special characters, NEC selected--IBM extended characters, and IBM
selected characters.

Many Japanese users have experienced problems using these extension
characters.  These problems stem from the following factors:

   * MySQL automatically converts character sets.

   * Character sets are converted using Unicode ('ucs2').

   * The 'sjis' character set does not support the conversion of these
     extension characters.

   * There are several conversion rules from so-called 'SHIFT JIS' to
     Unicode, and some characters are converted to Unicode differently
     depending on the conversion rule.  MySQL supports only one of these
     rules (described later).

The MySQL 'cp932' character set is designed to solve these problems.

Because MySQL supports character set conversion, it is important to
separate IANA 'Shift_JIS' and 'cp932' into two different character sets
because they provide different conversion rules.

*How does 'cp932' differ from 'sjis'?*

The 'cp932' character set differs from 'sjis' in the following ways:

   * 'cp932' supports NEC special characters, NEC selected--IBM extended
     characters, and IBM selected characters.

   * Some 'cp932' characters have two different code points, both of
     which convert to the same Unicode code point.  When converting from
     Unicode back to 'cp932', one of the code points must be selected.
     For this 'round trip conversion,' the rule recommended by Microsoft
     is used.  (See <http://support.microsoft.com/kb/170559/EN-US/>.)

     The conversion rule works like this:

        * If the character is in both JIS X 0208 and NEC special
          characters, use the code point of JIS X 0208.

        * If the character is in both NEC special characters and IBM
          selected characters, use the code point of NEC special
          characters.

        * If the character is in both IBM selected characters and NEC
          selected--IBM extended characters, use the code point of IBM
          extended characters.

     The table shown at
     <https://msdn.microsoft.com/en-us/goglobal/cc305152.aspx> provides
     information about the Unicode values of 'cp932' characters.  For
     'cp932' table entries with characters under which a four-digit
     number appears, the number represents the corresponding Unicode
     ('ucs2') encoding.  For table entries with an underlined two-digit
     value appears, there is a range of 'cp932' character values that
     begin with those two digits.  Clicking such a table entry takes you
     to a page that displays the Unicode value for each of the 'cp932'
     characters that begin with those digits.

     The following links are of special interest.  They correspond to
     the encodings for the following sets of characters:

        * NEC special characters (lead byte '0x87'):

               <https://msdn.microsoft.com/en-us/goglobal/gg674964>

        * NEC selected--IBM extended characters (lead byte '0xED' and
          '0xEE'):

               <https://msdn.microsoft.com/en-us/goglobal/gg671837>
               <https://msdn.microsoft.com/en-us/goglobal/gg671838>

        * IBM selected characters (lead byte '0xFA', '0xFB', '0xFC'):

               <https://msdn.microsoft.com/en-us/goglobal/gg671839>
               <https://msdn.microsoft.com/en-us/goglobal/gg671840>
               <https://msdn.microsoft.com/en-us/goglobal/gg671841>

   * 'cp932' supports conversion of user-defined characters in
     combination with 'eucjpms', and solves the problems with
     'sjis'/'ujis' conversion.  For details, please refer to
     <http://www.sljfaq.org/afaq/encodings.html>.

For some characters, conversion to and from 'ucs2' is different for
'sjis' and 'cp932'.  The following tables illustrate these differences.

Conversion to 'ucs2':

'sjis'/'cp932' Value     'sjis' -> 'ucs2'         'cp932' -> 'ucs2'
                         Conversion               Conversion
                                                  
5C                       005C                     005C
                                                  
7E                       007E                     007E
                                                  
815C                     2015                     2015
                                                  
815F                     005C                     FF3C
                                                  
8160                     301C                     FF5E
                                                  
8161                     2016                     2225
                                                  
817C                     2212                     FF0D
                                                  
8191                     00A2                     FFE0
                                                  
8192                     00A3                     FFE1
                                                  
81CA                     00AC                     FFE2
                         

Conversion from 'ucs2':

'ucs2' value             'ucs2' -> 'sjis'         'ucs2' -> 'cp932'
                         Conversion               Conversion
                                                  
005C                     815F                     5C
                                                  
007E                     7E                       7E
                                                  
00A2                     8191                     3F
                                                  
00A3                     8192                     3F
                                                  
00AC                     81CA                     3F
                                                  
2015                     815C                     815C
                                                  
2016                     8161                     3F
                                                  
2212                     817C                     3F
                                                  
2225                     3F                       8161
                                                  
301C                     8160                     3F
                                                  
FF0D                     3F                       817C
                                                  
FF3C                     3F                       815F
                                                  
FF5E                     3F                       8160
                                                  
FFE0                     3F                       8191
                                                  
FFE1                     3F                       8192
                                                  
FFE2                     3F                       81CA
                         

Users of any Japanese character sets should be aware that using
'--character-set-client-handshake' (or
'--skip-character-set-client-handshake') has an important effect.  See
*note server-options::.


File: manual.info.tmp,  Node: charset-binary-set,  Prev: charset-asian-sets,  Up: charset-charsets

10.10.8 The Binary Character Set
--------------------------------

The 'binary' character set is the character set for binary strings,
which are sequences of bytes.  The 'binary' character set has one
collation, also named 'binary'.  Comparison and sorting are based on
numeric byte values, rather than on numeric character code values (which
for multibyte characters differ from numeric byte values).  For
information about the differences between the 'binary' collation of the
'binary' character set and the '_bin' collations of nonbinary character
sets, see *note charset-binary-collations::.

For the 'binary' character set, the concepts of lettercase and accent
equivalence do not apply:

   * For single-byte characters stored as binary strings, character and
     byte boundaries are the same, so lettercase and accent differences
     are significant in comparisons.  That is, the 'binary' collation is
     case-sensitive and accent-sensitive.

          mysql> SET NAMES 'binary';
          mysql> SELECT CHARSET('abc'), COLLATION('abc');
          +----------------+------------------+
          | CHARSET('abc') | COLLATION('abc') |
          +----------------+------------------+
          | binary         | binary           |
          +----------------+------------------+
          mysql> SELECT 'abc' = 'ABC', 'a' = 'a"';
          +---------------+------------+
          | 'abc' = 'ABC' | 'a' = 'a"'  |
          +---------------+------------+
          |             0 |          0 |
          +---------------+------------+

   * For multibyte characters stored as binary strings, character and
     byte boundaries differ.  Character boundaries are lost, so
     comparisons that depend on them are not meaningful.

To perform lettercase conversion of a binary string, first convert it to
a nonbinary string using a character set appropriate for the data stored
in the string:

     mysql> SET @str = BINARY 'New York';
     mysql> SELECT LOWER(@str), LOWER(CONVERT(@str USING utf8mb4));
     +-------------+------------------------------------+
     | LOWER(@str) | LOWER(CONVERT(@str USING utf8mb4)) |
     +-------------+------------------------------------+
     | New York    | new york                           |
     +-------------+------------------------------------+

To convert a string expression to a binary string, these constructs are
equivalent:

     BINARY EXPR
     CAST(EXPR AS BINARY)
     CONVERT(EXPR USING BINARY)

If a value is a character string literal, the '_binary' introducer may
be used to designate it as a binary string.  For example:

     _binary 'a'

The '_binary' introducer is permitted for hexadecimal literals and
bit-value literals as well, but unnecessary; such literals are binary
strings by default.

For more information about introducers, see *note charset-introducer::.


File: manual.info.tmp,  Node: charset-restrictions,  Next: error-message-language,  Prev: charset-charsets,  Up: charset

10.11 Restrictions on Character Sets
====================================

   * Identifiers are stored in 'mysql' database tables ('user', 'db',
     and so forth) using 'utf8', but identifiers can contain only
     characters in the Basic Multilingual Plane (BMP). Supplementary
     characters are not permitted in identifiers.

   * The 'ucs2', 'utf16', and 'utf32' character sets have the following
     restrictions:

        * None of them can be used as the client character set.  See
          *note charset-connection-impermissible-client-charset::.

        * It is currently not possible to use *note 'LOAD DATA':
          load-data. to load data files that use these character sets.

        * 'FULLTEXT' indexes cannot be created on a column that uses any
          of these character sets.  However, you can perform 'IN BOOLEAN
          MODE' searches on the column without an index.

        * The use of 'ENCRYPT()' with these character sets is not
          recommended because the underlying system call expects a
          string terminated by a zero byte.

   * The 'REGEXP' and 'RLIKE' operators work in byte-wise fashion, so
     they are not multibyte safe and may produce unexpected results with
     multibyte character sets.  In addition, these operators compare
     characters by their byte values and accented characters may not
     compare as equal even if a given collation treats them as equal.


File: manual.info.tmp,  Node: error-message-language,  Next: adding-character-set,  Prev: charset-restrictions,  Up: charset

10.12 Setting the Error Message Language
========================================

By default, *note 'mysqld': mysqld. produces error messages in English,
but they can be displayed instead in any of several other languages:
Czech, Danish, Dutch, Estonian, French, German, Greek, Hungarian,
Italian, Japanese, Korean, Norwegian, Norwegian-ny, Polish, Portuguese,
Romanian, Russian, Slovak, Spanish, or Swedish.  This applies to
messages the server writes to the error log and sends to clients.

To select the language in which the server writes error messages, follow
the instructions in this section.  For information about changing the
character set for error messages (rather than the language), see *note
charset-errors::.  For general information about configuring error
logging, see *note error-log::.

The server searches for the error message file using these rules:

   * It looks for the file in a directory constructed from two system
     variable values, 'lc_messages_dir' and 'lc_messages', with the
     latter converted to a language name.  Suppose that you start the
     server using this command:

          mysqld --lc_messages_dir=/usr/share/mysql --lc_messages=fr_FR

     In this case, *note 'mysqld': mysqld. maps the locale 'fr_FR' to
     the language 'french' and looks for the error file in the
     '/usr/share/mysql/french' directory.

     By default, the language files are located in the
     'share/mysql/LANGUAGE' directory under the MySQL base directory.

   * If the message file cannot be found in the directory constructed as
     just described, the server ignores the 'lc_messages' value and uses
     only the 'lc_messages_dir' value as the location in which to look.

The 'lc_messages_dir' system variable can be set only at server startup
and has only a global read-only value at runtime.  'lc_messages' can be
set at server startup and has global and session values that can be
modified at runtime.  Thus, the error message language can be changed
while the server is running, and each client can have its own error
message language by setting its session 'lc_messages' value to the
desired locale name.  For example, if the server is using the 'fr_FR'
locale for error messages, a client can execute this statement to
receive error messages in English:

     SET lc_messages = 'en_US';


File: manual.info.tmp,  Node: adding-character-set,  Next: adding-collation,  Prev: error-message-language,  Up: charset

10.13 Adding a Character Set
============================

* Menu:

* character-arrays::             Character Definition Arrays
* string-collating::             String Collating Support for Complex Character Sets
* multibyte-characters::         Multi-Byte Character Support for Complex Character Sets

This section discusses the procedure for adding a character set to
MySQL. The proper procedure depends on whether the character set is
simple or complex:

   * If the character set does not need special string collating
     routines for sorting and does not need multibyte character support,
     it is simple.

   * If the character set needs either of those features, it is complex.

For example, 'greek' and 'swe7' are simple character sets, whereas
'big5' and 'czech' are complex character sets.

To use the following instructions, you must have a MySQL source
distribution.  In the instructions, MYSET represents the name of the
character set that you want to add.

  1. Add a '<charset>' element for MYSET to the
     'sql/share/charsets/Index.xml' file.  Use the existing contents in
     the file as a guide to adding new contents.  A partial listing for
     the 'latin1' '<charset>' element follows:

          <charset name="latin1">
            <family>Western</family>
            <description>cp1252 West European</description>
            ...
            <collation name="latin1_swedish_ci" id="8" order="Finnish, Swedish">
              <flag>primary</flag>
              <flag>compiled</flag>
            </collation>
            <collation name="latin1_danish_ci" id="15" order="Danish"/>
            ...
            <collation name="latin1_bin" id="47" order="Binary">
              <flag>binary</flag>
              <flag>compiled</flag>
            </collation>
            ...
          </charset>

     The '<charset>' element must list all the collations for the
     character set.  These must include at least a binary collation and
     a default (primary) collation.  The default collation is often
     named using a suffix of 'general_ci' (general, case-insensitive).
     It is possible for the binary collation to be the default
     collation, but usually they are different.  The default collation
     should have a 'primary' flag.  The binary collation should have a
     'binary' flag.

     You must assign a unique ID number to each collation.  The range of
     IDs from 1024 to 2047 is reserved for user-defined collations.  To
     find the maximum of the currently used collation IDs, use this
     query:

          SELECT MAX(ID) FROM INFORMATION_SCHEMA.COLLATIONS;

  2. This step depends on whether you are adding a simple or complex
     character set.  A simple character set requires only a
     configuration file, whereas a complex character set requires C
     source file that defines collation functions, multibyte functions,
     or both.

     For a simple character set, create a configuration file,
     'MYSET.xml', that describes the character set properties.  Create
     this file in the 'sql/share/charsets' directory.  You can use a
     copy of 'latin1.xml' as the basis for this file.  The syntax for
     the file is very simple:

        * Comments are written as ordinary XML comments ('<!-- TEXT
          -->').

        * Words within '<map>' array elements are separated by arbitrary
          amounts of whitespace.

        * Each word within '<map>' array elements must be a number in
          hexadecimal format.

        * The '<map>' array element for the '<ctype>' element has 257
          words.  The other '<map>' array elements after that have 256
          words.  See *note character-arrays::.

        * For each collation listed in the '<charset>' element for the
          character set in 'Index.xml', 'MYSET.xml' must contain a
          '<collation>' element that defines the character ordering.

     For a complex character set, create a C source file that describes
     the character set properties and defines the support routines
     necessary to properly perform operations on the character set:

        * Create the file 'ctype-MYSET.c' in the 'strings' directory.
          Look at one of the existing 'ctype-*.c' files (such as
          'ctype-big5.c') to see what needs to be defined.  The arrays
          in your file must have names like 'ctype_MYSET',
          'to_lower_MYSET', and so on.  These correspond to the arrays
          for a simple character set.  See *note character-arrays::.

        * For each '<collation>' element listed in the '<charset>'
          element for the character set in 'Index.xml', the
          'ctype-MYSET.c' file must provide an implementation of the
          collation.

        * If the character set requires string collating functions, see
          *note string-collating::.

        * If the character set requires multibyte character support, see
          *note multibyte-characters::.

  3. Modify the configuration information.  Use the existing
     configuration information as a guide to adding information for
     MYSYS.  The example here assumes that the character set has default
     and binary collations, but more lines are needed if MYSET has
     additional collations.

       1. Edit 'mysys/charset-def.c', and 'register' the collations for
          the new character set.

          Add these lines to the 'declaration' section:

               #ifdef HAVE_CHARSET_MYSET
               extern CHARSET_INFO my_charset_MYSET_general_ci;
               extern CHARSET_INFO my_charset_MYSET_bin;
               #endif

          Add these lines to the 'registration' section:

               #ifdef HAVE_CHARSET_MYSET
                 add_compiled_collation(&my_charset_MYSET_general_ci);
                 add_compiled_collation(&my_charset_MYSET_bin);
               #endif

       2. If the character set uses 'ctype-MYSET.c', edit
          'strings/CMakeLists.txt' and add 'ctype-MYSET.c' to the
          definition of the 'STRINGS_SOURCES' variable.

       3. Edit 'cmake/character_sets.cmake':

            1. Add MYSET to the value of with 'CHARSETS_AVAILABLE' in
               alphabetic order.

            2. Add MYSET to the value of 'CHARSETS_COMPLEX' in
               alphabetic order.  This is needed even for simple
               character sets, or 'CMake' will not recognize
               '-DDEFAULT_CHARSET=MYSET'.

  4. Reconfigure, recompile, and test.


File: manual.info.tmp,  Node: character-arrays,  Next: string-collating,  Prev: adding-character-set,  Up: adding-character-set

10.13.1 Character Definition Arrays
-----------------------------------

Each simple character set has a configuration file located in the
'sql/share/charsets' directory.  For a character set named MYSYS, the
file is named 'MYSET.xml'.  It uses '<map>' array elements to list
character set properties.  '<map>' elements appear within these
elements:

   * '<ctype>' defines attributes for each character.

   * '<lower>' and '<upper>' list the lowercase and uppercase
     characters.

   * '<unicode>' maps 8-bit character values to Unicode values.

   * '<collation>' elements indicate character ordering for comparison
     and sorting, one element per collation.  Binary collations need no
     '<map>' element because the character codes themselves provide the
     ordering.

For a complex character set as implemented in a 'ctype-MYSET.c' file in
the 'strings' directory, there are corresponding arrays:
'ctype_MYSET[]', 'to_lower_MYSET[]', and so forth.  Not every complex
character set has all of the arrays.  See also the existing 'ctype-*.c'
files for examples.  See the 'CHARSET_INFO.txt' file in the 'strings'
directory for additional information.

Most of the arrays are indexed by character value and have 256 elements.
The '<ctype>' array is indexed by character value + 1 and has 257
elements.  This is a legacy convention for handling 'EOF'.

'<ctype>' array elements are bit values.  Each element describes the
attributes of a single character in the character set.  Each attribute
is associated with a bitmask, as defined in 'include/m_ctype.h':

     #define _MY_U   01      /* Upper case */
     #define _MY_L   02      /* Lower case */
     #define _MY_NMR 04      /* Numeral (digit) */
     #define _MY_SPC 010     /* Spacing character */
     #define _MY_PNT 020     /* Punctuation */
     #define _MY_CTR 040     /* Control character */
     #define _MY_B   0100    /* Blank */
     #define _MY_X   0200    /* heXadecimal digit */

The '<ctype>' value for a given character should be the union of the
applicable bitmask values that describe the character.  For example,
''A'' is an uppercase character ('_MY_U') as well as a hexadecimal digit
('_MY_X'), so its 'ctype' value should be defined like this:

     ctype['A'+1] = _MY_U | _MY_X = 01 | 0200 = 0201

The bitmask values in 'm_ctype.h' are octal values, but the elements of
the '<ctype>' array in 'MYSET.xml' should be written as hexadecimal
values.

The '<lower>' and '<upper>' arrays hold the lowercase and uppercase
characters corresponding to each member of the character set.  For
example:

     lower['A'] should contain 'a'
     upper['a'] should contain 'A'

Each '<collation>' array indicates how characters should be ordered for
comparison and sorting purposes.  MySQL sorts characters based on the
values of this information.  In some cases, this is the same as the
'<upper>' array, which means that sorting is case-insensitive.  For more
complicated sorting rules (for complex character sets), see the
discussion of string collating in *note string-collating::.


File: manual.info.tmp,  Node: string-collating,  Next: multibyte-characters,  Prev: character-arrays,  Up: adding-character-set

10.13.2 String Collating Support for Complex Character Sets
-----------------------------------------------------------

For a simple character set named MYSET, sorting rules are specified in
the 'MYSET.xml' configuration file using '<map>' array elements within
'<collation>' elements.  If the sorting rules for your language are too
complex to be handled with simple arrays, you must define string
collating functions in the 'ctype-MYSET.c' source file in the 'strings'
directory.

The existing character sets provide the best documentation and examples
to show how these functions are implemented.  Look at the 'ctype-*.c'
files in the 'strings' directory, such as the files for the 'big5',
'czech', 'gbk', 'sjis', and 'tis160' character sets.  Take a look at the
'MY_COLLATION_HANDLER' structures to see how they are used.  See also
the 'CHARSET_INFO.txt' file in the 'strings' directory for additional
information.


File: manual.info.tmp,  Node: multibyte-characters,  Prev: string-collating,  Up: adding-character-set

10.13.3 Multi-Byte Character Support for Complex Character Sets
---------------------------------------------------------------

If you want to add support for a new character set named MYSET that
includes multibyte characters, you must use multibyte character
functions in the 'ctype-MYSET.c' source file in the 'strings' directory.

The existing character sets provide the best documentation and examples
to show how these functions are implemented.  Look at the 'ctype-*.c'
files in the 'strings' directory, such as the files for the 'euc_kr',
'gb2312', 'gbk', 'sjis', and 'ujis' character sets.  Take a look at the
'MY_CHARSET_HANDLER' structures to see how they are used.  See also the
'CHARSET_INFO.txt' file in the 'strings' directory for additional
information.


File: manual.info.tmp,  Node: adding-collation,  Next: charset-configuration,  Prev: adding-character-set,  Up: charset

10.14 Adding a Collation to a Character Set
===========================================

* Menu:

* charset-collation-implementations::  Collation Implementation Types
* adding-collation-choosing-id::  Choosing a Collation ID
* adding-collation-simple-8bit::  Adding a Simple Collation to an 8-Bit Character Set
* adding-collation-unicode-uca::  Adding a UCA Collation to a Unicode Character Set

A collation is a set of rules that defines how to compare and sort
character strings.  Each collation in MySQL belongs to a single
character set.  Every character set has at least one collation, and most
have two or more collations.

A collation orders characters based on weights.  Each character in a
character set maps to a weight.  Characters with equal weights compare
as equal, and characters with unequal weights compare according to the
relative magnitude of their weights.

MySQL supports several collation implementations, as discussed in *note
charset-collation-implementations::.  Some of these can be added to
MySQL without recompiling:

   * Simple collations for 8-bit character sets.

   * UCA-based collations for Unicode character sets.

   * Binary ('XXX_bin') collations.

The following sections describe how to add collations of the first two
types to existing character sets.  All existing character sets already
have a binary collation, so there is no need here to describe how to add
one.

Summary of the procedure for adding a new collation:

  1. Choose a collation ID.

  2. Add configuration information that names the collation and
     describes the character-ordering rules.

  3. Restart the server.

  4. Verify that the collation is present.

The instructions here cover only collations that can be added without
recompiling MySQL. To add a collation that does require recompiling (as
implemented by means of functions in a C source file), use the
instructions in *note adding-character-set::.  However, instead of
adding all the information required for a complete character set, just
modify the appropriate files for an existing character set.  That is,
based on what is already present for the character set's current
collations, add data structures, functions, and configuration
information for the new collation.

*Note*:

If you modify an existing collation, that may affect the ordering of
rows for indexes on columns that use the collation.  In this case,
rebuild any such indexes to avoid problems such as incorrect query
results.  See *note rebuilding-tables::.

*Additional Resources*

   * The Unicode Collation Algorithm (UCA) specification:
     <http://www.unicode.org/reports/tr10/>

   * The Locale Data Markup Language (LDML) specification:
     <http://www.unicode.org/reports/tr35/>


File: manual.info.tmp,  Node: charset-collation-implementations,  Next: adding-collation-choosing-id,  Prev: adding-collation,  Up: adding-collation

10.14.1 Collation Implementation Types
--------------------------------------

MySQL implements several types of collations:

*Simple collations for 8-bit character sets*

This kind of collation is implemented using an array of 256 weights that
defines a one-to-one mapping from character codes to weights.
'latin1_swedish_ci' is an example.  It is a case-insensitive collation,
so the uppercase and lowercase versions of a character have the same
weights and they compare as equal.

     mysql> SET NAMES 'latin1' COLLATE 'latin1_swedish_ci';
     Query OK, 0 rows affected (0.00 sec)

     mysql> SELECT 'a' = 'A';
     +-----------+
     | 'a' = 'A' |
     +-----------+
     |         1 |
     +-----------+
     1 row in set (0.00 sec)

For implementation instructions, see *note
adding-collation-simple-8bit::.

*Complex collations for 8-bit character sets*

This kind of collation is implemented using functions in a C source file
that define how to order characters, as described in *note
adding-character-set::.

*Collations for non-Unicode multibyte character sets*

For this type of collation, 8-bit (single-byte) and multibyte characters
are handled differently.  For 8-bit characters, character codes map to
weights in case-insensitive fashion.  (For example, the single-byte
characters ''a'' and ''A'' both have a weight of '0x41'.)  For multibyte
characters, there are two types of relationship between character codes
and weights:

   * Weights equal character codes.  'sjis_japanese_ci' is an example of
     this kind of collation.  The multibyte character ''ぢ'' has a
     character code of '0x82C0', and the weight is also '0x82C0'.

   * Character codes map one-to-one to weights, but a code is not
     necessarily equal to the weight.  'gbk_chinese_ci' is an example of
     this kind of collation.  The multibyte character ''膰'' has a
     character code of '0x81B0' but a weight of '0xC286'.

For implementation instructions, see *note adding-character-set::.

*Collations for Unicode multibyte character sets*

Some of these collations are based on the Unicode Collation Algorithm
(UCA), others are not.

Non-UCA collations have a one-to-one mapping from character code to
weight.  In MySQL, such collations are case-insensitive and
accent-insensitive.  'utf8_general_ci' is an example: ''a'', ''A'',
''A`'', and ''a''' each have different character codes but all have a
weight of '0x0041' and compare as equal.

     mysql> SET NAMES 'utf8' COLLATE 'utf8_general_ci';
     Query OK, 0 rows affected (0.00 sec)

     mysql> SELECT 'a' = 'A', 'a' = 'A`', 'a' = 'a'';
     +-----------+-----------+-----------+
     | 'a' = 'A' | 'a' = 'A`' | 'a' = 'a'' |
     +-----------+-----------+-----------+
     |         1 |         1 |         1 |
     +-----------+-----------+-----------+
     1 row in set (0.06 sec)

UCA-based collations in MySQL have these properties:

   * If a character has weights, each weight uses 2 bytes (16 bits).

   * A character may have zero weights (or an empty weight).  In this
     case, the character is ignorable.  Example: "U+0000 NULL" does not
     have a weight and is ignorable.

   * A character may have one weight.  Example: ''a'' has a weight of
     '0x0E33'.

   * A character may have many weights.  This is an expansion.  Example:
     The German letter ''ss'' (SZ ligature, or SHARP S) has a weight of
     '0x0FEA0FEA'.

   * Many characters may have one weight.  This is a contraction.
     Example: ''ch'' is a single letter in Czech and has a weight of
     '0x0EE2'.

A many-characters-to-many-weights mapping is also possible (this is
contraction with expansion), but is not supported by MySQL.

For implementation instructions, for a non-UCA collation, see *note
adding-character-set::.  For a UCA collation, see *note
adding-collation-unicode-uca::.

*Miscellaneous collations*

There are also a few collations that do not fall into any of the
previous categories.


File: manual.info.tmp,  Node: adding-collation-choosing-id,  Next: adding-collation-simple-8bit,  Prev: charset-collation-implementations,  Up: adding-collation

10.14.2 Choosing a Collation ID
-------------------------------

Each collation must have a unique ID. To add a collation, you must
choose an ID value that is not currently used.  The range of IDs from
1024 to 2047 is reserved for user-defined collations.  Before MySQL 5.5,
an ID must be chosen from the range 1 to 254.  As of MySQL 5.5, the 254
limit is removed for 'MyISAM' tables with the introduction of support
for two-byte collation IDs.  Two-byte collation ID support for 'InnoDB'
tables is not added until MySQL 5.6.

The collation ID that you choose will appear in these contexts:

   * The 'ID' column of the *note 'INFORMATION_SCHEMA.COLLATIONS':
     collations-table. table.

   * The 'Id' column of *note 'SHOW COLLATION': show-collation. output.

   * The 'charsetnr' member of the 'MYSQL_FIELD' C API data structure.

   * The 'number' member of the 'MY_CHARSET_INFO' data structure
     returned by the *note 'mysql_get_character_set_info()':
     mysql-get-character-set-info. C API function.

To determine the largest currently used ID, issue the following
statement:

     mysql> SELECT MAX(ID) FROM INFORMATION_SCHEMA.COLLATIONS;
     +---------+
     | MAX(ID) |
     +---------+
     |     210 |
     +---------+

To display a list of all currently used IDs, issue this statement:

     mysql> SELECT ID FROM INFORMATION_SCHEMA.COLLATIONS ORDER BY ID;
     +-----+
     | ID  |
     +-----+
     |   1 |
     |   2 |
     | ... |
     |  52 |
     |  53 |
     |  57 |
     |  58 |
     | ... |
     |  98 |
     |  99 |
     | 128 |
     | 129 |
     | ... |
     | 210 |
     +-----+

*Warning*:

Before upgrading, you should save the configuration files that you
change.  If you upgrade in place, the process will replace the your
modified files.


File: manual.info.tmp,  Node: adding-collation-simple-8bit,  Next: adding-collation-unicode-uca,  Prev: adding-collation-choosing-id,  Up: adding-collation

10.14.3 Adding a Simple Collation to an 8-Bit Character Set
-----------------------------------------------------------

This section describes how to add a simple collation for an 8-bit
character set by writing the '<collation>' elements associated with a
'<charset>' character set description in the MySQL 'Index.xml' file.
The procedure described here does not require recompiling MySQL. The
example adds a collation named 'latin1_test_ci' to the 'latin1'
character set.

  1. Choose a collation ID, as shown in *note
     adding-collation-choosing-id::.  The following steps use an ID of
     1024.

  2. Modify the 'Index.xml' and 'latin1.xml' configuration files.  These
     files are located in the directory named by the
     'character_sets_dir' system variable.  You can check the variable
     value as follows, although the path name might be different on your
     system:

          mysql> SHOW VARIABLES LIKE 'character_sets_dir';
          +--------------------+-----------------------------------------+
          | Variable_name      | Value                                   |
          +--------------------+-----------------------------------------+
          | character_sets_dir | /user/local/mysql/share/mysql/charsets/ |
          +--------------------+-----------------------------------------+

  3. Choose a name for the collation and list it in the 'Index.xml'
     file.  Find the '<charset>' element for the character set to which
     the collation is being added, and add a '<collation>' element that
     indicates the collation name and ID, to associate the name with the
     ID. For example:

          <charset name="latin1">
            ...
            <collation name="latin1_test_ci" id="1024"/>
            ...
          </charset>

  4. In the 'latin1.xml' configuration file, add a '<collation>' element
     that names the collation and that contains a '<map>' element that
     defines a character code-to-weight mapping table for character
     codes 0 to 255.  Each value within the '<map>' element must be a
     number in hexadecimal format.

          <collation name="latin1_test_ci">
          <map>
           00 01 02 03 04 05 06 07 08 09 0A 0B 0C 0D 0E 0F
           10 11 12 13 14 15 16 17 18 19 1A 1B 1C 1D 1E 1F
           20 21 22 23 24 25 26 27 28 29 2A 2B 2C 2D 2E 2F
           30 31 32 33 34 35 36 37 38 39 3A 3B 3C 3D 3E 3F
           40 41 42 43 44 45 46 47 48 49 4A 4B 4C 4D 4E 4F
           50 51 52 53 54 55 56 57 58 59 5A 5B 5C 5D 5E 5F
           60 41 42 43 44 45 46 47 48 49 4A 4B 4C 4D 4E 4F
           50 51 52 53 54 55 56 57 58 59 5A 7B 7C 7D 7E 7F
           80 81 82 83 84 85 86 87 88 89 8A 8B 8C 8D 8E 8F
           90 91 92 93 94 95 96 97 98 99 9A 9B 9C 9D 9E 9F
           A0 A1 A2 A3 A4 A5 A6 A7 A8 A9 AA AB AC AD AE AF
           B0 B1 B2 B3 B4 B5 B6 B7 B8 B9 BA BB BC BD BE BF
           41 41 41 41 5B 5D 5B 43 45 45 45 45 49 49 49 49
           44 4E 4F 4F 4F 4F 5C D7 5C 55 55 55 59 59 DE DF
           41 41 41 41 5B 5D 5B 43 45 45 45 45 49 49 49 49
           44 4E 4F 4F 4F 4F 5C F7 5C 55 55 55 59 59 DE FF
          </map>
          </collation>

  5. Restart the server and use this statement to verify that the
     collation is present:

          mysql> SHOW COLLATION WHERE Collation = 'latin1_test_ci';
          +----------------+---------+------+---------+----------+---------+
          | Collation      | Charset | Id   | Default | Compiled | Sortlen |
          +----------------+---------+------+---------+----------+---------+
          | latin1_test_ci | latin1  | 1024 |         |          |       1 |
          +----------------+---------+------+---------+----------+---------+


File: manual.info.tmp,  Node: adding-collation-unicode-uca,  Prev: adding-collation-simple-8bit,  Up: adding-collation

10.14.4 Adding a UCA Collation to a Unicode Character Set
---------------------------------------------------------

* Menu:

* ldml-collation-example::       Defining a UCA Collation Using LDML Syntax
* ldml-rules::                   LDML Syntax Supported in MySQL

This section describes how to add a UCA collation for a Unicode
character set by writing the '<collation>' element within a '<charset>'
character set description in the MySQL 'Index.xml' file.  The procedure
described here does not require recompiling MySQL. It uses a subset of
the Locale Data Markup Language (LDML) specification, which is available
at <http://www.unicode.org/reports/tr35/>.  With this method, you need
not define the entire collation.  Instead, you begin with an existing
'base' collation and describe the new collation in terms of how it
differs from the base collation.  The following table lists the base
collations of the Unicode character sets for which UCA collations can be
defined.

*MySQL Character Sets Available for User-Defined UCA Collations*

Character Set          Base Collation
                       
'utf8'

'utf8_unicode_ci'

'ucs2'

'ucs2_unicode_ci'

'utf16'

'utf16_unicode_ci'

'utf32'

'utf32_unicode_ci'

The following sections show how to add a collation that is defined using
LDML syntax, and provide a summary of LDML rules supported in MySQL.


File: manual.info.tmp,  Node: ldml-collation-example,  Next: ldml-rules,  Prev: adding-collation-unicode-uca,  Up: adding-collation-unicode-uca

10.14.4.1 Defining a UCA Collation Using LDML Syntax
....................................................

To add a UCA collation for a Unicode character set without recompiling
MySQL, use the following procedure.  If you are unfamiliar with the LDML
rules used to describe the collation's sort characteristics, see *note
ldml-rules::.

The example adds a collation named 'utf8_phone_ci' to the 'utf8'
character set.  The collation is designed for a scenario involving a Web
application for which users post their names and phone numbers.  Phone
numbers can be given in very different formats:

     +7-12345-67
     +7-12-345-67
     +7 12 345 67
     +7 (12) 345 67
     +71234567

The problem raised by dealing with these kinds of values is that the
varying permissible formats make searching for a specific phone number
very difficult.  The solution is to define a new collation that reorders
punctuation characters, making them ignorable.

  1. Choose a collation ID, as shown in *note
     adding-collation-choosing-id::.  The following steps use an ID of
     1029.

  2. To modify the 'Index.xml' configuration file.  This file is located
     in the directory named by the 'character_sets_dir' system variable.
     You can check the variable value as follows, although the path name
     might be different on your system:

          mysql> SHOW VARIABLES LIKE 'character_sets_dir';
          +--------------------+-----------------------------------------+
          | Variable_name      | Value                                   |
          +--------------------+-----------------------------------------+
          | character_sets_dir | /user/local/mysql/share/mysql/charsets/ |
          +--------------------+-----------------------------------------+

  3. Choose a name for the collation and list it in the 'Index.xml'
     file.  In addition, you'll need to provide the collation ordering
     rules.  Find the '<charset>' element for the character set to which
     the collation is being added, and add a '<collation>' element that
     indicates the collation name and ID, to associate the name with the
     ID. Within the '<collation>' element, provide a '<rules>' element
     containing the ordering rules:

          <charset name="utf8">
            ...
            <collation name="utf8_phone_ci" id="1029">
              <rules>
                <reset>\u0000</reset>
                <i>\u0020</i> <!-- space -->
                <i>\u0028</i> <!-- left parenthesis -->
                <i>\u0029</i> <!-- right parenthesis -->
                <i>\u002B</i> <!-- plus -->
                <i>\u002D</i> <!-- hyphen -->
              </rules>
            </collation>
            ...
          </charset>

  4. If you want a similar collation for other Unicode character sets,
     add other '<collation>' elements.  For example, to define
     'ucs2_phone_ci', add a '<collation>' element to the '<charset
     name="ucs2">' element.  Remember that each collation must have its
     own unique ID.

  5. Restart the server and use this statement to verify that the
     collation is present:

          mysql> SHOW COLLATION WHERE Collation = 'utf8_phone_ci';
          +---------------+---------+------+---------+----------+---------+
          | Collation     | Charset | Id   | Default | Compiled | Sortlen |
          +---------------+---------+------+---------+----------+---------+
          | utf8_phone_ci | utf8    | 1029 |         |          |       8 |
          +---------------+---------+------+---------+----------+---------+

Now test the collation to make sure that it has the desired properties.

Create a table containing some sample phone numbers using the new
collation:

     mysql> CREATE TABLE phonebook (
              name VARCHAR(64),
              phone VARCHAR(64) CHARACTER SET utf8 COLLATE utf8_phone_ci
            );
     Query OK, 0 rows affected (0.09 sec)

     mysql> INSERT INTO phonebook VALUES ('Svoj','+7 912 800 80 02');
     Query OK, 1 row affected (0.00 sec)

     mysql> INSERT INTO phonebook VALUES ('Hf','+7 (912) 800 80 04');
     Query OK, 1 row affected (0.00 sec)

     mysql> INSERT INTO phonebook VALUES ('Bar','+7-912-800-80-01');
     Query OK, 1 row affected (0.00 sec)

     mysql> INSERT INTO phonebook VALUES ('Ramil','(7912) 800 80 03');
     Query OK, 1 row affected (0.00 sec)

     mysql> INSERT INTO phonebook VALUES ('Sanja','+380 (912) 8008005');
     Query OK, 1 row affected (0.00 sec)

Run some queries to see whether the ignored punctuation characters are
in fact ignored for comparison and sorting:

     mysql> SELECT * FROM phonebook ORDER BY phone;
     +-------+--------------------+
     | name  | phone              |
     +-------+--------------------+
     | Sanja | +380 (912) 8008005 |
     | Bar   | +7-912-800-80-01   |
     | Svoj  | +7 912 800 80 02   |
     | Ramil | (7912) 800 80 03   |
     | Hf    | +7 (912) 800 80 04 |
     +-------+--------------------+
     5 rows in set (0.00 sec)

     mysql> SELECT * FROM phonebook WHERE phone='+7(912)800-80-01';
     +------+------------------+
     | name | phone            |
     +------+------------------+
     | Bar  | +7-912-800-80-01 |
     +------+------------------+
     1 row in set (0.00 sec)

     mysql> SELECT * FROM phonebook WHERE phone='79128008001';
     +------+------------------+
     | name | phone            |
     +------+------------------+
     | Bar  | +7-912-800-80-01 |
     +------+------------------+
     1 row in set (0.00 sec)

     mysql> SELECT * FROM phonebook WHERE phone='7 9 1 2 8 0 0 8 0 0 1';
     +------+------------------+
     | name | phone            |
     +------+------------------+
     | Bar  | +7-912-800-80-01 |
     +------+------------------+
     1 row in set (0.00 sec)


File: manual.info.tmp,  Node: ldml-rules,  Prev: ldml-collation-example,  Up: adding-collation-unicode-uca

10.14.4.2 LDML Syntax Supported in MySQL
........................................

This section describes the LDML syntax that MySQL recognizes.  This is a
subset of the syntax described in the LDML specification available at
<http://www.unicode.org/reports/tr35/>, which should be consulted for
further information.  The rules described here are all supported except
that character sorting occurs only at the primary level.  Rules that
specify differences at secondary or higher sort levels are recognized
(and thus can be included in collation definitions) but are treated as
equality at the primary level.

*Character Representation*

Characters named in LDML rules can be written in '\uNNNN' format, where
NNNN is the hexadecimal Unicode code point value.  Within hexadecimal
values, the digits 'A' through 'F' are not case-sensitive; '\u00E1' and
'\u00e1' are equivalent.  Basic Latin letters 'A-Z' and 'a-z' can also
be written literally (this is a MySQL limitation; the LDML specification
permits literal non-Latin1 characters in the rules).  Only characters in
the Basic Multilingual Plane can be specified.  This notation does not
apply to characters outside the BMP range of '0000' to 'FFFF'.

The 'Index.xml' file itself should be written using ASCII encoding.

*Syntax Rules*

LDML has reset rules and shift rules to specify character ordering.
Orderings are given as a set of rules that begin with a reset rule that
establishes an anchor point, followed by shift rules that indicate how
characters sort relative to the anchor point.

   * A '<reset>' rule does not specify any ordering in and of itself.
     Instead, it 'resets' the ordering for subsequent shift rules to
     cause them to be taken in relation to a given character.  Either of
     the following rules resets subsequent shift rules to be taken in
     relation to the letter ''A'':

          <reset>A</reset>

          <reset>\u0041</reset>

   * The '<p>', '<s>', and '<t>' shift rules define primary, secondary,
     and tertiary differences of a character from another character:

        * Use primary differences to distinguish separate letters.

        * Use secondary differences to distinguish accent variations.

        * Use tertiary differences to distinguish lettercase variations.

     Either of these rules specifies a primary shift rule for the ''G''
     character:

          <p>G</p>

          <p>\u0047</p>

   * The '<i>' shift rule indicates that one character sorts identically
     to another.  The following rules cause ''b'' to sort the same as
     ''a'':

          <reset>a</reset>
          <i>b</i>


File: manual.info.tmp,  Node: charset-configuration,  Next: locale-support,  Prev: adding-collation,  Up: charset

10.15 Character Set Configuration
=================================

The MySQL server has a compiled-in default character set and collation.
To change these defaults, use the '--character-set-server' and
'--collation-server' options when you start the server.  See *note
server-options::.  The collation must be a legal collation for the
default character set.  To determine which collations are available for
each character set, use the *note 'SHOW COLLATION': show-collation.
statement or query the 'INFORMATION_SCHEMA' *note 'COLLATIONS':
collations-table. table.

If you try to use a character set that is not compiled into your binary,
you might run into the following problems:

   * If your program uses an incorrect path to determine where the
     character sets are stored (which is typically the
     'share/mysql/charsets' or 'share/charsets' directory under the
     MySQL installation directory), this can be fixed by using the
     '--character-sets-dir' option when you run the program.  For
     example, to specify a directory to be used by MySQL client
     programs, list it in the '[client]' group of your option file.  The
     examples given here show what the setting might look like for Unix
     or Windows, respectively:

          [client]
          character-sets-dir=/usr/local/mysql/share/mysql/charsets

          [client]
          character-sets-dir="C:/Program Files/MySQL/MySQL Server 5.5/share/charsets"

   * If the character set is a complex character set that cannot be
     loaded dynamically, you must recompile the program with support for
     the character set.

     For Unicode character sets, you can define collations without
     recompiling by using LDML notation.  See *note
     adding-collation-unicode-uca::.

   * If the character set is a dynamic character set, but you do not
     have a configuration file for it, you should install the
     configuration file for the character set from a new MySQL
     distribution.

   * If your character set index file ('Index.xml') does not contain the
     name for the character set, your program displays an error message:

          Character set 'CHARSET_NAME' is not a compiled character set and is not
          specified in the '/usr/share/mysql/charsets/Index.xml' file

     To solve this problem, you should either get a new index file or
     manually add the name of any missing character sets to the current
     file.

You can force client programs to use specific character set as follows:

     [client]
     default-character-set=CHARSET_NAME

This is normally unnecessary.  However, when 'character_set_system'
differs from 'character_set_server' or 'character_set_client', and you
input characters manually (as database object identifiers, column
values, or both), these may be displayed incorrectly in output from the
client or the output itself may be formatted incorrectly.  In such
cases, starting the mysql client with
'--default-character-set=SYSTEM_CHARACTER_SET'--that is, setting the
client character set to match the system character set--should fix the
problem.


File: manual.info.tmp,  Node: locale-support,  Prev: charset-configuration,  Up: charset

10.16 MySQL Server Locale Support
=================================

The locale indicated by the 'lc_time_names' system variable controls the
language used to display day and month names and abbreviations.  This
variable affects the output from the 'DATE_FORMAT()', 'DAYNAME()', and
'MONTHNAME()' functions.

'lc_time_names' does not affect the 'STR_TO_DATE()' or 'GET_FORMAT()'
function.

The 'lc_time_names' value does not affect the result from 'FORMAT()',
but this function takes an optional third parameter that enables a
locale to be specified to be used for the result number's decimal point,
thousands separator, and grouping between separators.  Permissible
locale values are the same as the legal values for the 'lc_time_names'
system variable.

Locale names have language and region subtags listed by IANA
(<http://www.iana.org/assignments/language-subtag-registry>) such as
''ja_JP'' or ''pt_BR''.  The default value is ''en_US'' regardless of
your system's locale setting, but you can set the value at server
startup, or set the 'GLOBAL' value at runtime if you have privileges
sufficient to set global system variables; see *note
system-variable-privileges::.  Any client can examine the value of
'lc_time_names' or set its 'SESSION' value to affect the locale for its
own connection.

     mysql> SET NAMES 'utf8';
     Query OK, 0 rows affected (0.09 sec)

     mysql> SELECT @@lc_time_names;
     +-----------------+
     | @@lc_time_names |
     +-----------------+
     | en_US           |
     +-----------------+
     1 row in set (0.00 sec)

     mysql> SELECT DAYNAME('2010-01-01'), MONTHNAME('2010-01-01');
     +-----------------------+-------------------------+
     | DAYNAME('2010-01-01') | MONTHNAME('2010-01-01') |
     +-----------------------+-------------------------+
     | Friday                | January                 |
     +-----------------------+-------------------------+
     1 row in set (0.00 sec)

     mysql> SELECT DATE_FORMAT('2010-01-01','%W %a %M %b');
     +-----------------------------------------+
     | DATE_FORMAT('2010-01-01','%W %a %M %b') |
     +-----------------------------------------+
     | Friday Fri January Jan                  |
     +-----------------------------------------+
     1 row in set (0.00 sec)

     mysql> SET lc_time_names = 'es_MX';
     Query OK, 0 rows affected (0.00 sec)

     mysql> SELECT @@lc_time_names;
     +-----------------+
     | @@lc_time_names |
     +-----------------+
     | es_MX           |
     +-----------------+
     1 row in set (0.00 sec)

     mysql> SELECT DAYNAME('2010-01-01'), MONTHNAME('2010-01-01');
     +-----------------------+-------------------------+
     | DAYNAME('2010-01-01') | MONTHNAME('2010-01-01') |
     +-----------------------+-------------------------+
     | viernes               | enero                   |
     +-----------------------+-------------------------+
     1 row in set (0.00 sec)

     mysql> SELECT DATE_FORMAT('2010-01-01','%W %a %M %b');
     +-----------------------------------------+
     | DATE_FORMAT('2010-01-01','%W %a %M %b') |
     +-----------------------------------------+
     | viernes vie enero ene                   |
     +-----------------------------------------+
     1 row in set (0.00 sec)

The day or month name for each of the affected functions is converted
from 'utf8' to the character set indicated by the
'character_set_connection' system variable.

'lc_time_names' may be set to any of the following locale values.  The
set of locales supported by MySQL may differ from those supported by
your operating system.

Locale Value                         Meaning
                                     
'ar_AE': Arabic - United Arab        'ar_BH': Arabic - Bahrain
Emirates                             

'ar_DZ': Arabic - Algeria            'ar_EG': Arabic - Egypt
                                     
'ar_IN': Arabic - India              'ar_IQ': Arabic - Iraq
                                     
'ar_JO': Arabic - Jordan             'ar_KW': Arabic - Kuwait
                                     
'ar_LB': Arabic - Lebanon            'ar_LY': Arabic - Libya
                                     
'ar_MA': Arabic - Morocco            'ar_OM': Arabic - Oman
                                     
'ar_QA': Arabic - Qatar              'ar_SA': Arabic - Saudi Arabia
                                     
'ar_SD': Arabic - Sudan              'ar_SY': Arabic - Syria
                                     
'ar_TN': Arabic - Tunisia            'ar_YE': Arabic - Yemen
                                     
'be_BY': Belarusian - Belarus        'bg_BG': Bulgarian - Bulgaria
                                     
'ca_ES': Catalan - Spain             'cs_CZ': Czech - Czech Republic
                                     
'da_DK': Danish - Denmark            'de_AT': German - Austria
                                     
'de_BE': German - Belgium            'de_CH': German - Switzerland
                                     
'de_DE': German - Germany            'de_LU': German - Luxembourg
                                     
'el_GR': Greek - Greece              'en_AU': English - Australia
                                     
'en_CA': English - Canada            'en_GB': English - United Kingdom
                                     
'en_IN': English - India             'en_NZ': English - New Zealand
                                     
'en_PH': English - Philippines       'en_US': English - United States
                                     
'en_ZA': English - South Africa      'en_ZW': English - Zimbabwe
                                     
'es_AR': Spanish - Argentina         'es_BO': Spanish - Bolivia
                                     
'es_CL': Spanish - Chile             'es_CO': Spanish - Colombia
                                     
'es_CR': Spanish - Costa Rica        'es_DO': Spanish - Dominican
                                     Republic
                                     
'es_EC': Spanish - Ecuador           'es_ES': Spanish - Spain
                                     
'es_GT': Spanish - Guatemala         'es_HN': Spanish - Honduras
                                     
'es_MX': Spanish - Mexico            'es_NI': Spanish - Nicaragua
                                     
'es_PA': Spanish - Panama            'es_PE': Spanish - Peru
                                     
'es_PR': Spanish - Puerto Rico       'es_PY': Spanish - Paraguay
                                     
'es_SV': Spanish - El Salvador       'es_US': Spanish - United States
                                     
'es_UY': Spanish - Uruguay           'es_VE': Spanish - Venezuela
                                     
'et_EE': Estonian - Estonia          'eu_ES': Basque - Basque
                                     
'fi_FI': Finnish - Finland           'fo_FO': Faroese - Faroe Islands
                                     
'fr_BE': French - Belgium            'fr_CA': French - Canada
                                     
'fr_CH': French - Switzerland        'fr_FR': French - France
                                     
'fr_LU': French - Luxembourg         'gl_ES': Galician - Spain
                                     
'gu_IN': Gujarati - India            'he_IL': Hebrew - Israel
                                     
'hi_IN': Hindi - India               'hr_HR': Croatian - Croatia
                                     
'hu_HU': Hungarian - Hungary         'id_ID': Indonesian - Indonesia
                                     
'is_IS': Icelandic - Iceland         'it_CH': Italian - Switzerland
                                     
'it_IT': Italian - Italy             'ja_JP': Japanese - Japan
                                     
'ko_KR': Korean - Republic of        'lt_LT': Lithuanian - Lithuania
Korea                                

'lv_LV': Latvian - Latvia            'mk_MK': Macedonian - FYROM
                                     
'mn_MN': Mongolia - Mongolian        'ms_MY': Malay - Malaysia
                                     
'nb_NO': Norwegian(Bokmaal) -        'nl_BE': Dutch - Belgium
Norway                               

'nl_NL': Dutch - The Netherlands     'no_NO': Norwegian - Norway
                                     
'pl_PL': Polish - Poland             'pt_BR': Portugese - Brazil
                                     
'pt_PT': Portugese - Portugal        'ro_RO': Romanian - Romania
                                     
'ru_RU': Russian - Russia            'ru_UA': Russian - Ukraine
                                     
'sk_SK': Slovak - Slovakia           'sl_SI': Slovenian - Slovenia
                                     
'sq_AL': Albanian - Albania          'sr_RS': Serbian - Yugoslavia
                                     
'sv_FI': Swedish - Finland           'sv_SE': Swedish - Sweden
                                     
'ta_IN': Tamil - India               'te_IN': Telugu - India
                                     
'th_TH': Thai - Thailand             'tr_TR': Turkish - Turkey
                                     
'uk_UA': Ukrainian - Ukraine         'ur_PK': Urdu - Pakistan
                                     
'vi_VN': Vietnamese - Viet Nam       'zh_CN': Chinese - China
                                     
'zh_HK': Chinese - Hong Kong         'zh_TW': Chinese - Taiwan Province
                                     of China


File: manual.info.tmp,  Node: data-types,  Next: functions,  Prev: charset,  Up: Top

11 Data Types
*************

* Menu:

* numeric-types::                Numeric Data Types
* date-and-time-types::          Date and Time Data Types
* string-types::                 String Data Types
* spatial-types::                Spatial Data Types
* data-type-defaults::           Data Type Default Values
* storage-requirements::         Data Type Storage Requirements
* choosing-types::               Choosing the Right Type for a Column
* other-vendor-data-types::      Using Data Types from Other Database Engines

MySQL supports SQL data types in several categories: numeric types, date
and time types, string (character and byte) types, and spatial types.
This chapter provides an overview and more detailed description of the
properties of the types in each category, and a summary of the data type
storage requirements.  The initial overviews are intentionally brief.
Consult the more detailed descriptions for additional information about
particular data types, such as the permissible formats in which you can
specify values.

Data type descriptions use these conventions:

   * For integer types, M indicates the maximum display width.  For
     floating-point and fixed-point types, M is the total number of
     digits that can be stored (the precision).  For string types, M is
     the maximum length.  The maximum permissible value of M depends on
     the data type.

   * D applies to floating-point and fixed-point types and indicates the
     number of digits following the decimal point (the scale).  The
     maximum possible value is 30, but should be no greater than M−2.

   * Square brackets ('[' and ']') indicate optional parts of type
     definitions.


File: manual.info.tmp,  Node: numeric-types,  Next: date-and-time-types,  Prev: data-types,  Up: data-types

11.1 Numeric Data Types
=======================

* Menu:

* numeric-type-syntax::          Numeric Data Type Syntax
* integer-types::                Integer Types (Exact Value) - INTEGER, INT, SMALLINT, TINYINT, MEDIUMINT, BIGINT
* fixed-point-types::            Fixed-Point Types (Exact Value) - DECIMAL, NUMERIC
* floating-point-types::         Floating-Point Types (Approximate Value) - FLOAT, DOUBLE
* bit-type::                     Bit-Value Type - BIT
* numeric-type-attributes::      Numeric Type Attributes
* out-of-range-and-overflow::    Out-of-Range and Overflow Handling

MySQL supports all standard SQL numeric data types.  These types include
the exact numeric data types (*note 'INTEGER': integer-types, *note
'SMALLINT': integer-types, *note 'DECIMAL': fixed-point-types, and *note
'NUMERIC': fixed-point-types.), as well as the approximate numeric data
types (*note 'FLOAT': floating-point-types, *note 'REAL':
floating-point-types, and *note 'DOUBLE PRECISION':
floating-point-types.).  The keyword *note 'INT': integer-types. is a
synonym for *note 'INTEGER': integer-types, and the keywords *note
'DEC': fixed-point-types. and *note 'FIXED': fixed-point-types. are
synonyms for *note 'DECIMAL': fixed-point-types.  MySQL treats *note
'DOUBLE': floating-point-types. as a synonym for *note 'DOUBLE
PRECISION': floating-point-types. (a nonstandard extension).  MySQL also
treats *note 'REAL': floating-point-types. as a synonym for *note
'DOUBLE PRECISION': floating-point-types. (a nonstandard variation),
unless the 'REAL_AS_FLOAT' SQL mode is enabled.

The *note 'BIT': bit-type. data type stores bit values and is supported
for *note 'MyISAM': myisam-storage-engine, *note 'MEMORY':
memory-storage-engine, *note 'InnoDB': innodb-storage-engine, and *note
'NDBCLUSTER': mysql-cluster. tables.

For information about how MySQL handles assignment of out-of-range
values to columns and overflow during expression evaluation, see *note
out-of-range-and-overflow::.

For information about storage requirements of the numeric data types,
see *note storage-requirements::.

For descriptions of functions that operate on numeric values, see *note
numeric-functions::.  The data type used for the result of a calculation
on numeric operands depends on the types of the operands and the
operations performed on them.  For more information, see *note
arithmetic-functions::.


File: manual.info.tmp,  Node: numeric-type-syntax,  Next: integer-types,  Prev: numeric-types,  Up: numeric-types

11.1.1 Numeric Data Type Syntax
-------------------------------

For integer data types, M indicates the maximum display width.  The
maximum display width is 255.  Display width is unrelated to the range
of values a type can store, as described in *note
numeric-type-attributes::.

For floating-point and fixed-point data types, M is the total number of
digits that can be stored.

If you specify 'ZEROFILL' for a numeric column, MySQL automatically adds
the 'UNSIGNED' attribute to the column.

Numeric data types that permit the 'UNSIGNED' attribute also permit
'SIGNED'.  However, these data types are signed by default, so the
'SIGNED' attribute has no effect.

'SERIAL' is an alias for 'BIGINT UNSIGNED NOT NULL AUTO_INCREMENT
UNIQUE'.

'SERIAL DEFAULT VALUE' in the definition of an integer column is an
alias for 'NOT NULL AUTO_INCREMENT UNIQUE'.

*Warning*:

When you use subtraction between integer values where one is of type
'UNSIGNED', the result is unsigned unless the 'NO_UNSIGNED_SUBTRACTION'
SQL mode is enabled.  See *note cast-functions::.

   * 
     *note 'BIT[(M)]': bit-type.

     A bit-value type.  M indicates the number of bits per value, from 1
     to 64.  The default is 1 if M is omitted.

   * 
     *note 'TINYINT[(M)] [UNSIGNED] [ZEROFILL]': integer-types.

     A very small integer.  The signed range is '-128' to '127'.  The
     unsigned range is '0' to '255'.

   * 
     *note 'BOOL': integer-types, *note 'BOOLEAN': integer-types.

     These types are synonyms for *note 'TINYINT(1)': integer-types.  A
     value of zero is considered false.  Nonzero values are considered
     true:

          mysql> SELECT IF(0, 'true', 'false');
          +------------------------+
          | IF(0, 'true', 'false') |
          +------------------------+
          | false                  |
          +------------------------+

          mysql> SELECT IF(1, 'true', 'false');
          +------------------------+
          | IF(1, 'true', 'false') |
          +------------------------+
          | true                   |
          +------------------------+

          mysql> SELECT IF(2, 'true', 'false');
          +------------------------+
          | IF(2, 'true', 'false') |
          +------------------------+
          | true                   |
          +------------------------+

     However, the values 'TRUE' and 'FALSE' are merely aliases for '1'
     and '0', respectively, as shown here:

          mysql> SELECT IF(0 = FALSE, 'true', 'false');
          +--------------------------------+
          | IF(0 = FALSE, 'true', 'false') |
          +--------------------------------+
          | true                           |
          +--------------------------------+

          mysql> SELECT IF(1 = TRUE, 'true', 'false');
          +-------------------------------+
          | IF(1 = TRUE, 'true', 'false') |
          +-------------------------------+
          | true                          |
          +-------------------------------+

          mysql> SELECT IF(2 = TRUE, 'true', 'false');
          +-------------------------------+
          | IF(2 = TRUE, 'true', 'false') |
          +-------------------------------+
          | false                         |
          +-------------------------------+

          mysql> SELECT IF(2 = FALSE, 'true', 'false');
          +--------------------------------+
          | IF(2 = FALSE, 'true', 'false') |
          +--------------------------------+
          | false                          |
          +--------------------------------+

     The last two statements display the results shown because '2' is
     equal to neither '1' nor '0'.

   * 
     *note 'SMALLINT[(M)] [UNSIGNED] [ZEROFILL]': integer-types.

     A small integer.  The signed range is '-32768' to '32767'.  The
     unsigned range is '0' to '65535'.

   * 
     *note 'MEDIUMINT[(M)] [UNSIGNED] [ZEROFILL]': integer-types.

     A medium-sized integer.  The signed range is '-8388608' to
     '8388607'.  The unsigned range is '0' to '16777215'.

   * 
     *note 'INT[(M)] [UNSIGNED] [ZEROFILL]': integer-types.

     A normal-size integer.  The signed range is '-2147483648' to
     '2147483647'.  The unsigned range is '0' to '4294967295'.

   * 
     *note 'INTEGER[(M)] [UNSIGNED] [ZEROFILL]': integer-types.

     This type is a synonym for *note 'INT': integer-types.

   * 
     *note 'BIGINT[(M)] [UNSIGNED] [ZEROFILL]': integer-types.

     A large integer.  The signed range is '-9223372036854775808' to
     '9223372036854775807'.  The unsigned range is '0' to
     '18446744073709551615'.

     'SERIAL' is an alias for 'BIGINT UNSIGNED NOT NULL AUTO_INCREMENT
     UNIQUE'.

     Some things you should be aware of with respect to *note 'BIGINT':
     integer-types. columns:

        * 
          All arithmetic is done using signed *note 'BIGINT':
          integer-types. or *note 'DOUBLE': floating-point-types.
          values, so you should not use unsigned big integers larger
          than '9223372036854775807' (63 bits) except with bit
          functions!  If you do that, some of the last digits in the
          result may be wrong because of rounding errors when converting
          a *note 'BIGINT': integer-types. value to a *note 'DOUBLE':
          floating-point-types.

          MySQL can handle *note 'BIGINT': integer-types. in the
          following cases:

             * When using integers to store large unsigned values in a
               *note 'BIGINT': integer-types. column.

             * In 'MIN(COL_NAME)' or 'MAX(COL_NAME)', where COL_NAME
               refers to a *note 'BIGINT': integer-types. column.

             * When using operators ('+', '-', '*', and so on) where
               both operands are integers.

        * You can always store an exact integer value in a *note
          'BIGINT': integer-types. column by storing it using a string.
          In this case, MySQL performs a string-to-number conversion
          that involves no intermediate double-precision representation.

        * The '-', '+', and '*' operators use *note 'BIGINT':
          integer-types. arithmetic when both operands are integer
          values.  This means that if you multiply two big integers (or
          results from functions that return integers), you may get
          unexpected results when the result is larger than
          '9223372036854775807'.

   * 
     *note 'DECIMAL[(M[,D])] [UNSIGNED] [ZEROFILL]': fixed-point-types.

     A packed 'exact' fixed-point number.  M is the total number of
     digits (the precision) and D is the number of digits after the
     decimal point (the scale).  The decimal point and (for negative
     numbers) the '-' sign are not counted in M.  If D is 0, values have
     no decimal point or fractional part.  The maximum number of digits
     (M) for *note 'DECIMAL': fixed-point-types. is 65.  The maximum
     number of supported decimals (D) is 30.  If D is omitted, the
     default is 0.  If M is omitted, the default is 10.

     'UNSIGNED', if specified, disallows negative values.

     All basic calculations ('+, -, *, /') with *note 'DECIMAL':
     fixed-point-types. columns are done with a precision of 65 digits.

   * 
     *note 'DEC[(M[,D])] [UNSIGNED] [ZEROFILL]': fixed-point-types,
     *note 'NUMERIC[(M[,D])] [UNSIGNED] [ZEROFILL]': fixed-point-types,
     *note 'FIXED[(M[,D])] [UNSIGNED] [ZEROFILL]': fixed-point-types.

     These types are synonyms for *note 'DECIMAL': fixed-point-types.
     The *note 'FIXED': fixed-point-types. synonym is available for
     compatibility with other database systems.

   * 
     *note 'FLOAT[(M,D)] [UNSIGNED] [ZEROFILL]': floating-point-types.

     A small (single-precision) floating-point number.  Permissible
     values are '-3.402823466E+38' to '-1.175494351E-38', '0', and
     '1.175494351E-38' to '3.402823466E+38'.  These are the theoretical
     limits, based on the IEEE standard.  The actual range might be
     slightly smaller depending on your hardware or operating system.

     M is the total number of digits and D is the number of digits
     following the decimal point.  If M and D are omitted, values are
     stored to the limits permitted by the hardware.  A single-precision
     floating-point number is accurate to approximately 7 decimal
     places.

     'FLOAT(M,D)' is a nonstandard MySQL extension.

     'UNSIGNED', if specified, disallows negative values.

     Using *note 'FLOAT': floating-point-types. might give you some
     unexpected problems because all calculations in MySQL are done with
     double precision.  See *note no-matching-rows::.

   * 
     *note 'FLOAT(P) [UNSIGNED] [ZEROFILL]': floating-point-types.

     A floating-point number.  P represents the precision in bits, but
     MySQL uses this value only to determine whether to use *note
     'FLOAT': floating-point-types. or *note 'DOUBLE':
     floating-point-types. for the resulting data type.  If P is from 0
     to 24, the data type becomes *note 'FLOAT': floating-point-types.
     with no M or D values.  If P is from 25 to 53, the data type
     becomes *note 'DOUBLE': floating-point-types. with no M or D
     values.  The range of the resulting column is the same as for the
     single-precision *note 'FLOAT': floating-point-types. or
     double-precision *note 'DOUBLE': floating-point-types. data types
     described earlier in this section.

     *note 'FLOAT(P)': floating-point-types. syntax is provided for ODBC
     compatibility.

   * 
     *note 'DOUBLE[(M,D)] [UNSIGNED] [ZEROFILL]': floating-point-types.

     A normal-size (double-precision) floating-point number.
     Permissible values are '-1.7976931348623157E+308' to
     '-2.2250738585072014E-308', '0', and '2.2250738585072014E-308' to
     '1.7976931348623157E+308'.  These are the theoretical limits, based
     on the IEEE standard.  The actual range might be slightly smaller
     depending on your hardware or operating system.

     M is the total number of digits and D is the number of digits
     following the decimal point.  If M and D are omitted, values are
     stored to the limits permitted by the hardware.  A double-precision
     floating-point number is accurate to approximately 15 decimal
     places.

     'DOUBLE(M,D)' is a nonstandard MySQL extension.

     'UNSIGNED', if specified, disallows negative values.

   * 
     *note 'DOUBLE PRECISION[(M,D)] [UNSIGNED] [ZEROFILL]':
     floating-point-types, *note 'REAL[(M,D)] [UNSIGNED] [ZEROFILL]':
     floating-point-types.

     These types are synonyms for *note 'DOUBLE': floating-point-types.
     Exception: If the 'REAL_AS_FLOAT' SQL mode is enabled, *note
     'REAL': floating-point-types. is a synonym for *note 'FLOAT':
     floating-point-types. rather than *note 'DOUBLE':
     floating-point-types.


File: manual.info.tmp,  Node: integer-types,  Next: fixed-point-types,  Prev: numeric-type-syntax,  Up: numeric-types

11.1.2 Integer Types (Exact Value) - INTEGER, INT, SMALLINT, TINYINT, MEDIUMINT, BIGINT
---------------------------------------------------------------------------------------

MySQL supports the SQL standard integer types 'INTEGER' (or 'INT') and
'SMALLINT'.  As an extension to the standard, MySQL also supports the
integer types 'TINYINT', 'MEDIUMINT', and 'BIGINT'.  The following table
shows the required storage and range for each integer type.

*Required Storage and Range for Integer Types Supported by MySQL*

Type         Storage      Minimum      Minimum      Maximum      Maximum
             (Bytes)      Value        Value        Value        Value
                          Signed       Unsigned     Signed       Unsigned
                                                                 
'TINYINT'    1            '-128'       '0'          '127'        '255'
                                                                 
'SMALLINT'   2            '-32768'     '0'          '32767'      '65535'
                                                                 
'MEDIUMINT'  3            '-8388608'   '0'          '8388607'    '16777215'
                                                                 
'INT'        4            '-2147483648''0'          '2147483647' '4294967295'
                                                                 
'BIGINT'     8            '-2^63'      '0'          '2^63-1'     '2^64-1'
                                                    


File: manual.info.tmp,  Node: fixed-point-types,  Next: floating-point-types,  Prev: integer-types,  Up: numeric-types

11.1.3 Fixed-Point Types (Exact Value) - DECIMAL, NUMERIC
---------------------------------------------------------

The 'DECIMAL' and 'NUMERIC' types store exact numeric data values.
These types are used when it is important to preserve exact precision,
for example with monetary data.  In MySQL, 'NUMERIC' is implemented as
'DECIMAL', so the following remarks about 'DECIMAL' apply equally to
'NUMERIC'.

MySQL stores 'DECIMAL' values in binary format.  See *note
precision-math::.

In a 'DECIMAL' column declaration, the precision and scale can be (and
usually is) specified.  For example:

     salary DECIMAL(5,2)

In this example, '5' is the precision and '2' is the scale.  The
precision represents the number of significant digits that are stored
for values, and the scale represents the number of digits that can be
stored following the decimal point.

Standard SQL requires that 'DECIMAL(5,2)' be able to store any value
with five digits and two decimals, so values that can be stored in the
'salary' column range from '-999.99' to '999.99'.

In standard SQL, the syntax 'DECIMAL(M)' is equivalent to
'DECIMAL(M,0)'.  Similarly, the syntax 'DECIMAL' is equivalent to
'DECIMAL(M,0)', where the implementation is permitted to decide the
value of M.  MySQL supports both of these variant forms of 'DECIMAL'
syntax.  The default value of M is 10.

If the scale is 0, 'DECIMAL' values contain no decimal point or
fractional part.

The maximum number of digits for 'DECIMAL' is 65, but the actual range
for a given 'DECIMAL' column can be constrained by the precision or
scale for a given column.  When such a column is assigned a value with
more digits following the decimal point than are permitted by the
specified scale, the value is converted to that scale.  (The precise
behavior is operating system-specific, but generally the effect is
truncation to the permissible number of digits.)


File: manual.info.tmp,  Node: floating-point-types,  Next: bit-type,  Prev: fixed-point-types,  Up: numeric-types

11.1.4 Floating-Point Types (Approximate Value) - FLOAT, DOUBLE
---------------------------------------------------------------

The 'FLOAT' and 'DOUBLE' types represent approximate numeric data
values.  MySQL uses four bytes for single-precision values and eight
bytes for double-precision values.

For 'FLOAT', the SQL standard permits an optional specification of the
precision (but not the range of the exponent) in bits following the
keyword 'FLOAT' in parentheses; ; that is, *note 'FLOAT(P)':
floating-point-types.  MySQL also supports this optional precision
specification, but the precision value in *note 'FLOAT(P)':
floating-point-types. is used only to determine storage size.  A
precision from 0 to 23 results in a 4-byte single-precision 'FLOAT'
column.  A precision from 24 to 53 results in an 8-byte double-precision
'DOUBLE' column.

MySQL permits a nonstandard syntax: 'FLOAT(M,D)' or 'REAL(M,D)' or
'DOUBLE PRECISION(M,D)'.  Here, '(M,D)' means than values can be stored
with up to M digits in total, of which D digits may be after the decimal
point.  For example, a column defined as 'FLOAT(7,4)' will look like
'-999.9999' when displayed.  MySQL performs rounding when storing
values, so if you insert '999.00009' into a 'FLOAT(7,4)' column, the
approximate result is '999.0001'.

Because floating-point values are approximate and not stored as exact
values, attempts to treat them as exact in comparisons may lead to
problems.  They are also subject to platform or implementation
dependencies.  For more information, see *note problems-with-float::

For maximum portability, code requiring storage of approximate numeric
data values should use 'FLOAT' or 'DOUBLE PRECISION' with no
specification of precision or number of digits.


File: manual.info.tmp,  Node: bit-type,  Next: numeric-type-attributes,  Prev: floating-point-types,  Up: numeric-types

11.1.5 Bit-Value Type - BIT
---------------------------

The 'BIT' data type is used to store bit values.  A type of 'BIT(M)'
enables storage of M-bit values.  M can range from 1 to 64.

To specify bit values, 'b'VALUE'' notation can be used.  VALUE is a
binary value written using zeros and ones.  For example, 'b'111'' and
'b'10000000'' represent 7 and 128, respectively.  See *note
bit-value-literals::.

If you assign a value to a 'BIT(M)' column that is less than M bits
long, the value is padded on the left with zeros.  For example,
assigning a value of 'b'101'' to a 'BIT(6)' column is, in effect, the
same as assigning 'b'000101''.

NDB Cluster

The maximum combined size of all 'BIT' columns used in a given *note
'NDB': mysql-cluster. table must not exceed 4096 bits.


File: manual.info.tmp,  Node: numeric-type-attributes,  Next: out-of-range-and-overflow,  Prev: bit-type,  Up: numeric-types

11.1.6 Numeric Type Attributes
------------------------------

MySQL supports an extension for optionally specifying the display width
of integer data types in parentheses following the base keyword for the
type.  For example, *note 'INT(4)': integer-types. specifies an *note
'INT': integer-types. with a display width of four digits.  This
optional display width may be used by applications to display integer
values having a width less than the width specified for the column by
left-padding them with spaces.  (That is, this width is present in the
metadata returned with result sets.  Whether it is used is up to the
application.)

The display width does _not_ constrain the range of values that can be
stored in the column.  Nor does it prevent values wider than the column
display width from being displayed correctly.  For example, a column
specified as *note 'SMALLINT(3)': integer-types. has the usual *note
'SMALLINT': integer-types. range of '-32768' to '32767', and values
outside the range permitted by three digits are displayed in full using
more than three digits.

When used in conjunction with the optional (nonstandard) 'ZEROFILL'
attribute, the default padding of spaces is replaced with zeros.  For
example, for a column declared as *note 'INT(4) ZEROFILL':
integer-types, a value of '5' is retrieved as '0005'.

*Note*:

The 'ZEROFILL' attribute is ignored for columns involved in expressions
or *note 'UNION': union. queries.

If you store values larger than the display width in an integer column
that has the 'ZEROFILL' attribute, you may experience problems when
MySQL generates temporary tables for some complicated joins.  In these
cases, MySQL assumes that the data values fit within the column display
width.

All integer types can have an optional (nonstandard) 'UNSIGNED'
attribute.  An unsigned type can be used to permit only nonnegative
numbers in a column or when you need a larger upper numeric range for
the column.  For example, if an *note 'INT': integer-types. column is
'UNSIGNED', the size of the column's range is the same but its endpoints
shift up, from '-2147483648' and '2147483647' to '0' and '4294967295'.

Floating-point and fixed-point types also can be 'UNSIGNED'.  As with
integer types, this attribute prevents negative values from being stored
in the column.  Unlike the integer types, the upper range of column
values remains the same.

If you specify 'ZEROFILL' for a numeric column, MySQL automatically adds
the 'UNSIGNED' attribute.

Integer or floating-point data types can have the 'AUTO_INCREMENT'
attribute.  When you insert a value of 'NULL' into an indexed
'AUTO_INCREMENT' column, the column is set to the next sequence value.
Typically this is 'VALUE+1', where VALUE is the largest value for the
column currently in the table.  ('AUTO_INCREMENT' sequences begin with
'1'.)

Storing '0' into an 'AUTO_INCREMENT' column has the same effect as
storing 'NULL', unless the 'NO_AUTO_VALUE_ON_ZERO' SQL mode is enabled.

Inserting 'NULL' to generate 'AUTO_INCREMENT' values requires that the
column be declared 'NOT NULL'.  If the column is declared 'NULL',
inserting 'NULL' stores a 'NULL'.  When you insert any other value into
an 'AUTO_INCREMENT' column, the column is set to that value and the
sequence is reset so that the next automatically generated value follows
sequentially from the inserted value.

As of MySQL 5.5.29, negative values for 'AUTO_INCREMENT' columns are not
supported.


File: manual.info.tmp,  Node: out-of-range-and-overflow,  Prev: numeric-type-attributes,  Up: numeric-types

11.1.7 Out-of-Range and Overflow Handling
-----------------------------------------

When MySQL stores a value in a numeric column that is outside the
permissible range of the column data type, the result depends on the SQL
mode in effect at the time:

   * If strict SQL mode is enabled, MySQL rejects the out-of-range value
     with an error, and the insert fails, in accordance with the SQL
     standard.

   * If no restrictive modes are enabled, MySQL clips the value to the
     appropriate endpoint of the column data type range and stores the
     resulting value instead.

     When an out-of-range value is assigned to an integer column, MySQL
     stores the value representing the corresponding endpoint of the
     column data type range.

     When a floating-point or fixed-point column is assigned a value
     that exceeds the range implied by the specified (or default)
     precision and scale, MySQL stores the value representing the
     corresponding endpoint of that range.

Suppose that a table 't1' has this definition:

     CREATE TABLE t1 (i1 TINYINT, i2 TINYINT UNSIGNED);

With strict SQL mode enabled, an out of range error occurs:

     mysql> SET sql_mode = 'TRADITIONAL';
     mysql> INSERT INTO t1 (i1, i2) VALUES(256, 256);
     ERROR 1264 (22003): Out of range value for column 'i1' at row 1
     mysql> SELECT * FROM t1;
     Empty set (0.00 sec)

With strict SQL mode not enabled, clipping with warnings occurs:

     mysql> SET sql_mode = '';
     mysql> INSERT INTO t1 (i1, i2) VALUES(256, 256);
     mysql> SHOW WARNINGS;
     +---------+------+---------------------------------------------+
     | Level   | Code | Message                                     |
     +---------+------+---------------------------------------------+
     | Warning | 1264 | Out of range value for column 'i1' at row 1 |
     | Warning | 1264 | Out of range value for column 'i2' at row 1 |
     +---------+------+---------------------------------------------+
     mysql> SELECT * FROM t1;
     +------+------+
     | i1   | i2   |
     +------+------+
     |  127 |  255 |
     +------+------+

When strict SQL mode is not enabled, column-assignment conversions that
occur due to clipping are reported as warnings for *note 'ALTER TABLE':
alter-table, *note 'LOAD DATA': load-data, *note 'UPDATE': update, and
multiple-row *note 'INSERT': insert. statements.  In strict mode, these
statements fail, and some or all the values are not inserted or changed,
depending on whether the table is a transactional table and other
factors.  For details, see *note sql-mode::.

Overflow during numeric expression evaluation results in an error.  For
example, the largest signed *note 'BIGINT': integer-types. value is
9223372036854775807, so the following expression produces an error:

     mysql> SELECT 9223372036854775807 + 1;
     ERROR 1690 (22003): BIGINT value is out of range in '(9223372036854775807 + 1)'

To enable the operation to succeed in this case, convert the value to
unsigned;

     mysql> SELECT CAST(9223372036854775807 AS UNSIGNED) + 1;
     +-------------------------------------------+
     | CAST(9223372036854775807 AS UNSIGNED) + 1 |
     +-------------------------------------------+
     |                       9223372036854775808 |
     +-------------------------------------------+

Whether overflow occurs depends on the range of the operands, so another
way to handle the preceding expression is to use exact-value arithmetic
because *note 'DECIMAL': fixed-point-types. values have a larger range
than integers:

     mysql> SELECT 9223372036854775807.0 + 1;
     +---------------------------+
     | 9223372036854775807.0 + 1 |
     +---------------------------+
     |     9223372036854775808.0 |
     +---------------------------+

Subtraction between integer values, where one is of type 'UNSIGNED',
produces an unsigned result by default.  If the result would otherwise
have been negative, an error results:

     mysql> SET sql_mode = '';
     Query OK, 0 rows affected (0.00 sec)

     mysql> SELECT CAST(0 AS UNSIGNED) - 1;
     ERROR 1690 (22003): BIGINT UNSIGNED value is out of range in '(cast(0 as unsigned) - 1)'

If the 'NO_UNSIGNED_SUBTRACTION' SQL mode is enabled, the result is
negative:

     mysql> SET sql_mode = 'NO_UNSIGNED_SUBTRACTION';
     mysql> SELECT CAST(0 AS UNSIGNED) - 1;
     +-------------------------+
     | CAST(0 AS UNSIGNED) - 1 |
     +-------------------------+
     |                      -1 |
     +-------------------------+

If the result of such an operation is used to update an 'UNSIGNED'
integer column, the result is clipped to the maximum value for the
column type, or clipped to 0 if 'NO_UNSIGNED_SUBTRACTION' is enabled.
If strict SQL mode is enabled, an error occurs and the column remains
unchanged.


File: manual.info.tmp,  Node: date-and-time-types,  Next: string-types,  Prev: numeric-types,  Up: data-types

11.2 Date and Time Data Types
=============================

* Menu:

* date-and-time-type-syntax::    Date and Time Data Type Syntax
* datetime::                     The DATE, DATETIME, and TIMESTAMP Types
* time::                         The TIME Type
* year::                         The YEAR Type
* migrating-from-year2::         2-Digit YEAR(2) Limitations and Migrating to 4-Digit YEAR
* timestamp-initialization::     Automatic Initialization and Updating for TIMESTAMP
* fractional-seconds::           Fractional Seconds in Time Values
* date-and-time-type-conversion::  Conversion Between Date and Time Types
* two-digit-years::              2-Digit Years in Dates

The date and time data types for representing temporal values are *note
'DATE': datetime, *note 'TIME': time, *note 'DATETIME': datetime, *note
'TIMESTAMP': datetime, and *note 'YEAR': year.  Each temporal type has a
range of valid values, as well as a 'zero' value that may be used when
you specify an invalid value that MySQL cannot represent.  The *note
'TIMESTAMP': datetime. type has special automatic updating behavior,
described in *note timestamp-initialization::.

For information about storage requirements of the temporal data types,
see *note storage-requirements::.

For descriptions of functions that operate on temporal values, see *note
date-and-time-functions::.

Keep in mind these general considerations when working with date and
time types:

   * MySQL retrieves values for a given date or time type in a standard
     output format, but it attempts to interpret a variety of formats
     for input values that you supply (for example, when you specify a
     value to be assigned to or compared to a date or time type).  For a
     description of the permitted formats for date and time types, see
     *note date-and-time-literals::.  It is expected that you supply
     valid values.  Unpredictable results may occur if you use values in
     other formats.

   * Although MySQL tries to interpret values in several formats, date
     parts must always be given in year-month-day order (for example,
     ''98-09-04''), rather than in the month-day-year or day-month-year
     orders commonly used elsewhere (for example, ''09-04-98'',
     ''04-09-98'').  To convert strings in other orders to
     year-month-day order, the 'STR_TO_DATE()' function may be useful.

   * Dates containing 2-digit year values are ambiguous because the
     century is unknown.  MySQL interprets 2-digit year values using
     these rules:

        * Year values in the range '70-99' become '1970-1999'.

        * Year values in the range '00-69' become '2000-2069'.

     See also *note two-digit-years::.

   * Conversion of values from one temporal type to another occurs
     according to the rules in *note date-and-time-type-conversion::.

   * MySQL automatically converts a date or time value to a number if
     the value is used in numeric context and vice versa.

   * By default, when MySQL encounters a value for a date or time type
     that is out of range or otherwise invalid for the type, it converts
     the value to the 'zero' value for that type.  The exception is that
     out-of-range *note 'TIME': time. values are clipped to the
     appropriate endpoint of the *note 'TIME': time. range.

   * By setting the SQL mode to the appropriate value, you can specify
     more exactly what kind of dates you want MySQL to support.  (See
     *note sql-mode::.)  You can get MySQL to accept certain dates, such
     as ''2009-11-31'', by enabling the 'ALLOW_INVALID_DATES' SQL mode.
     This is useful when you want to store a 'possibly wrong' value
     which the user has specified (for example, in a web form) in the
     database for future processing.  Under this mode, MySQL verifies
     only that the month is in the range from 1 to 12 and that the day
     is in the range from 1 to 31.

   * MySQL permits you to store dates where the day or month and day are
     zero in a *note 'DATE': datetime. or *note 'DATETIME': datetime.
     column.  This is useful for applications that need to store
     birthdates for which you may not know the exact date.  In this
     case, you simply store the date as ''2009-00-00'' or
     ''2009-01-00''.  However, with dates such as these, you should not
     expect to get correct results for functions such as 'DATE_SUB()' or
     'DATE_ADD()' that require complete dates.  To disallow zero month
     or day parts in dates, enable the 'NO_ZERO_IN_DATE' SQL mode.

   * MySQL permits you to store a 'zero' value of ''0000-00-00'' as a
     'dummy date.' In some cases, this is more convenient than using
     'NULL' values, and uses less data and index space.  To disallow
     ''0000-00-00'', enable the 'NO_ZERO_DATE' SQL mode.

   * 'Zero' date or time values used through Connector/ODBC are
     converted automatically to 'NULL' because ODBC cannot handle such
     values.

The following table shows the format of the 'zero' value for each type.
The 'zero' values are special, but you can store or refer to them
explicitly using the values shown in the table.  You can also do this
using the values ''0'' or '0', which are easier to write.  For temporal
types that include a date part (*note 'DATE': datetime, *note
'DATETIME': datetime, and *note 'TIMESTAMP': datetime.), use of these
values may produce warning or errors.  The precise behavior depends on
which, if any, of the strict and 'NO_ZERO_DATE' SQL modes are enabled;
see *note sql-mode::.

Data Type              'Zero' Value
                       
*note 'DATE': datetime.''0000-00-00''
                       
*note 'TIME': time.    ''00:00:00''
                       
*note 'DATETIME': datetime.''0000-00-00 00:00:00''
                       
*note 'TIMESTAMP': datetime.''0000-00-00 00:00:00''
                       
*note 'YEAR': year.    '0000'


File: manual.info.tmp,  Node: date-and-time-type-syntax,  Next: datetime,  Prev: date-and-time-types,  Up: date-and-time-types

11.2.1 Date and Time Data Type Syntax
-------------------------------------

The date and time data types for representing temporal values are *note
'DATE': datetime, *note 'TIME': time, *note 'DATETIME': datetime, *note
'TIMESTAMP': datetime, and *note 'YEAR': year.

For the *note 'DATE': datetime. and *note 'DATETIME': datetime. range
descriptions, 'supported' means that although earlier values might work,
there is no guarantee.

   * 
     *note 'DATE': datetime.

     A date.  The supported range is ''1000-01-01'' to ''9999-12-31''.
     MySQL displays *note 'DATE': datetime. values in ''YYYY-MM-DD''
     format, but permits assignment of values to *note 'DATE': datetime.
     columns using either strings or numbers.

   * 
     *note 'DATETIME': datetime.

     A date and time combination.  The supported range is ''1000-01-01
     00:00:00'' to ''9999-12-31 23:59:59''.  MySQL displays *note
     'DATETIME': datetime. values in 'YYYY-MM-DD HH:MM:SS' format, but
     permits assignment of values to *note 'DATETIME': datetime. columns
     using either strings or numbers.

   * 
     *note 'TIMESTAMP': datetime.

     A timestamp.  The range is ''1970-01-01 00:00:01'' UTC to
     ''2038-01-19 03:14:07'' UTC. *note 'TIMESTAMP': datetime. values
     are stored as the number of seconds since the epoch (''1970-01-01
     00:00:00'' UTC). A *note 'TIMESTAMP': datetime. cannot represent
     the value ''1970-01-01 00:00:00'' because that is equivalent to 0
     seconds from the epoch and the value 0 is reserved for representing
     ''0000-00-00 00:00:00'', the 'zero' *note 'TIMESTAMP': datetime.
     value.

     Unless specified otherwise, the first *note 'TIMESTAMP': datetime.
     column in a table is defined to be automatically set to the date
     and time of the most recent modification if not explicitly assigned
     a value.  This makes *note 'TIMESTAMP': datetime. useful for
     recording the timestamp of an *note 'INSERT': insert. or *note
     'UPDATE': update. operation.  You can also set any *note
     'TIMESTAMP': datetime. column to the current date and time by
     assigning it a 'NULL' value, unless it has been defined with the
     'NULL' attribute to permit 'NULL' values.  The automatic
     initialization and updating to the current date and time can be
     specified using 'DEFAULT CURRENT_TIMESTAMP' and 'ON UPDATE
     CURRENT_TIMESTAMP' clauses, as described in *note
     timestamp-initialization::.

   * 
     *note 'TIME': time.

     A time.  The range is ''-838:59:59'' to ''838:59:59''.  MySQL
     displays *note 'TIME': time. values in 'HH:MM:SS' format, but
     permits assignment of values to *note 'TIME': time. columns using
     either strings or numbers.

   * 
     *note 'YEAR[(2|4)]': year.

     A year in 2-digit or 4-digit format.  The default is 4-digit
     format.  'YEAR(2)' or 'YEAR(4)' differ in display format, but have
     the same range of values.  In 4-digit format, values display as
     '1901' to '2155', or '0000'.  In 2-digit format, values display as
     '70' to '69', representing years from 1970 to 2069.  MySQL displays
     *note 'YEAR': year. values in YYYY or YY format, but permits
     assignment of values to *note 'YEAR': year. columns using either
     strings or numbers.

     *Note*:

     The 'YEAR(2)' data type has certain issues that you should consider
     before choosing to use it.  As of MySQL 5.5.27, 'YEAR(2)' is
     deprecated.  For more information, see *note
     migrating-from-year2::.

     For additional information about 'YEAR' display format and
     interpretation of input values, see *note year::.

The 'SUM()' and 'AVG()' aggregate functions do not work with temporal
values.  (They convert the values to numbers, losing everything after
the first nonnumeric character.)  To work around this problem, convert
to numeric units, perform the aggregate operation, and convert back to a
temporal value.  Examples:

     SELECT SEC_TO_TIME(SUM(TIME_TO_SEC(TIME_COL))) FROM TBL_NAME;
     SELECT FROM_DAYS(SUM(TO_DAYS(DATE_COL))) FROM TBL_NAME;

*Note*:

The MySQL server can be run with the 'MAXDB' SQL mode enabled.  In this
case, *note 'TIMESTAMP': datetime. is identical with *note 'DATETIME':
datetime.  If this mode is enabled at the time that a table is created,
*note 'TIMESTAMP': datetime. columns are created as *note 'DATETIME':
datetime. columns.  As a result, such columns use *note 'DATETIME':
datetime. display format, have the same range of values, and there is no
automatic initialization or updating to the current date and time.  See
*note sql-mode::.


File: manual.info.tmp,  Node: datetime,  Next: time,  Prev: date-and-time-type-syntax,  Up: date-and-time-types

11.2.2 The DATE, DATETIME, and TIMESTAMP Types
----------------------------------------------

The 'DATE', 'DATETIME', and 'TIMESTAMP' types are related.  This section
describes their characteristics, how they are similar, and how they
differ.  MySQL recognizes 'DATE', 'DATETIME', and 'TIMESTAMP' values in
several formats, described in *note date-and-time-literals::.  For the
'DATE' and 'DATETIME' range descriptions, 'supported' means that
although earlier values might work, there is no guarantee.

The 'DATE' type is used for values with a date part but no time part.
MySQL retrieves and displays 'DATE' values in ''YYYY-MM-DD'' format.
The supported range is ''1000-01-01'' to ''9999-12-31''.

The 'DATETIME' type is used for values that contain both date and time
parts.  MySQL retrieves and displays 'DATETIME' values in ''YYYY-MM-DD
HH:MM:SS'' format.  The supported range is ''1000-01-01 00:00:00'' to
''9999-12-31 23:59:59''.

The 'TIMESTAMP' data type is used for values that contain both date and
time parts.  'TIMESTAMP' has a range of ''1970-01-01 00:00:01'' UTC to
''2038-01-19 03:14:07'' UTC.

MySQL converts 'TIMESTAMP' values from the current time zone to UTC for
storage, and back from UTC to the current time zone for retrieval.
(This does not occur for other types such as 'DATETIME'.)  By default,
the current time zone for each connection is the server's time.  The
time zone can be set on a per-connection basis.  As long as the time
zone setting remains constant, you get back the same value you store.
If you store a 'TIMESTAMP' value, and then change the time zone and
retrieve the value, the retrieved value is different from the value you
stored.  This occurs because the same time zone was not used for
conversion in both directions.  The current time zone is available as
the value of the 'time_zone' system variable.  For more information, see
*note time-zone-support::.

The 'TIMESTAMP' data type offers automatic initialization and updating
to the current date and time.  For more information, see *note
timestamp-initialization::.

A 'DATETIME' or 'TIMESTAMP' value can include a trailing fractional
seconds part in up to microseconds (6 digits) precision.  Although this
fractional part is recognized, it is discarded from values stored into
'DATETIME' or 'TIMESTAMP' columns.  For information about fractional
seconds support in MySQL, see *note fractional-seconds::.

Invalid 'DATE', 'DATETIME', or 'TIMESTAMP' values are converted to the
'zero' value of the appropriate type (''0000-00-00'' or ''0000-00-00
00:00:00''), if the SQL mode permits this conversion.  The precise
behavior depends on which if any of strict SQL mode and the
'NO_ZERO_DATE' SQL mode are enabled; see *note sql-mode::.

Be aware of certain properties of date value interpretation in MySQL:

   * MySQL permits a 'relaxed' format for values specified as strings,
     in which any punctuation character may be used as the delimiter
     between date parts or time parts.  In some cases, this syntax can
     be deceiving.  For example, a value such as ''10:11:12'' might look
     like a time value because of the ':', but is interpreted as the
     year ''2010-11-12'' if used in date context.  The value
     ''10:45:15'' is converted to ''0000-00-00'' because ''45'' is not a
     valid month.

   * The server requires that month and day values be valid, and not
     merely in the range 1 to 12 and 1 to 31, respectively.  With strict
     mode disabled, invalid dates such as ''2004-04-31'' are converted
     to ''0000-00-00'' and a warning is generated.  With strict mode
     enabled, invalid dates generate an error.  To permit such dates,
     enable 'ALLOW_INVALID_DATES'.  See *note sql-mode::, for more
     information.

   * MySQL does not accept 'TIMESTAMP' values that include a zero in the
     day or month column or values that are not a valid date.  The sole
     exception to this rule is the special 'zero' value ''0000-00-00
     00:00:00'', if the SQL mode permits this value.  The precise
     behavior depends on which if any of strict SQL mode and the
     'NO_ZERO_DATE' SQL mode are enabled; see *note sql-mode::.

   * 'CAST()' treats a 'TIMESTAMP' value as a string when not selecting
     from a table.  (This is true even if you specify 'FROM DUAL'.)  See
     *note cast-functions::.

   * Dates containing 2-digit year values are ambiguous because the
     century is unknown.  MySQL interprets 2-digit year values using
     these rules:

        * Year values in the range '00-69' become '2000-2069'.

        * Year values in the range '70-99' become '1970-1999'.

     See also *note two-digit-years::.

*Note*:

The MySQL server can be run with the 'MAXDB' SQL mode enabled.  In this
case, 'TIMESTAMP' is identical with 'DATETIME'.  If this mode is enabled
at the time that a table is created, 'TIMESTAMP' columns are created as
'DATETIME' columns.  As a result, such columns use 'DATETIME' display
format, have the same range of values, and there is no automatic
initialization or updating to the current date and time.  See *note
sql-mode::.


File: manual.info.tmp,  Node: time,  Next: year,  Prev: datetime,  Up: date-and-time-types

11.2.3 The TIME Type
--------------------

MySQL retrieves and displays 'TIME' values in 'HH:MM:SS' format (or
'HHH:MM:SS' format for large hours values).  'TIME' values may range
from ''-838:59:59'' to ''838:59:59''.  The hours part may be so large
because the 'TIME' type can be used not only to represent a time of day
(which must be less than 24 hours), but also elapsed time or a time
interval between two events (which may be much greater than 24 hours, or
even negative).

MySQL recognizes 'TIME' values in several formats, described in *note
date-and-time-literals::.  Some of these formats can include a trailing
fractional seconds part in up to microseconds (6 digits) precision.
Although this fractional part is recognized, it is discarded from values
stored into 'TIME' columns.  For information about fractional seconds
support in MySQL, see *note fractional-seconds::.

Be careful about assigning abbreviated values to a 'TIME' column.  MySQL
interprets abbreviated 'TIME' values with colons as time of the day.
That is, ''11:12'' means ''11:12:00'', not ''00:11:12''.  MySQL
interprets abbreviated values without colons using the assumption that
the two rightmost digits represent seconds (that is, as elapsed time
rather than as time of day).  For example, you might think of ''1112''
and '1112' as meaning ''11:12:00'' (12 minutes after 11 o'clock), but
MySQL interprets them as ''00:11:12'' (11 minutes, 12 seconds).
Similarly, ''12'' and '12' are interpreted as ''00:00:12''.

By default, values that lie outside the 'TIME' range but are otherwise
valid are clipped to the closest endpoint of the range.  For example,
''-850:00:00'' and ''850:00:00'' are converted to ''-838:59:59'' and
''838:59:59''.  Invalid 'TIME' values are converted to ''00:00:00''.
Note that because ''00:00:00'' is itself a valid 'TIME' value, there is
no way to tell, from a value of ''00:00:00'' stored in a table, whether
the original value was specified as ''00:00:00'' or whether it was
invalid.

For more restrictive treatment of invalid 'TIME' values, enable strict
SQL mode to cause errors to occur.  See *note sql-mode::.


File: manual.info.tmp,  Node: year,  Next: migrating-from-year2,  Prev: time,  Up: date-and-time-types

11.2.4 The YEAR Type
--------------------

The 'YEAR' type is a 1-byte type used to represent year values.  It can
be declared as 'YEAR' with an implicit display width of 4 characters, or
as 'YEAR(4)' or 'YEAR(2)' with an explicit display width of 4 or 2
characters.

*Note*:

The 2-digit 'YEAR(2)' data type has certain issues that you should
consider before choosing to use it.  As of MySQL 5.5.27, 'YEAR(2)' is
deprecated.  For more information, see *note migrating-from-year2::.

'YEAR'/'YEAR(4)' and 'YEAR(2)' differ in display format, but have the
same range of values.  For 4-digit format, MySQL displays 'YEAR' values
in YYYY format, with a range of '1901' to 2155, and '0000'.  For 2-digit
format, MySQL displays only the last (least significant) 2 digits; for
example, '70' (1970 or 2070) or '69' (2069).

'YEAR' accepts input values in a variety of formats:

   * As 4-digit strings in the range ''1901'' to ''2155''.

   * As 4-digit numbers in the range '1901' to '2155'.

   * As 1- or 2-digit strings in the range ''0'' to ''99''.  MySQL
     converts values in the ranges ''0'' to ''69'' and ''70'' to ''99''
     to 'YEAR' values in the ranges '2000' to '2069' and '1970' to
     '1999'.

   * As 1- or 2-digit numbers in the range '0' to '99'.  MySQL converts
     values in the ranges '1' to '69' and '70' to '99' to 'YEAR' values
     in the ranges '2001' to '2069' and '1970' to '1999'.

     Inserting a numeric '0' has different effects for 'YEAR'/'YEAR(4)'
     and 'YEAR(2)':

        * For 'YEAR'/'YEAR(4)', the result has a display value of '0000'
          and an internal value of '0000'.  To specify zero and have it
          be interpreted as '2000', specify it as a string ''0'' or
          ''00''.

        * For 'YEAR(2)', the result has a display value of '00' and an
          internal value of '2000'.

   * As the result of functions that return a value that is acceptable
     in 'YEAR' context, such as 'NOW()'.

If strict SQL mode is not enabled, MySQL converts invalid 'YEAR' values
to '0000'.  In strict SQL mode, attempting to insert an invalid 'YEAR'
value produces an error.

See also *note two-digit-years::.


File: manual.info.tmp,  Node: migrating-from-year2,  Next: timestamp-initialization,  Prev: year,  Up: date-and-time-types

11.2.5 2-Digit YEAR(2) Limitations and Migrating to 4-Digit YEAR
----------------------------------------------------------------

This section describes problems that can occur when using the 2-digit
*note 'YEAR(2)': year. data type and provides information about
converting existing *note 'YEAR(2)': year. columns to 4-digit
year-valued columns, which can be declared as 'YEAR' with an implicit
display width of 4 characters, or equivalently as 'YEAR(4)' with an
explicit display width.

Although the internal range of values for 'YEAR'/*note 'YEAR(4)': year.
and the deprecated *note 'YEAR(2)': year. type is the same ('1901' to
'2155', and '0000'), the display width for *note 'YEAR(2)': year. makes
that type inherently ambiguous because displayed values indicate only
the last two digits of the internal values and omit the century digits.
The result can be a loss of information under certain circumstances.
For this reason, avoid using *note 'YEAR(2)': year. in your applications
and use 'YEAR'/*note 'YEAR(4)': year. wherever you need a year-valued
data type.  Note that conversion is required at some point because
support for *note 'YEAR': year. data types with display values other
than 4, most notably *note 'YEAR(2)': year, is reduced as of MySQL 5.6.6
and is removed entirely in MySQL 5.7.

*YEAR(2) Limitations*

Issues with the *note 'YEAR(2)': year. data type include ambiguity of
displayed values, and possible loss of information when values are
dumped and reloaded or converted to strings.

   * Displayed *note 'YEAR(2)': year. values can be ambiguous.  It is
     possible for up to three *note 'YEAR(2)': year. values that have
     different internal values to have the same displayed value, as the
     following example demonstrates:

          mysql> CREATE TABLE t (y2 YEAR(2), y4 YEAR);
          Query OK, 0 rows affected, 1 warning (0.01 sec)

          mysql> INSERT INTO t (y2) VALUES(1912),(2012),(2112);
          Query OK, 3 rows affected (0.00 sec)
          Records: 3  Duplicates: 0  Warnings: 0

          mysql> UPDATE t SET y4 = y2;
          Query OK, 3 rows affected (0.00 sec)
          Rows matched: 3  Changed: 3  Warnings: 0

          mysql> SELECT * FROM t;
          +------+------+
          | y2   | y4   |
          +------+------+
          |   12 | 1912 |
          |   12 | 2012 |
          |   12 | 2112 |
          +------+------+
          3 rows in set (0.00 sec)

   * If you use *note 'mysqldump': mysqldump. to dump the table created
     in the preceding example, the dump file represents all 'y2' values
     using the same 2-digit representation ('12').  If you reload the
     table from the dump file, all resulting rows have internal value
     '2012' and display value '12', thus losing the distinctions between
     them.

   * Conversion of a 2-digit or 4-digit *note 'YEAR': year. data value
     to string form uses the data type display width.  Suppose that a
     *note 'YEAR(2)': year. column and a 'YEAR'/*note 'YEAR(4)': year.
     column both contain the value '1970'.  Assigning each column to a
     string results in a value of ''70'' or ''1970'', respectively.
     That is, loss of information occurs for conversion from *note
     'YEAR(2)': year. to string.

   * Values outside the range from '1970' to '2069' are stored
     incorrectly when inserted into a *note 'YEAR(2)': year. column in a
     *note 'CSV': csv-storage-engine. table.  For example, inserting
     '2211' results in a display value of '11' but an internal value of
     '2011'.

To avoid these problems, use the 4-digit *note 'YEAR': year. or *note
'YEAR(4)': year. data type rather than the 2-digit *note 'YEAR(2)':
year. data type.  Suggestions regarding migration strategies appear
later in this section.

*Migrating from YEAR(2) to 4-Digit YEAR*

To convert 2-digit *note 'YEAR(2)': year. columns to 4-digit *note
'YEAR': year. columns, use *note 'ALTER TABLE': alter-table.  Suppose
that a table 't1' has this definition:

     CREATE TABLE t1 (ycol YEAR(2) NOT NULL DEFAULT '70');

Modify the column using 'ALTER TABLE' as follows.  Remember to include
any column attributes such as 'NOT NULL' or 'DEFAULT':

     ALTER TABLE t1 MODIFY ycol YEAR NOT NULL DEFAULT '1970';

The *note 'ALTER TABLE': alter-table. statement converts the table
without changing *note 'YEAR(2)': year. values.  If the server is a
replication master, the *note 'ALTER TABLE': alter-table. statement
replicates to slaves and makes the corresponding table change on each
one.

One migration method should be avoided: Do not dump your data with *note
'mysqldump': mysqldump. and reload the dump file after upgrading.  That
has the potential to change *note 'YEAR(2)': year. values, as described
previously.

A migration from 2-digit *note 'YEAR(2)': year. columns to 4-digit *note
'YEAR': year. columns should also involve examining application code for
the possibility of changed behavior under conditions such as these:

   * Code that expects selecting a *note 'YEAR': year. column to produce
     exactly two digits.

   * Code that does not account for different handling for inserts of
     numeric '0': Inserting '0' into *note 'YEAR(2)': year. or *note
     'YEAR(4)': year. results in an internal value of '2000' or '0000',
     respectively.


File: manual.info.tmp,  Node: timestamp-initialization,  Next: fractional-seconds,  Prev: migrating-from-year2,  Up: date-and-time-types

11.2.6 Automatic Initialization and Updating for TIMESTAMP
----------------------------------------------------------

*Note*:

In older versions of MySQL (prior to 4.1), the properties of the *note
'TIMESTAMP': datetime. data type differed significantly in several ways
from what is described in this section (see the 'MySQL 3.23, 4.0, 4.1
Reference Manual' for details); these include syntax extensions which
are deprecated in MySQL 5.1, and no longer supported in MySQL 5.5.  This
has implications for performing a dump and restore or replicating
between MySQL Server versions.  If you are using columns that are
defined using the old *note 'TIMESTAMP(N)': datetime. syntax, see *note
upgrading-from-previous-series::, prior to upgrading to MySQL 5.5.

The *note 'TIMESTAMP': datetime. data type offers automatic
initialization and updating to the current date and time (that is, the
current timestamp).  You can choose whether to use these properties and
which column should have them:

   * One *note 'TIMESTAMP': datetime. column in a table can have the
     current timestamp as the default value for initializing the column,
     as the auto-update value, or both.  It is not possible to have the
     current timestamp be the default value for one column and the
     auto-update value for another column.

   * If the column is auto-initialized, it is set to the current
     timestamp for inserted rows that specify no value for the column.

   * If the column is auto-updated, it is automatically updated to the
     current timestamp when the value of any other column in the row is
     changed from its current value.  The column remains unchanged if
     all other columns are set to their current values.  To prevent the
     column from updating when other columns change, explicitly set it
     to its current value.  To update the column even when other columns
     do not change, explicitly set it to the value it should have (for
     example, set it to 'CURRENT_TIMESTAMP').

In addition, you can initialize or update any *note 'TIMESTAMP':
datetime. column to the current date and time by assigning it a 'NULL'
value, unless it has been defined with the 'NULL' attribute to permit
'NULL' values.

To specify automatic properties, use the 'DEFAULT CURRENT_TIMESTAMP' and
'ON UPDATE CURRENT_TIMESTAMP' clauses.  The order of the clauses does
not matter.  If both are present in a column definition, either can
occur first.  Any of the synonyms for 'CURRENT_TIMESTAMP' have the same
meaning as 'CURRENT_TIMESTAMP'.  These are 'CURRENT_TIMESTAMP()',
'NOW()', 'LOCALTIME', 'LOCALTIME()', 'LOCALTIMESTAMP', and
'LOCALTIMESTAMP()'.

Use of 'DEFAULT CURRENT_TIMESTAMP' and 'ON UPDATE CURRENT_TIMESTAMP' is
specific to *note 'TIMESTAMP': datetime.  The 'DEFAULT' clause also can
be used to specify a constant (nonautomatic) default value (for example,
'DEFAULT 0' or 'DEFAULT '2000-01-01 00:00:00'').

*Note*:

The following examples use 'DEFAULT 0', a default that can produce
warnings or errors depending on whether strict SQL mode or the
'NO_ZERO_DATE' SQL mode is enabled.  Be aware that the 'TRADITIONAL' SQL
mode includes strict mode and 'NO_ZERO_DATE'.  See *note sql-mode::.

The following rules describe the possibilities for defining the first
*note 'TIMESTAMP': datetime. column in a table with the current
timestamp for both the default and auto-update values, for one but not
the other, or for neither:

   * With both 'DEFAULT CURRENT_TIMESTAMP' and 'ON UPDATE
     CURRENT_TIMESTAMP', the column has the current timestamp for its
     default value and is automatically updated to the current
     timestamp.

          CREATE TABLE t1 (
            ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
          );

   * With neither 'DEFAULT CURRENT_TIMESTAMP' nor 'ON UPDATE
     CURRENT_TIMESTAMP', it is the same as specifying both 'DEFAULT
     CURRENT_TIMESTAMP' and 'ON UPDATE CURRENT_TIMESTAMP'.

          CREATE TABLE t1 (
            ts TIMESTAMP
          );

   * With a 'DEFAULT' clause but no 'ON UPDATE CURRENT_TIMESTAMP'
     clause, the column has the given default value and is not
     automatically updated to the current timestamp.

     The default depends on whether the 'DEFAULT' clause specifies
     'CURRENT_TIMESTAMP' or a constant value.  With 'CURRENT_TIMESTAMP',
     the default is the current timestamp.

          CREATE TABLE t1 (
            ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP
          );

     With a constant, the default is the given value.  In this case, the
     column has no automatic properties at all.

          CREATE TABLE t1 (
            ts TIMESTAMP DEFAULT 0
          );

   * With an 'ON UPDATE CURRENT_TIMESTAMP' clause and a constant
     'DEFAULT' clause, the column is automatically updated to the
     current timestamp and has the given constant default value.

          CREATE TABLE t1 (
            ts TIMESTAMP DEFAULT 0 ON UPDATE CURRENT_TIMESTAMP
          );

   * With an 'ON UPDATE CURRENT_TIMESTAMP' clause but no 'DEFAULT'
     clause, the column is automatically updated to the current
     timestamp.  The default is 0 unless the column is defined with the
     'NULL' attribute, in which case the default is 'NULL'.

          CREATE TABLE t1 (
            ts TIMESTAMP ON UPDATE CURRENT_TIMESTAMP      -- default 0
          );
          CREATE TABLE t2 (
            ts TIMESTAMP NULL ON UPDATE CURRENT_TIMESTAMP -- default NULL
          );

It need not be the first *note 'TIMESTAMP': datetime. column in a table
that is automatically initialized or updated to the current timestamp.
However, to specify automatic initialization or updating for a different
*note 'TIMESTAMP': datetime. column, you must suppress the automatic
properties for the first one.  Then, for the other *note 'TIMESTAMP':
datetime. column, the rules for the 'DEFAULT' and 'ON UPDATE' clauses
are the same as for the first *note 'TIMESTAMP': datetime. column,
except that if you omit both clauses, no automatic initialization or
updating occurs.

To suppress automatic properties for the first *note 'TIMESTAMP':
datetime. column, do either of the following:

   * Define the column with a 'DEFAULT' clause that specifies a constant
     default value.

   * Specify the 'NULL' attribute.  This also causes the column to
     permit 'NULL' values, which means that you cannot assign the
     current timestamp by setting the column to 'NULL'.  Assigning
     'NULL' sets the column to 'NULL'.

Consider these table definitions:

     CREATE TABLE t1 (
       ts1 TIMESTAMP DEFAULT 0,
       ts2 TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                     ON UPDATE CURRENT_TIMESTAMP);
     CREATE TABLE t2 (
       ts1 TIMESTAMP NULL,
       ts2 TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                     ON UPDATE CURRENT_TIMESTAMP);
     CREATE TABLE t3 (
       ts1 TIMESTAMP NULL DEFAULT 0,
       ts2 TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                     ON UPDATE CURRENT_TIMESTAMP);

The tables have these properties:

   * In each table definition, the first *note 'TIMESTAMP': datetime.
     column has no automatic initialization or updating.

   * The tables differ in how the 'ts1' column handles 'NULL' values.
     For 't1', 'ts1' is 'NOT NULL' and assigning it a value of 'NULL'
     sets it to the current timestamp.  For 't2' and 't3', 'ts1' permits
     'NULL' and assigning it a value of 'NULL' sets it to 'NULL'.

   * 't2' and 't3' differ in the default value for 'ts1'.  For 't2',
     'ts1' is defined to permit 'NULL', so the default is also 'NULL' in
     the absence of an explicit 'DEFAULT' clause.  For 't3', 'ts1'
     permits 'NULL' but has an explicit default of 0.

*TIMESTAMP Initialization and the NULL Attribute*

By default, *note 'TIMESTAMP': datetime. columns are 'NOT NULL', cannot
contain 'NULL' values, and assigning 'NULL' assigns the current
timestamp.  To permit a *note 'TIMESTAMP': datetime. column to contain
'NULL', explicitly declare it with the 'NULL' attribute.  In this case,
the default value also becomes 'NULL' unless overridden with a 'DEFAULT'
clause that specifies a different default value.  'DEFAULT NULL' can be
used to explicitly specify 'NULL' as the default value.  (For a *note
'TIMESTAMP': datetime. column not declared with the 'NULL' attribute,
'DEFAULT NULL' is invalid.)  If a *note 'TIMESTAMP': datetime. column
permits 'NULL' values, assigning 'NULL' sets it to 'NULL', not to the
current timestamp.

The following table contains several *note 'TIMESTAMP': datetime.
columns that permit 'NULL' values:

     CREATE TABLE t
     (
       ts1 TIMESTAMP NULL DEFAULT NULL,
       ts2 TIMESTAMP NULL DEFAULT 0,
       ts3 TIMESTAMP NULL DEFAULT CURRENT_TIMESTAMP
     );

A *note 'TIMESTAMP': datetime. column that permits 'NULL' values does
_not_ take on the current timestamp at insert time except under one of
the following conditions:

   * Its default value is defined as 'CURRENT_TIMESTAMP' and no value is
     specified for the column

   * 'CURRENT_TIMESTAMP' or any of its synonyms such as 'NOW()' is
     explicitly inserted into the column

In other words, a *note 'TIMESTAMP': datetime. column defined to permit
'NULL' values auto-initializes only if its definition includes 'DEFAULT
CURRENT_TIMESTAMP':

     CREATE TABLE t (ts TIMESTAMP NULL DEFAULT CURRENT_TIMESTAMP);

If the *note 'TIMESTAMP': datetime. column permits 'NULL' values but its
definition does not include 'DEFAULT CURRENT_TIMESTAMP', you must
explicitly insert a value corresponding to the current date and time.
Suppose that tables 't1' and 't2' have these definitions:

     CREATE TABLE t1 (ts TIMESTAMP NULL DEFAULT '0000-00-00 00:00:00');
     CREATE TABLE t2 (ts TIMESTAMP NULL DEFAULT NULL);

To set the *note 'TIMESTAMP': datetime. column in either table to the
current timestamp at insert time, explicitly assign it that value.  For
example:

     INSERT INTO t2 VALUES (CURRENT_TIMESTAMP);
     INSERT INTO t1 VALUES (NOW());


File: manual.info.tmp,  Node: fractional-seconds,  Next: date-and-time-type-conversion,  Prev: timestamp-initialization,  Up: date-and-time-types

11.2.7 Fractional Seconds in Time Values
----------------------------------------

A trailing fractional seconds part is permissible for temporal values in
contexts such as literal values, and in the arguments to or return
values from some temporal functions.  Example:

     mysql> SELECT MICROSECOND('2010-12-10 14:12:09.019473');
     +-------------------------------------------+
     | MICROSECOND('2010-12-10 14:12:09.019473') |
     +-------------------------------------------+
     |                                     19473 |
     +-------------------------------------------+

However, when MySQL stores a value into a column of any temporal data
type, it discards any fractional part and does not store it.


File: manual.info.tmp,  Node: date-and-time-type-conversion,  Next: two-digit-years,  Prev: fractional-seconds,  Up: date-and-time-types

11.2.8 Conversion Between Date and Time Types
---------------------------------------------

To some extent, you can convert a value from one temporal type to
another.  However, there may be some alteration of the value or loss of
information.  In all cases, conversion between temporal types is subject
to the range of valid values for the resulting type.  For example,
although *note 'DATE': datetime, *note 'DATETIME': datetime, and *note
'TIMESTAMP': datetime. values all can be specified using the same set of
formats, the types do not all have the same range of values.  *note
'TIMESTAMP': datetime. values cannot be earlier than '1970' UTC or later
than ''2038-01-19 03:14:07'' UTC. This means that a date such as
''1968-01-01'', while valid as a *note 'DATE': datetime. or *note
'DATETIME': datetime. value, is not valid as a *note 'TIMESTAMP':
datetime. value and is converted to '0'.

Conversion of *note 'DATE': datetime. values:

   * Conversion to a *note 'DATETIME': datetime. or *note 'TIMESTAMP':
     datetime. value adds a time part of ''00:00:00'' because the *note
     'DATE': datetime. value contains no time information.

   * Conversion to a *note 'TIME': time. value is not useful; the result
     is ''00:00:00''.

Conversion of *note 'DATETIME': datetime. and *note 'TIMESTAMP':
datetime. values:

   * Conversion to a *note 'DATE': datetime. value discards the time
     part because the *note 'DATE': datetime. type contains no time
     information.

   * Conversion to a *note 'TIME': time. value discards the date part
     because the *note 'TIME': time. type contains no date information.

Conversion of *note 'TIME': time. values:

MySQL converts a time value to a date or date-and-time value by parsing
the string value of the time as a date or date-and-time.  This is
unlikely to be useful.  For example, ''23:12:31'' interpreted as a date
becomes ''2023-12-31''.  Time values not valid as dates become
''0000-00-00'' or 'NULL'.

Explicit conversion can be used to override implicit conversion.  For
example, in comparison of *note 'DATE': datetime. and *note 'DATETIME':
datetime. values, the *note 'DATE': datetime. value is coerced to the
*note 'DATETIME': datetime. type by adding a time part of ''00:00:00''.
To perform the comparison by ignoring the time part of the *note
'DATETIME': datetime. value instead, use the 'CAST()' function in the
following way:

     DATE_COL = CAST(DATETIME_COL AS DATE)

Conversion of *note 'TIME': time. or *note 'DATETIME': datetime. values
to numeric form (for example, by adding '+0') results in a
double-precision value with a microseconds part of '.000000':

     mysql> SELECT CURTIME(), CURTIME()+0;
     +-----------+---------------+
     | CURTIME() | CURTIME()+0   |
     +-----------+---------------+
     | 10:41:36  | 104136.000000 |
     +-----------+---------------+
     mysql> SELECT NOW(), NOW()+0;
     +---------------------+-----------------------+
     | NOW()               | NOW()+0               |
     +---------------------+-----------------------+
     | 2007-11-30 10:41:47 | 20071130104147.000000 |
     +---------------------+-----------------------+


File: manual.info.tmp,  Node: two-digit-years,  Prev: date-and-time-type-conversion,  Up: date-and-time-types

11.2.9 2-Digit Years in Dates
-----------------------------

Date values with 2-digit years are ambiguous because the century is
unknown.  Such values must be interpreted into 4-digit form because
MySQL stores years internally using 4 digits.

For *note 'DATETIME': datetime, *note 'DATE': datetime, and *note
'TIMESTAMP': datetime. types, MySQL interprets dates specified with
ambiguous year values using these rules:

   * Year values in the range '00-69' become '2000-2069'.

   * Year values in the range '70-99' become '1970-1999'.

For 'YEAR', the rules are the same, with this exception: A numeric '00'
inserted into 'YEAR(4)' results in '0000' rather than '2000'.  To
specify zero for 'YEAR(4)' and have it be interpreted as '2000', specify
it as a string ''0'' or ''00''.

Remember that these rules are only heuristics that provide reasonable
guesses as to what your data values mean.  If the rules used by MySQL do
not produce the values you require, you must provide unambiguous input
containing 4-digit year values.

'ORDER BY' properly sorts *note 'YEAR': year. values that have 2-digit
years.

Some functions like 'MIN()' and 'MAX()' convert a *note 'YEAR': year. to
a number.  This means that a value with a 2-digit year does not work
properly with these functions.  The fix in this case is to convert the
*note 'YEAR': year. to 4-digit year format.


File: manual.info.tmp,  Node: string-types,  Next: spatial-types,  Prev: date-and-time-types,  Up: data-types

11.3 String Data Types
======================

* Menu:

* string-type-syntax::           String Data Type Syntax
* char::                         The CHAR and VARCHAR Types
* binary-varbinary::             The BINARY and VARBINARY Types
* blob::                         The BLOB and TEXT Types
* enum::                         The ENUM Type
* set::                          The SET Type

The string data types are *note 'CHAR': char, *note 'VARCHAR': char,
*note 'BINARY': binary-varbinary, *note 'VARBINARY': binary-varbinary,
*note 'BLOB': blob, *note 'TEXT': blob, *note 'ENUM': enum, and *note
'SET': set.

For information about storage requirements of the string data types, see
*note storage-requirements::.

For descriptions of functions that operate on string values, see *note
string-functions::.


File: manual.info.tmp,  Node: string-type-syntax,  Next: char,  Prev: string-types,  Up: string-types

11.3.1 String Data Type Syntax
------------------------------

The string data types are *note 'CHAR': char, *note 'VARCHAR': char,
*note 'BINARY': binary-varbinary, *note 'VARBINARY': binary-varbinary,
*note 'BLOB': blob, *note 'TEXT': blob, *note 'ENUM': enum, and *note
'SET': set.

In some cases, MySQL may change a string column to a type different from
that given in a *note 'CREATE TABLE': create-table. or *note 'ALTER
TABLE': alter-table. statement.  See *note silent-column-changes::.

For definitions of character string columns (*note 'CHAR': char, *note
'VARCHAR': char, and the *note 'TEXT': blob. types), MySQL interprets
length specifications in character units.  For definitions of binary
string columns (*note 'BINARY': binary-varbinary, *note 'VARBINARY':
binary-varbinary, and the *note 'BLOB': blob. types), MySQL interprets
length specifications in byte units.

Column definitions for character string data types *note 'CHAR': char,
*note 'VARCHAR': char, the *note 'TEXT': blob. types, *note 'ENUM':
enum, *note 'SET': set, and any synonyms) can specify the column
character set and collation:

   * 'CHARACTER SET' specifies the character set.  If desired, a
     collation for the character set can be specified with the 'COLLATE'
     attribute, along with any other attributes.  For example:

          CREATE TABLE t
          (
              c1 VARCHAR(20) CHARACTER SET utf8,
              c2 TEXT CHARACTER SET latin1 COLLATE latin1_general_cs
          );

     This table definition creates a column named 'c1' that has a
     character set of 'utf8' with the default collation for that
     character set, and a column named 'c2' that has a character set of
     'latin1' and a case-sensitive ('_cs') collation.

     The rules for assigning the character set and collation when either
     or both of 'CHARACTER SET' and the 'COLLATE' attribute are missing
     are described in *note charset-column::.

     'CHARSET' is a synonym for 'CHARACTER SET'.

   * Specifying the 'CHARACTER SET binary' attribute for a character
     string data type causes the column to be created as the
     corresponding binary string data type: *note 'CHAR': char. becomes
     *note 'BINARY': binary-varbinary, *note 'VARCHAR': char. becomes
     *note 'VARBINARY': binary-varbinary, and *note 'TEXT': blob.
     becomes *note 'BLOB': blob.  For the *note 'ENUM': enum. and *note
     'SET': set. data types, this does not occur; they are created as
     declared.  Suppose that you specify a table using this definition:

          CREATE TABLE t
          (
            c1 VARCHAR(10) CHARACTER SET binary,
            c2 TEXT CHARACTER SET binary,
            c3 ENUM('a','b','c') CHARACTER SET binary
          );

     The resulting table has this definition:

          CREATE TABLE t
          (
            c1 VARBINARY(10),
            c2 BLOB,
            c3 ENUM('a','b','c') CHARACTER SET binary
          );

   * The 'BINARY' attribute is a nonstandard MySQL extension that is
     shorthand for specifying the binary ('_bin') collation of the
     column character set (or of the table default character set if no
     column character set is specified).  In this case, comparison and
     sorting are based on numeric character code values.  Suppose that
     you specify a table using this definition:

          CREATE TABLE t
          (
            c1 VARCHAR(10) CHARACTER SET latin1 BINARY,
            c2 TEXT BINARY
          ) CHARACTER SET utf8mb4;

     The resulting table has this definition:

          CREATE TABLE t (
            c1 VARCHAR(10) CHARACTER SET latin1 COLLATE latin1_bin,
            c2 TEXT CHARACTER SET utf8mb4 COLLATE utf8mb4_bin
          ) CHARACTER SET utf8mb4;

   * The 'ASCII' attribute is shorthand for 'CHARACTER SET latin1'.

   * The 'UNICODE' attribute is shorthand for 'CHARACTER SET ucs2'.

Character column comparison and sorting are based on the collation
assigned to the column.  For the *note 'CHAR': char, *note 'VARCHAR':
char, *note 'TEXT': blob, *note 'ENUM': enum, and *note 'SET': set. data
types, you can declare a column with a binary ('_bin') collation or the
'BINARY' attribute to cause comparison and sorting to use the underlying
character code values rather than a lexical ordering.

For additional information about use of character sets in MySQL, see
*note charset::.

   * 
     '[NATIONAL] CHAR[(M)] [CHARACTER SET CHARSET_NAME] [COLLATE
     COLLATION_NAME]'

     A fixed-length string that is always right-padded with spaces to
     the specified length when stored.  M represents the column length
     in characters.  The range of M is 0 to 255.  If M is omitted, the
     length is 1.

     *Note*:

     Trailing spaces are removed when *note 'CHAR': char. values are
     retrieved unless the 'PAD_CHAR_TO_FULL_LENGTH' SQL mode is enabled.

     *note 'CHAR': char. is shorthand for *note 'CHARACTER': char.
     *note 'NATIONAL CHAR': char. (or its equivalent short form, *note
     'NCHAR': char.) is the standard SQL way to define that a *note
     'CHAR': char. column should use some predefined character set.
     MySQL uses 'utf8' as this predefined character set.  *note
     charset-national::.

     The *note 'CHAR BYTE': binary-varbinary. data type is an alias for
     the *note 'BINARY': binary-varbinary. data type.  This is a
     compatibility feature.

     MySQL permits you to create a column of type 'CHAR(0)'.  This is
     useful primarily when you must be compliant with old applications
     that depend on the existence of a column but that do not actually
     use its value.  'CHAR(0)' is also quite nice when you need a column
     that can take only two values: A column that is defined as 'CHAR(0)
     NULL' occupies only one bit and can take only the values 'NULL' and
     '''' (the empty string).

   * 
     '[NATIONAL] VARCHAR(M) [CHARACTER SET CHARSET_NAME] [COLLATE
     COLLATION_NAME]'

     A variable-length string.  M represents the maximum column length
     in characters.  The range of M is 0 to 65,535.  The effective
     maximum length of a *note 'VARCHAR': char. is subject to the
     maximum row size (65,535 bytes, which is shared among all columns)
     and the character set used.  For example, 'utf8' characters can
     require up to three bytes per character, so a *note 'VARCHAR':
     char. column that uses the 'utf8' character set can be declared to
     be a maximum of 21,844 characters.  See *note column-count-limit::.

     MySQL stores *note 'VARCHAR': char. values as a 1-byte or 2-byte
     length prefix plus data.  The length prefix indicates the number of
     bytes in the value.  A *note 'VARCHAR': char. column uses one
     length byte if values require no more than 255 bytes, two length
     bytes if values may require more than 255 bytes.

     *Note*:

     MySQL follows the standard SQL specification, and does _not_ remove
     trailing spaces from *note 'VARCHAR': char. values.

     *note 'VARCHAR': char. is shorthand for *note 'CHARACTER VARYING':
     char.  *note 'NATIONAL VARCHAR': char. is the standard SQL way to
     define that a *note 'VARCHAR': char. column should use some
     predefined character set.  MySQL uses 'utf8' as this predefined
     character set.  *note charset-national::.  *note 'NVARCHAR': char.
     is shorthand for *note 'NATIONAL VARCHAR': char.

   * 
     *note 'BINARY[(M)]': binary-varbinary.

     The *note 'BINARY': binary-varbinary. type is similar to the *note
     'CHAR': char. type, but stores binary byte strings rather than
     nonbinary character strings.  An optional length M represents the
     column length in bytes.  If omitted, M defaults to 1.

   * 
     *note 'VARBINARY(M)': binary-varbinary.

     The *note 'VARBINARY': binary-varbinary. type is similar to the
     *note 'VARCHAR': char. type, but stores binary byte strings rather
     than nonbinary character strings.  M represents the maximum column
     length in bytes.

   * 
     *note 'TINYBLOB': blob.

     A *note 'BLOB': blob. column with a maximum length of 255 (2^8 −
     1) bytes.  Each *note 'TINYBLOB': blob. value is stored using a
     1-byte length prefix that indicates the number of bytes in the
     value.

   * 
     *note 'TINYTEXT [CHARACTER SET CHARSET_NAME] [COLLATE
     COLLATION_NAME]': blob.

     A *note 'TEXT': blob. column with a maximum length of 255 (2^8 −
     1) characters.  The effective maximum length is less if the value
     contains multibyte characters.  Each *note 'TINYTEXT': blob. value
     is stored using a 1-byte length prefix that indicates the number of
     bytes in the value.

   * 
     *note 'BLOB[(M)]': blob.

     A *note 'BLOB': blob. column with a maximum length of 65,535 (2^16
     − 1) bytes.  Each *note 'BLOB': blob. value is stored using a
     2-byte length prefix that indicates the number of bytes in the
     value.

     An optional length M can be given for this type.  If this is done,
     MySQL creates the column as the smallest *note 'BLOB': blob. type
     large enough to hold values M bytes long.

   * 
     *note 'TEXT[(M)] [CHARACTER SET CHARSET_NAME] [COLLATE
     COLLATION_NAME]': blob.

     A *note 'TEXT': blob. column with a maximum length of 65,535 (2^16
     − 1) characters.  The effective maximum length is less if the
     value contains multibyte characters.  Each *note 'TEXT': blob.
     value is stored using a 2-byte length prefix that indicates the
     number of bytes in the value.

     An optional length M can be given for this type.  If this is done,
     MySQL creates the column as the smallest *note 'TEXT': blob. type
     large enough to hold values M characters long.

   * 
     *note 'MEDIUMBLOB': blob.

     A *note 'BLOB': blob. column with a maximum length of 16,777,215
     (2^24 − 1) bytes.  Each *note 'MEDIUMBLOB': blob. value is stored
     using a 3-byte length prefix that indicates the number of bytes in
     the value.

   * 
     *note 'MEDIUMTEXT [CHARACTER SET CHARSET_NAME] [COLLATE
     COLLATION_NAME]': blob.

     A *note 'TEXT': blob. column with a maximum length of 16,777,215
     (2^24 − 1) characters.  The effective maximum length is less if
     the value contains multibyte characters.  Each *note 'MEDIUMTEXT':
     blob. value is stored using a 3-byte length prefix that indicates
     the number of bytes in the value.

   * 
     *note 'LONGBLOB': blob.

     A *note 'BLOB': blob. column with a maximum length of 4,294,967,295
     or 4GB (2^32 − 1) bytes.  The effective maximum length of *note
     'LONGBLOB': blob. columns depends on the configured maximum packet
     size in the client/server protocol and available memory.  Each
     *note 'LONGBLOB': blob. value is stored using a 4-byte length
     prefix that indicates the number of bytes in the value.

   * 
     *note 'LONGTEXT [CHARACTER SET CHARSET_NAME] [COLLATE
     COLLATION_NAME]': blob.

     A *note 'TEXT': blob. column with a maximum length of 4,294,967,295
     or 4GB (2^32 − 1) characters.  The effective maximum length is
     less if the value contains multibyte characters.  The effective
     maximum length of *note 'LONGTEXT': blob. columns also depends on
     the configured maximum packet size in the client/server protocol
     and available memory.  Each *note 'LONGTEXT': blob. value is stored
     using a 4-byte length prefix that indicates the number of bytes in
     the value.

   * 
     *note 'ENUM('VALUE1','VALUE2',...) [CHARACTER SET CHARSET_NAME]
     [COLLATE COLLATION_NAME]': enum.

     An enumeration.  A string object that can have only one value,
     chosen from the list of values ''VALUE1'', ''VALUE2'', '...',
     'NULL' or the special '''' error value.  *note 'ENUM': enum. values
     are represented internally as integers.

     An *note 'ENUM': enum. column can have a maximum of 65,535 distinct
     elements.  (The practical limit is less than 3000.)  A table can
     have no more than 255 unique element list definitions among its
     *note 'ENUM': enum. and *note 'SET': set. columns considered as a
     group.  For more information on these limits, see *note
     limits-frm-file::.

   * 
     *note 'SET('VALUE1','VALUE2',...) [CHARACTER SET CHARSET_NAME]
     [COLLATE COLLATION_NAME]': set.

     A set.  A string object that can have zero or more values, each of
     which must be chosen from the list of values ''VALUE1'',
     ''VALUE2'', '...' *note 'SET': set. values are represented
     internally as integers.

     A *note 'SET': set. column can have a maximum of 64 distinct
     members.  A table can have no more than 255 unique element list
     definitions among its *note 'ENUM': enum. and *note 'SET': set.
     columns considered as a group.  For more information on this limit,
     see *note limits-frm-file::.


File: manual.info.tmp,  Node: char,  Next: binary-varbinary,  Prev: string-type-syntax,  Up: string-types

11.3.2 The CHAR and VARCHAR Types
---------------------------------

The 'CHAR' and 'VARCHAR' types are similar, but differ in the way they
are stored and retrieved.  They also differ in maximum length and in
whether trailing spaces are retained.

The 'CHAR' and 'VARCHAR' types are declared with a length that indicates
the maximum number of characters you want to store.  For example,
'CHAR(30)' can hold up to 30 characters.

The length of a 'CHAR' column is fixed to the length that you declare
when you create the table.  The length can be any value from 0 to 255.
When 'CHAR' values are stored, they are right-padded with spaces to the
specified length.  When 'CHAR' values are retrieved, trailing spaces are
removed unless the 'PAD_CHAR_TO_FULL_LENGTH' SQL mode is enabled.

Values in 'VARCHAR' columns are variable-length strings.  The length can
be specified as a value from 0 to 65,535.  The effective maximum length
of a 'VARCHAR' is subject to the maximum row size (65,535 bytes, which
is shared among all columns) and the character set used.  See *note
column-count-limit::.

In contrast to 'CHAR', 'VARCHAR' values are stored as a 1-byte or 2-byte
length prefix plus data.  The length prefix indicates the number of
bytes in the value.  A column uses one length byte if values require no
more than 255 bytes, two length bytes if values may require more than
255 bytes.

If strict SQL mode is not enabled and you assign a value to a 'CHAR' or
'VARCHAR' column that exceeds the column's maximum length, the value is
truncated to fit and a warning is generated.  For truncation of nonspace
characters, you can cause an error to occur (rather than a warning) and
suppress insertion of the value by using strict SQL mode.  See *note
sql-mode::.

For 'VARCHAR' columns, trailing spaces in excess of the column length
are truncated prior to insertion and a warning is generated, regardless
of the SQL mode in use.  For 'CHAR' columns, truncation of excess
trailing spaces from inserted values is performed silently regardless of
the SQL mode.

'VARCHAR' values are not padded when they are stored.  Trailing spaces
are retained when values are stored and retrieved, in conformance with
standard SQL.

The following table illustrates the differences between 'CHAR' and
'VARCHAR' by showing the result of storing various string values into
'CHAR(4)' and 'VARCHAR(4)' columns (assuming that the column uses a
single-byte character set such as 'latin1').

Value       'CHAR(4)'   Storage        'VARCHAR(4)'Storage
                        Required                   Required
                                                   
''''        ''    ''    4 bytes        ''''        1 byte
                                                   
''ab''      ''ab  ''    4 bytes        ''ab''      3 bytes
                                                   
''abcd''    ''abcd''    4 bytes        ''abcd''    5 bytes
                                                   
''abcdefgh''''abcd''    4 bytes        ''abcd''    5 bytes
                                       

The values shown as stored in the last row of the table apply _only when
not using strict mode_; if MySQL is running in strict mode, values that
exceed the column length are _not stored_, and an error results.

'InnoDB' encodes fixed-length fields greater than or equal to 768 bytes
in length as variable-length fields, which can be stored off-page.  For
example, a 'CHAR(255)' column can exceed 768 bytes if the maximum byte
length of the character set is greater than 3, as it is with 'utf8mb4'.

If a given value is stored into the 'CHAR(4)' and 'VARCHAR(4)' columns,
the values retrieved from the columns are not always the same because
trailing spaces are removed from 'CHAR' columns upon retrieval.  The
following example illustrates this difference:

     mysql> CREATE TABLE vc (v VARCHAR(4), c CHAR(4));
     Query OK, 0 rows affected (0.01 sec)

     mysql> INSERT INTO vc VALUES ('ab  ', 'ab  ');
     Query OK, 1 row affected (0.00 sec)

     mysql> SELECT CONCAT('(', v, ')'), CONCAT('(', c, ')') FROM vc;
     +---------------------+---------------------+
     | CONCAT('(', v, ')') | CONCAT('(', c, ')') |
     +---------------------+---------------------+
     | (ab  )              | (ab)                |
     +---------------------+---------------------+
     1 row in set (0.06 sec)

Values in 'CHAR' and 'VARCHAR' columns are sorted and compared according
to the character set collation assigned to the column.

All MySQL collations are of type PAD SPACE. This means that all 'CHAR',
'VARCHAR', and 'TEXT' values are compared without regard to any trailing
spaces.  'Comparison' in this context does not include the 'LIKE'
pattern-matching operator, for which trailing spaces are significant.
For example:

     mysql> CREATE TABLE names (myname CHAR(10));
     Query OK, 0 rows affected (0.03 sec)

     mysql> INSERT INTO names VALUES ('Jones');
     Query OK, 1 row affected (0.00 sec)

     mysql> SELECT myname = 'Jones', myname = 'Jones  ' FROM names;
     +------------------+--------------------+
     | myname = 'Jones' | myname = 'Jones  ' |
     +------------------+--------------------+
     |                1 |                  1 |
     +------------------+--------------------+
     1 row in set (0.00 sec)

     mysql> SELECT myname LIKE 'Jones', myname LIKE 'Jones  ' FROM names;
     +---------------------+-----------------------+
     | myname LIKE 'Jones' | myname LIKE 'Jones  ' |
     +---------------------+-----------------------+
     |                   1 |                     0 |
     +---------------------+-----------------------+
     1 row in set (0.00 sec)

This is true for all MySQL versions, and is not affected by the server
SQL mode.

*Note*:

For more information about MySQL character sets and collations, see
*note charset::.  For additional information about storage requirements,
see *note storage-requirements::.

For those cases where trailing pad characters are stripped or
comparisons ignore them, if a column has an index that requires unique
values, inserting into the column values that differ only in number of
trailing pad characters will result in a duplicate-key error.  For
example, if a table contains ''a'', an attempt to store ''a '' causes a
duplicate-key error.


File: manual.info.tmp,  Node: binary-varbinary,  Next: blob,  Prev: char,  Up: string-types

11.3.3 The BINARY and VARBINARY Types
-------------------------------------

The 'BINARY' and 'VARBINARY' types are similar to *note 'CHAR': char.
and *note 'VARCHAR': char, except that they store binary strings rather
than nonbinary strings.  That is, they store byte strings rather than
character strings.  This means they have the 'binary' character set and
collation, and comparison and sorting are based on the numeric values of
the bytes in the values.

The permissible maximum length is the same for 'BINARY' and 'VARBINARY'
as it is for *note 'CHAR': char. and *note 'VARCHAR': char, except that
the length for 'BINARY' and 'VARBINARY' is measured in bytes rather than
characters.

The 'BINARY' and 'VARBINARY' data types are distinct from the 'CHAR
BINARY' and 'VARCHAR BINARY' data types.  For the latter types, the
'BINARY' attribute does not cause the column to be treated as a binary
string column.  Instead, it causes the binary ('_bin') collation for the
column character set (or the table default character set if no column
character set is specified) to be used, and the column itself stores
nonbinary character strings rather than binary byte strings.  For
example, if the default character set is 'latin1', 'CHAR(5) BINARY' is
treated as 'CHAR(5) CHARACTER SET latin1 COLLATE latin1_bin'.  This
differs from 'BINARY(5)', which stores 5-byte binary strings that have
the 'binary' character set and collation.  For information about the
differences between the 'binary' collation of the 'binary' character set
and the '_bin' collations of nonbinary character sets, see *note
charset-binary-collations::.

If strict SQL mode is not enabled and you assign a value to a 'BINARY'
or 'VARBINARY' column that exceeds the column's maximum length, the
value is truncated to fit and a warning is generated.  For cases of
truncation, to cause an error to occur (rather than a warning) and
suppress insertion of the value, use strict SQL mode.  See *note
sql-mode::.

When 'BINARY' values are stored, they are right-padded with the pad
value to the specified length.  The pad value is '0x00' (the zero byte).
Values are right-padded with '0x00' for inserts, and no trailing bytes
are removed for retrievals.  All bytes are significant in comparisons,
including 'ORDER BY' and 'DISTINCT' operations.  '0x00' and space differ
in comparisons, with '0x00' sorting before space.

Example: For a 'BINARY(3)' column, ''a '' becomes ''a \0'' when
inserted.  ''a\0'' becomes ''a\0\0'' when inserted.  Both inserted
values remain unchanged for retrievals.

For 'VARBINARY', there is no padding for inserts and no bytes are
stripped for retrievals.  All bytes are significant in comparisons,
including 'ORDER BY' and 'DISTINCT' operations.  '0x00' and space differ
in comparisons, with '0x00' sorting before space.

For those cases where trailing pad bytes are stripped or comparisons
ignore them, if a column has an index that requires unique values,
inserting values into the column that differ only in number of trailing
pad bytes results in a duplicate-key error.  For example, if a table
contains ''a'', an attempt to store ''a\0'' causes a duplicate-key
error.

You should consider the preceding padding and stripping characteristics
carefully if you plan to use the 'BINARY' data type for storing binary
data and you require that the value retrieved be exactly the same as the
value stored.  The following example illustrates how '0x00'-padding of
'BINARY' values affects column value comparisons:

     mysql> CREATE TABLE t (c BINARY(3));
     Query OK, 0 rows affected (0.01 sec)

     mysql> INSERT INTO t SET c = 'a';
     Query OK, 1 row affected (0.01 sec)

     mysql> SELECT HEX(c), c = 'a', c = 'a\0\0' from t;
     +--------+---------+-------------+
     | HEX(c) | c = 'a' | c = 'a\0\0' |
     +--------+---------+-------------+
     | 610000 |       0 |           1 |
     +--------+---------+-------------+
     1 row in set (0.09 sec)

If the value retrieved must be the same as the value specified for
storage with no padding, it might be preferable to use 'VARBINARY' or
one of the *note 'BLOB': blob. data types instead.


File: manual.info.tmp,  Node: blob,  Next: enum,  Prev: binary-varbinary,  Up: string-types

11.3.4 The BLOB and TEXT Types
------------------------------

A 'BLOB' is a binary large object that can hold a variable amount of
data.  The four 'BLOB' types are 'TINYBLOB', 'BLOB', 'MEDIUMBLOB', and
'LONGBLOB'.  These differ only in the maximum length of the values they
can hold.  The four 'TEXT' types are 'TINYTEXT', 'TEXT', 'MEDIUMTEXT',
and 'LONGTEXT'.  These correspond to the four 'BLOB' types and have the
same maximum lengths and storage requirements.  See *note
storage-requirements::.

'BLOB' values are treated as binary strings (byte strings).  They have
the 'binary' character set and collation, and comparison and sorting are
based on the numeric values of the bytes in column values.  'TEXT'
values are treated as nonbinary strings (character strings).  They have
a character set other than 'binary', and values are sorted and compared
based on the collation of the character set.

If strict SQL mode is not enabled and you assign a value to a 'BLOB' or
'TEXT' column that exceeds the column's maximum length, the value is
truncated to fit and a warning is generated.  For truncation of nonspace
characters, you can cause an error to occur (rather than a warning) and
suppress insertion of the value by using strict SQL mode.  See *note
sql-mode::.

Truncation of excess trailing spaces from values to be inserted into
*note 'TEXT': blob. columns always generates a warning, regardless of
the SQL mode.

For 'TEXT' and 'BLOB' columns, there is no padding on insert and no
bytes are stripped on select.

If a 'TEXT' column is indexed, index entry comparisons are space-padded
at the end.  This means that, if the index requires unique values,
duplicate-key errors will occur for values that differ only in the
number of trailing spaces.  For example, if a table contains ''a'', an
attempt to store ''a '' causes a duplicate-key error.  This is not true
for 'BLOB' columns.

In most respects, you can regard a 'BLOB' column as a *note 'VARBINARY':
binary-varbinary. column that can be as large as you like.  Similarly,
you can regard a 'TEXT' column as a *note 'VARCHAR': char. column.
'BLOB' and 'TEXT' differ from *note 'VARBINARY': binary-varbinary. and
*note 'VARCHAR': char. in the following ways:

   * For indexes on 'BLOB' and 'TEXT' columns, you must specify an index
     prefix length.  For *note 'CHAR': char. and *note 'VARCHAR': char,
     a prefix length is optional.  See *note column-indexes::.

   * 
     'BLOB' and 'TEXT' columns cannot have 'DEFAULT' values.

If you use the 'BINARY' attribute with a 'TEXT' data type, the column is
assigned the binary ('_bin') collation of the column character set.

'LONG' and 'LONG VARCHAR' map to the 'MEDIUMTEXT' data type.  This is a
compatibility feature.

MySQL Connector/ODBC defines 'BLOB' values as 'LONGVARBINARY' and 'TEXT'
values as 'LONGVARCHAR'.

Because 'BLOB' and 'TEXT' values can be extremely long, you might
encounter some constraints in using them:

   * Only the first 'max_sort_length' bytes of the column are used when
     sorting.  The default value of 'max_sort_length' is 1024.  You can
     make more bytes significant in sorting or grouping by increasing
     the value of 'max_sort_length' at server startup or runtime.  Any
     client can change the value of its session 'max_sort_length'
     variable:

          mysql> SET max_sort_length = 2000;
          mysql> SELECT id, comment FROM t
              -> ORDER BY comment;

   * Instances of 'BLOB' or 'TEXT' columns in the result of a query that
     is processed using a temporary table causes the server to use a
     table on disk rather than in memory because the 'MEMORY' storage
     engine does not support those data types (see *note
     internal-temporary-tables::).  Use of disk incurs a performance
     penalty, so include 'BLOB' or 'TEXT' columns in the query result
     only if they are really needed.  For example, avoid using *note
     'SELECT *': select, which selects all columns.

   * The maximum size of a 'BLOB' or 'TEXT' object is determined by its
     type, but the largest value you actually can transmit between the
     client and server is determined by the amount of available memory
     and the size of the communications buffers.  You can change the
     message buffer size by changing the value of the
     'max_allowed_packet' variable, but you must do so for both the
     server and your client program.  For example, both *note 'mysql':
     mysql. and *note 'mysqldump': mysqldump. enable you to change the
     client-side 'max_allowed_packet' value.  See *note
     server-configuration::, *note mysql::, and *note mysqldump::.  You
     may also want to compare the packet sizes and the size of the data
     objects you are storing with the storage requirements, see *note
     storage-requirements::

Each 'BLOB' or 'TEXT' value is represented internally by a separately
allocated object.  This is in contrast to all other data types, for
which storage is allocated once per column when the table is opened.

In some cases, it may be desirable to store binary data such as media
files in 'BLOB' or 'TEXT' columns.  You may find MySQL's string handling
functions useful for working with such data.  See *note
string-functions::.  For security and other reasons, it is usually
preferable to do so using application code rather than giving
application users the 'FILE' privilege.  You can discuss specifics for
various languages and platforms in the MySQL Forums
(<http://forums.mysql.com/>).


File: manual.info.tmp,  Node: enum,  Next: set,  Prev: blob,  Up: string-types

11.3.5 The ENUM Type
--------------------

An 'ENUM' is a string object with a value chosen from a list of
permitted values that are enumerated explicitly in the column
specification at table creation time.  It has these advantages:

   * Compact data storage in situations where a column has a limited set
     of possible values.  The strings you specify as input values are
     automatically encoded as numbers.  See *note storage-requirements::
     for the storage requirements for 'ENUM' types.

   * Readable queries and output.  The numbers are translated back to
     the corresponding strings in query results.

and these potential issues to consider:

   * If you make enumeration values that look like numbers, it is easy
     to mix up the literal values with their internal index numbers, as
     explained in *note enum-limits::.

   * Using 'ENUM' columns in 'ORDER BY' clauses requires extra care, as
     explained in *note enum-sorting::.

   * *note enum-using::

   * *note enum-indexes::

   * *note enum-literals::

   * *note enum-nulls::

   * *note enum-sorting::

   * *note enum-limits::

*Creating and Using ENUM Columns*

An enumeration value must be a quoted string literal.  For example, you
can create a table with an 'ENUM' column like this:

     CREATE TABLE shirts (
         name VARCHAR(40),
         size ENUM('x-small', 'small', 'medium', 'large', 'x-large')
     );
     INSERT INTO shirts (name, size) VALUES ('dress shirt','large'), ('t-shirt','medium'),
       ('polo shirt','small');
     SELECT name, size FROM shirts WHERE size = 'medium';
     +---------+--------+
     | name    | size   |
     +---------+--------+
     | t-shirt | medium |
     +---------+--------+
     UPDATE shirts SET size = 'small' WHERE size = 'large';
     COMMIT;

Inserting 1 million rows into this table with a value of ''medium''
would require 1 million bytes of storage, as opposed to 6 million bytes
if you stored the actual string ''medium'' in a 'VARCHAR' column.

*Index Values for Enumeration Literals*

Each enumeration value has an index:

   * The elements listed in the column specification are assigned index
     numbers, beginning with 1.

   * The index value of the empty string error value is 0.  This means
     that you can use the following *note 'SELECT': select. statement to
     find rows into which invalid 'ENUM' values were assigned:

          mysql> SELECT * FROM TBL_NAME WHERE ENUM_COL=0;

   * The index of the 'NULL' value is 'NULL'.

   * The term 'index' here refers to a position within the list of
     enumeration values.  It has nothing to do with table indexes.

For example, a column specified as 'ENUM('Mercury', 'Venus', 'Earth')'
can have any of the values shown here.  The index of each value is also
shown.

Value       Index
            
'NULL'      'NULL'
            
''''        0
            
''Mercury'' 1
            
''Venus''   2
            
''Earth''   3

An *note 'ENUM': enum. column can have a maximum of 65,535 distinct
elements.  (The practical limit is less than 3000.)  A table can have no
more than 255 unique element list definitions among its *note 'ENUM':
enum. and *note 'SET': set. columns considered as a group.  For more
information on these limits, see *note limits-frm-file::.

If you retrieve an 'ENUM' value in a numeric context, the column value's
index is returned.  For example, you can retrieve numeric values from an
'ENUM' column like this:

     mysql> SELECT ENUM_COL+0 FROM TBL_NAME;

Functions such as 'SUM()' or 'AVG()' that expect a numeric argument cast
the argument to a number if necessary.  For 'ENUM' values, the index
number is used in the calculation.

*Handling of Enumeration Literals*

Trailing spaces are automatically deleted from 'ENUM' member values in
the table definition when a table is created.

When retrieved, values stored into an 'ENUM' column are displayed using
the lettercase that was used in the column definition.  Note that 'ENUM'
columns can be assigned a character set and collation.  For binary or
case-sensitive collations, lettercase is taken into account when
assigning values to the column.

If you store a number into an 'ENUM' column, the number is treated as
the index into the possible values, and the value stored is the
enumeration member with that index.  (However, this does _not_ work with
*note 'LOAD DATA': load-data, which treats all input as strings.)  If
the numeric value is quoted, it is still interpreted as an index if
there is no matching string in the list of enumeration values.  For
these reasons, it is not advisable to define an 'ENUM' column with
enumeration values that look like numbers, because this can easily
become confusing.  For example, the following column has enumeration
members with string values of ''0'', ''1'', and ''2'', but numeric index
values of '1', '2', and '3':

     numbers ENUM('0','1','2')

If you store '2', it is interpreted as an index value, and becomes ''1''
(the value with index 2).  If you store ''2'', it matches an enumeration
value, so it is stored as ''2''.  If you store ''3'', it does not match
any enumeration value, so it is treated as an index and becomes ''2''
(the value with index 3).

     mysql> INSERT INTO t (numbers) VALUES(2),('2'),('3');
     mysql> SELECT * FROM t;
     +---------+
     | numbers |
     +---------+
     | 1       |
     | 2       |
     | 2       |
     +---------+

To determine all possible values for an 'ENUM' column, use *note 'SHOW
COLUMNS FROM TBL_NAME LIKE 'ENUM_COL'': show-columns. and parse the
'ENUM' definition in the 'Type' column of the output.

In the C API, 'ENUM' values are returned as strings.  For information
about using result set metadata to distinguish them from other strings,
see *note c-api-data-structures::.

*Empty or NULL Enumeration Values*

An enumeration value can also be the empty string ('''') or 'NULL' under
certain circumstances:

   * If you insert an invalid value into an 'ENUM' (that is, a string
     not present in the list of permitted values), the empty string is
     inserted instead as a special error value.  This string can be
     distinguished from a 'normal' empty string by the fact that this
     string has the numeric value 0.  See *note enum-indexes:: for
     details about the numeric indexes for the enumeration values.

     If strict SQL mode is enabled, attempts to insert invalid 'ENUM'
     values result in an error.

   * If an 'ENUM' column is declared to permit 'NULL', the 'NULL' value
     is a valid value for the column, and the default value is 'NULL'.
     If an 'ENUM' column is declared 'NOT NULL', its default value is
     the first element of the list of permitted values.

*Enumeration Sorting*

'ENUM' values are sorted based on their index numbers, which depend on
the order in which the enumeration members were listed in the column
specification.  For example, ''b'' sorts before ''a'' for 'ENUM('b',
'a')'.  The empty string sorts before nonempty strings, and 'NULL'
values sort before all other enumeration values.

To prevent unexpected results when using the 'ORDER BY' clause on an
'ENUM' column, use one of these techniques:

   * Specify the 'ENUM' list in alphabetic order.

   * Make sure that the column is sorted lexically rather than by index
     number by coding 'ORDER BY CAST(COL AS CHAR)' or 'ORDER BY
     CONCAT(COL)'.

*Enumeration Limitations*

An enumeration value cannot be an expression, even one that evaluates to
a string value.

For example, this *note 'CREATE TABLE': create-table. statement does
_not_ work because the 'CONCAT' function cannot be used to construct an
enumeration value:

     CREATE TABLE sizes (
         size ENUM('small', CONCAT('med','ium'), 'large')
     );

You also cannot employ a user variable as an enumeration value.  This
pair of statements do _not_ work:

     SET @mysize = 'medium';

     CREATE TABLE sizes (
         size ENUM('small', @mysize, 'large')
     );

We strongly recommend that you do _not_ use numbers as enumeration
values, because it does not save on storage over the appropriate *note
'TINYINT': integer-types. or *note 'SMALLINT': integer-types. type, and
it is easy to mix up the strings and the underlying number values (which
might not be the same) if you quote the 'ENUM' values incorrectly.  If
you do use a number as an enumeration value, always enclose it in
quotation marks.  If the quotation marks are omitted, the number is
regarded as an index.  See *note enum-literals:: to see how even a
quoted number could be mistakenly used as a numeric index value.

Duplicate values in the definition cause a warning, or an error if
strict SQL mode is enabled.


File: manual.info.tmp,  Node: set,  Prev: enum,  Up: string-types

11.3.6 The SET Type
-------------------

A 'SET' is a string object that can have zero or more values, each of
which must be chosen from a list of permitted values specified when the
table is created.  'SET' column values that consist of multiple set
members are specified with members separated by commas (',').  A
consequence of this is that 'SET' member values should not themselves
contain commas.

For example, a column specified as 'SET('one', 'two') NOT NULL' can have
any of these values:

     ''
     'one'
     'two'
     'one,two'

A *note 'SET': set. column can have a maximum of 64 distinct members.  A
table can have no more than 255 unique element list definitions among
its *note 'ENUM': enum. and *note 'SET': set. columns considered as a
group.  For more information on this limit, see *note limits-frm-file::.

Duplicate values in the definition cause a warning, or an error if
strict SQL mode is enabled.

Trailing spaces are automatically deleted from 'SET' member values in
the table definition when a table is created.

When retrieved, values stored in a 'SET' column are displayed using the
lettercase that was used in the column definition.  Note that 'SET'
columns can be assigned a character set and collation.  For binary or
case-sensitive collations, lettercase is taken into account when
assigning values to the column.

MySQL stores 'SET' values numerically, with the low-order bit of the
stored value corresponding to the first set member.  If you retrieve a
'SET' value in a numeric context, the value retrieved has bits set
corresponding to the set members that make up the column value.  For
example, you can retrieve numeric values from a 'SET' column like this:

     mysql> SELECT SET_COL+0 FROM TBL_NAME;

If a number is stored into a 'SET' column, the bits that are set in the
binary representation of the number determine the set members in the
column value.  For a column specified as 'SET('a','b','c','d')', the
members have the following decimal and binary values.

'SET'       Decimal        Binary Value
Member      Value          
            
''a''       '1'            '0001'
                           
''b''       '2'            '0010'
                           
''c''       '4'            '0100'
                           
''d''       '8'            '1000'
            

If you assign a value of '9' to this column, that is '1001' in binary,
so the first and fourth 'SET' value members ''a'' and ''d'' are selected
and the resulting value is ''a,d''.

For a value containing more than one 'SET' element, it does not matter
what order the elements are listed in when you insert the value.  It
also does not matter how many times a given element is listed in the
value.  When the value is retrieved later, each element in the value
appears once, with elements listed according to the order in which they
were specified at table creation time.  Suppose that a column is
specified as 'SET('a','b','c','d')':

     mysql> CREATE TABLE myset (col SET('a', 'b', 'c', 'd'));

If you insert the values ''a,d'', ''d,a'', ''a,d,d'', ''a,d,a'', and
''d,a,d'':

     mysql> INSERT INTO myset (col) VALUES
     -> ('a,d'), ('d,a'), ('a,d,a'), ('a,d,d'), ('d,a,d');
     Query OK, 5 rows affected (0.01 sec)
     Records: 5  Duplicates: 0  Warnings: 0

Then all these values appear as ''a,d'' when retrieved:

     mysql> SELECT col FROM myset;
     +------+
     | col  |
     +------+
     | a,d  |
     | a,d  |
     | a,d  |
     | a,d  |
     | a,d  |
     +------+
     5 rows in set (0.04 sec)

If you set a 'SET' column to an unsupported value, the value is ignored
and a warning is issued:

     mysql> INSERT INTO myset (col) VALUES ('a,d,d,s');
     Query OK, 1 row affected, 1 warning (0.03 sec)

     mysql> SHOW WARNINGS;
     +---------+------+------------------------------------------+
     | Level   | Code | Message                                  |
     +---------+------+------------------------------------------+
     | Warning | 1265 | Data truncated for column 'col' at row 1 |
     +---------+------+------------------------------------------+
     1 row in set (0.04 sec)

     mysql> SELECT col FROM myset;
     +------+
     | col  |
     +------+
     | a,d  |
     | a,d  |
     | a,d  |
     | a,d  |
     | a,d  |
     | a,d  |
     +------+
     6 rows in set (0.01 sec)

If strict SQL mode is enabled, attempts to insert invalid 'SET' values
result in an error.

'SET' values are sorted numerically.  'NULL' values sort before
non-'NULL' 'SET' values.

Functions such as 'SUM()' or 'AVG()' that expect a numeric argument cast
the argument to a number if necessary.  For 'SET' values, the cast
operation causes the numeric value to be used.

Normally, you search for 'SET' values using the 'FIND_IN_SET()' function
or the 'LIKE' operator:

     mysql> SELECT * FROM TBL_NAME WHERE FIND_IN_SET('VALUE',SET_COL)>0;
     mysql> SELECT * FROM TBL_NAME WHERE SET_COL LIKE '%VALUE%';

The first statement finds rows where SET_COL contains the VALUE set
member.  The second is similar, but not the same: It finds rows where
SET_COL contains VALUE anywhere, even as a substring of another set
member.

The following statements also are permitted:

     mysql> SELECT * FROM TBL_NAME WHERE SET_COL & 1;
     mysql> SELECT * FROM TBL_NAME WHERE SET_COL = 'VAL1,VAL2';

The first of these statements looks for values containing the first set
member.  The second looks for an exact match.  Be careful with
comparisons of the second type.  Comparing set values to ''VAL1,VAL2''
returns different results than comparing values to ''VAL2,VAL1''.  You
should specify the values in the same order they are listed in the
column definition.

To determine all possible values for a 'SET' column, use 'SHOW COLUMNS
FROM TBL_NAME LIKE SET_COL' and parse the 'SET' definition in the 'Type'
column of the output.

In the C API, 'SET' values are returned as strings.  For information
about using result set metadata to distinguish them from other strings,
see *note c-api-data-structures::.


File: manual.info.tmp,  Node: spatial-types,  Next: data-type-defaults,  Prev: string-types,  Up: data-types

11.4 Spatial Data Types
=======================

* Menu:

* spatial-type-overview::        Spatial Data Types
* opengis-geometry-model::       The OpenGIS Geometry Model
* gis-data-formats::             Supported Spatial Data Formats
* creating-spatial-columns::     Creating Spatial Columns
* populating-spatial-columns::   Populating Spatial Columns
* fetching-spatial-data::        Fetching Spatial Data
* optimizing-spatial-analysis::  Optimizing Spatial Analysis
* creating-spatial-indexes::     Creating Spatial Indexes
* using-spatial-indexes::        Using Spatial Indexes

The Open Geospatial Consortium (http://www.opengeospatial.org) (OGC) is
an international consortium of more than 250 companies, agencies, and
universities participating in the development of publicly available
conceptual solutions that can be useful with all kinds of applications
that manage spatial data.

The Open Geospatial Consortium publishes the 'OpenGIS(R) Implementation
Standard for Geographic information - Simple Feature Access - Part 2:
SQL Option', a document that proposes several conceptual ways for
extending an SQL RDBMS to support spatial data.  This specification is
available from the OGC website at
<http://www.opengeospatial.org/standards/sfs>.

Following the OGC specification, MySQL implements spatial extensions as
a subset of the *SQL with Geometry Types* environment.  This term refers
to an SQL environment that has been extended with a set of geometry
types.  A geometry-valued SQL column is implemented as a column that has
a geometry type.  The specification describes a set of SQL geometry
types, as well as functions on those types to create and analyze
geometry values.

MySQL spatial extensions enable the generation, storage, and analysis of
geographic features:

   * Data types for representing spatial values

   * Functions for manipulating spatial values

   * Spatial indexing for improved access times to spatial columns

The spatial data types and functions are available for *note 'MyISAM':
myisam-storage-engine, *note 'InnoDB': innodb-storage-engine, *note
'NDB': mysql-cluster, and *note 'ARCHIVE': archive-storage-engine.
tables.  For indexing spatial columns, 'MyISAM' supports both 'SPATIAL'
and non-'SPATIAL' indexes.  The other storage engines support
non-'SPATIAL' indexes, as described in *note create-index::.

A *geographic feature* is anything in the world that has a location.  A
feature can be:

   * An entity.  For example, a mountain, a pond, a city.

   * A space.  For example, town district, the tropics.

   * A definable location.  For example, a crossroad, as a particular
     place where two streets intersect.

Some documents use the term *geospatial feature* to refer to geographic
features.

*Geometry* is another word that denotes a geographic feature.
Originally the word *geometry* meant measurement of the earth.  Another
meaning comes from cartography, referring to the geometric features that
cartographers use to map the world.

The discussion here considers these terms synonymous: *geographic
feature*, *geospatial feature*, *feature*, or *geometry*.  The term most
commonly used is *geometry*, defined as _a point or an aggregate of
points representing anything in the world that has a location_.

The following material covers these topics:

   * The spatial data types implemented in MySQL model

   * The basis of the spatial extensions in the OpenGIS geometry model

   * Data formats for representing spatial data

   * How to use spatial data in MySQL

   * Use of indexing for spatial data

   * MySQL differences from the OpenGIS specification

For information about functions that operate on spatial data, see *note
spatial-analysis-functions::.

*MySQL GIS Conformance and Compatibility*

MySQL does not implement the following GIS features:

   * Additional Metadata Views

     OpenGIS specifications propose several additional metadata views.
     For example, a system view named 'GEOMETRY_COLUMNS' contains a
     description of geometry columns, one row for each geometry column
     in the database.

   * The OpenGIS function 'Length()' on 'LineString' and
     'MultiLineString' should be called in MySQL as 'GLength()'

     The problem is that there is an existing SQL function 'Length()'
     that calculates the length of string values, and sometimes it is
     not possible to distinguish whether the function is called in a
     textual or spatial context.

*Additional Resources*

The Open Geospatial Consortium publishes the 'OpenGIS(R) Implementation
Standard for Geographic information - Simple feature access - Part 2:
SQL option', a document that proposes several conceptual ways for
extending an SQL RDBMS to support spatial data.  The Open Geospatial
Consortium (OGC) maintains a website at
<http://www.opengeospatial.org/>.  The specification is available there
at <http://www.opengeospatial.org/standards/sfs>.  It contains
additional information relevant to the material here.

If you have questions or concerns about the use of the spatial
extensions to MySQL, you can discuss them in the GIS forum:
<https://forums.mysql.com/list.php?23>.


File: manual.info.tmp,  Node: spatial-type-overview,  Next: opengis-geometry-model,  Prev: spatial-types,  Up: spatial-types

11.4.1 Spatial Data Types
-------------------------

MySQL has spatial data types that correspond to OpenGIS classes.  The
basis for these types is described in *note opengis-geometry-model::.

Some spatial data types hold single geometry values:

   * 'GEOMETRY'

   * 'POINT'

   * 'LINESTRING'

   * 'POLYGON'

'GEOMETRY' can store geometry values of any type.  The other
single-value types ('POINT', 'LINESTRING', and 'POLYGON') restrict their
values to a particular geometry type.

The other spatial data types hold collections of values:

   * 'MULTIPOINT'

   * 'MULTILINESTRING'

   * 'MULTIPOLYGON'

   * 'GEOMETRYCOLLECTION'

'GEOMETRYCOLLECTION' can store a collection of objects of any type.  The
other collection types ('MULTIPOINT', 'MULTILINESTRING', and
'MULTIPOLYGON') restrict collection members to those having a particular
geometry type.

Example: To create a table named 'geom' that has a column named 'g' that
can store values of any geometry type, use this statement:

     CREATE TABLE geom (g GEOMETRY);

'SPATIAL' indexes can be created on 'NOT NULL' spatial columns, so if
you plan to index the column, declare it 'NOT NULL':

     CREATE TABLE geom (g GEOMETRY NOT NULL);

For other examples showing how to use spatial data types in MySQL, see
*note creating-spatial-columns::.


File: manual.info.tmp,  Node: opengis-geometry-model,  Next: gis-data-formats,  Prev: spatial-type-overview,  Up: spatial-types

11.4.2 The OpenGIS Geometry Model
---------------------------------

* Menu:

* gis-geometry-class-hierarchy::  The Geometry Class Hierarchy
* gis-class-geometry::           Geometry Class
* gis-class-point::              Point Class
* gis-class-curve::              Curve Class
* gis-class-linestring::         LineString Class
* gis-class-surface::            Surface Class
* gis-class-polygon::            Polygon Class
* gis-class-geometrycollection::  GeometryCollection Class
* gis-class-multipoint::         MultiPoint Class
* gis-class-multicurve::         MultiCurve Class
* gis-class-multilinestring::    MultiLineString Class
* gis-class-multisurface::       MultiSurface Class
* gis-class-multipolygon::       MultiPolygon Class

The set of geometry types proposed by OGC's *SQL with Geometry Types*
environment is based on the *OpenGIS Geometry Model*.  In this model,
each geometric object has the following general properties:

   * It is associated with a spatial reference system, which describes
     the coordinate space in which the object is defined.

   * It belongs to some geometry class.


File: manual.info.tmp,  Node: gis-geometry-class-hierarchy,  Next: gis-class-geometry,  Prev: opengis-geometry-model,  Up: opengis-geometry-model

11.4.2.1 The Geometry Class Hierarchy
.....................................

The geometry classes define a hierarchy as follows:

   * 'Geometry' (noninstantiable)

        * 'Point' (instantiable)

        * 'Curve' (noninstantiable)

             * 'LineString' (instantiable)

                  * 'Line'

                  * 'LinearRing'

        * 'Surface' (noninstantiable)

             * 'Polygon' (instantiable)

        * 'GeometryCollection' (instantiable)

             * 'MultiPoint' (instantiable)

             * 'MultiCurve' (noninstantiable)

                  * 'MultiLineString' (instantiable)

             * 'MultiSurface' (noninstantiable)

                  * 'MultiPolygon' (instantiable)

It is not possible to create objects in noninstantiable classes.  It is
possible to create objects in instantiable classes.  All classes have
properties, and instantiable classes may also have assertions (rules
that define valid class instances).

'Geometry' is the base class.  It is an abstract class.  The
instantiable subclasses of 'Geometry' are restricted to zero-, one-, and
two-dimensional geometric objects that exist in two-dimensional
coordinate space.  All instantiable geometry classes are defined so that
valid instances of a geometry class are topologically closed (that is,
all defined geometries include their boundary).

The base 'Geometry' class has subclasses for 'Point', 'Curve',
'Surface', and 'GeometryCollection':

   * 'Point' represents zero-dimensional objects.

   * 'Curve' represents one-dimensional objects, and has subclass
     'LineString', with sub-subclasses 'Line' and 'LinearRing'.

   * 'Surface' is designed for two-dimensional objects and has subclass
     'Polygon'.

   * 'GeometryCollection' has specialized zero-, one-, and
     two-dimensional collection classes named 'MultiPoint',
     'MultiLineString', and 'MultiPolygon' for modeling geometries
     corresponding to collections of 'Points', 'LineStrings', and
     'Polygons', respectively.  'MultiCurve' and 'MultiSurface' are
     introduced as abstract superclasses that generalize the collection
     interfaces to handle 'Curves' and 'Surfaces'.

'Geometry', 'Curve', 'Surface', 'MultiCurve', and 'MultiSurface' are
defined as noninstantiable classes.  They define a common set of methods
for their subclasses and are included for extensibility.

'Point', 'LineString', 'Polygon', 'GeometryCollection', 'MultiPoint',
'MultiLineString', and 'MultiPolygon' are instantiable classes.


File: manual.info.tmp,  Node: gis-class-geometry,  Next: gis-class-point,  Prev: gis-geometry-class-hierarchy,  Up: opengis-geometry-model

11.4.2.2 Geometry Class
.......................

'Geometry' is the root class of the hierarchy.  It is a noninstantiable
class but has a number of properties, described in the following list,
that are common to all geometry values created from any of the
'Geometry' subclasses.  Particular subclasses have their own specific
properties, described later.

*Geometry Properties*

A geometry value has the following properties:

   * Its *type*.  Each geometry belongs to one of the instantiable
     classes in the hierarchy.

   * Its *SRID*, or spatial reference identifier.  This value identifies
     the geometry's associated spatial reference system that describes
     the coordinate space in which the geometry object is defined.

     In MySQL, the SRID value is an integer associated with the geometry
     value.  The maximum usable SRID value is 2^32−1.  If a larger
     value is given, only the lower 32 bits are used.  All computations
     are done assuming SRID 0, regardless of the actual SRID value.
     SRID 0 represents an infinite flat Cartesian plane with no units
     assigned to its axes.

   * Its *coordinates* in its spatial reference system, represented as
     double-precision (8-byte) numbers.  All nonempty geometries include
     at least one pair of (X,Y) coordinates.  Empty geometries contain
     no coordinates.

     Coordinates are related to the SRID. For example, in different
     coordinate systems, the distance between two objects may differ
     even when objects have the same coordinates, because the distance
     on the *planar* coordinate system and the distance on the
     *geodetic* system (coordinates on the Earth's surface) are
     different things.

   * Its *interior*, *boundary*, and *exterior*.

     Every geometry occupies some position in space.  The exterior of a
     geometry is all space not occupied by the geometry.  The interior
     is the space occupied by the geometry.  The boundary is the
     interface between the geometry's interior and exterior.

   * Its *MBR* (minimum bounding rectangle), or envelope.  This is the
     bounding geometry, formed by the minimum and maximum (X,Y)
     coordinates:

          ((MINX MINY, MAXX MINY, MAXX MAXY, MINX MAXY, MINX MINY))

   * Whether the value is *simple* or *nonsimple*.  Geometry values of
     types ('LineString', 'MultiPoint', 'MultiLineString') are either
     simple or nonsimple.  Each type determines its own assertions for
     being simple or nonsimple.

   * Whether the value is *closed* or *not closed*.  Geometry values of
     types ('LineString', 'MultiString') are either closed or not
     closed.  Each type determines its own assertions for being closed
     or not closed.

   * Whether the value is *empty* or *nonempty* A geometry is empty if
     it does not have any points.  Exterior, interior, and boundary of
     an empty geometry are not defined (that is, they are represented by
     a 'NULL' value).  An empty geometry is defined to be always simple
     and has an area of 0.

   * Its *dimension*.  A geometry can have a dimension of −1, 0, 1, or
     2:

        * −1 for an empty geometry.

        * 0 for a geometry with no length and no area.

        * 1 for a geometry with nonzero length and zero area.

        * 2 for a geometry with nonzero area.

     'Point' objects have a dimension of zero.  'LineString' objects
     have a dimension of 1.  'Polygon' objects have a dimension of 2.
     The dimensions of 'MultiPoint', 'MultiLineString', and
     'MultiPolygon' objects are the same as the dimensions of the
     elements they consist of.


File: manual.info.tmp,  Node: gis-class-point,  Next: gis-class-curve,  Prev: gis-class-geometry,  Up: opengis-geometry-model

11.4.2.3 Point Class
....................

A 'Point' is a geometry that represents a single location in coordinate
space.

*'Point' Examples*

   * Imagine a large-scale map of the world with many cities.  A 'Point'
     object could represent each city.

   * On a city map, a 'Point' object could represent a bus stop.

*'Point' Properties*

   * X-coordinate value.

   * Y-coordinate value.

   * 'Point' is defined as a zero-dimensional geometry.

   * The boundary of a 'Point' is the empty set.


File: manual.info.tmp,  Node: gis-class-curve,  Next: gis-class-linestring,  Prev: gis-class-point,  Up: opengis-geometry-model

11.4.2.4 Curve Class
....................

A 'Curve' is a one-dimensional geometry, usually represented by a
sequence of points.  Particular subclasses of 'Curve' define the type of
interpolation between points.  'Curve' is a noninstantiable class.

*'Curve' Properties*

   * A 'Curve' has the coordinates of its points.

   * A 'Curve' is defined as a one-dimensional geometry.

   * A 'Curve' is simple if it does not pass through the same point
     twice, with the exception that a curve can still be simple if the
     start and end points are the same.

   * A 'Curve' is closed if its start point is equal to its endpoint.

   * The boundary of a closed 'Curve' is empty.

   * The boundary of a nonclosed 'Curve' consists of its two endpoints.

   * A 'Curve' that is simple and closed is a 'LinearRing'.


File: manual.info.tmp,  Node: gis-class-linestring,  Next: gis-class-surface,  Prev: gis-class-curve,  Up: opengis-geometry-model

11.4.2.5 LineString Class
.........................

A 'LineString' is a 'Curve' with linear interpolation between points.

*'LineString' Examples*

   * On a world map, 'LineString' objects could represent rivers.

   * In a city map, 'LineString' objects could represent streets.

*'LineString' Properties*

   * A 'LineString' has coordinates of segments, defined by each
     consecutive pair of points.

   * A 'LineString' is a 'Line' if it consists of exactly two points.

   * A 'LineString' is a 'LinearRing' if it is both closed and simple.


File: manual.info.tmp,  Node: gis-class-surface,  Next: gis-class-polygon,  Prev: gis-class-linestring,  Up: opengis-geometry-model

11.4.2.6 Surface Class
......................

A 'Surface' is a two-dimensional geometry.  It is a noninstantiable
class.  Its only instantiable subclass is 'Polygon'.

*'Surface' Properties*

   * A 'Surface' is defined as a two-dimensional geometry.

   * The OpenGIS specification defines a simple 'Surface' as a geometry
     that consists of a single 'patch' that is associated with a single
     exterior boundary and zero or more interior boundaries.

   * The boundary of a simple 'Surface' is the set of closed curves
     corresponding to its exterior and interior boundaries.


File: manual.info.tmp,  Node: gis-class-polygon,  Next: gis-class-geometrycollection,  Prev: gis-class-surface,  Up: opengis-geometry-model

11.4.2.7 Polygon Class
......................

A 'Polygon' is a planar 'Surface' representing a multisided geometry.
It is defined by a single exterior boundary and zero or more interior
boundaries, where each interior boundary defines a hole in the
'Polygon'.

*'Polygon' Examples*

   * On a region map, 'Polygon' objects could represent forests,
     districts, and so on.

*'Polygon' Assertions*

   * The boundary of a 'Polygon' consists of a set of 'LinearRing'
     objects (that is, 'LineString' objects that are both simple and
     closed) that make up its exterior and interior boundaries.

   * A 'Polygon' has no rings that cross.  The rings in the boundary of
     a 'Polygon' may intersect at a 'Point', but only as a tangent.

   * A 'Polygon' has no lines, spikes, or punctures.

   * A 'Polygon' has an interior that is a connected point set.

   * A 'Polygon' may have holes.  The exterior of a 'Polygon' with holes
     is not connected.  Each hole defines a connected component of the
     exterior.

The preceding assertions make a 'Polygon' a simple geometry.


File: manual.info.tmp,  Node: gis-class-geometrycollection,  Next: gis-class-multipoint,  Prev: gis-class-polygon,  Up: opengis-geometry-model

11.4.2.8 GeometryCollection Class
.................................

A 'GeometryCollection' is a geometry that is a collection of zero or
more geometries of any class.

All the elements in a geometry collection must be in the same spatial
reference system (that is, in the same coordinate system).  There are no
other constraints on the elements of a geometry collection, although the
subclasses of 'GeometryCollection' described in the following sections
may restrict membership.  Restrictions may be based on:

   * Element type (for example, a 'MultiPoint' may contain only 'Point'
     elements)

   * Dimension

   * Constraints on the degree of spatial overlap between elements


File: manual.info.tmp,  Node: gis-class-multipoint,  Next: gis-class-multicurve,  Prev: gis-class-geometrycollection,  Up: opengis-geometry-model

11.4.2.9 MultiPoint Class
.........................

A 'MultiPoint' is a geometry collection composed of 'Point' elements.
The points are not connected or ordered in any way.

*'MultiPoint' Examples*

   * On a world map, a 'MultiPoint' could represent a chain of small
     islands.

   * On a city map, a 'MultiPoint' could represent the outlets for a
     ticket office.

*'MultiPoint' Properties*

   * A 'MultiPoint' is a zero-dimensional geometry.

   * A 'MultiPoint' is simple if no two of its 'Point' values are equal
     (have identical coordinate values).

   * The boundary of a 'MultiPoint' is the empty set.


File: manual.info.tmp,  Node: gis-class-multicurve,  Next: gis-class-multilinestring,  Prev: gis-class-multipoint,  Up: opengis-geometry-model

11.4.2.10 MultiCurve Class
..........................

A 'MultiCurve' is a geometry collection composed of 'Curve' elements.
'MultiCurve' is a noninstantiable class.

*'MultiCurve' Properties*

   * A 'MultiCurve' is a one-dimensional geometry.

   * A 'MultiCurve' is simple if and only if all of its elements are
     simple; the only intersections between any two elements occur at
     points that are on the boundaries of both elements.

   * A 'MultiCurve' boundary is obtained by applying the 'mod 2 union
     rule' (also known as the 'odd-even rule'): A point is in the
     boundary of a 'MultiCurve' if it is in the boundaries of an odd
     number of 'Curve' elements.

   * A 'MultiCurve' is closed if all of its elements are closed.

   * The boundary of a closed 'MultiCurve' is always empty.


File: manual.info.tmp,  Node: gis-class-multilinestring,  Next: gis-class-multisurface,  Prev: gis-class-multicurve,  Up: opengis-geometry-model

11.4.2.11 MultiLineString Class
...............................

A 'MultiLineString' is a 'MultiCurve' geometry collection composed of
'LineString' elements.

*'MultiLineString' Examples*

   * On a region map, a 'MultiLineString' could represent a river system
     or a highway system.


File: manual.info.tmp,  Node: gis-class-multisurface,  Next: gis-class-multipolygon,  Prev: gis-class-multilinestring,  Up: opengis-geometry-model

11.4.2.12 MultiSurface Class
............................

A 'MultiSurface' is a geometry collection composed of surface elements.
'MultiSurface' is a noninstantiable class.  Its only instantiable
subclass is 'MultiPolygon'.

*'MultiSurface' Assertions*

   * Surfaces within a 'MultiSurface' have no interiors that intersect.

   * Surfaces within a 'MultiSurface' have boundaries that intersect at
     most at a finite number of points.


File: manual.info.tmp,  Node: gis-class-multipolygon,  Prev: gis-class-multisurface,  Up: opengis-geometry-model

11.4.2.13 MultiPolygon Class
............................

A 'MultiPolygon' is a 'MultiSurface' object composed of 'Polygon'
elements.

*'MultiPolygon' Examples*

   * On a region map, a 'MultiPolygon' could represent a system of
     lakes.

*'MultiPolygon' Assertions*

   * A 'MultiPolygon' has no two 'Polygon' elements with interiors that
     intersect.

   * A 'MultiPolygon' has no two 'Polygon' elements that cross (crossing
     is also forbidden by the previous assertion), or that touch at an
     infinite number of points.

   * A 'MultiPolygon' may not have cut lines, spikes, or punctures.  A
     'MultiPolygon' is a regular, closed point set.

   * A 'MultiPolygon' that has more than one 'Polygon' has an interior
     that is not connected.  The number of connected components of the
     interior of a 'MultiPolygon' is equal to the number of 'Polygon'
     values in the 'MultiPolygon'.

*'MultiPolygon' Properties*

   * A 'MultiPolygon' is a two-dimensional geometry.

   * A 'MultiPolygon' boundary is a set of closed curves ('LineString'
     values) corresponding to the boundaries of its 'Polygon' elements.

   * Each 'Curve' in the boundary of the 'MultiPolygon' is in the
     boundary of exactly one 'Polygon' element.

   * Every 'Curve' in the boundary of an 'Polygon' element is in the
     boundary of the 'MultiPolygon'.


File: manual.info.tmp,  Node: gis-data-formats,  Next: creating-spatial-columns,  Prev: opengis-geometry-model,  Up: spatial-types

11.4.3 Supported Spatial Data Formats
-------------------------------------

Two standard spatial data formats are used to represent geometry objects
in queries:

   * Well-Known Text (WKT) format

   * Well-Known Binary (WKB) format

Internally, MySQL stores geometry values in a format that is not
identical to either WKT or WKB format.  (Internal format is like WKB but
with an initial 4 bytes to indicate the SRID.)

There are functions available to convert between different data formats;
see *note gis-format-conversion-functions::.

The following sections describe the spatial data formats MySQL uses:

   * *note gis-wkt-format::

   * *note gis-wkb-format::

   * *note gis-internal-format::

*Well-Known Text (WKT) Format*

The Well-Known Text (WKT) representation of geometry values is designed
for exchanging geometry data in ASCII form.  The OpenGIS specification
provides a Backus-Naur grammar that specifies the formal production
rules for writing WKT values (see *note spatial-types::).

Examples of WKT representations of geometry objects:

   * A 'Point':

          POINT(15 20)

     The point coordinates are specified with no separating comma.  This
     differs from the syntax for the SQL 'Point()' function, which
     requires a comma between the coordinates.  Take care to use the
     syntax appropriate to the context of a given spatial operation.
     For example, the following statements both use 'X()' to extract the
     X-coordinate from a 'Point' object.  The first produces the object
     directly using the 'Point()' function.  The second uses a WKT
     representation converted to a 'Point' with 'GeomFromText()'.

          mysql> SELECT X(Point(15, 20));
          +------------------+
          | X(POINT(15, 20)) |
          +------------------+
          |               15 |
          +------------------+

          mysql> SELECT X(GeomFromText('POINT(15 20)'));
          +---------------------------------+
          | X(GeomFromText('POINT(15 20)')) |
          +---------------------------------+
          |                              15 |
          +---------------------------------+

   * A 'LineString' with four points:

          LINESTRING(0 0, 10 10, 20 25, 50 60)

     The point coordinate pairs are separated by commas.

   * A 'Polygon' with one exterior ring and one interior ring:

          POLYGON((0 0,10 0,10 10,0 10,0 0),(5 5,7 5,7 7,5 7, 5 5))

   * A 'MultiPoint' with three 'Point' values:

          MULTIPOINT(0 0, 20 20, 60 60)

   * A 'MultiLineString' with two 'LineString' values:

          MULTILINESTRING((10 10, 20 20), (15 15, 30 15))

   * A 'MultiPolygon' with two 'Polygon' values:

          MULTIPOLYGON(((0 0,10 0,10 10,0 10,0 0)),((5 5,7 5,7 7,5 7, 5 5)))

   * A 'GeometryCollection' consisting of two 'Point' values and one
     'LineString':

          GEOMETRYCOLLECTION(POINT(10 10), POINT(30 30), LINESTRING(15 15, 20 20))

*Well-Known Binary (WKB) Format*

The Well-Known Binary (WKB) representation of geometric values is used
for exchanging geometry data as binary streams represented by *note
'BLOB': blob. values containing geometric WKB information.  This format
is defined by the OpenGIS specification (see *note spatial-types::).  It
is also defined in the ISO 'SQL/MM Part 3: Spatial' standard.

WKB uses 1-byte unsigned integers, 4-byte unsigned integers, and 8-byte
double-precision numbers (IEEE 754 format).  A byte is eight bits.

For example, a WKB value that corresponds to 'POINT(1 -1)' consists of
this sequence of 21 bytes, each represented by two hexadecimal digits:

     0101000000000000000000F03F000000000000F0BF

The sequence consists of the components shown in the following table.

*WKB Components Example*

Component              Size                   Value
                                              
Byte order             1 byte                 '01'
                                              
WKB type               4 bytes                '01000000'
                                              
X coordinate           8 bytes                '000000000000F03F'
                                              
Y coordinate           8 bytes                '000000000000F0BF'
                       

Component representation is as follows:

   * The byte order indicator is either 1 or 0 to signify little-endian
     or big-endian storage.  The little-endian and big-endian byte
     orders are also known as Network Data Representation (NDR) and
     External Data Representation (XDR), respectively.

   * The WKB type is a code that indicates the geometry type.  MySQL
     uses values from 1 through 7 to indicate 'Point', 'LineString',
     'Polygon', 'MultiPoint', 'MultiLineString', 'MultiPolygon', and
     'GeometryCollection'.

   * A 'Point' value has X and Y coordinates, each represented as a
     double-precision value.

WKB values for more complex geometry values have more complex data
structures, as detailed in the OpenGIS specification.

*Internal Geometry Storage Format*

MySQL stores geometry values using 4 bytes to indicate the SRID followed
by the WKB representation of the value.  For a description of WKB
format, see *note gis-wkb-format::.

For the WKB part, these MySQL-specific considerations apply:

   * The byte-order indicator byte is 1 because MySQL stores geometries
     as little-ending values.

   * MySQL supports geometry types of 'Point', 'LineString', 'Polygon',
     'MultiPoint', 'MultiLineString', 'MultiPolygon', and
     'GeometryCollection'.  Other geometry types are not supported.

The 'LENGTH()' function returns the space in bytes required for value
storage.  Example:

     mysql> SET @g = GeomFromText('POINT(1 -1)');
     mysql> SELECT LENGTH(@g);
     +------------+
     | LENGTH(@g) |
     +------------+
     |         25 |
     +------------+
     mysql> SELECT HEX(@g);
     +----------------------------------------------------+
     | HEX(@g)                                            |
     +----------------------------------------------------+
     | 000000000101000000000000000000F03F000000000000F0BF |
     +----------------------------------------------------+

The value length is 25 bytes, made up of these components (as can be
seen from the hexadecimal value):

   * 4 bytes for integer SRID (0)

   * 1 byte for integer byte order (1 = little-endian)

   * 4 bytes for integer type information (1 = 'Point')

   * 8 bytes for double-precision X coordinate (1)

   * 8 bytes for double-precision Y coordinate (−1)


File: manual.info.tmp,  Node: creating-spatial-columns,  Next: populating-spatial-columns,  Prev: gis-data-formats,  Up: spatial-types

11.4.4 Creating Spatial Columns
-------------------------------

MySQL provides a standard way of creating spatial columns for geometry
types, for example, with *note 'CREATE TABLE': create-table. or *note
'ALTER TABLE': alter-table.  Spatial columns are supported for *note
'MyISAM': myisam-storage-engine, *note 'InnoDB': innodb-storage-engine,
*note 'NDB': mysql-cluster, and *note 'ARCHIVE': archive-storage-engine.
tables.  See also the notes about spatial indexes under *note
creating-spatial-indexes::.

   * Use the *note 'CREATE TABLE': create-table. statement to create a
     table with a spatial column:

          CREATE TABLE geom (g GEOMETRY);

   * Use the *note 'ALTER TABLE': alter-table. statement to add or drop
     a spatial column to or from an existing table:

          ALTER TABLE geom ADD pt POINT;
          ALTER TABLE geom DROP pt;


File: manual.info.tmp,  Node: populating-spatial-columns,  Next: fetching-spatial-data,  Prev: creating-spatial-columns,  Up: spatial-types

11.4.5 Populating Spatial Columns
---------------------------------

After you have created spatial columns, you can populate them with
spatial data.

Values should be stored in internal geometry format, but you can convert
them to that format from either Well-Known Text (WKT) or Well-Known
Binary (WKB) format.  The following examples demonstrate how to insert
geometry values into a table by converting WKT values to internal
geometry format:

   * Perform the conversion directly in the *note 'INSERT': insert.
     statement:

          INSERT INTO geom VALUES (GeomFromText('POINT(1 1)'));

          SET @g = 'POINT(1 1)';
          INSERT INTO geom VALUES (GeomFromText(@g));

   * Perform the conversion prior to the *note 'INSERT': insert.:

          SET @g = GeomFromText('POINT(1 1)');
          INSERT INTO geom VALUES (@g);

The following examples insert more complex geometries into the table:

     SET @g = 'LINESTRING(0 0,1 1,2 2)';
     INSERT INTO geom VALUES (GeomFromText(@g));

     SET @g = 'POLYGON((0 0,10 0,10 10,0 10,0 0),(5 5,7 5,7 7,5 7, 5 5))';
     INSERT INTO geom VALUES (GeomFromText(@g));

     SET @g =
     'GEOMETRYCOLLECTION(POINT(1 1),LINESTRING(0 0,1 1,2 2,3 3,4 4))';
     INSERT INTO geom VALUES (GeomFromText(@g));

The preceding examples use 'GeomFromText()' to create geometry values.
You can also use type-specific functions:

     SET @g = 'POINT(1 1)';
     INSERT INTO geom VALUES (PointFromText(@g));

     SET @g = 'LINESTRING(0 0,1 1,2 2)';
     INSERT INTO geom VALUES (LineStringFromText(@g));

     SET @g = 'POLYGON((0 0,10 0,10 10,0 10,0 0),(5 5,7 5,7 7,5 7, 5 5))';
     INSERT INTO geom VALUES (PolygonFromText(@g));

     SET @g =
     'GEOMETRYCOLLECTION(POINT(1 1),LINESTRING(0 0,1 1,2 2,3 3,4 4))';
     INSERT INTO geom VALUES (GeomCollFromText(@g));

A client application program that wants to use WKB representations of
geometry values is responsible for sending correctly formed WKB in
queries to the server.  There are several ways to satisfy this
requirement.  For example:

   * Inserting a 'POINT(1 1)' value with hex literal syntax:

          INSERT INTO geom VALUES
          (GeomFromWKB(X'0101000000000000000000F03F000000000000F03F'));

   * An ODBC application can send a WKB representation, binding it to a
     placeholder using an argument of *note 'BLOB': blob. type:

          INSERT INTO geom VALUES (GeomFromWKB(?))

     Other programming interfaces may support a similar placeholder
     mechanism.

   * In a C program, you can escape a binary value using *note
     'mysql_real_escape_string()': mysql-real-escape-string. and include
     the result in a query string that is sent to the server.  See *note
     mysql-real-escape-string::.


File: manual.info.tmp,  Node: fetching-spatial-data,  Next: optimizing-spatial-analysis,  Prev: populating-spatial-columns,  Up: spatial-types

11.4.6 Fetching Spatial Data
----------------------------

Geometry values stored in a table can be fetched in internal format.
You can also convert them to WKT or WKB format.

   * Fetching spatial data in internal format:

     Fetching geometry values using internal format can be useful in
     table-to-table transfers:

          CREATE TABLE geom2 (g GEOMETRY) SELECT g FROM geom;

   * Fetching spatial data in WKT format:

     The 'AsText()' function converts a geometry from internal format to
     a WKT string.

          SELECT AsText(g) FROM geom;

   * Fetching spatial data in WKB format:

     The 'AsBinary()' function converts a geometry from internal format
     to a *note 'BLOB': blob. containing the WKB value.

          SELECT AsBinary(g) FROM geom;


File: manual.info.tmp,  Node: optimizing-spatial-analysis,  Next: creating-spatial-indexes,  Prev: fetching-spatial-data,  Up: spatial-types

11.4.7 Optimizing Spatial Analysis
----------------------------------

For *note 'MyISAM': myisam-storage-engine. tables, search operations in
columns containing spatial data can be optimized using 'SPATIAL'
indexes.  The most typical operations are:

   * Point queries that search for all objects that contain a given
     point

   * Region queries that search for all objects that overlap a given
     region

MySQL uses *R-Trees with quadratic splitting* for 'SPATIAL' indexes on
spatial columns.  A 'SPATIAL' index is built using the minimum bounding
rectangle (MBR) of a geometry.  For most geometries, the MBR is a
minimum rectangle that surrounds the geometries.  For a horizontal or a
vertical linestring, the MBR is a rectangle degenerated into the
linestring.  For a point, the MBR is a rectangle degenerated into the
point.

It is also possible to create normal indexes on spatial columns.  In a
non-'SPATIAL' index, you must declare a prefix for any spatial column
except for 'POINT' columns.

'MyISAM' supports both 'SPATIAL' and non-'SPATIAL' indexes.  Other
storage engines support non-'SPATIAL' indexes, as described in *note
create-index::.


File: manual.info.tmp,  Node: creating-spatial-indexes,  Next: using-spatial-indexes,  Prev: optimizing-spatial-analysis,  Up: spatial-types

11.4.8 Creating Spatial Indexes
-------------------------------

For 'MyISAM' tables, MySQL can create spatial indexes using syntax
similar to that for creating regular indexes, but using the 'SPATIAL'
keyword.  Columns in spatial indexes must be declared 'NOT NULL'.  The
following examples demonstrate how to create spatial indexes:

   * With *note 'CREATE TABLE': create-table.:

          CREATE TABLE geom (g GEOMETRY NOT NULL, SPATIAL INDEX(g)) ENGINE=MyISAM;

   * With *note 'ALTER TABLE': alter-table.:

          CREATE TABLE geom (g GEOMETRY NOT NULL) ENGINE=MyISAM;
          ALTER TABLE geom ADD SPATIAL INDEX(g);

   * With *note 'CREATE INDEX': create-index.:

          CREATE TABLE geom (g GEOMETRY NOT NULL) ENGINE=MyISAM;
          CREATE SPATIAL INDEX g ON geom (g);

'SPATIAL INDEX' creates an R-tree index.  For storage engines that
support nonspatial indexing of spatial columns, the engine creates a
B-tree index.  A B-tree index on spatial values is useful for
exact-value lookups, but not for range scans.

For more information on indexing spatial columns, see *note
create-index::.

To drop spatial indexes, use *note 'ALTER TABLE': alter-table. or *note
'DROP INDEX': drop-index.:

   * With *note 'ALTER TABLE': alter-table.:

          ALTER TABLE geom DROP INDEX g;

   * With *note 'DROP INDEX': drop-index.:

          DROP INDEX g ON geom;

Example: Suppose that a table 'geom' contains more than 32,000
geometries, which are stored in the column 'g' of type 'GEOMETRY'.  The
table also has an 'AUTO_INCREMENT' column 'fid' for storing object ID
values.

     mysql> DESCRIBE geom;
     +-------+----------+------+-----+---------+----------------+
     | Field | Type     | Null | Key | Default | Extra          |
     +-------+----------+------+-----+---------+----------------+
     | fid   | int(11)  |      | PRI | NULL    | auto_increment |
     | g     | geometry |      |     |         |                |
     +-------+----------+------+-----+---------+----------------+
     2 rows in set (0.00 sec)

     mysql> SELECT COUNT(*) FROM geom;
     +----------+
     | count(*) |
     +----------+
     |    32376 |
     +----------+
     1 row in set (0.00 sec)

To add a spatial index on the column 'g', use this statement:

     mysql> ALTER TABLE geom ADD SPATIAL INDEX(g) ENGINE=MyISAM;
     Query OK, 32376 rows affected (4.05 sec)
     Records: 32376  Duplicates: 0  Warnings: 0


File: manual.info.tmp,  Node: using-spatial-indexes,  Prev: creating-spatial-indexes,  Up: spatial-types

11.4.9 Using Spatial Indexes
----------------------------

The optimizer investigates whether available spatial indexes can be
involved in the search for queries that use a function such as
'MBRContains()' or 'MBRWithin()' in the 'WHERE' clause.  The following
query finds all objects that are in the given rectangle:

     mysql> SET @poly =
         -> 'Polygon((30000 15000,
                      31000 15000,
                      31000 16000,
                      30000 16000,
                      30000 15000))';
     mysql> SELECT fid,AsText(g) FROM geom WHERE
         -> MBRContains(GeomFromText(@poly),g);
     +-----+---------------------------------------------------------------+
     | fid | AsText(g)                                                     |
     +-----+---------------------------------------------------------------+
     |  21 | LINESTRING(30350.4 15828.8,30350.6 15845,30333.8 15845,30 ... |
     |  22 | LINESTRING(30350.6 15871.4,30350.6 15887.8,30334 15887.8, ... |
     |  23 | LINESTRING(30350.6 15914.2,30350.6 15930.4,30334 15930.4, ... |
     |  24 | LINESTRING(30290.2 15823,30290.2 15839.4,30273.4 15839.4, ... |
     |  25 | LINESTRING(30291.4 15866.2,30291.6 15882.4,30274.8 15882. ... |
     |  26 | LINESTRING(30291.6 15918.2,30291.6 15934.4,30275 15934.4, ... |
     | 249 | LINESTRING(30337.8 15938.6,30337.8 15946.8,30320.4 15946. ... |
     |   1 | LINESTRING(30250.4 15129.2,30248.8 15138.4,30238.2 15136. ... |
     |   2 | LINESTRING(30220.2 15122.8,30217.2 15137.8,30207.6 15136, ... |
     |   3 | LINESTRING(30179 15114.4,30176.6 15129.4,30167 15128,3016 ... |
     |   4 | LINESTRING(30155.2 15121.4,30140.4 15118.6,30142 15109,30 ... |
     |   5 | LINESTRING(30192.4 15085,30177.6 15082.2,30179.2 15072.4, ... |
     |   6 | LINESTRING(30244 15087,30229 15086.2,30229.4 15076.4,3024 ... |
     |   7 | LINESTRING(30200.6 15059.4,30185.6 15058.6,30186 15048.8, ... |
     |  10 | LINESTRING(30179.6 15017.8,30181 15002.8,30190.8 15003.6, ... |
     |  11 | LINESTRING(30154.2 15000.4,30168.6 15004.8,30166 15014.2, ... |
     |  13 | LINESTRING(30105 15065.8,30108.4 15050.8,30118 15053,3011 ... |
     | 154 | LINESTRING(30276.2 15143.8,30261.4 15141,30263 15131.4,30 ... |
     | 155 | LINESTRING(30269.8 15084,30269.4 15093.4,30258.6 15093,30 ... |
     | 157 | LINESTRING(30128.2 15011,30113.2 15010.2,30113.6 15000.4, ... |
     +-----+---------------------------------------------------------------+
     20 rows in set (0.00 sec)

Use *note 'EXPLAIN': explain. to check the way this query is executed:

     mysql> SET @poly =
         -> 'Polygon((30000 15000,
                      31000 15000,
                      31000 16000,
                      30000 16000,
                      30000 15000))';
     mysql> EXPLAIN SELECT fid,AsText(g) FROM geom WHERE
         -> MBRContains(GeomFromText(@poly),g)\G
     *************************** 1. row ***************************
                id: 1
       select_type: SIMPLE
             table: geom
              type: range
     possible_keys: g
               key: g
           key_len: 32
               ref: NULL
              rows: 50
             Extra: Using where
     1 row in set (0.00 sec)

Check what would happen without a spatial index:

     mysql> SET @poly =
         -> 'Polygon((30000 15000,
                      31000 15000,
                      31000 16000,
                      30000 16000,
                      30000 15000))';
     mysql> EXPLAIN SELECT fid,AsText(g) FROM g IGNORE INDEX (g) WHERE
         -> MBRContains(GeomFromText(@poly),g)\G
     *************************** 1. row ***************************
                id: 1
       select_type: SIMPLE
             table: geom
              type: ALL
     possible_keys: NULL
               key: NULL
           key_len: NULL
               ref: NULL
              rows: 32376
             Extra: Using where
     1 row in set (0.00 sec)

Executing the *note 'SELECT': select. statement without the spatial
index yields the same result but causes the execution time to rise from
0.00 seconds to 0.46 seconds:

     mysql> SET @poly =
         -> 'Polygon((30000 15000,
                      31000 15000,
                      31000 16000,
                      30000 16000,
                      30000 15000))';
     mysql> SELECT fid,AsText(g) FROM geom IGNORE INDEX (g) WHERE
         -> MBRContains(GeomFromText(@poly),g);
     +-----+---------------------------------------------------------------+
     | fid | AsText(g)                                                     |
     +-----+---------------------------------------------------------------+
     |   1 | LINESTRING(30250.4 15129.2,30248.8 15138.4,30238.2 15136. ... |
     |   2 | LINESTRING(30220.2 15122.8,30217.2 15137.8,30207.6 15136, ... |
     |   3 | LINESTRING(30179 15114.4,30176.6 15129.4,30167 15128,3016 ... |
     |   4 | LINESTRING(30155.2 15121.4,30140.4 15118.6,30142 15109,30 ... |
     |   5 | LINESTRING(30192.4 15085,30177.6 15082.2,30179.2 15072.4, ... |
     |   6 | LINESTRING(30244 15087,30229 15086.2,30229.4 15076.4,3024 ... |
     |   7 | LINESTRING(30200.6 15059.4,30185.6 15058.6,30186 15048.8, ... |
     |  10 | LINESTRING(30179.6 15017.8,30181 15002.8,30190.8 15003.6, ... |
     |  11 | LINESTRING(30154.2 15000.4,30168.6 15004.8,30166 15014.2, ... |
     |  13 | LINESTRING(30105 15065.8,30108.4 15050.8,30118 15053,3011 ... |
     |  21 | LINESTRING(30350.4 15828.8,30350.6 15845,30333.8 15845,30 ... |
     |  22 | LINESTRING(30350.6 15871.4,30350.6 15887.8,30334 15887.8, ... |
     |  23 | LINESTRING(30350.6 15914.2,30350.6 15930.4,30334 15930.4, ... |
     |  24 | LINESTRING(30290.2 15823,30290.2 15839.4,30273.4 15839.4, ... |
     |  25 | LINESTRING(30291.4 15866.2,30291.6 15882.4,30274.8 15882. ... |
     |  26 | LINESTRING(30291.6 15918.2,30291.6 15934.4,30275 15934.4, ... |
     | 154 | LINESTRING(30276.2 15143.8,30261.4 15141,30263 15131.4,30 ... |
     | 155 | LINESTRING(30269.8 15084,30269.4 15093.4,30258.6 15093,30 ... |
     | 157 | LINESTRING(30128.2 15011,30113.2 15010.2,30113.6 15000.4, ... |
     | 249 | LINESTRING(30337.8 15938.6,30337.8 15946.8,30320.4 15946. ... |
     +-----+---------------------------------------------------------------+
     20 rows in set (0.46 sec)


File: manual.info.tmp,  Node: data-type-defaults,  Next: storage-requirements,  Prev: spatial-types,  Up: data-types

11.5 Data Type Default Values
=============================

Data type specifications can have explicit or implicit default values.

   * *note data-types-defaults-explicit::

   * *note data-types-defaults-implicit::

*Handling of Explicit Defaults*

A 'DEFAULT VALUE' clause in a data type specification explicitly
indicates a default value for a column.  Examples:

     CREATE TABLE t1 (
       i     INT DEFAULT -1,
       c     VARCHAR(10) DEFAULT '',
       price DOUBLE(16,2) DEFAULT '0.00'
     );

'SERIAL DEFAULT VALUE' is a special case.  In the definition of an
integer column, it is an alias for 'NOT NULL AUTO_INCREMENT UNIQUE'.

With one exception, the default value specified in a 'DEFAULT' clause
must be a literal constant; it cannot be a function or an expression.
This means, for example, that you cannot set the default for a date
column to be the value of a function such as 'NOW()' or 'CURRENT_DATE'.
The exception is that, for a *note 'TIMESTAMP': datetime. column, you
can specify 'CURRENT_TIMESTAMP' as the default.  See *note
timestamp-initialization::.

The *note 'BLOB': blob. and *note 'TEXT': blob. data types cannot be
assigned a default value.

*Handling of Implicit Defaults*

If a data type specification includes no explicit 'DEFAULT' value, MySQL
determines the default value as follows:

If the column can take 'NULL' as a value, the column is defined with an
explicit 'DEFAULT NULL' clause.

If the column cannot take 'NULL' as a value, MySQL defines the column
with no explicit 'DEFAULT' clause.  Exception: If the column is defined
as part of a 'PRIMARY KEY' but not explicitly as 'NOT NULL', MySQL
creates it as a 'NOT NULL' column (because 'PRIMARY KEY' columns must be
'NOT NULL'), but also assigns it a 'DEFAULT' clause using the implicit
default value.  To prevent this, include an explicit 'NOT NULL' in the
definition of any 'PRIMARY KEY' column.

For data entry into a 'NOT NULL' column that has no explicit 'DEFAULT'
clause, if an *note 'INSERT': insert. or *note 'REPLACE': replace.
statement includes no value for the column, or an *note 'UPDATE':
update. statement sets the column to 'NULL', MySQL handles the column
according to the SQL mode in effect at the time:

   * If strict SQL mode is enabled, an error occurs for transactional
     tables and the statement is rolled back.  For nontransactional
     tables, an error occurs, but if this happens for the second or
     subsequent row of a multiple-row statement, the preceding rows will
     have been inserted.

   * If strict mode is not enabled, MySQL sets the column to the
     implicit default value for the column data type.

Suppose that a table 't' is defined as follows:

     CREATE TABLE t (i INT NOT NULL);

In this case, 'i' has no explicit default, so in strict mode each of the
following statements produce an error and no row is inserted.  When not
using strict mode, only the third statement produces an error; the
implicit default is inserted for the first two statements, but the third
fails because 'DEFAULT(i)' cannot produce a value:

     INSERT INTO t VALUES();
     INSERT INTO t VALUES(DEFAULT);
     INSERT INTO t VALUES(DEFAULT(i));

See *note sql-mode::.

For a given table, the *note 'SHOW CREATE TABLE': show-create-table.
statement displays which columns have an explicit 'DEFAULT' clause.

Implicit defaults are defined as follows:

   * For numeric types, the default is '0', with the exception that for
     integer or floating-point types declared with the 'AUTO_INCREMENT'
     attribute, the default is the next value in the sequence.

   * For date and time types other than *note 'TIMESTAMP': datetime, the
     default is the appropriate 'zero' value for the type.  For the
     first *note 'TIMESTAMP': datetime. column in a table, the default
     value is the current date and time.  See *note
     date-and-time-types::.

   * For string types other than *note 'ENUM': enum, the default value
     is the empty string.  For *note 'ENUM': enum, the default is the
     first enumeration value.


File: manual.info.tmp,  Node: storage-requirements,  Next: choosing-types,  Prev: data-type-defaults,  Up: data-types

11.6 Data Type Storage Requirements
===================================

   * *note data-types-storage-reqs-innodb::

   * *note data-types-storage-reqs-ndb::

   * *note data-types-storage-reqs-numeric::

   * *note data-types-storage-reqs-date-time::

   * *note data-types-storage-reqs-strings::

   * *note data-types-storage-reqs-gis::

The storage requirements for data vary, according to the storage engine
being used for the table in question.  Different storage engines use
different methods for recording the raw data and different data types.
In addition, some engines may compress the information in a given row,
either on a column or entire row basis, making calculation of the
storage requirements for a given table or column structure.

However, all storage engines must communicate and exchange information
on a given row within a table using the same structure, and this
information is consistent, irrespective of the storage engine used to
write the information to disk.

This sections includes some guideliness and information for the storage
requirements for each data type supported by MySQL, including details
for the internal format and the sizes used by storage engines that used
a fixed size representation for different types.  Information is listed
by category or storage engine.

The internal representation of a table has a maximum row size of 65,535
bytes, even if the storage engine is capable of supporting larger rows.
This figure excludes *note 'BLOB': blob. or *note 'TEXT': blob. columns,
which contribute only 9 to 12 bytes toward this size.  For *note 'BLOB':
blob. and *note 'TEXT': blob. data, the information is stored internally
in a different area of memory than the row buffer.  Different storage
engines handle the allocation and storage of this data in different
ways, according to the method they use for handling the corresponding
types.  For more information, see *note storage-engines::, and *note
column-count-limit::.

*InnoDB Table Storage Requirements*

See *note innodb-row-format:: for information about storage requirements
for 'InnoDB' tables.

*NDBCLUSTER Table Storage Requirements*

*Important*:

For tables using the *note 'NDBCLUSTER': mysql-cluster. storage engine,
there is the factor of _4-byte alignment_ to be taken into account when
calculating storage requirements.  This means that all *note 'NDB':
mysql-cluster. data storage is done in multiples of 4 bytes.  Thus, a
column value that would take 15 bytes in a table using a storage engine
other than *note 'NDB': mysql-cluster. requires 16 bytes in an *note
'NDB': mysql-cluster. table.  This requirement applies in addition to
any other considerations that are discussed in this section.  For
example, in *note 'NDBCLUSTER': mysql-cluster. tables, the *note
'TINYINT': integer-types, *note 'SMALLINT': integer-types, *note
'MEDIUMINT': integer-types, and *note 'INTEGER': integer-types. (*note
'INT': integer-types.) column types each require 4 bytes storage per
record due to the alignment factor.

An exception to this rule is the *note 'BIT': bit-type. type, which is
_not_ 4-byte aligned.  In NDB Cluster tables, a 'BIT(M)' column takes M
bits of storage space.  However, if a table definition contains 1 or
more *note 'BIT': bit-type. columns (up to 32 *note 'BIT': bit-type.
columns), then *note 'NDBCLUSTER': mysql-cluster. reserves 4 bytes (32
bits) per row for these.  If a table definition contains more than 32
*note 'BIT': bit-type. columns (up to 64 such columns), then *note
'NDBCLUSTER': mysql-cluster. reserves 8 bytes (that is, 64 bits) per
row.

In addition, while a 'NULL' itself does not require any storage space,
*note 'NDBCLUSTER': mysql-cluster. reserves 4 bytes per row if the table
definition contains any columns defined as 'NULL', up to 32 'NULL'
columns.  (If an NDB Cluster table is defined with more than 32 'NULL'
columns up to 64 'NULL' columns, then 8 bytes per row is reserved.)

When calculating storage requirements for NDB Cluster tables, you must
also remember that every table using the *note 'NDBCLUSTER':
mysql-cluster. storage engine requires a primary key; if no primary key
is defined by the user, then a 'hidden' primary key will be created by
*note 'NDB': mysql-cluster.  This hidden primary key consumes 31-35
bytes per table record.

You may find the 'ndb_size.pl' utility to be useful for estimating *note
'NDB': mysql-cluster. storage requirements.  This Perl script connects
to a current MySQL (non-Cluster) database and creates a report on how
much space that database would require if it used the *note
'NDBCLUSTER': mysql-cluster. storage engine.  See *note
mysql-cluster-programs-ndb-size-pl::, for more information.

*Numeric Type Storage Requirements*

Data Type                     Storage Required
                              
*note 'TINYINT': integer-types.1 byte
                              
*note 'SMALLINT': integer-types.2 bytes
                              
*note 'MEDIUMINT': integer-types.3 bytes
                              
*note 'INT': integer-types,   4 bytes
*note 'INTEGER': integer-types.

*note 'BIGINT': integer-types.8 bytes
                              
'FLOAT(P)'                    4 bytes if 0 <= P <= 24, 8 bytes if 25 <=
                              P <= 53
                              
*note 'FLOAT': floating-point-types.4 bytes
                              
'DOUBLE [PRECISION]',         8 bytes
*note 'REAL': floating-point-types.

'DECIMAL(M,D)',               Varies; see following discussion
'NUMERIC(M,D)'                

'BIT(M)'                      approximately (M+7)/8 bytes

Values for *note 'DECIMAL': fixed-point-types. (and *note 'NUMERIC':
fixed-point-types.) columns are represented using a binary format that
packs nine decimal (base 10) digits into four bytes.  Storage for the
integer and fractional parts of each value are determined separately.
Each multiple of nine digits requires four bytes, and the 'leftover'
digits require some fraction of four bytes.  The storage required for
excess digits is given by the following table.

Leftover Digits    Number of Bytes
                   
0                  0
                   
1                  1
                   
2                  1
                   
3                  2
                   
4                  2
                   
5                  3
                   
6                  3
                   
7                  4
                   
8                  4

*Date and Time Type Storage Requirements*

Data Type                     Storage Required
                              
*note 'DATE': datetime.       3 bytes
                              
*note 'TIME': time.           3 bytes
                              
*note 'DATETIME': datetime.   8 bytes
                              
*note 'TIMESTAMP': datetime.  4 bytes
                              
*note 'YEAR': year.           1 byte

For details about internal representation of temporal values, see MySQL
Internals: Important Algorithms and Structures
(https://dev.mysql.com/doc/internals/en/algorithms.html).

*String Type Storage Requirements*

In the following table, M represents the declared column length in
characters for nonbinary string types and bytes for binary string types.
L represents the actual length in bytes of a given string value.

Data Type                     Storage Required
                              
'CHAR(M)'                     The compact family of InnoDB row formats
                              optimize storage for variable-length
                              character sets.  See
                              *note innodb-compact-row-format-characteristics::.
                              Otherwise, M x W bytes, '<= M <=' 255,
                              where W is the number of bytes required
                              for the maximum-length character in the
                              character set.
                              
'BINARY(M)'                   M bytes, 0 '<= M <=' 255
                              
'VARCHAR(M)',                 L + 1 bytes if column values require 0
'VARBINARY(M)'                − 255 bytes, L + 2 bytes if values may
                              require more than 255 bytes
                              
*note 'TINYBLOB': blob,       L + 1 bytes, where L < 2^8
*note 'TINYTEXT': blob.       

*note 'BLOB': blob,           L + 2 bytes, where L < 2^16
*note 'TEXT': blob.           

*note 'MEDIUMBLOB': blob,     L + 3 bytes, where L < 2^24
*note 'MEDIUMTEXT': blob.     

*note 'LONGBLOB': blob,       L + 4 bytes, where L < 2^32
*note 'LONGTEXT': blob.       

'ENUM('VALUE1','VALUE2',...)' 1 or 2 bytes, depending on the number of
                              enumeration values (65,535 values
                              maximum)
                              
'SET('VALUE1','VALUE2',...)'  1, 2, 3, 4, or 8 bytes, depending on the
                              number of set members (64 members
                              maximum)

Variable-length string types are stored using a length prefix plus data.
The length prefix requires from one to four bytes depending on the data
type, and the value of the prefix is L (the byte length of the string).
For example, storage for a *note 'MEDIUMTEXT': blob. value requires L
bytes to store the value plus three bytes to store the length of the
value.

To calculate the number of bytes used to store a particular *note
'CHAR': char, *note 'VARCHAR': char, or *note 'TEXT': blob. column
value, you must take into account the character set used for that column
and whether the value contains multibyte characters.  In particular,
when using a 'utf8' Unicode character set, you must keep in mind that
not all characters use the same number of bytes.  'utf8mb3' and
'utf8mb4' character sets can require up to three and four bytes per
character, respectively.  For a breakdown of the storage used for
different categories of 'utf8mb3' or 'utf8mb4' characters, see *note
charset-unicode::.

*note 'VARCHAR': char, *note 'VARBINARY': binary-varbinary, and the
*note 'BLOB': blob. and *note 'TEXT': blob. types are variable-length
types.  For each, the storage requirements depend on these factors:

   * The actual length of the column value

   * The column's maximum possible length

   * The character set used for the column, because some character sets
     contain multibyte characters

For example, a 'VARCHAR(255)' column can hold a string with a maximum
length of 255 characters.  Assuming that the column uses the 'latin1'
character set (one byte per character), the actual storage required is
the length of the string (L), plus one byte to record the length of the
string.  For the string ''abcd'', L is 4 and the storage requirement is
five bytes.  If the same column is instead declared to use the 'ucs2'
double-byte character set, the storage requirement is 10 bytes: The
length of ''abcd'' is eight bytes and the column requires two bytes to
store lengths because the maximum length is greater than 255 (up to 510
bytes).

The effective maximum number of _bytes_ that can be stored in a *note
'VARCHAR': char. or *note 'VARBINARY': binary-varbinary. column is
subject to the maximum row size of 65,535 bytes, which is shared among
all columns.  For a *note 'VARCHAR': char. column that stores multibyte
characters, the effective maximum number of _characters_ is less.  For
example, 'utf8mb3' characters can require up to three bytes per
character, so a *note 'VARCHAR': char. column that uses the 'utf8mb3'
character set can be declared to be a maximum of 21,844 characters.  See
*note column-count-limit::.

'InnoDB' encodes fixed-length fields greater than or equal to 768 bytes
in length as variable-length fields, which can be stored off-page.  For
example, a 'CHAR(255)' column can exceed 768 bytes if the maximum byte
length of the character set is greater than 3, as it is with 'utf8mb4'.

The *note 'NDBCLUSTER': mysql-cluster. storage engine supports
variable-width columns.  This means that a *note 'VARCHAR': char. column
in an NDB Cluster table requires the same amount of storage as would any
other storage engine, with the exception that such values are 4-byte
aligned.  Thus, the string ''abcd'' stored in a 'VARCHAR(50)' column
using the 'latin1' character set requires 8 bytes (rather than 5 bytes
for the same column value in a 'MyISAM' table).

*note 'TEXT': blob. and *note 'BLOB': blob. columns are implemented
differently in the NDB Cluster storage engine, wherein each row in a
*note 'TEXT': blob. column is made up of two separate parts.  One of
these is of fixed size (256 bytes), and is actually stored in the
original table.  The other consists of any data in excess of 256 bytes,
which is stored in a hidden table.  The rows in this second table are
always 2000 bytes long.  This means that the size of a *note 'TEXT':
blob. column is 256 if SIZE <= 256 (where SIZE represents the size of
the row); otherwise, the size is 256 + SIZE + (2000 x (SIZE − 256) %
2000).

The size of an *note 'ENUM': enum. object is determined by the number of
different enumeration values.  One byte is used for enumerations with up
to 255 possible values.  Two bytes are used for enumerations having
between 256 and 65,535 possible values.  See *note enum::.

The size of a *note 'SET': set. object is determined by the number of
different set members.  If the set size is N, the object occupies
'(N+7)/8' bytes, rounded up to 1, 2, 3, 4, or 8 bytes.  A *note 'SET':
set. can have a maximum of 64 members.  See *note set::.

*Spatial Type Storage Requirements*

MySQL stores geometry values using 4 bytes to indicate the SRID followed
by the WKB representation of the value.  The 'LENGTH()' function returns
the space i